@INPROCEEDINGS{5670084, 
author={L. Zhang and J. Wu and Z. C. Wang and C. J. Wang}, 
booktitle={2010 22nd IEEE International Conference on Tools with Artificial Intelligence}, 
title={Multi-relational Topic Model for Social Recommendation}, 
year={2010}, 
volume={2}, 
pages={349-350}, 
abstract={Various attribute and relation information is used in social recommendation systems. However, previous approaches fail to use them in a unified way. In this paper, we propose a unified framework for social recommendation. Entities like users and items are described by their tags. We model each entity using topic models like Latent Dirichlet Allocation(LDA) and then connect these topic models to form a multi-relational network. Various relations between entities in recommender systems such as rating relation or user friend relation can be expressed as edges in the multi-relational network. We evaluate our model on a real-life dataset collected from a commercial recommender website. Experiments validate the generative performance and predictive performance of our model.}, 
keywords={Web sites;recommender systems;statistical analysis;commercial recommender Website;latent dirichlet allocation;multirelational topic model;social recommendation systems;Data models;Predictive models;Recommender systems;Resource management;Training;Vocabulary}, 
doi={10.1109/ICTAI.2010.123}, 
ISSN={1082-3409}, 
month={Oct},}
@INPROCEEDINGS{1026196, 
author={G. D. Horn and A. K. Milne}, 
booktitle={IEEE International Geoscience and Remote Sensing Symposium}, 
title={Segmentation and classification of multitemporal data: methodology and results of a modified Gaussian Markov random field model classification system}, 
year={2002}, 
volume={3}, 
pages={1609-1611 vol.3}, 
abstract={Constructive and destructive interference have long been a significant problem for the classification of radar imagery. Several authors have noted the ability of segmentation to work around the problem by incorporation of spatially adaptive filters and fuzzy logic. We present the methodology and results of such a study using multitemporal datasets in the place of multispectral data. For the study 27 standard and ScanSAR images were collected over the region from 1996 to 2001. The majority of the images were S4 ascending, with ScanSAR narrow collected simultaneously. The ScanSAR images allowed us to assess the methodology over a wider swath, yielding results on a regional rather than local scale. Northern Australia exhibits an extremely seasonal climate. The monsoonal nature of this climate means a huge variation in the amount of available water to plant life both on and surrounding the flood plains of the major river systems. Within this area is Kakadu National Park, a heritage listed and Ramsar identified wetland of international significance. Kakadu has a long history of scientific study, and is an ideal site on which to conduct studies such as this. The seasonal nature of this area presents a unique opportunity, that of a worst case scenario for classification schemes, whereby each location on the flood plain undergoes significant change over the seasonal cycle, similar in many respects to the change between bands of a multi-spectral dataset. A modified Gaussian Markov random field model segmentation routine was used to cluster areas exhibiting similar radar response (both numerical and textural) at each successive date in the time series. For each date cluster statistics were then generated. This allowed the construction of temporal curves due to the fact that as the target material dries the material's dielectric constant decreases. As dielectric constant is significantly dependent on water content, these temporal curves act as a proxy measure for water availability, and hence aid in the discrimination of wetland from non-wetland areas and any change in size and location of these areas throughout the season. The output of the segmentation routine is a series of three bands. The first and second bands are the statistics (mean and standard deviation) of t- he original image and the third a vector file of the edge locations. By outputting statistical information instead of arbitrary segment number standard classification routines may be used successfully on radar imagery. Classification of the results of the segmentation may be either supervised or unsupervised, however analysis indicated that the Isodata algorithm adequately addressed classification needs. As a combined system the segmentation and classification system used in this paper has shown some excellent results when applied to multi-date radar imagery of Kakadu National Park in Northern Australia. Results for a segmentation of multitemporal radar dataset are presented alongside the results of the subsequent classification. By creating a system that requires minimal user input a wide variety of applications may be addressed. Indeed, the registration of the imagery becomes the most user intensive portion of the process. Automated routines such as this allow for analysis of large areas on a regular basis, an exceptional result for monitoring change and establishing baselines for later comparison.}, 
keywords={hydrology;image classification;image segmentation;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping;AD 1996 to 2001;Isodata algorithm;Kakadu National Park;Northern Australia;ScanSAR images;classification schemes;edge locations;flood plain;methodology;modified Gaussian Markov random field model classification system;multitemporal data;radar imagery;radar response;seasonal cycle;segmentation;statistical information;target material dielectric constant;water availability;water content;wetland;Australia;Building materials;Dielectric constant;Dielectric materials;Floods;Image segmentation;Interference;Markov random fields;Radar imaging;Statistics}, 
doi={10.1109/IGARSS.2002.1026196}, 
month={},}
@INPROCEEDINGS{7900063, 
author={S. Bacha and M. S. Allili and N. Benblidia}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Event recognition in photo albums using probabilistic graphical model and feature relevance}, 
year={2016}, 
pages={2819-2823}, 
abstract={The exponential use of digital cameras has raised a new problem: how to store/retrieve images/albums in very large photo databases that correspond to special events. In this paper, we propose a new probabilistic graphical model (PGM) to recognize events in photo albums stored by users. The PGM combines high-level image features consisting of scenes and objects detected in images. To consider the discriminative power of features, our model integrates the object/scene relevance for more precise prediction of semantic events in photo albums. Experimental results carried out on the challenging PEC dataset with 807 photo albums are presented.}, 
keywords={cameras;graph theory;object detection;probability;very large databases;visual databases;PGM;digital cameras;event recognition;feature relevance;high-level image features;image retrieval;image storage;object detection;object-scene relevance;photo albums;probabilistic graphical model;scene detection;semantic event prediction;very large photo databases;Estimation;Graphical models;Image recognition;Pattern recognition;Random variables;Roads;Semantics;event recognition;feature relevance;probabilistic graphical models}, 
doi={10.1109/ICPR.2016.7900063}, 
month={Dec},}
@INPROCEEDINGS{7727668, 
author={Q. N. T. Do and A. Zhilin and C. Z. P. Junior and G. Wang and F. K. Hussain}, 
booktitle={2016 International Joint Conference on Neural Networks (IJCNN)}, 
title={A network-based approach to detect spammer groups}, 
year={2016}, 
pages={3642-3648}, 
abstract={Online reviews nowadays are an important source of information for consumers to evaluate online services and products before deciding which product and which provider to choose. Therefore, online reviews have significant power to influence consumers' purchase decisions. Being aware of this, an increasing number of companies have organized spammer review campaigns, in order to promote their products and gain an advantage over their competitors by manipulating and misleading consumers. To make sure the Internet remains a reliable source of information, we propose a method to identify both individual and group spamming reviews by assigning a suspicion score to each user. The proposed method is a network-based approach combining clustering techniques. We demonstrate the efficiency and effectiveness of our approach on a real-world and manipulated dataset that contains over 8000 restaurants and 600,000 restaurant reviews from TripAdvisor website. We tested our method in three testing scenarios. The method was able to detect all spammers in two testing scenarios, however it did not detect all in the last scenario.}, 
keywords={Internet;pattern clustering;unsolicited e-mail;Internet;TripAdvisor Website;clustering techniques;consumer purchase decisions;group spamming reviews;individual spamming reviews;network-based approach;online reviews;online service evaluation;spammer group detection;Australia;Computational modeling;Data mining;Feature extraction;Pragmatics;Software;Unsolicited electronic mail;K-means clustering;graph mining;network method;spammer groups}, 
doi={10.1109/IJCNN.2016.7727668}, 
month={July},}
@INPROCEEDINGS{6930523, 
author={D. Yu and Y. Liu and Y. Xu and Y. Yin}, 
booktitle={2014 IEEE International Conference on Services Computing}, 
title={Personalized QoS Prediction for Web Services Using Latent Factor Models}, 
year={2014}, 
pages={107-114}, 
abstract={Recommending the suitable Web service is an important topic in today's society. The critical step is to accurately predict QoS of Web services. However, the highly sparse QoS data complicate the challenges. In the real world, since QoS delivery can be significantly affected by some dominant factors in the service environment (e.g., network delay and the location of user or service), Web services which are published by the same provider usually have the similar fundamental network environment. These factors can be leveraged for accurate QoS predictions, leading to high-quality service recommendations. In this paper, we expound how Latent Factor Models (LFM) can be utilized to predict the unknown QoS values. Meanwhile, we take the factors of provider and its country into consideration, which imply the latent physical location and network status information, as the latent neighbor for the set of Web services. Hence, the novel neighbor factor model is built to evaluate the personalized connection quality of latent neighbors for each service user. Then, we propose an integrated model based on LFM. Finally, we conduct a group of experiments on a large-scale real-world QoS dataset and the results demonstrate that our approach is effective, especially in the situation of data sparsity.}, 
keywords={Web services;quality of service;LFM;QoS delivery;QoS values;Web service;high-quality service recommendations;highly sparse QoS data;latent factor models;latent physical location;neighbor factor model;network delay;network environment;network status information;personalized QoS prediction;personalized connection quality;service environment;service location;user location;Accuracy;Educational institutions;Mathematical model;Predictive models;Quality of service;Vectors;Web services;Latent Factor Models;QoS prediction;SVD;Web Service;data sparsity}, 
doi={10.1109/SCC.2014.23}, 
month={June},}
@INPROCEEDINGS{7883032, 
author={Wang Chen and Qiang Gao and Huagang Xiong}, 
booktitle={2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
title={Predictability of action time for online human behaviors}, 
year={2016}, 
pages={131-134}, 
abstract={With the widespread use of internet technology, the online behaviors become a more and more important part in human's daily lives. Knowing the time of user's next action in online activities is quite valuable for improving online services, which prompts us to wonder whether the time of user's next online activity is predictable? In this paper, we study the predictability of action time for human online activities using the dataset from a social network. To this end, we map the inter-event time sequence of user's online activities to a sequence of inter-event time symbols and analyze it using information-theoretic method. Results show that knowing the time interval between the current activity and previous activity decreases the entropy about the time interval between the next activity and current activity, i.e., in the inter-event time sequence, the knowledge of an inter-event time can help decrease the entropy about its next one, which indicates that the time of next online activity is predictable. Moreover, the short and long inter-event times decrease the entropy about the next inter-event time more largely than the medium ones, which indicates that the short and long inter-event times have higher predictive powers. Furthermore, our results show that the action time of online activities in weekdays is more predictable than that in weekends.}, 
keywords={Internet;behavioural sciences computing;entropy;social networking (online);Internet technology;action time predictability;entropy;information-theoretic method;interevent time sequence;interevent time symbols;online human behaviors;online services;social network;user online activities;Entropy;Indexes;Predictive models;human activity;online behavior;social network;temporal predictability}, 
doi={10.1109/ICSESS.2016.7883032}, 
month={Aug},}
@INPROCEEDINGS{7561174, 
author={P. N. Karthik and R. Ramakrishna and G. Joseph and C. R. Murthy and J. Sebastian and N. B. Mehta}, 
booktitle={2016 Twenty Second National Conference on Communication (NCC)}, 
title={Model-based interference cartography and visualization}, 
year={2016}, 
pages={1-6}, 
abstract={In this work, we present a tool to construct and visualize the spatio-temporal variations of power. A dataset of real-world power measurements is collected over a geographical area of interest. Relevant parameters of the environment such as the path loss exponent and the decorrelation time of the lognormal shadow fading are extracted from the dataset. Also, the average powers measured at a finite set of known locations are interpolated to obtain the average power distribution over the area. Using the parameters of the lognormal shadow fading, synthetic data with the same temporal behavior of the dataset is generated, and multiplied with the average power distribution. The resulting spatio-temporal power map is displayed on the screen through a graphical user interface developed in-house. The proposed approaches for interpolation and parameter extraction are validated using test datasets generated using the well-accepted modified Gudmundson model for the spatio-temporal correlation of lognormal shadow fading. We also undertake a comparative study of three different interpolation techniques: linear interpolation, inverse distance weighing and ordinary kriging. Further, we compare a model-based approach with a model-free approach for interpolation, and find that model-based ordinary kriging provides the best mean absolute percentage error performance.}, 
keywords={cartography;decorrelation;fading channels;graphical user interfaces;interpolation;power measurement;radiofrequency interference;radiowave propagation;spatiotemporal phenomena;Gudmundson model;decorrelation time;finite set;geographical area;graphical user interface;interpolation techniques;inverse distance weighing;linear interpolation;lognormal shadow fading;mean absolute percentage error performance;model-based interference cartography;ordinary kriging;parameter extraction;path loss exponent;power distribution;power measurements;spatiotemporal correlation;spatiotemporal power map;Correlation;Decorrelation;Fading channels;Graphical user interfaces;Interference;Interpolation;Power measurement}, 
doi={10.1109/NCC.2016.7561174}, 
month={March},}
@INPROCEEDINGS{7195575, 
author={J. Zhu and P. He and Z. Zheng and M. R. Lyu}, 
booktitle={2015 IEEE International Conference on Web Services}, 
title={A Privacy-Preserving QoS Prediction Framework for Web Service Recommendation}, 
year={2015}, 
pages={241-248}, 
abstract={QoS-based Web service recommendation has recently gained much attention for providing a promising way to help users find high-quality services. To facilitate such recommendations, existing studies suggest the use of collaborative filtering techniques for personalized QoS prediction. These approaches, by leveraging partially observed QoS values from users, can achieve high accuracy of QoS predictions on the unobserved ones. However, the requirement to collect users' QoS data likely puts user privacy at risk, thus making them unwilling to contribute their usage data to a Web service recommender system. As a result, privacy becomes a critical challenge in developing practical Web service recommender systems. In this paper, we make the first attempt to cope with the privacy concerns for Web service recommendation. Specifically, we propose a simple yet effective privacy-preserving framework by applying data obfuscation techniques, and further develop two representative privacy-preserving QoS prediction approaches under this framework. Evaluation results from a publicly-available QoS dataset of real-world Web services demonstrate the feasibility and effectiveness of our privacy-preserving QoS prediction approaches. We believe our work can serve as a good starting point to inspire more research efforts on privacy-preserving Web service recommendation.}, 
keywords={Web services;collaborative filtering;data privacy;quality of service;recommender systems;QoS dataset;QoS values;QoS-based Web service recommendation system;collaborative filtering techniques;data obfuscation techniques;high-quality services;personalized QoS prediction;privacy-preserving QoS prediction framework;user privacy;Collaboration;Data privacy;Predictive models;Privacy;Quality of service;Servers;Web services;QoS prediction;Web service recommendation;collaborative filtering;privacy preservation}, 
doi={10.1109/ICWS.2015.41}, 
month={June},}
@INPROCEEDINGS{6173539, 
author={W. W. Kim and S. Park and J. Hwang and S. Lee}, 
booktitle={2011 8th International Conference on Information, Communications Signal Processing}, 
title={Automatic head pose estimation from a single camera using projective geometry}, 
year={2011}, 
pages={1-5}, 
abstract={Estimation of human head position and orientation has become an increasingly important issue in human-computer interaction field. Over the last decade, many approaches have been introduced to achieve head pose estimation in both academical and industrial fields, but the low-cost and real-time application still proves to be a difficult task. Motivated by the past researches, we propose an automatic and monocular head pose estimation system. We applied a number of improvements to a direct linear transformation algorithm called Pose from Orthography and Scaling with ITerations (POSIT) and applied it for the head pose estimation. User's virtual head model is also recovered by analyzing laser scan database and corrected by the head pose. The entire process is completely automatic with no need for users to pre-register for identification or initialize their head position. Experiments on a public dataset show realtime performance with lower errors than previous head pose estimation methods.}, 
keywords={cameras;pose estimation;POSIT;automatic head pose estimation;camera;human head position estimation;human-computer interaction;pose from orthography and scaling with iterations;projective geometry;virtual head model;Computational modeling;Estimation;Head;Magnetic heads;Solid modeling;Three dimensional displays;Vectors}, 
doi={10.1109/ICICS.2011.6173539}, 
month={Dec},}
@INPROCEEDINGS{6208957, 
author={A. Cufoglu and M. Lohi and C. Everiss}, 
booktitle={2012 IEEE 10th International Symposium on Applied Machine Intelligence and Informatics (SAMI)}, 
title={Weighted Instance Based Learner (WIBL) for user profiling}, 
year={2012}, 
pages={201-205}, 
abstract={With an increase in web-based products and services, user profiling has created opportunities for both businesses and other organizations to provide a channel for user awareness as well as to achieve high user satisfaction. Apart from traditional collaborative and content-based methods, a number of classification and clustering algorithms have been used for user profiling. Instance Based Learner (IBL) classifier is a comprehensive form of the Nearest Neighbour (NN) algorithm and it is suitable for user profiling as users with similar profiles are likely to share similar personal interests and preferences. In IBL every attribute has an equal effect on the classification regardless of their relevance. In this paper, we proposed a weighted classification method, namely Weighted Instance Based Learner (WIBL), to build and handle user profiles. With WIBL, we introduce Per Category Feature (PCF) method to IBL in order to distinguish the effect of attributes on classification. PCF is an attribute weighting method and it assigns weights to attributes using conditional probabilities. The direct use of this method with IBL is not possible. Hence, two possible solutions were also proposed to address this problem. This study is aimed to test the performance of WIBL for user profiling. To validate the performance of WIBL, a series of computer simulations were carried out. These simulations were conducted using a large user profile database that includes 5000 training and 1000 test instances (users). Here, each user is represented with three sets of profile information; demographic, interest and preference data. The results illustrate that WIBL with PCF methods performs better than IBL on user profiling by reducing the error up to 28% on the selected dataset.}, 
keywords={Internet;learning (artificial intelligence);pattern classification;pattern clustering;probability;user modelling;IBL classifier;NN algorithm;PCF method;WIBL;Web-based products;Web-based services;attribute weighting method;classification algorithms;clustering algorithms;computer simulations;conditional probabilities;demographic data;instance based learner classifier;interest data;nearest neighbour algorithm;per category feature method;preference data;profile information;user awareness;user profiling;user satisfaction;weight assignment;weighted instance based learner;Bayesian methods;Clustering algorithms;Collaboration;Educational institutions;Error analysis;Machine learning;Training}, 
doi={10.1109/SAMI.2012.6208957}, 
month={Jan},}
@INPROCEEDINGS{6927544, 
author={F. Xie and Z. Chen and J. Shang and X. Feng and W. Huang and J. Li}, 
booktitle={2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)}, 
title={A Link Prediction Approach for Item Recommendation with Complex Number}, 
year={2014}, 
volume={1}, 
pages={205-212}, 
abstract={Recommendation can be reduced to a sub-problem of link prediction, with specific nodes (users and items) and links (similar relations among users/items, and interactions between users and items). However, the previous link prediction algorithms need to be modified to suit the recommendation cases since they do not consider the separation of these two fundamental relations: similar or dissimilar and like or dislike. In this paper, we propose a novel and unified way to solve this problem, which models the relation duality using complex number. Under this representation, the previous works can directly reuse. In experiments with the Movie Lens dataset and the Android software website AppChina.com, the presented approach achieves significant performance improvement comparing with other popular recommendation algorithms both in accuracy and coverage. Besides, our results revealed some new findings. First, it is observed that the performance is improved when the user and item popularities are taken into account. Second, the item popularity plays a more important role than the user popularity does in final recommendation. Since its notable performance, we are working to apply it in a commercial setting, AppChina.com website, for application recommendation.}, 
keywords={Web sites;recommender systems;Android software Website;AppChina.com;Movie Lens dataset;complex number;item recommendation;link prediction approach;relation duality;Accuracy;Collaboration;Measurement;Prediction algorithms;Recommender systems;Symmetric matrices;Training;Complex Number;Data Sparsity;Link Prediction;Recommender Systems}, 
doi={10.1109/WI-IAT.2014.35}, 
month={Aug},}
@INPROCEEDINGS{7042486, 
author={F. Wang and W. Chen and F. Wu and Y. Zhao and H. Hong and T. Gu and L. Wang and R. Liang and H. Bao}, 
booktitle={2014 IEEE Conference on Visual Analytics Science and Technology (VAST)}, 
title={A visual reasoning approach for data-driven transport assessment on urban roads}, 
year={2014}, 
pages={103-112}, 
abstract={Transport assessment plays a vital role in urban planning and traffic control, which are influenced by multi-faceted traffic factors involving road infrastructure and traffic flow. Conventional solutions can hardly meet the requirements and expectations of domain experts. In this paper we present a data-driven solution by leveraging a visual analysis system to evaluate the real traffic situations based on taxi trajectory data. A sketch-based visual interface is designed to support dynamic query and visual reasoning of traffic situations within multiple coordinated views. In particular, we propose a novel road-based query model for analysts to interactively conduct evaluation tasks. This model is supported by a bi-directional hash structure, TripHash, which enables real-time responses to the data queries over a huge amount of trajectory data. Case studies with a real taxi GPS trajectory dataset (> 30GB) show that our system performs well for on-demand transport assessment and reasoning.}, 
keywords={data visualisation;road traffic;traffic engineering computing;user interfaces;TripHash bidirectional hash structure;data queries;data-driven transport assessment;road-based query model;sketch-based visual interface;taxi GPS trajectory dataset;taxi trajectory data;traffic control;urban planning;urban roads;visual analysis system;visual reasoning approach;Global Positioning System;Indexes;Roads;Topology;Trajectory;Visualization;Hash Index;Road-based Query;Taxi Trajectory;Visual Analysis}, 
doi={10.1109/VAST.2014.7042486}, 
month={Oct},}
@INPROCEEDINGS{6978966, 
author={N. Tsapatsoulis and F. Mendez}, 
booktitle={2014 9th International Workshop on Semantic and Social Media Adaptation and Personalization}, 
title={Social Vote Recommendation: Building Party Models Using the Probability to Vote Feedback of VAA Users}, 
year={2014}, 
pages={124-129}, 
abstract={Voting Advice Applications (VAAs) are online tools that match the policy preferences of voters' with the policy positions of political parties or candidates. A recent, innovative extension of VAAs has been to draw on the field of computer science to introduce a social vote recommendation borrowing the basic principles of collaborative filtering. The latter takes advantage of the community of VAA users to provide a vote recommendation. This paper presents a comparative study of social vote recommendation approaches that are based on machine learning. We build party models by utilizing both categorical variables, i.e., Voting intention and ordinal variables, i.e., Probability to vote for each one of the competing parties. The latter were first introduced in a practical VAA during the federal election in Germany in September 2013. The dataset from this election, consisting of more than 150.000 users, was used in our experiments.}, 
keywords={collaborative filtering;probability;recommender systems;social sciences computing;Germany;VAA users;candidate policy positions;categorical variables;collaborative filtering;computer science;federal election;machine learning;online tools;ordinal variables;party models;political party policy positions;probability-to-vote feedback;social vote recommendation approaches;voter policy preferences;voting advice applications;voting intention;Computational modeling;Estimation;Mathematical model;Nominations and elections;Sections;Training;Vectors;Voting advice applications;artificial neural networks;collaborative filtering;political party modeling;social vote recommendation}, 
doi={10.1109/SMAP.2014.17}, 
month={Nov},}
@INPROCEEDINGS{7045288, 
booktitle={2014 International Conference on High Performance Computing and Applications (ICHPCA)}, 
title={Table of contents}, 
year={2014}, 
pages={iii-x}, 
abstract={The following topics are dealt with: greedy load balancing algorithms; heterogeneous distributed computing system; exploiting fault tolerance; cache memory structures; reconfigurable cache architecture; multiphase applications scheduling; identical parallel multiprocessor; MATLAB distributed computing server; AIM; resource provisioning strategy; dynamic cloud environment; cloud computing memory allocation; timestamped signature scheme; message recovery; CMMSPEED; reliable real-time protocol; industrial mesh network; MANET; TCP variants protocols; wireless network; selective cooperation method; dynamic traffic pattern; NS2; data hiding; halftone images; mathematical morphology; conjugate ordered dithering; secure group key agreement protocol design; elliptic curve cryptography; fingerprint based symmetric cryptography; BER performance comparison; MIMO System; STBC; MRC; different fading channels; fault node detection algorithm; wireless sensor networks; secured packet inspection; hierarchical pattern matching; incremental clustering algorithm; GPU; STBC-OFDM WiMAX system performance evaluation; graphics processing unit; string sorting; many-threaded architectures; multithreaded architectures; differential evolution; cost reduction cellular network; game theory; mobile wireless sensor network; GNU radio; error rate performance enhancement; hybrid equalization technique; MIMO-OFDM systems; BER; Brewster's angle; polarization diversity technique; indoor visible light communication system; route stability; AODV; parallel Hadamard transform; multimesh network; dense matrix multiplication; 2D mesh; scalable parallel clustering approach; parallel K means; firefly algorithms; apriori algorithm; HDFS; parameter optimization; nonlinear fitting; epidemic disease propagation detection algorithm; MapReduce; realistic social contact networks; leather defects identification; auto adaptive edge detection image processing algorithm; list scheduling algorithms analysis; parallel sys- em; evolutionary Jordan Pi-sigma neural network; gradient descent learning; HDFS performance evaluation; big data management; securing financial network system; multilevel security; cyber physical system; data mining concepts; Web crawling; convex points; convex hull; image searching algorithm development; social network; graph mining techniques; clustering algorithm; binary data sets; human facial expression recognition; image color analysis; FPGA realization; particle swarm optimisation algorithm; floating point arithmetic; gradient local auto correlation; handwritten devanagari character recognition; feature extraction; feature matching; speech recognition; intelligent steganography detection; fuzzy logic; wind energy conversion system; solide oxide fuel; biobjective portfolio optimisation; muliobjective simulated annealing; test case prioritization technique; free-text user authentication technique; keystroke dynamics; weighted bag hybrid multiple classifier machine; boosting prediction accuracy; automated document indexing; intelligent hierarchical clustering; training feedforward neural network; novel gravitational search optimization; real time robotic arm control; hand gestures; hybrid differential evolution-flower pollination algorithm; function minimization; bijective mapping; arbitrary finite state machine; high performance computing application; lattice physics code VISWAM; stencil computation; CPU-GPU; threshold voltage roll-off; triple gate FinFET analysis; Ultra-Thin Si directly; cardiac pulse measurement; multicore architecture; optimum gene selection; gene expression dataset; cancer dataset; PKI; timestamped secure signing tool; e-documents; trigger action reaction model; artificial energy drinks; stability enhancement; differential evolution algorithm; molecular dynamics application optimization; Intel Xeon Phi Coprocessor; pair programming team;UTB-SG; DG MOSFET; security robot-SECBOT;AODV routing protocol modification; VANET; lifting biorthogona}, 
keywords={Hadamard transforms;MIMO communication;OFDM modulation;cache storage;cellular automata;cloud computing;data mining;distributed processing;edge detection;feature extraction;floating point arithmetic;graphics processing units;greedy algorithms;image processing;image texture;mathematical morphology;median filters;mobile communication;multi-threading;multiprocessing systems;neural nets;optimisation;particle swarm optimisation;pattern matching;public key cryptography;reconfigurable architectures;resource allocation;routing protocols;simulated annealing;speech recognition;steganography;transport protocols;wavelet transforms;wireless sensor networks;AIM;AODV;AODV routing protocol modification;BER;BER performance comparison;Brewster' angle;CMMSPEED;CPU-GPU;GNU radio;HDFS;HDFS performance evaluation;MANET;MATLAB distributed computing server;MIMO System;MIMO-OFDM systems;MRC;MapReduce;ROM design;SDR;STBC;STBC-OFDM WiMAX system;TCP variants protocols;Web crawling;adaptive background subtraction;apriori algorithm;arbitrary finite state machine;auto adaptive edge detection image processing algorithm;automated document indexing;big data management;bijective mapping;binary data sets;biobjective portfolio optimisation;biorthogonal wavelet design;boosting prediction accuracy;cache memory structures;cancer dataset;cardiac pulse measurement;cellular network;cloud computing memory allocation;clustering algorithm;conjugate ordered dithering;convex hull;convex points;cyber physical system;data hiding;data mining concepts;decoder segment optimization;dense matrix multiplication;differential evolution;dynamic cloud environment;dynamic traffic pattern;e-documents;elliptic curve cryptography;epidemic disease propagation detection algorithm;error rate performance enhancement;evolutionary Jordan Pi-sigma neural network;exploiting fault tolerance;fading channels;fault node detection algorithm;feature extraction;feature matching;financial network system security;fingerprint based symmetric cryptography;firefly algorithms;floating point arithmetic;free-text user authentication technique;function minimization;fuzzy based median filter;fuzzy logic;game theory;gene expression dataset;generation reliability evaluation;gradient descent learning;gradient local auto correlation;graph mining techniques;graphics processing unit;greedy load balancing algorithms;halftone images;hand gestures;handwritten devanagari character recognition;heterogeneous distributed computing system;hierarchical pattern matching;human facial expression recognition;hybrid differential evolution-flower pollination algorithm;hybrid equalization technique;identical parallel multiprocessor;image color analysis;image searching algorithm development;impulse noise detection;incremental clustering algorithm;indoor visible light communication system;industrial mesh network;intelligent hierarchical clustering;intelligent steganography detection;intrusion detection systems;keystroke dynamics;lattice physics code VISWAM;leather defects identification;list scheduling algorithms analysis;many-threaded architectures;mathematical morphology;medical images;message recovery;mobile communications;mobile wireless sensor network;muliobjective simulated annealing;multicore architecture;multilevel security;multimesh network;multiphase applications scheduling;multithreaded architectures;neighborhood switching median filter;nonlinear fitting;novel gravitational search optimization;optimum gene selection;parallel Hadamard transform;parallel K means;parallel system;parameter optimization;particle swarm optimisation algorithm;quantum dot cellular automata;real time robotic arm control;reconfigurable cache architecture;reconfigurable viterbi decoder;reliable real-time protocol;resource provisioning strategy;route stability;scalable parallel clustering approach;secure group key agreement protocol design;secured packet inspection;selective cooperation method;social contact networks;social network;solide oxide fuel;speech recognition;stencil computation;string sorting;texture analysis;threshold voltage roll-off;timestamped secure signing tool;timestamped signature scheme;training feedforward neural network;trigger action reaction model;triple gate FinFET analysis;weighted bag hybrid multiple classifier machine;wind energy conversion system;wind energy penetrated power system;wireless network;wireless sensor networks}, 
doi={10.1109/ICHPCA.2014.7045288}, 
month={Dec},}
@ARTICLE{7360932, 
author={S. Qian and T. Zhang and C. Xu and J. Shao}, 
journal={IEEE Transactions on Multimedia}, 
title={Multi-Modal Event Topic Model for Social Event Analysis}, 
year={2016}, 
volume={18}, 
number={2}, 
pages={233-246}, 
abstract={With the massive growth of social events in Internet , it has become more and more difficult to exactly find and organize the interesting events from massive social media data, which is useful to browse, search, and monitor social events by users or governments . To deal with this problem, we propose a novel multi-modal social event tracking and evolution framework to not only effectively capture multi-modal topics of social events, but also obtain the evolutionary trends of social events and generate effective event summary details over time. To achieve this goal, we propose a novel multi-modal event topic model (mmETM), which can effectively model social media documents, including long text with related images, and learn the correlations between textual and visual modalities to separate the visual-representative topics and non-visual-representative topics. To apply the mmETM model to social event tracking, we adopt an incremental learning strategy denoted as incremental mmETM, which can obtain informative textual and visual topics of social events over time to help understand these events and their evolutionary trends. To evaluate the effectiveness of our proposed algorithm, we collect a real-world dataset to conduct various experiments. Both qualitative and quantitative evaluations demonstrate that the proposed mmETM algorithm performs favorably against several state-of-the-art methods.}, 
keywords={Internet;data mining;evolutionary computation;learning (artificial intelligence);social networking (online);text analysis;IEEE textual modalities;Internet;incremental learning strategy;incremental mmETM algorithm;mmETM model;multimodal event topic model;multimodal social event tracking framework;social event analysis;social media data;social media documents;visual modalities;visual-representative topics;Google;Hidden Markov models;Market research;Media;Multimedia communication;Streaming media;Visualization;Event evolution;multi-modality;social event tracking;social media;topic model}, 
doi={10.1109/TMM.2015.2510329}, 
ISSN={1520-9210}, 
month={Feb},}
@INPROCEEDINGS{7518350, 
author={Y. Zhao and Y. Yang and Z. Mi and Z. Xiong}, 
booktitle={2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)}, 
title={Combining Clustering Algorithm with Factorization Machine for Friend Recommendation in Social Network}, 
year={2015}, 
pages={887-893}, 
abstract={Social Network Service (SNS) has been explosively growing and generating huge amounts of data every day, it is a meaningful job to mine useful information from the big data which generated from the social networks. In this paper, we study the relationship and behavior of social network users, and then put forward a model which combines Clustering Algorithm with Factorization Machine (FM) for SNS Friend Recommendation. With the help of Clustering Algorithm, we classified the users and make it easy to locate users' characteristics and interests, and by using FM we can solve the Data Sparseness problem effectively. We trained this model by Markov Chain Monte Carlo (MCMC) algorithm and verified our model using Ten cent Webo's real dataset and proved it has a better computational efficiency and better accuracy in recommending friends.}, 
keywords={Big Data;Markov processes;Monte Carlo methods;data mining;pattern clustering;recommender systems;social networking (online);Big Data;FM;MCMC algorithm;Markov Chain Monte Carlo algorithm;SNS friend recommendation;Ten cent Webo real dataset;clustering algorithm;data sparseness problem;factorization machine;information mining;social network service;social network user behavior;user characteristics;user interests;Algorithm design and analysis;Clustering algorithms;Computational modeling;Frequency modulation;Prediction algorithms;Social network services;Sparse matrices;Clustering Algorithm;Factorization Machine;SNS;recommendation}, 
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.171}, 
month={Aug},}
@INPROCEEDINGS{1437848, 
author={B. K. Szymanski and Y. Zhang}, 
booktitle={Proceedings from the Fifth Annual IEEE SMC Information Assurance Workshop, 2004.}, 
title={Recursive data mining for masquerade detection and author identification}, 
year={2004}, 
pages={424-431}, 
abstract={In this paper, a novel recursive data mining method based on the simple but powerful model of cognition called a conceptor is introduced and applied to computer security. The method recursively mines a string of symbols by finding frequent patterns, encoding them with unique symbols and rewriting the string using this new coding. We apply this technique to two related but important problems in computer security: (i) masquerade detection to prevent a security attack in which an intruder impersonates a legitimate user to gain access to the resources, and (ii) author identification, in which anonymous or disputed computer session needs to be attributed to one of a set of potential authors. Many methods based on automata theory, hidden Markov models, Bayesian models or even matching algorithms from bioinformatics have been proposed to solve the masquerading detection problem but less work has been done on the author identification. We used recursive data mining to characterize the structure and high-level symbols in user signatures and the monitored sessions. We used one-class SVM to measure the similarity of these two characterizations. We applied weighting prediction scheme to author identification. On the SEA dataset that we used in our experiments, the results were very promising.}, 
keywords={authorisation;data mining;digital signatures;Bayesian model;author identification;automata theory;bioinformatics;cognition model;computer security attack;hidden Markov model;intrusion detection;masquerade detection problem;matching algorithm;recursive data mining method;user signature;weighting prediction scheme;Automata;Bayesian methods;Bioinformatics;Cognition;Computer security;Data mining;Encoding;Hidden Markov models;Monitoring;Support vector machines}, 
doi={10.1109/IAW.2004.1437848}, 
month={June},}
@INPROCEEDINGS{7860415, 
author={N. Shaker and M. Abou-Zleikha}, 
booktitle={2016 IEEE Conference on Computational Intelligence and Games (CIG)}, 
title={Transfer learning for cross-game prediction of player experience}, 
year={2016}, 
pages={1-8}, 
abstract={Several studies on cross-domain users' behaviour revealed generic personality trails and behavioural patterns. This paper, proposes quantitative approaches to use the knowledge of player behaviour in one game to seed the process of building player experience models in another. We investigate two settings: in the supervised feature mapping method, we use labeled datasets about players' behaviour in two games. The goal is to establish a mapping between the features so that the models build on one dataset could be used on the other by simple feature replacement. For the unsupervised transfer learning scenario, our goal is to find a shared space of correlated features based on unlabelled data. The features in the shared space are then used to construct models for one game that directly work on the transferred features of the other game. We implemented and analysed the two approaches and we show that transferring the knowledge of player experience between domains is indeed possible and ultimately useful when studying players' behaviour and when designing user studies.}, 
keywords={behavioural sciences;computer games;unsupervised learning;behavioural patterns;cross-domain users behaviour;cross-game prediction;feature replacement;features mapping;knowledge transfer;personality trails;player behaviour;player experience;supervised feature mapping;unsupervised transfer learning;Computational modeling;Context modeling;Data collection;Data models;Feature extraction;Games;Predictive models}, 
doi={10.1109/CIG.2016.7860415}, 
month={Sept},}
@INPROCEEDINGS{4630586, 
author={P. Gursky and V. Vanekova and J. Pribolova}, 
booktitle={2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence)}, 
title={Fuzzy user preference model for top-k search}, 
year={2008}, 
pages={1606-1612}, 
abstract={The task of modeling complex user preferences face the problem of understandability to common users and the problem of querying the dataset for preferred objects. We propose natural and complex model of user preferences decomposed with respect to particular attribute values. Partial preferences are combined by monotone aggregation function. The user model is stored in ontology structure. We also present an extension of top-k objects search algorithm to provide a query evaluation of the proposed model.}, 
keywords={fuzzy set theory;ontologies (artificial intelligence);query processing;complex user preferences;fuzzy user preference model;monotone aggregation function;natural-complex model;ontology structure;query evaluation;Conferences;Fuzzy systems}, 
doi={10.1109/FUZZY.2008.4630586}, 
ISSN={1098-7584}, 
month={June},}
@ARTICLE{7192729, 
author={J. Wang and K. Mueller}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={The Visual Causality Analyst: An Interactive Interface for Causal Reasoning}, 
year={2016}, 
volume={22}, 
number={1}, 
pages={230-239}, 
abstract={Uncovering the causal relations that exist among variables in multivariate datasets is one of the ultimate goals in data analytics. Causation is related to correlation but correlation does not imply causation. While a number of casual discovery algorithms have been devised that eliminate spurious correlations from a network, there are no guarantees that all of the inferred causations are indeed true. Hence, bringing a domain expert into the casual reasoning loop can be of great benefit in identifying erroneous casual relationships suggested by the discovery algorithm. To address this need we present the Visual Causal Analyst - a novel visual causal reasoning framework that allows users to apply their expertise, verify and edit causal links, and collaborate with the causal discovery algorithm to identify a valid causal network. Its interface consists of both an interactive 2D graph view and a numerical presentation of salient statistical parameters, such as regression coefficients, p-values, and others. Both help users in gaining a good understanding of the landscape of causal structures particularly when the number of variables is large. Our framework is also novel in that it can handle both numerical and categorical variables within one unified model and return plausible results. We demonstrate its use via a set of case studies using multiple practical datasets.}, 
keywords={data analysis;data mining;data visualisation;inference mechanisms;statistical testing;casual discovery algorithms;categorical variables;causal reasoning framework;data analytics;interactive 2D graph view;multivariate dataset;numerical variables;salient statistical parameters;visual causality analyst;Correlation;Inference algorithms;Layout;Linear regression;Optimization;Visualization;Causality;High-dimensional data;Hypothesis testing;Visual evidence;Visual knowledge discovery}, 
doi={10.1109/TVCG.2015.2467931}, 
ISSN={1077-2626}, 
month={Jan},}
@INPROCEEDINGS{6746793, 
author={Q. Zhu and M. L. Shyu and H. Wang}, 
booktitle={2013 IEEE International Symposium on Multimedia}, 
title={VideoTopic: Content-Based Video Recommendation Using a Topic Model}, 
year={2013}, 
pages={219-222}, 
abstract={Most video recommender systems limit the content to the metadata associated with the videos, which could lead to poor results since metadata is not always available or correct. Meanwhile, the visual information of videos is typically not fully explored, which is especially important for recommending new items with limited metadata information. In this paper, a novel content-based video recommendation framework, called Video Topic, that utilizes a topic model is proposed. It decomposes the recommendation process into video representation and recommendation generation. It aims to capture user interests in videos by using a topic model to represent the videos, and then generates recommendations by finding those videos that most fit to the topic distribution of the user interests. Experimental results on the Movie Lens dataset validate the effectiveness of Video Topic by evaluating each of its components and the whole framework.}, 
keywords={content-based retrieval;feature extraction;video signal processing;VideoTopic;content-based video recommendation;topic distribution;topic model;video representation;visual information;Computational modeling;Equations;Feature extraction;Mathematical model;Measurement;Motion pictures;Visualization;VideoTopic;content-based video recommendation;topic model;video presentation}, 
doi={10.1109/ISM.2013.41}, 
month={Dec},}
@INPROCEEDINGS{6486813, 
author={C. J. Song and H. Park and C. M. Yang and S. J. Jang and S. P. Lee}, 
booktitle={2013 IEEE International Conference on Consumer Electronics (ICCE)}, 
title={Implementation of a practical query-by-singing/humming (QbSH) system and its commercial applications}, 
year={2013}, 
pages={102-103}, 
abstract={This paper proposes a practical query-by-singing/humming (QbSH) system that retrieves polyphonic music such as an MP3 and its commercial applications. The performance of music retrieval system is mainly affected by the server. This paper discusses developing the state-of-the-art server side software stack which has several managers and plug-in engines. It also describes implementation of digital signal processor (DSP) module for stand-alone embedded platforms. The paper shows three different models for its commercial applications like smart phone, laptop and karaoke. We evaluate the performance of the proposed system with polyphonic music datasets using users' humming datasets as the input query.}, 
keywords={digital signal processing chips;embedded systems;information retrieval systems;music;query processing;DSP module;QbSH system;digital signal processor;karaoke;laptop;music retrieval system;polyphonic music dataset;polyphonic music retrieval;query-by-humming system;query-by-singing system;server side software stack;smart phone;stand-alone embedded platform;user humming dataset;Accuracy;Digital signal processing;Engines;Feature extraction;Servers;Smart phones;Software}, 
doi={10.1109/ICCE.2013.6486813}, 
ISSN={2158-3994}, 
month={Jan},}
@INPROCEEDINGS{6978189, 
author={J. Srisuan and A. Hanskunatai}, 
booktitle={2014 International Computer Science and Engineering Conference (ICSEC)}, 
title={The ensemble of Na #x00EF;ve Bayes classifiers for hotel searching}, 
year={2014}, 
pages={168-173}, 
abstract={The objective of the paper is to present a new ensemble of Naïve Bayes classifiers model for an application of hotel searching. The dataset were collected from 293 reviews of 15 hotels in Phuket. The main idea of the proposed model is to combine two models of Naïve Bayes classifiers with different feature selection techniques. The output of the searching model is a list of hotel names ranking by hotel probability related to user keywords. The searching performance of the ensemble model was compared with two classical searching methods: Boolean searching and Boyer-Moore searching. The results show that the ensemble of Naïve Bayes classifiers model provides the highest average rank_accuracy. In addition, the proposed model also takes the fastest time in searching when compared with the other techniques.}, 
keywords={Bayes methods;Boolean functions;hotel industry;information retrieval;pattern classification;Boolean searching;Boyer-Moore searching;Phuket;average rank_accuracy;ensemble model;hotel names;hotel probability;hotel searching;naïve Bayes classifiers;user keywords;Classification algorithms;Computational modeling;Computer science;Data models;Equations;Mathematical model;Probability;Ensemble model;Naïve Bayes classifier;hotel searchin;opinion mining}, 
doi={10.1109/ICSEC.2014.6978189}, 
month={July},}
@ARTICLE{5430993, 
author={W. J. J. Roberts}, 
journal={IEEE Signal Processing Letters}, 
title={Application of a Gaussian, Missing-Data Model to Product Recommendation}, 
year={2010}, 
volume={17}, 
number={5}, 
pages={509-512}, 
abstract={A Gaussian, missing-data model is applied to predict product ratings. Vectors of product ratings from users are assumed to be independent and identically distributed. Two approaches for parameter estimation in this model are studied: Little and Rubin's expectation-maximization algorithm and McMichael's modified stochastic gradient descent approach. The resulting estimates are used in minimum mean squared error prediction of product ratings using the conditional mean. On a large dataset, performance using McMichael's approach is better than reported performance of the popular matrix factorization approach.}, 
keywords={Gaussian processes;data models;expectation-maximisation algorithm;matrix decomposition;mean square error methods;recommender systems;Gaussian model;McMichael algorithm;conditional mean;expectation maximization algorithm;matrix factorization;mean squared error prediction;missing data model;parameter estimation;product rating;product recommendation;recommender system;stochastic gradient;Covariance matrix;Expectation-maximization algorithms;Iterative algorithms;Maximum likelihood estimation;Parameter estimation;Predictive models;Recommender systems;Social network services;Sparse matrices;Stochastic processes;Conditional mean;expectation maximization;gradient descent;maximum likelihood}, 
doi={10.1109/LSP.2010.2045543}, 
ISSN={1070-9908}, 
month={May},}
@INPROCEEDINGS{7463221, 
author={K. Nitschke and E. B. de Azevedo}, 
booktitle={2015 3rd Experiment International Conference (exp.at'15)}, 
title={3D atmosphere column evaluation and clouds tomography: At Eastern North Atlantic (ENA), Graciosa Island ARM facility}, 
year={2015}, 
pages={91-92}, 
abstract={One source of uncertainty that thwarts accurate and comprehensive representation of present and future climate processes in models is the role of marine stratocumulus clouds. In particular, clouds that prevail over the eastern subtropical oceans have proven to play a critical role in boundary layer dynamics and in the global climate. Azores have been identified as having the mix condition for research on the life cycle and characteristics of marine startocumulos clouds and for a better understanding of the complex ocean atmosphere interactions. The Atmospheric Radiation Measurement (ARM) Climate Research Program from the Department of Energy (DOE), with 20 years of operations, has been providing data to advance research from atmospheric observations at diverse climatic regimes around the world. Since 2009 Azores has been included in this global program. The campaign of the ARM Mobile Facility at Graciosa Island, Azores, in the context of the Clouds, Aerosol and Precipitation in the Marine Boundary Layer (CAP-MBL) project, added the most extensive and comprehensive dataset of marine boundary layer (MBL) clouds to date. Solid preliminary findings an valuable data sets have been used since that to promote a true climatology of marine cloud structure over the north Atlantic. As a result, the facility becomes a fixed site on the 1st of October of 2013 and has joined the fixed network of the ARM Climate Research Facilities around the world. Identified broadly as the Eastern North Atlantic (ENA), this user facility has augmented the measurement capability with the addition of a Ka-/W-Band scanning cloud radar, a X-Band precipitation radar, Doppler lidar and an extensive set of radiometric measurements and routine radiosonde soundings, which, taken together, provide a three-dimensional view of the atmospheric phenomena, some of them poorly understood until now.}, 
keywords={atmospheric boundary layer;clouds;remote sensing by radar;3D atmosphere column evaluation;AD 2009;AD 2013 10 01;ARM climate research facilities;ARM climate research program;ARM mobile facility;Azores;CAP-MBL project;Clouds Aerosol and Precipitation in the Marine Boundary Layer;Department of Energy;Doppler lidar;ENA;Eastern North Atlantic;Graciosa Island climate process;Ka-W-Band scanning cloud radar;X-band precipitation radar;atmospheric radiation measurement;boundary layer dynamics;climatic regime;climatology;cloud tomography;marine boundary layer cloud;marine cloud structure;marine stratocumulus cloud;ocean-atmosphere interaction;radiometric measurement;radiosonde sounding;Clouds;Maintenance engineering;Thermodynamics;ARM facility;Atmospheric Radiation Measurements;Climate;Esatern Nort Atlantic;Graciosa}, 
doi={10.1109/EXPAT.2015.7463221}, 
month={June},}
@INPROCEEDINGS{7473083, 
author={R. Huang and X. Wei and C. Lv and X. Li and S. Zhang}, 
booktitle={2015 First International Conference on Computational Intelligence Theory, Systems and Applications (CCITSA)}, 
title={Prediction Model for User's QoE in Imbalanced Dataset}, 
year={2015}, 
pages={41-45}, 
abstract={Nowadays, the users of IPTV require better user experience and the media providers are interested in finding the key factors which will influence the Quality of Experience (QoE) and the way to predict the QoE. In this paper, we discuss the relationship between the status of IPTV set-top box and user's QoE. We first clean the dataset and conduct some statistical analysis in dataset. Then, we select the important features in key performance indicator (KPI) dataset by the feature selection technology. In addition, we compare the decision tree model and Adaboost model in KPI-user's complaint dataset which is a classical imbalanced dataset. Extensive experimental results demonstrate that in imbalanced KPI-user's complaint dataset Adaboost model performs better than the decision tree in terms of predicting the user's complaint, and importantly we design a cost coefficient for Adaboost model to achieve a higher accuracy.}, 
keywords={IPTV;decision trees;feature selection;learning (artificial intelligence);quality of experience;set-top boxes;statistical analysis;Adaboost model;IPTV set-top box;KPI-user complaint dataset;decision tree model;feature selection technology;imbalanced dataset;key performance indicator;prediction model;quality of experience;statistical analysis;user QoE;user experience;Computational intelligence;Adaboost;QoE;decision tree;imbalanced dataset}, 
doi={10.1109/CCITSA.2015.33}, 
month={Dec},}
@INPROCEEDINGS{7886077, 
author={R. Jiamthapthaksin and T. H. Aung}, 
booktitle={2017 9th International Conference on Knowledge and Smart Technology (KST)}, 
title={User preferences profiling based on user behaviors on Facebook page categories}, 
year={2017}, 
pages={248-253}, 
abstract={User preference profiling is important in both social networking mining and recommender systems. Facebook provides information of page category over two hundred relating to user preferences, but the predefined categories may not fit application well. Explicitly mapping those categories to a desirable set of user preferences is a tedious task. This paper proposes an effective user profiling technique using features constructed from user behaviours on Facebook page in different categories. The models created from three well known classification algorithms: Naïve Bayes (NB), Artificial Neural Network (ANN), and Support Vector Machine (SVM) turn the raw data of user behaviours into a set of user preferences defined by application. The experiments performed on Facebook dataset show that the constructed features are implicitly reflecting user preferences and can be used to tailor the preferences as needed. Among the three algorithms SVM leverages classification performance the most with accuracy over seventy-two percent.}, 
keywords={Bayes methods;behavioural sciences computing;data mining;neural nets;pattern classification;recommender systems;social networking (online);support vector machines;Facebook dataset;Facebook page categories;artificial neural network classification;feature construction;naive Bayes classification;recommender systems;social networking mining;support vector machine classification;user behaviors;user preference profiling;Artificial neural networks;Decision support systems;Facebook;Handheld computers;Social computing;Support vector machines;Facebook page category;Naïve Bayes;Support Vector Machine;profiling;social computing;social networking;user preferences}, 
doi={10.1109/KST.2017.7886077}, 
month={Feb},}
@INPROCEEDINGS{6253560, 
author={Y. Kang and Z. Zheng and M. R. Lyu}, 
booktitle={2012 IEEE Fifth International Conference on Cloud Computing}, 
title={A Latency-Aware Co-deployment Mechanism for Cloud-Based Services}, 
year={2012}, 
pages={630-637}, 
abstract={Cloud computing attracts considerable attention from both industry and academic these years. Nowadays, a number of research investigations have been conducted on cloud-based services (e.g., IaaS, PaaS, SaaS, etc.). Deployment of cloud-based services is one of the most important research problems. In cloud computing, multiple services tend to cooperate with each other to accomplish complicated tasks. Deploying these services independently may not lead to good overall performance, since there are a lot of interactions among different services. Making an optimal co-deployment of multiple services is critical for reducing latency of user requests. When deploying highly related services, taking only distribution of users into consideration is not enough, since the deployment of one service would affect others. To attack this challenge, we employ cross service information as well as user locations to build a new model in integer programming formulation. To reduce the computation time of the model, we purpose a sequential model running iteratively to obtain approximate solution. Extensive experiments have been conducted over a large real-world dataset, involving 307 distributed computers in about 40 countries, and 1881 real-world Internet-based services in about 60 countries. The experimental results show the effectiveness of our proposed model. Our real-world dataset is publicly released to promote future research, which also makes our experiments reproducible.}, 
keywords={cloud computing;integer programming;cloud computing;cloud-based services;cross service information;distributed computers;integer programming formulation;latency-aware codeployment mechanism;optimal codeployment;real-world Internet-based services;sequential model;Cloud computing;Companies;Computational modeling;Computers;Facebook;Google;Linear programming;cloud;deployment;integer programming;multi-service}, 
doi={10.1109/CLOUD.2012.90}, 
ISSN={2159-6182}, 
month={June},}
@INPROCEEDINGS{1287344, 
author={Shu-Tzu Tsai and Chao-Tung Yang}, 
booktitle={e-Technology, e-Commerce and e-Service, 2004. EEE '04. 2004 IEEE International Conference on}, 
title={Decision tree construction for data mining on grid computing}, 
year={2004}, 
pages={441-447}, 
abstract={Decision tree is one of the frequently used methods in data mining for searching prediction information. Due to its characteristics which are suitable for parallelism, it has been widely adopted in high performance field and developed into various parallel decision tree algorithms to deal with huge data and complex computation. Following the development of other technology fields, grid computing is regarded as the extension of PC cluster and therefore it future research development is highly valued. This new wave of Internet application is the 3rd generation of Internet applications following the traditional Internet and Web application. We have presented a grid-based decision tree architecture, and hope it can be applied on both parallel and sequential algorithms for the decision tree applications. Also, based on the scope and model of data mining applied in grid environment as well as user equivalent perspective, grid roles can be categorized into three types. We are hoping that through these definitions, software developers can define clear system processes and differentiate the application scope for software applications. To fulfil our architecture, we first apply an existing parallel decision tree algorithm-SPRINT algorithm in the grid environment. The performance and differences in many other areas are compared using different sizes of dataset. The experimental results are used for future reference and further development.}, 
keywords={Internet;data mining;decision trees;grid computing;parallel algorithms;workstation clusters;Internet application;PC cluster;SPRINT algorithm;Web application;cluster computing;data mining;decision tree construction;grid computing;parallel algorithm;parallel decision tree algorithm;searching prediction information;sequential algorithm;software application;Application software;Clustering algorithms;Computer architecture;Concurrent computing;Data mining;Decision trees;Grid computing;High performance computing;Internet;Parallel processing}, 
doi={10.1109/EEE.2004.1287344}, 
month={March},}
@INPROCEEDINGS{6972321, 
author={H. Zhao and D. Guo and Q. Chen and L. Gao}, 
booktitle={2014 IEEE International Conference on Progress in Informatics and Computing}, 
title={A hybrid recommendation approach based on social tagging data preprocession}, 
year={2014}, 
pages={185-189}, 
abstract={As an important explicit rating approach, social tagging can not only describe resources but also reflect user's preferences. Therefore personalized recommendation based on social tagging has becoming a hot research direction. However, recommendation algorithms based on tags will encounter great data sparseness problem. In this paper, we process the original dataset by applying similarity propagation algorithm and popularity dimensionality reduction techniques. Hence the sparseness problem of the dataset can be partially solved. Finally, based on the high-quality dataset, we propose a hybrid recommendation algorithm. The experimental results show that our algorithm has a better performance than traditional pure content based or collaborative filtering recommendation algorithms.}, 
keywords={collaborative filtering;recommender systems;social networking (online);collaborative filtering;explicit rating approach;high-quality dataset;hybrid recommendation approach;personalized recommendation;recommendation algorithms;social tagging data preprocession;Algorithm design and analysis;Collaboration;Data models;Filtering;Filtering algorithms;Sparse matrices;Tagging;popularity dimension reduction;propagation;recommendation;sparseness;tag}, 
doi={10.1109/PIC.2014.6972321}, 
month={May},}
@INPROCEEDINGS{6686180, 
author={R. Bassily and A. Groce and J. Katz and A. Smith}, 
booktitle={2013 IEEE 54th Annual Symposium on Foundations of Computer Science}, 
title={Coupled-Worlds Privacy: Exploiting Adversarial Uncertainty in Statistical Data Privacy}, 
year={2013}, 
pages={439-448}, 
abstract={We propose a new framework for defining privacy in statistical databases that enables reasoning about and exploiting adversarial uncertainty about the data. Roughly, our framework requires indistinguishability of the real world in which a mechanism is computed over the real dataset, and an ideal world in which a simulator outputs some function of a "scrubbed" version of the dataset (e.g., one in which an individual user's data is removed). In each world, the underlying dataset is drawn from the same distribution in some class (specified as part of the definition), which models the adversary's uncertainty about the dataset. We argue that our framework provides meaningful guarantees in a broader range of settings as compared to previous efforts to model privacy in the presence of adversarial uncertainty. We also show that several natural, "noiseless" mechanisms satisfy our definitional framework under realistic assumptions on the distribution of the underlying data.}, 
keywords={data privacy;statistical databases;adversarial uncertainty;coupled-worlds privacy;natural noiseless mechanisms;statistical data privacy;statistical databases;Computer science;Data privacy;Databases;Educational institutions;Privacy;Random variables;Uncertainty;data privacy}, 
doi={10.1109/FOCS.2013.54}, 
ISSN={0272-5428}, 
month={Oct},}
@INPROCEEDINGS{1398497, 
author={J. Milles and A. van Susteren and T. Arts and P. Clarysse and P. Croisille and I. E. Magnin}, 
booktitle={2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro (IEEE Cat No. 04EX821)}, 
title={Automatic 2D segmentation of the left ventricle in tagged cardiac MRI using motion information}, 
year={2004}, 
pages={153-156 Vol. 1}, 
abstract={A new algorithm is presented to automatically segment the left ventricle from tagged MR images using motion information. This segmentation method is based on a deformable template which is fitted to tags' envelope image through an iterative process. Motion information is used for initializing the template. The method has been evaluated both on simulated and 5 in vivo datasets. A 5 slice levels, 36 frames per slice dataset is fully segmented in less than 10 minutes. The method proposed here has the potential to overcome the main drawbacks of previous approaches, such as user supervision and computing time, to foresee routine clinical use of tagged cardiac MRI.}, 
keywords={biomedical MRI;cardiology;image motion analysis;image segmentation;medical image processing;automatic 2D segmentation;deformable template;iterative process;left ventricle;motion information;tagged cardiac MRI;Art;Deformable models;Image segmentation;Iterative algorithms;Magnetic resonance imaging;Milling machines;Motion estimation;Myocardium;Solid modeling;Strain measurement}, 
doi={10.1109/ISBI.2004.1398497}, 
month={April},}
@ARTICLE{1323116, 
author={A. Freeman and S. S. Saatchi}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={On the detection of Faraday rotation in linearly polarized L-band SAR backscatter signatures}, 
year={2004}, 
volume={42}, 
number={8}, 
pages={1607-1616}, 
abstract={The potentially measurable effects of Faraday rotation on linearly polarized backscatter measurements from space are addressed. Single-polarized, dual-polarized, and quad-polarized backscatter measurements subject to Faraday rotation are first modeled. Then, the impacts are assessed using L-band polarimetric synthetic aperture radar (SAR) data. Due to Faraday rotation, the received signal will include other polarization characteristics of the surface, which may be detectable under certain conditions. Model results are used to suggest data characteristics that will reveal the presence of Faraday rotation in a given single-polarized, dual-polarized, or quad-polarized L-band SAR dataset, provided the user can identify scatterers within the scene whose general behavior is known or can compare the data to another, similar dataset with zero Faraday rotation. The data characteristics found to be most sensitive to a small amount of Faraday rotation (i.e., a one-way rotation <20°) are the cross-pol backscatter [σ°(HV)] and the like-to-cross-pol correlation [e.g., ρ(HHHV*)]. For a diverse, but representative, set of natural terrain, the level of distortion across a range of backscatter measures is shown to be acceptable (i.e., minimal) for one-way Faraday rotations of less than 5°, and 3° if the radiometric uncertainty in the HV backscatter is specified to be less than 0.5 dB.}, 
keywords={Faraday effect;backscatter;geophysical signal processing;radar polarimetry;radar theory;remote sensing by radar;synthetic aperture radar;terrain mapping;Faraday rotation detection;HV backscatter;L-band polarimetric SAR data;cross-pol backscatter;data characteristics;distortion level;dual-polarized backscatter measurements;like-to-cross-pol correlation;linearly polarized L-band SAR backscatter signatures;natural terrain;one-way rotation;quadpolarized backscatter measurements;radar polarimetry;radiometric uncertainty;single-polarized backscatter measurements;surface polarization characteristics;synthetic aperture radar;Backscatter;Extraterrestrial measurements;L-band;Layout;Polarimetric synthetic aperture radar;Polarization;Radar detection;Radar scattering;Rotation measurement;Synthetic aperture radar;Faraday rotation;SAR;polarimetry;synthetic aperture radar}, 
doi={10.1109/TGRS.2004.830163}, 
ISSN={0196-2892}, 
month={Aug},}
@INPROCEEDINGS{6999370, 
author={X. Liu and K. Tang and L. Xiao and M. Song and R. Xu}, 
booktitle={2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
title={Nanotoxicity modeling in multidimentional cube}, 
year={2014}, 
pages={25-28}, 
abstract={Nanotoxicity modeling can reveal the relationship between nanomaterial properties and unintended adverse effects. Traditional modeling usually builds a prediction model on the whole dataset. It does not examine the subsets of the data and their quality for model building. In this paper, we introduce a prediction cube approach to nanotoxicity modeling. Prediction cube is a new type of data cube for data exploration and predictive analytics. It can help researchers slice/dice a large amount of nanotoxicity data and measure the quality of different subsets for building prediction models. We constructed a prediction cube using a sample nanotoxicity data on zebrafish. The results show that the prediction cube can help identify useful subsets for building high quality prediction models. And the cube interface facilitates the exploration of data subsets and associated models.}, 
keywords={biology computing;chemical hazards;data analysis;data mining;nanostructured materials;toxicology;user interfaces;zoology;associated model exploration;cube interface;data cube;data exploration;data subset exploration;multidimentional cube;nanomaterial properties;nanotoxicity data dicing;nanotoxicity data slicing;nanotoxicity modeling;prediction cube approach;prediction model quality;predictive analytics;sample nanotoxicity data;subset identification;subset quality measurement;unintended adverse effect;zebrafish;Accuracy;Data mining;Data models;Machine learning algorithms;Materials;Nanobioscience;Predictive models;Nanotoxicity;data cube;data mining;modeling;prediction cube}, 
doi={10.1109/BIBM.2014.6999370}, 
month={Nov},}
@INPROCEEDINGS{7022574, 
author={F. Wang and G. Wang and P. S. Yu}, 
booktitle={2014 IEEE International Conference on Data Mining Workshop}, 
title={Why Checkins: Exploring User Motivation on Location Based Social Networks}, 
year={2014}, 
pages={27-34}, 
abstract={Checkins, the niche service provided by location based social networks (LBSN), bridge users' online activities and offline social lives in a seamless way. Therefore, knowledge discovery on check in data has become an important research direction [1], [2], [3], [4]. However, a fundamental and interesting question about checkins remains unanswered yet. What are people's motivations behind those checkins? We give the first attempt to answer this question. Motivation studies first appear in social psychology in a less quantitative way. For example, the goal-directed behavior (MGB) model [5] uncovers the association between behaviors and motivations. Following a similar rationale, we design a computational model for the mining of user check in motivations from large scale real world data. We assume that the check in motivation has two types: social motivation and individual motivation. Social motivation is the type of check in incentive that stimulates interactions or influences among friends. Individual motivation is another type of check in incentive that aims to explore and share attractive places. Following the structure of the MGB model, we construct user check in motivation prediction model (UCMP) and then formalize the motivation prediction problem as an optimization problem. The idea is minimizing the difference between the estimated behavior and the true behavior to get the predicted motivations. The experiment on this GOWALLA dataset shows not only prediction results, but also very interesting phenomenons about social users and social locations.}, 
keywords={data mining;mobile computing;optimisation;social networking (online);GOWALLA dataset;LBSN;MGB;UCMP;checkin data;checkin incentive;goal-directed behavior model;knowledge discovery;location based social networks;motivation prediction problem;niche service;offline social lives;optimization problem;social locations;social users;user checkin motivation prediction model;user checkin motivations;user online activities;Attitude control;Computational modeling;Optimization;Prediction algorithms;Predictive models;Psychology;Social network services;location-based social networks;user behavior}, 
doi={10.1109/ICDMW.2014.175}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{7105624, 
author={T. Zhu and J. Yan and Y. Zhao}, 
booktitle={Proceedings of 2nd International Conference on Information Technology and Electronic Commerce}, 
title={What special about Top-N recommendation for mobile app stores}, 
year={2014}, 
pages={306-310}, 
abstract={As the number of mobile applications grows rapidly, personalized recommendation for mobile apps becomes more and more important to mobile App stores. While most of the current works focused on proposing novel recommendation models for mobile apps, the lack of understanding of user download behaviors still remains a problem. Towards this end, we conducted a user behavior study on a large scale real world dataset, focusing on three kinds of biases to user download behaviors, namely, browsing history, app update history, and app categories. The dataset we used in this paper was constructed from Mobile Market (MM), an online mobile app store released by China Mobile, server logs from April 16, 2013 to June 13, 2013, containing 12,125,702 users and 67,577 apps.}, 
keywords={mobile computing;recommender systems;app category;app update history;browsing history;large scale real world dataset;mobile market;online mobile app store;personalized recommendation;top-N recommendation model;user download behaviors;Computational modeling;Entropy;Games;History;Mobile communication;Recommender systems;Servers}, 
doi={10.1109/ICITEC.2014.7105624}, 
month={Dec},}
@INPROCEEDINGS{7474332, 
author={A. Kalaitzis and M. I. Gorinova and Y. Lewenberg and Y. Bachrach and M. Fagan and D. Carignan and N. Gautam}, 
booktitle={2016 IEEE Second International Conference on Big Data Computing Service and Applications (BigDataService)}, 
title={Predicting Gaming Related Properties from Twitter Profiles}, 
year={2016}, 
pages={28-35}, 
abstract={We present a system for predicting gaming-related properties from Twitter profiles. Our system predicts various traits of users based on the tweets publicly available on their profiles. Such inferred traits include degrees of tech-savviness, knowledge on computer games, actual gaming performance, preferred platform, degree of originality, humor and influence on others. Our approach is based on machine learning models trained on crowd-sourced data. Our system enables people to select Twitter profiles of their fellow gamers, examine the trait predictions made by our system, and the main drivers of these predictions. We present empirical results on the performance of our system based on its accuracy on our crowd-sourced dataset. Ultimately, we are motivated by the automated discovery of influential gamers in social media, and its potential for streamlining product campaigns.}, 
keywords={advertising;computer games;learning (artificial intelligence);social networking (online);Twitter profile;actual gaming performance;computer games;crowd-sourced dataset;degree of originality;degree of tech-savviness;humor;machine learning;product campaign;social media;trait prediction;Correlation;Feature extraction;Games;Media;Predictive models;Reliability;Twitter;Social network analytics;Twitter;gamers}, 
doi={10.1109/BigDataService.2016.24}, 
month={March},}
@INPROCEEDINGS{7033096, 
author={M. Zhu and W. Xiong and Y. F. B. Wu}, 
booktitle={2014 13th International Conference on Machine Learning and Applications}, 
title={Learning to Rank with Only Positive Examples}, 
year={2014}, 
pages={87-92}, 
abstract={Search By Multiple Examples (SBME) is a new search paradigm that allows users to specify their information needs as a set of relevant documents rather than as a set of keywords. In this study, we propose a Transductive Positive Unlabeled learning (TPU learning) based framework for SBME. The framework consists of two steps: 1) identifying potential relevant documents for searching space reduction, and 2) adopting TPU learning methods to re-rank the documents in the new searching space. Using MAP and p@k, we evaluate two state-of-the-art PU learning algorithms and the Rocchio classifier (Rc) for document ranking in the proposed framework. We then adopt the idea of ensemble learning to combine Rc with the two state-of-the-art PU learning algorithms respectively. Experiments conducted on a benchmark dataset show that the ensemble learning based methods lead to a significant improvement in effectiveness.}, 
keywords={document handling;information needs;learning (artificial intelligence);pattern classification;MAP;PU learning algorithms;Rocchio classifier;SBME;TPU learning methods;document ranking;ensemble learning;information needs;p@k;relevant documents;search by multiple examples;search paradigm;searching space reduction;transductive positive unlabeled learning based framework;Classification algorithms;Data collection;Prediction algorithms;Support vector machine classification;Text categorization;Training data;Information Need Modeling;Positive Unlabeled Learning;Search by Multiple Examples;Transductive Learning}, 
doi={10.1109/ICMLA.2014.19}, 
month={Dec},}
@INPROCEEDINGS{7275635, 
author={A. Gautam and P. Bedi}, 
booktitle={2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
title={MR-VSM: Map Reduce based vector Space Model for user profiling-an empirical study on News data}, 
year={2015}, 
pages={355-360}, 
abstract={Velocity of data generation has increased over a period of decade which is expected to further increase exponentially with the passage of time. To mine the useful nuggets of information, satisfying a large community of users it is preferred to capture the interest of the user, i.e., to create a user profile, and then filter the content according to his taste. A user may traverse through a large number of documents, requiring a user profiling technique to support the scalability of growing number of documents. This paper proposes a novel technique of user profiling - Map Reduce based Vector Space Model (MR-VSM). MR-VSM is a technique for user profiling where the user interacts with data rich in text and volume. MR-VSM implements traditional VSM to use Map Reduce, a parallel programming paradigm to increase the computational efficiency and support scalability of documents. It works by parallelizing the task of creating a term-document class of VSM by using TF-IDF to create term vector. For experimental study this paper makes use of the News dataset which is rich in text and volume and is collected from the web using RSS feeds. The proposed system creates user profile by taking into consideration the News item read by the user and creating a term vector for each news item read. Resulting user profile is set of Top-n terms. To test the computational efficiency and scalability of MR-VSM for growing number of news items read by user, MR-VSM is made to run on a cluster of Hadoop for 12,000, 24,000 and 48000 news items. VSM is also run for 1,500 news items to show the computational efficiency of the proposed approach. It is observed that for MR-VSM computational time for user profiling and scalability of news item read by the user are improved with the increase in the number of nodes in a Hadoop cluster.}, 
keywords={document handling;information resources;information retrieval;parallel programming;pattern clustering;Hadoop cluster;MR-VSM;Mapreduce based vector space model;RSS feeds;TF-IDF;computational efficiency;data generation;documents scalability;information retrieval;news dataset;news item;parallel programming;term vector;term-document class;top-n terms;user profiling;Computational efficiency;Computational modeling;Databases;Feeds;Filtering;Informatics;Scalability;Hadoop;Information Retrieval;Map Reduce;User Profiling;Vector Space Model}, 
doi={10.1109/ICACCI.2015.7275635}, 
month={Aug},}
@INPROCEEDINGS{7064450, 
author={N. D. Thai and T. S. Le and N. Thoai and K. Hamamoto}, 
booktitle={2014 13th International Conference on Control Automation Robotics Vision (ICARCV)}, 
title={Learning bag of visual words for motorbike detection}, 
year={2014}, 
pages={1045-1050}, 
abstract={Recent growth of traffic surveillance based on computer vision techniques has caught more and more attentions from researchers. Since the detection of vehicles is the primary step of such system, there is a large body of works towards developing an efficient detection scheme on various operating conditions. However, those works mainly focus on the detection of car and pedestrian. In this paper, we shift our attention to motorbike, which is also a common road user, especially in developing country. In comparison to other target objects, motorbike is rather small in size but has more complex structure. Thus, detecting motorbike is not a trivial task in real-life context where there is variance due to presence of motorbike drivers and high degree of occlusions. To address this problem, we propose a method for detecting motorbike from the scenes. Our method can achieve robustness to changes in illuminations, affine transformations and occlusions. Firstly, local features are extracted from images which contain one single target object. These local features represent parts of objects and are used to construct a Bag-of-Visual Words model. Using this model, each object is represented as a histogram of their parts. Next, a Support Vector Machine classifier is trained with these representations for classifying motorbike and non-motorbike objects. Finally, we develop an algorithm to form a detection hypothesis to detect multiple target objects from the scene. We collect a dataset of 3000 images for evaluating our proposed method. The experimental results indicate that our method can achieve high accuracy in the context of real-life motorbike detection applications.}, 
keywords={affine transforms;computer vision;feature extraction;image classification;image representation;motorcycles;object detection;object recognition;support vector machines;surveillance;traffic engineering computing;affine transformations;bag-of-visual words learning;computer vision techniques;detection hypothesis;illuminations;local feature extraction;motorbike classification;multiple target object detection;object representation;occlusions;real-life motorbike detection;support vector machine classifier;traffic surveillance;vehicle detection;Accuracy;Feature extraction;Lighting;Motorcycles;Support vector machines;Training;bag of visual words;local features;vehicle detection and classification}, 
doi={10.1109/ICARCV.2014.7064450}, 
month={Dec},}
@INPROCEEDINGS{6246142, 
author={C. Amma and M. Georgi and T. Schultz}, 
booktitle={2012 16th International Symposium on Wearable Computers}, 
title={Airwriting: Hands-Free Mobile Text Input by Spotting and Continuous Recognition of 3d-Space Handwriting with Inertial Sensors}, 
year={2012}, 
pages={52-59}, 
abstract={We present an input method which enables complex hands-free interaction through 3d handwriting recognition. Users can write text in the air as if they were using an imaginary blackboard. Motion sensing is done wirelessly by accelerometers and gyroscopes which are attached to the back of the hand. We propose a two-stage approach for spotting and recognition of handwriting gestures. The spotting stage uses a Support Vector Machine to identify data segments which contain handwriting. The recognition stage uses Hidden Markov Models (HMM) to generate the text representation from the motion sensor data. Individual characters are modeled by HMMs and concatenated to word models. Our system can continuously recognize arbitrary sentences, based on a freely definable vocabulary with over 8000 words. A statistical language model is used to enhance recognition performance and restrict the search space. We report the results from a nine-user experiment on sentence recognition for person dependent and person independent setups on 3d-space handwriting data. For the person independent setup, a word error rate of 11% is achieved, for the person dependent setup 3% are achieved. We evaluate the spotting algorithm in a second experiment on a realistic dataset including everyday activities and achieve a sample based recall of 99% and a precision of 25%. We show that additional filtering in the recognition stage can detect up to 99% of the false positive segments.}, 
keywords={accelerometers;data gloves;filtering theory;gesture recognition;gyroscopes;handwriting recognition;hidden Markov models;image sensors;search problems;support vector machines;text detection;3D handwriting recognition;3D-space handwriting data;HMM;accelerometers;airwriting;arbitrary sentences;complex hands-free interaction;continuous recognition;data gloves;data segments;false positive segments;freely definable vocabulary;gyroscopes;hands-free mobile text input;handwriting gestures recognition;handwriting gestures spotting;hidden Markov models;imaginary blackboard;individual characters;inertial sensors;motion sensing;motion sensor data;person dependent setups;person independent setups;recognition performance;recognition stage filtering;search space;sentence recognition;spotting algorithm;spotting recognition;spotting stage;statistical language model;support vector machine;text representation;word error rate;word models;Handwriting recognition;Hidden Markov models;Motion segmentation;Sensors;Support vector machines;Text recognition;Vocabulary;Accelerometers;Handwriting recognition;User interfaces;Wearable computers}, 
doi={10.1109/ISWC.2012.21}, 
ISSN={1550-4816}, 
month={June},}
@INPROCEEDINGS{7814067, 
author={A. Santoro and A. Parziale and A. Marcelli}, 
booktitle={2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR)}, 
title={A Human in the Loop Approach to Historical Handwritten Documents Transcription}, 
year={2016}, 
pages={222-227}, 
abstract={We propose a novel approach for helping content transcription of handwritten digital documents. The approach adopts a segmentation based keyword retrieval approach that follows query-by-string paradigm and exploits the user validation of the retrieved words to improve its performance during operation. Our approach starts with an initial training set, which contains only a few pages and a tentative list of words supposedly in the document, and iteratively interleaves a word retrieval step by the system with a validation step by the user. After each iteration, the system exploits the results of the validation to update its internal model, so as to use that evidence in further iterations of the search. Experimental results on the Bentham dataset show that the system may start with a few word images and their transcripts, exhibits an improvement of the performance during operation, and after a few iterations is able to correctly transcribe more than 68% of the word of the list.}, 
keywords={document image processing;handwritten character recognition;image retrieval;image segmentation;Bentham dataset;content transcription;historical handwritten digital document transcription;initial training set;internal model;query-by-string paradigm;segmentation based keyword retrieval;validation step;word retrieval step;Handwriting recognition;Hidden Markov models;Image segmentation;Ink;Knowledge based systems;Training;Writing;Historical handwritten documents;human in the loop;word retrieval}, 
doi={10.1109/ICFHR.2016.0051}, 
ISSN={2167-6445}, 
month={Oct},}
@INPROCEEDINGS{7373391, 
author={S. Kataria and A. Agarwal}, 
booktitle={2015 IEEE International Conference on Data Mining}, 
title={Supervised Topic Models for Microblog Classification}, 
year={2015}, 
pages={793-798}, 
abstract={In this paper we present a topic model based approach for classifying micro-blog posts into a given topics of interests. The short nature of micro-blog posts make them challenging for directly learning a classification model. To overcome this limitation, we use content of the links embedded in these posts to improve the topic learning. The hypothesis is that since the link content is far richer than the content of the post itself, using link content along with the content of the post will help learning. However, how this link content can be used to construct features for classification remains a challenging issue. Furthermore, in previous methods, user based information is utilized in an ad-hoc manner that only work for certain type of classification, such as characterizing content of microblogs. In this paper, we propose supervised topic model, User-Labeled-LDA and its nonparametric variant that can avoid the ad-hoc feature construction task and model the topics in a discriminative way. Our experiments on a Twitter dataset shows that modeling user interests and link information helps in learning quality topics for sparse tweets as well as helps significantly in classification task. Our experiments further show that modeling this information in a principled way through topic models helps more than simply adding this information through features.}, 
keywords={pattern classification;social networking (online);Twitter;latent Dirichlet allocation;microblog classification;supervised topic model;topic classification;tweets;user-labeled-LDA;Blogs;Data mining;Data models;Electronic publishing;Encyclopedias;Internet}, 
doi={10.1109/ICDM.2015.148}, 
ISSN={1550-4786}, 
month={Nov},}
@INPROCEEDINGS{7457108, 
author={Y. Jagadeesan and P. Hu and C. R. Rivero}, 
booktitle={2016 IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops)}, 
title={PLOMaR: An ontology framework for context modeling and reasoning on crowd-sensing platform}, 
year={2016}, 
pages={1-6}, 
abstract={Crowd-sensing is a popular way to sense and collect data using smartphones that reveals user behaviors and their correlations with device performance. PhoneLab is one of the largest crowd-sensing platform based on the Android system. Through experimental instrumentations and system modifications, researchers can tap into a sea of insightful information that can be further processed to reveal valuable context information about the device, user and the environment. However, the PhoneLab data is in JSON format. The process of inferring reasons from data in this format is not straightforward. In this paper, we introduce PLOMaR - an ontology framework that uses SPARQL rules to help researchers access information and derive new information without complex data processing. The goals are to (i) make the measurement data more accessible, (ii) increase interoperability and reusability of data gathered from different sources, (iii) develop extensible data representation to support future development of the PhoneLab platform. We describe the models, the JSON to RDF mapping processes, and the SPARQL rules used for deriving new information. We evaluate our framework with three application examples based on the sample dataset provided.}, 
keywords={inference mechanisms;ontologies (artificial intelligence);open systems;smart phones;Android system;JSON format;PLOMaR;PhoneLab;RDF mapping processes;SPARQL rules;context modeling;crowd-sensing platform;data interoperability;data representation;data reusability;device performance;experimental instrumentations;ontology framework;reasoning;smartphones;system modifications;Batteries;Context;Context modeling;Data models;Ontologies;Smart phones;Temperature measurement}, 
doi={10.1109/PERCOMW.2016.7457108}, 
month={March},}
@INPROCEEDINGS{7195573, 
author={X. Fan and Y. Hu and R. Zhang and W. Chen and P. Brézillon and X. Fan}, 
booktitle={2015 IEEE International Conference on Web Services}, 
title={Modeling Temporal Effectiveness for Context-Aware Web Services Recommendation}, 
year={2015}, 
pages={225-232}, 
abstract={Context-Aware Recommender System (CARS) aims to not only recommend services similar to those already rated with the highest score, but also provide opportunities for exploring the important role of temporal, spatial and social contexts for personalized web services recommendation. A key step for temporal-based CARS methods is to explore the time decay process of past invocation records to make the Quality of Services (QoS) prediction. However, it is a nontrivial task to model the temporal effects on web services recommendation, due to the dynamic features of contextual information in view of temporal spatial correlations. For instance, in location-aware services recommendation, the user's geographical position would change very frequently as time goes on. In this paper, we propose a Context-Aware Services Recommendation based on Temporal Effectiveness (CASR-TE) method. Inspired by existing time decay approaches, we first present an enhanced temporal decay model combining the time decay function with traditional similarity measurement methods. Then, we model temporal spatial correlations as well as their impacts on the user preference expansion. Finally, we evaluate the CASR-TE method on WS-Dream dataset by evaluation matrices of both RMSE and MAE. Experimental results show that our approach outperforms several benchmark methods with a significant margin.}, 
keywords={Web services;mean square error methods;mobile computing;quality of service;recommender systems;CASR-TE method;MAE;QoS prediction;RMSE;WS-Dream dataset;context-aware recommender system;context-aware services recommendation based on temporal effectiveness method;enhanced temporal decay model;location-aware services recommendation;personalized Web service recommendation;quality of services prediction;similarity measurement methods;social context;spatial context;temporal context;temporal effectiveness modeling;temporal spatial correlations;temporal-based CARS methods;time decay process;user geographical position;user preference expansion;Context;Correlation;Quality of service;Recommender systems;Weather forecasting;Web services;QoS;context awareness;recommender system;temporal effectiveness;web services}, 
doi={10.1109/ICWS.2015.39}, 
month={June},}
@INPROCEEDINGS{7838204, 
author={S. Banitaan and M. Azzeh and A. B. Nassif}, 
booktitle={2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
title={User Movement Prediction: The Contribution of Machine Learning Techniques}, 
year={2016}, 
pages={571-575}, 
abstract={Ambient Assisted Living (AAL) aims to increase the time older people or disabled people can live in their home environment by assisting them in performing activities of daily living by the use of intelligent products. Localization and tracking of users in indoor environment are the main components of AAL. Wireless sensor networks is an effective technology to accomplish these services by using Received Signal Strength (RSS) information. This work seeks to investigate the effect of machine learning techniques on the accuracy of user movement prediction. Five base classifiers and two ensemble learning approaches are employed and the results are evaluated in terms of precision recall, and F-measure. A real-life benchmark dataset in the area of AAL is used for evaluation. The results show that J48 is the best performing model compared to the other base-level classifiers. It also shows that Bagged J48 achieves the best performance.}, 
keywords={RSSI;handicapped aids;learning (artificial intelligence);wireless sensor networks;AAL;RSS information;ambient assisted living;disabled people;home environment;indoor environment;intelligent products;machine learning techniques;movement prediction;real-life benchmark dataset;received signal strength;user movement prediction;wireless sensor networks;Bagging;Benchmark testing;Boosting;Radar tracking;Tracking;Training;Wireless sensor networks;Bagging;Boosting;Classification;Ensemble Learning;Indoor User Movement;Machine Learning}, 
doi={10.1109/ICMLA.2016.0100}, 
month={Dec},}
@INPROCEEDINGS{7532541, 
author={A. Bushnevskiy and L. Sorgi and B. Rosenhahn}, 
booktitle={2016 IEEE International Conference on Image Processing (ICIP)}, 
title={Multimode camera calibration}, 
year={2016}, 
pages={1165-1169}, 
abstract={Camera calibration denotes the task of estimating the projective mapping between 3D world and the camera image plane. Most of the modern cameras have multiple image and video acquisition modes, which differ in image resolution, field of view or aspect ratio. Each mode should be treated as an independent device, and therefore independently calibrated. This straightforward solution implies the acquisition of the calibration dataset and execution of the calibration routine for each of the camera modes, a time-consuming and error-prone task, especially in a multi-camera scenario. In this paper we propose a multimode camera calibration approach that noticeably relieves this task. We show how the calibration model of one camera mode can be easily transferred to the other modes without the need of multiple calibrations. The evaluation test performed on off-the-shelf consumer cameras shows that the proposed method not only reduces the required amount of user interaction, but also allows for the calibration accuracy improvement.}, 
keywords={calibration;cameras;image resolution;3D world;aspect ratio;calibration dataset acquisition;calibration routine execution;camera image plane;field of view;image resolution;multicamera scenario;multimode camera calibration;projective mapping estimation;Calibration;Cameras;Computational modeling;Estimation;Image resolution;Lenses;Three-dimensional displays;Camera calibration;calibration transfer;multicamera calibration;multimode calibration}, 
doi={10.1109/ICIP.2016.7532541}, 
month={Sept},}
@INPROCEEDINGS{6529573, 
author={P. Cottone and G. Lo Re and G. Maida and M. Morana}, 
booktitle={2013 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)}, 
title={Motion sensors for activity recognition in an ambient-intelligence scenario}, 
year={2013}, 
pages={646-651}, 
abstract={In recent years, Ambient Intelligence (AmI) has attracted a number of researchers due to the widespread diffusion of unobtrusive sensing devices. The availability of such a great amount of acquired data has driven the interest of the scientific community in producing novel methods for combining raw measurements in order to understand what is happening in the monitored scenario. Moreover, due the primary role of the end user, an additional requirement of any AmI system is to maintain a high level of pervasiveness. In this paper we propose a method for recognizing human activities by means of a time of flight (ToF) depth and RGB camera device, namely Microsoft Kinect. The proposed approach is based on the estimation of some relevant joints of the human body by using Kinect depth information. The most significative configurations of joints positions are combined by a clustering approach and classified by means of a multi-class Support Vector Machine. Then, Hidden Markov Models (HMMs) are applied to model each activity as a sequence of known postures. The proposed solution has been tested on a public dataset while considering four different configurations corresponding to some state-of-the-art approaches and results are very promising. Moreover, in order to maintain a high level of pervasiveness, we implemented a real prototype by connecting Kinect sensor to a miniature computer capable of real-time processing.}, 
keywords={ambient intelligence;behavioural sciences;cameras;hidden Markov models;pattern clustering;pattern recognition;support vector machines;wireless sensor networks;AmI system;HMM;Kinect sensor;Microsoft Kinect depth information;RGB camera device;ToF depth;ambient-intelligence scenario;clustering approach;hidden Markov models;human activity recognition;human body joint estimation;joint position configurations;motion sensors;multiclass support vector machine;pervasiveness level;public dataset;time-of-flight depth;unobtrusive sensing devices;Accuracy;Hidden Markov models;Joints;Principal component analysis;Sensors;Support vector machines;Vocabulary;Activity Recognition;Ambient Intelligence}, 
doi={10.1109/PerComW.2013.6529573}, 
month={March},}
@INPROCEEDINGS{6346874, 
author={M. Arvaneh and C. Guan and K. K. Ang and C. Quek}, 
booktitle={2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 
title={Omitting the intra-session calibration in EEG-based brain computer interface used for stroke rehabilitation}, 
year={2012}, 
pages={4124-4127}, 
abstract={Brain-computer interface (BCI) as a rehabilitation tool has been used in restoring motor functions in patients with moderate to sever stroke impairments. To achieve the best possible outcome in such an application, it is highly desirable to have a stable and accurate operation of BCI. However, since electroencephalogram (EEG) signals considerably vary between sessions of even the same user, typically a long calibration session is recorded at the beginning of each session. This process is time-consuming and inconvenient for stroke patients who undergo long-term BCI sessions with repeating same mental tasks. This paper investigates the possibility of omitting the intra-session calibration for BCI-based stroke rehabilitation when large data recorded from the same user are available. For this purpose, a large dataset of EEG signals from 11 stroke patients performing 12 BCI-based stroke rehabilitation sessions over one month is used. Our offline results suggest that after recording a number of stroke rehabilitation sessions, the patient does not require calibration any more. The experimental results show that combining 11 sessions, which each session comprises minimum 60 trials per class, yields a model that averagely outperforms the standard calibration model trained by the data recorded directly before the test session.}, 
keywords={brain-computer interfaces;electroencephalography;medical signal processing;patient rehabilitation;BCI;EEG signals;brain computer interface;electroencephalogram;intrasession calibration;motor function restoration;rehabilitation tool;sever stroke impairment;stroke patient rehabilitation;Accuracy;Brain computer interfaces;Brain modeling;Calibration;Data models;Electroencephalography;Reliability;Adult;Brain Mapping;Brain-Computer Interfaces;Electroencephalography;Humans;Middle Aged;Motor Cortex;Movement;Paresis;Reproducibility of Results;Sensitivity and Specificity;Stroke}, 
doi={10.1109/EMBC.2012.6346874}, 
ISSN={1094-687X}, 
month={Aug},}
@INPROCEEDINGS{7023970, 
author={T. Komamizu and T. Amagasa and H. Kitagawa}, 
booktitle={2014 17th International Conference on Network-Based Information Systems}, 
title={Frequent-Pattern Based Facet Extraction from Graph Data}, 
year={2014}, 
pages={318-323}, 
abstract={Graph is a general data model which is applicable to complex data structures. Hence, there are lots of data which can be expressed using graph data models. For example, Web, social networking services, bibliographic database, and Linked Open Data. Since such graph data can be heterogeneous, users are imposed a huge burden on searching over the graph data to find desired sub graphs. Faceted search is a promising approach to reduce the burden of searching graph data. Applying faceted search for graph data requires to determine objects (target sub graphs) and facets. To achieve this, in this paper, we propose a framework for faceted search over graph data. The framework is organized into two phases, namely, extraction phase and search phase. The main objective of this paper is to develop the extraction phase which has two main tasks, one is to extract target sub graphs, and the other is to extract facets. This paper applies frequent sub graph mining techniques to extract target sub graphs and facets. The proposed framework is experimentally evaluated using publicly available graph datasets, namely, citation network data and review network data, which show the proposed framework works as expected.}, 
keywords={data mining;data models;graph theory;search problems;World Wide Web;bibliographic database;citation network data;complex data structure;extraction phase;faceted search;frequent sub graph mining technique;frequent-pattern based facet extraction;graph data model;graph dataset;linked open data;social networking services;Data mining;Indexes;Keyword search;Motion pictures;Search problems;XML;faceted search;frequent-pattern;graph data}, 
doi={10.1109/NBiS.2014.77}, 
ISSN={2157-0418}, 
month={Sept},}
@INPROCEEDINGS{7377693, 
author={H. M. K. G. Bandara and H. A. Caldera}, 
booktitle={2015 Fifteenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
title={Towards optimising Wi-Fi energy consumption in mobile phones: A data driven approach}, 
year={2015}, 
pages={226-235}, 
abstract={Contemporary mobile devices are equipped with multiple network interfaces with diverse characteristics. Although the Wi-Fi interface bestows commendable throughput and data transfer efficiency, it is least power efficient in the idle state and causes highest energy overhead when scanning for networks. In this paper we present a data driven approach to alleviate this issue, focusing on Wi-Fi usage by the user's perspective. We model the Wi-Fi usage of mobile users based on their past usage to predict usage requirements. This allows intelligently switching on the Wi-Fi interface only if the user context demands. Thus, it reduces long periods of time being in the idle state and significantly lessens the number of futile network scans. Based on the trace data collected from Rice-Livelab study, we extract temporal, application usage, operational state and location context data to build our prediction model. This study includes a systematic feature engineering process followed by the deployment of machine learning algorithms on the target dataset. We used Sampling, Ensemble and Hybrid techniques to mitigate the class imbalance problem of our prediction model. Evaluated metrics indicate that decision tree based classification algorithms perform well with the dataset and suit for working with mobile usage data, which are mostly conflated with noise, data imbalance.}, 
keywords={decision trees;learning (artificial intelligence);mobile computing;mobile handsets;pattern classification;power aware computing;sampling methods;wireless LAN;Rice-Livelab study;Wi-Fi energy consumption optimisation;Wi-Fi interface;Wi-Fi usage;contemporary mobile devices;data driven approach;data imbalance;data transfer efficiency;decision tree based classification algorithms;energy overhead;ensemble techniques;hybrid techniques;location context data;machine learning algorithms;mobile phones;mobile users;multiple network interfaces;operational state;sampling techniques;systematic feature engineering process;usage requirements;user context;Context;IEEE 802.11 Standard;Context-awareness;Mobile usage data pre-processing;Mobile usage prediction;Wi-Fi energy dissemination}, 
doi={10.1109/ICTER.2015.7377693}, 
month={Aug},}
@INPROCEEDINGS{4344949, 
author={Z. Xiaofang and L. Jin and L. Qi}, 
booktitle={Third International Conference on Natural Computation (ICNC 2007)}, 
title={Personalized Information Recommendation Service System Based on GIS}, 
year={2007}, 
volume={5}, 
pages={805-808}, 
abstract={To realize personalized location service, personalized information recommendation service system based on GIS is proposed and realized. The structure of system, workflow and key technologies of realizing feature selection module, user interest module, personalized maps resources filtering module are introduced in the paper. After the test use of the system by some certain users, we found that the system could improve the learners' initiative participation in location based service. The paper serves as a case study tries to expose the potential of wireless technology to serve LBSs. The future study focuses on an object-oriented database system capable for large spatial dataset.}, 
keywords={geographic information systems;information filters;object-oriented databases;personal computing;GIS;feature selection module;object-oriented database system;personalized information recommendation service system;personalized location service;personalized maps resources filtering module;user interest module;Database systems;Filtering;Geographic Information Systems;Geology;Information systems;Object oriented modeling;Paper technology;Position measurement;System testing;Wireless networks}, 
doi={10.1109/ICNC.2007.552}, 
ISSN={2157-9555}, 
month={Aug},}
@ARTICLE{6289356, 
author={M. A. Pathak and B. Raj}, 
journal={IEEE Transactions on Audio, Speech, and Language Processing}, 
title={Privacy-Preserving Speaker Verification and Identification Using Gaussian Mixture Models}, 
year={2013}, 
volume={21}, 
number={2}, 
pages={397-406}, 
abstract={Speech being a unique characteristic of an individual is widely used in speaker verification and speaker identification tasks in applications such as authentication and surveillance respectively. In this article, we present frameworks for privacy-preserving speaker verification and speaker identification systems, where the system is able to perform the necessary operations without being able to observe the speech input provided by the user. In a speech-based authentication setting, this privacy constraint protect against an adversary who can break into the system and use the speech models to impersonate legitimate users. In surveillance applications, we require the system to first identify if the speech recording belongs to a suspect while preserving the privacy constraints. This prevents the system from listening in on conversations of innocent individuals. In this paper we formalize the privacy criteria for the speaker verification and speaker identification problems and construct Gaussian mixture model-based protocols. We also report experiments with a prototype implementation of the protocols on a standardized dataset for execution time and accuracy.}, 
keywords={Gaussian processes;speaker recognition;surveillance;Gaussian mixture model-based protocol;authentication application;privacy constraint;privacy-preserving speaker identification;privacy-preserving speaker verification;speech-based authentication setting;surveillance application;Adaptation models;Authentication;Computational modeling;Privacy;Speech;Speech processing;Surveillance;Secure multiparty computation;speaker identification;speaker verification}, 
doi={10.1109/TASL.2012.2215602}, 
ISSN={1558-7916}, 
month={Feb},}
@INPROCEEDINGS{7181430, 
author={R. Minelli and A. Mocci and M. Lanza}, 
booktitle={2015 IEEE 23rd International Conference on Program Comprehension}, 
title={I Know What You Did Last Summer - An Investigation of How Developers Spend Their Time}, 
year={2015}, 
pages={25-35}, 
abstract={Developing software is a complex mental activity, requiring extensive technical knowledge and abstraction capabilities. The tangible part of development is the use of tools to read, inspect, edit, and manipulate source code, usually through an IDE (integrated development environment). Common claims about software development include that program comprehension takes up half of the time of a developer, or that certain UI (user interface) paradigms of IDEs offer insufficient support to developers. Such claims are often based on anecdotal evidence, throwing up the question of whether they can be corroborated on more solid grounds. We present an in-depth analysis of how developers spend their time, based on a fine-grained IDE interaction dataset consisting of ca. 740 development sessions by 18 developers, amounting to 200 hours of development time and 5 million of IDE events. We propose an inference model of development activities to precisely measure the time spent in editing, navigating and searching for artifacts, interacting with the UI of the IDE, and performing corollary activities, such as inspection and debugging. We report several interesting findings which in part confirm and reinforce some common claims, but also disconfirm other beliefs about software development.}, 
keywords={software engineering;IDE;fine-grained IDE interaction dataset;software development;user interface;Browsers;History;Inspection;Keyboards;Mice;Navigation;Software;empirical study;interaction data;program understanding;user interface}, 
doi={10.1109/ICPC.2015.12}, 
ISSN={1092-8138}, 
month={May},}
@INPROCEEDINGS{7020819, 
author={A. Socievole and F. De Rango and A. Caputo}, 
booktitle={2014 IFIP Wireless Days (WD)}, 
title={Wireless contacts, Facebook friendships and interests: Analysis of a multi-layer social network in an academic environment}, 
year={2014}, 
pages={1-7}, 
abstract={Human mobility traces have drawn increasing attention in recent years due to their usefulness for constructing mobility models and evaluating mobile opportunistic communication systems. Even if human mobility provides us insights into the social behavior of mobile users, there is a growing awareness that human sociality is expressed simultaneously on multiple layers. The multilayered complex network composed by the social network constructed on wireless contacts and other types of social network layers needs still to be analyzed and understood in depth. In this paper, we describe the experiment we performed in a campus environment to trace the wireless contacts in terms of Bluetooth encounters, occurring between nodes inside and outside the group of experimenters carrying smartphones, and to gather the profiles, Facebook friendships, and interests of the participants. By analyzing the multilayer social network constructed on this dataset, we contribute to novel understanding of human behavior at different social dimensions. We study the relationship between offline encounters detected through mobile devices, Facebook friendships and shared interests in terms of closeness between the corresponding social graphs, matching between strong offline ties and the other social ties, and similarity between communities. We show that Bluetooth contacts network layer and Facebook friendships network layer are similar.}, 
keywords={information analysis;smart phones;social networking (online);Bluetooth encounters;Facebook friendships;Facebook interests;academic environment;human mobility traces;human sociality;mobile opportunistic communication systems;mobile user social behavior;mobility models;multilayer social network analysis;multilayered complex network;smart phones;social graph;wireless contacts;Analytical models;Bluetooth;Communities;Educational institutions;Facebook;Wireless communication;Bluetooth contacts;Facebook;Multi-layer network;mobility trace analytics;social network}, 
doi={10.1109/WD.2014.7020819}, 
ISSN={2156-9711}, 
month={Nov},}
@INPROCEEDINGS{964524, 
author={M. Gavriliu and J. Carranza and D. E. Breen and A. H. Barr}, 
booktitle={Visualization, 2001. VIS '01. Proceedings}, 
title={Fast extraction of adaptive multiresolution meshes with guaranteed properties from volumetric data}, 
year={2001}, 
pages={295-565}, 
abstract={We present a new algorithm for extracting adaptive multiresolution triangle meshes from volume datasets. The algorithm guarantees that the topological genus of the generated mesh is the same as the genus of the surface embedded in the volume dataset at all levels of detail. In addition to this "hard constraint" on the genus of the mesh, the user can choose to specify some number of soft geometric constraints, such as triangle aspect ratio, minimum or maximum total number of vertices, minimum and/or maximum triangle edge lengths, maximum magnitude of various error metrics per triangle or vertex, including maximum curvature (area) error, maximum distance to the surface, and others. The mesh extraction process is fully automatic and does not require manual adjusting of parameters to produce the desired results as long as the user does not specify incompatible constraints. The algorithm robustly handles special topological cases, such as trimmed surfaces (intersections of the surface with the volume boundary), and manifolds with multiple disconnected components (several closed surfaces embedded in the same volume dataset). The meshes may self-intersect at coarse resolutions. However, the self-intersections are corrected automatically as the resolution of the meshes increase. We show several examples of meshes extracted from complex volume datasets.}, 
keywords={data visualisation;interpolation;rendering (computer graphics);adaptive multiresolution meshes extraction;guaranteed properties;maximum curvature trimmed surfaces;volume datasets;volumetric data;Application software;Biomedical imaging;Computational modeling;Computer science;Data mining;Data visualization;Fluid dynamics;Medical simulation;Mesh generation;Robustness}, 
doi={10.1109/VISUAL.2001.964524}, 
month={Oct},}
@INPROCEEDINGS{7023421, 
author={G. Liu and Y. Fu and T. Xu and H. Xiong and G. Chen}, 
booktitle={2014 IEEE International Conference on Data Mining}, 
title={Discovering Temporal Retweeting Patterns for Social Media Marketing Campaigns}, 
year={2014}, 
pages={905-910}, 
abstract={Social media has become one of the most popular marketing channels for many companies, which aims at maximizing their influence by various marketing campaigns conducted from their official accounts on social networks. However, most of these marketing accounts merely focus on the contents of their tweets. Less effort has been made on understanding tweeting time, which is a major contributing factor in terms of attracting customers' attention and maximizing the influence of a social marketing campaign. To that end, in this paper, we provide a focused study of temporal retweeting patterns and their influence on social media marketing campaigns. Specifically, we investigate the users' retweeting patterns by modeling their retweeting behaviors as a generative process, which considers temporal, social, and topical factors. Moreover, we validate the predictive power of the model on the dataset collected from Sina Weibo, the most popular micro blog platform in China. By discovering the temporal retweeting patterns, we analyze the temporal popular topics and recommend tweets to users in a time-aware manner. Finally, experimental results show that the proposed algorithm outperforms other baseline methods. This model is applicable for companies to conduct their marketing campaigns at the right time on social media.}, 
keywords={marketing data processing;social networking (online);China;Sina Weibo;generative process;microblog platform;retweeting behaviors;social factor;social media marketing campaigns;temporal factor;temporal retweeting patterns;topical factor;Companies;Context;Context modeling;Educational institutions;History;Media;Predictive models}, 
doi={10.1109/ICDM.2014.48}, 
ISSN={1550-4786}, 
month={Dec},}
@ARTICLE{7883939, 
author={X. Liu and X. Lu and H. Li and T. Xie and Q. Mei and H. Mei and F. Feng}, 
journal={IEEE Transactions on Software Engineering}, 
title={Understanding Diverse Usage Patterns from Large-Scale Appstore-Service Profiles}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={The prevalence of smart mobile devices has promoted the popularity of mobile applications (a.k.a. apps). Supporting mobility has become a promising trend in software engineering research. This article presents an empirical study of behavioral service profiles collected from millions of users whose devices are deployed with Wandoujia, a leading Android app store service in China. The dataset of Wandoujia service profiles consists of two kinds of user behavioral data from using 0.28 million free Android apps, including (1) app management activities (i.e., downloading, updating, and uninstalling apps) from over 17 million unique users and (2) app network usage from over 6 million unique users. We explore multiple aspects of such behavioral data and present patterns of app usage. Based on the findings as well as derived knowledge, we also suggest some new open opportunities and challenges that can be explored by the research community, including app development, deployment, delivery, revenue, etc.}, 
keywords={Androids;Biological system modeling;Electronic mail;Humanoid robots;Mobile communication;Software;Software engineering;app store;mobile apps;user behavior analysis}, 
doi={10.1109/TSE.2017.2685387}, 
ISSN={0098-5589}, 
month={},}
@INPROCEEDINGS{5070814, 
author={S. Uppalapati and J. C. Femiani and A. Razdan and K. Gary}, 
booktitle={2009 Sixth International Conference on Information Technology: New Generations}, 
title={3D VQI: 3D Visual Query Interface}, 
year={2009}, 
pages={1347-1354}, 
abstract={The 3D Visual Query Interface (3D VQI) is a Web-based database application that allows for shape-based searches in addition to more traditional text-based queries of the 3D vessel dataset. This interface allows researchers remote access to vessel collection that can be analyzed from anywhere in the world. The interface is divided into two sections with right half supporting the ability to draw profile curve both symmetric and asymmetric. This query can be further refined using the left half which provides contextual based information. Using a curve-matching algorithm the software then searches the database and returns a series of potential matches sorted by the percentage of similarity between the curves. Each match can then be selected for more detailed viewing of the 3D model and data or for further analysis.}, 
keywords={Internet;curve fitting;pattern matching;query processing;user interfaces;3D visual query interface;Web-based database application;curve-matching algorithm;shape-based search;Application software;Computer interfaces;Information technology;Mathematical model;Shape measurement;Software algorithms;Solid modeling;Spatial databases;Three dimensional displays;Visual databases}, 
doi={10.1109/ITNG.2009.331}, 
month={April},}
@ARTICLE{7837622, 
author={Q. Liu and S. Wu and L. Wang}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Multi-Behavioral Sequential Prediction with Recurrent Log-Bilinear Model}, 
year={2017}, 
volume={29}, 
number={6}, 
pages={1254-1267}, 
abstract={With the rapid growth of Internet applications, sequential prediction in collaborative filtering has become an emerging and crucial task. Given the behavioral history of a specific user, predicting his or her next choice plays a key role in improving various online services. Meanwhile, there are more and more scenarios with multiple types of behaviors, while existing works mainly study sequences with a single type of behavior. As a widely used approach, Markov chain based models are based on a strong independence assumption. As two classical neural network methods for modeling sequences, recurrent neural networks cannot well model short-term contexts, and the log-bilinear model is not suitable for long-term contexts. In this paper, we propose a Recurrent Log-BiLinear (RLBL) model. It can model multiple types of behaviors in historical sequences with behavior-specific transition matrices. RLBL applies a recurrent structure for modeling long-term contexts. It models several items in each hidden layer and employs position-specific transition matrices for modeling short-term contexts. Moreover, considering continuous time difference in behavioral history is a key factor for dynamic prediction, we further extend RLBL and replace position-specific transition matrices with time-specific transition matrices, and accordingly propose a Time-Aware Recurrent Log-BiLinear (TA-RLBL) model. Experimental results show that the proposed RLBL model and TA-RLBL model yield significant improvements over the competitive compared methods on three datasets, i.e., Movielens-1M dataset, Global Terrorism Database and Tmall dataset with different numbers of behavior types.}, 
keywords={collaborative filtering;recurrent neural nets;TA-RLBL model;behavior-specific transition matrices;collaborative filtering;continuous time difference;multibehavioral sequential prediction;position-specific transition matrices;recurrent neural network methods;sequential prediction;time-aware recurrent log-bilinear model;time-specific transition;Context;Context modeling;Correlation;History;Markov processes;Predictive models;Recurrent neural networks;Collaborative filtering;multi-behavior;recurrent log-bilinear;sequential prediction}, 
doi={10.1109/TKDE.2017.2661760}, 
ISSN={1041-4347}, 
month={June},}
@INPROCEEDINGS{5716326, 
author={Y. Usui and K. Kondo}, 
booktitle={2010 Second World Congress on Nature and Biologically Inspired Computing (NaBIC)}, 
title={3D object recognition based on confidence LUT of SIFT feature distance}, 
year={2010}, 
pages={293-297}, 
abstract={In this paper, a confidence-based matching method for three dimensional (3D) object recognition is proposed. We are developing a remote control system combined with a camera and image recognition system that can recognize specific objects that a user wants to control. Previously, Scale Invariant Feature Transform (SIFT) feature point-based recognition algorithms have been proposed by numerous researchers. However, it is difficult to apply the conventional recognition methods to remote control systems because home appliances tend to have simple shapes, and thus normally produce very few SIFT feature points. To improve the performance under such low feature count situations, a confidence-based 3D feature point matching method is proposed. This method is a modified Best Bin First (BBF) approach that uses a trained confidence look up table (LUT) for decision making. An evaluation of this method is demonstrated on a dataset of 2,432 images.}, 
keywords={decision making;feature extraction;image matching;object recognition;solid modelling;table lookup;telecontrol;3D feature point matching method;3D object recognition;SIFT feature distance;camera;confidence LUT;confidence look up table;decision making;image recognition system;modified best bin first approach;remote control system;scale invariant feature transform;Robots;3D Image Matching;Affine SIFT;Bag of Words;Best Bin First;Confidence based Matching}, 
doi={10.1109/NABIC.2010.5716326}, 
month={Dec},}
@INPROCEEDINGS{7164742, 
author={D. P. Mandal and D. Kundu and S. Maiti}, 
booktitle={2015 International Conference on Advances in Computer Engineering and Applications}, 
title={Finding experts in community question answering services: A theme based query likelihood language approach}, 
year={2015}, 
pages={423-427}, 
abstract={Community question answering services provide an open platform for users to acquire and share their knowledge. In the last decade, popularity of such services has increased noticeably. Large number of unanswered questions is a major problem for the growth of such services. A common way to address this issue is to route a new question to some selected users who have the potentiality in answering the question. Expert finding is the process of selecting such potential answerers. In this article, we have introduced an efficient method for expert finding using the theme in query likelihood language (QLL) model. Theme of a query is nothing but its subject matter and we have decided it based on the parts of speech (POS) of the words in the query. Depending on the theme of the given question, its similarity to a question in the archive is determined using the QLL model. Aggregating the similarity values of the questions a user answered previously (i.e., in the archive), his/her expertise for the given question is obtained. The performance of the proposed method is verified on a real world dataset (obtained from Yahoo! Answers) and it is found to be quite encouraging.}, 
keywords={query languages;question answering (information retrieval);POS;QLL model;community question answering services;parts of speech;theme based query likelihood language approach;Calculators;Communities;Computational modeling;Estimation;Knowledge discovery;Routing;Smoothing methods;Yahoo! Answers;community question answering;expert finding;query likelihood language model;theme}, 
doi={10.1109/ICACEA.2015.7164742}, 
month={March},}
@ARTICLE{7762123, 
author={H. Haddad Pajouh and R. Javidan and R. Khayami and D. Ali and K. K. R. Choo}, 
journal={IEEE Transactions on Emerging Topics in Computing}, 
title={A Two-layer Dimension Reduction and Two-tier Classification Model for Anomaly-Based Intrusion Detection in IoT Backbone Networks}, 
year={2016}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={With increasing reliance on Internet of Things (IoT) devices and services, the capability to detect intrusions and malicious activities within IoT networks is critical for resilience of the network infrastructure. In this paper, we present a novel model for intrusion detection based on two-layer dimension reduction and two-tier classification module, designed to detect malicious activities such as User to Root (U2R) and Remote to Local (R2L) attacks. The proposed model is using component analysis and linear discriminate analysis of dimension reduction module to spate the high dimensional dataset to a lower one with lesser features. We then apply a two-tier classification module utilizing Naïve Bayes and Certainty Factor version of K-Nearest Neighbor to identify suspicious behaviors. The experiment results using NSL-KDD dataset shows that our model outperforms previous models designed to detect U2R and R2L attacks.}, 
keywords={Anomaly Detection;CF-KNN;Intrusion Detection System;IoT;Multi-layer Classification}, 
doi={10.1109/TETC.2016.2633228}, 
ISSN={2168-6750}, 
month={},}
@INPROCEEDINGS{7341256, 
author={S. Sun and X. Wei and L. Wang and J. Chen}, 
booktitle={2015 International Conference on Wireless Communications Signal Processing (WCSP)}, 
title={Association analysis and prediction for IPTV service data and user's QoE}, 
year={2015}, 
pages={1-5}, 
abstract={In order to provide good service and improve user's feeling and degree of satisfaction, video service providers are now interested in understanding the influence of attributes on Quality of Experience (QoE). In this paper, based on IPTV business, we study the relationship between alarming data from IPTV set-top boxes and the user's QoE. First, data cleaning and analysis are performed. After these procedures, the important attributes influencing QoE are selected. Then decision tree is used for modeling dataset. Here information entropy minimization heuristic is adopted to discretize continuous-valued attributes and the improved C4.5 algorithm is designed to fastly and accurately build the tree. Experimental results show that the proposed scheme can indeed improve the prediction accuracy when compared with other competing schemes.}, 
keywords={IPTV;decision trees;entropy;minimisation;quality of experience;set-top boxes;IPTV business;IPTV service data;IPTV set-top boxes;QoE;alarming data;association analysis;continuous-valued attributes;data analysis;data cleaning;decision tree;improved C4.5 algorithm;information entropy minimization heuristic;quality of experience;video service providers;Correlation;Correlation coefficient;Decision trees;Entropy;IPTV;Media;Telecommunications;C4.5 algorithm;QoE;decision tree;discretization}, 
doi={10.1109/WCSP.2015.7341256}, 
month={Oct},}
@INPROCEEDINGS{4408861, 
author={H. Sahbi and J. Y. Audibert and R. Keriven}, 
booktitle={2007 IEEE 11th International Conference on Computer Vision}, 
title={Graph-Cut Transducers for Relevance Feedback in Content Based Image Retrieval}, 
year={2007}, 
pages={1-8}, 
abstract={Closing the semantic gap in content based image retrieval (CBIR) basically requires the knowledge of the user's intention which is usually translated into a sequence of questions and answers (Q&A). The user's feedback to these questions provides a CBIR system with a partial labeling of the data and makes it possible to iteratively refine a decision rule on the unlabeled data. Training of this decision rule is referred to as transductive learning. This work is an original approach to relevance feedback (RF) based on graph-cuts. Training consists in implicitly modeling the manifold enclosing both the labeled and unlabeled dataset and finding a partition of this manifold using a min-cut. The contribution of this work is two-fold (i) this is the first comprehensive study of relevance feedback using graph cuts and (ii) our RF model exploits the structure of the data manifold by considering also the structure of the unlabeled data. Experiments conducted on generic as well as specific databases show that our graph-cut based approach is very effective, outperforms other existing methods and makes it possible to converge to almost all the images of the user's "class of interest" with a very small labeling effort. A demo is available through our image retrieval tool kit (IRTK).}, 
keywords={content-based retrieval;graph theory;image retrieval;learning (artificial intelligence);relevance feedback;content based image retrieval;data manifold;decision rule;graph-cut transducers;image retrieval tool kit;relevance feedback;transductive learning;Bayesian methods;Content based retrieval;Displays;Feedback;Image converters;Image databases;Image retrieval;Labeling;Radio frequency;Transducers}, 
doi={10.1109/ICCV.2007.4408861}, 
ISSN={1550-5499}, 
month={Oct},}
@INPROCEEDINGS{7406414, 
author={Y. L. Lin and V. I. Morariu and W. Hsu}, 
booktitle={2015 IEEE International Conference on Computer Vision Workshop (ICCVW)}, 
title={Summarizing While Recording: Context-Based Highlight Detection for Egocentric Videos}, 
year={2015}, 
pages={443-451}, 
abstract={In conventional video summarization problems, contexts (e.g., scenes, activities) are often fixed or in a specific structure (e.g., movie, sport, surveillance videos). However, egocentric videos often include a variety of scene contexts as users can bring the cameras anywhere, which makes these conventional methods not directly applicable, especially because there is limited memory storage and computing power on the wearable devices. To resolve these difficulties, we propose a context-based highlight detection method that immediately generates summaries without watching the whole video sequences. In particular, our method automatically predicts the contexts of each video segment and uses a context-specific highlight model to generate the summaries. To further reduce computational and storage cost, we develop a joint approach that simultaneously optimizes the context and highlight models in an unified learning framework. We evaluate our method on a public Youtube dataset, demonstrating our method outperforms state-of-the-art approaches. In addition, we show the utility of our joint approach and early prediction for achieving competitive highlight detection results while requiring less computational and storage cost.}, 
keywords={image segmentation;learning (artificial intelligence);video signal processing;context-based highlight detection method;egocentric videos;movie;scene contexts;sport;surveillance videos;unified learning framework;video segment;video summarization problems;wearable devices;Computational modeling;Context;Context modeling;Predictive models;Support vector machines;Video sequences;Videos}, 
doi={10.1109/ICCVW.2015.65}, 
month={Dec},}
@INPROCEEDINGS{6921699, 
author={Y. Liu and B. Wu and B. Wang and G. Li}, 
booktitle={2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)}, 
title={SDHM: A hybrid model for spammer detection in Weibo}, 
year={2014}, 
pages={942-947}, 
abstract={As the microblogging service (such as Weibo) is becoming popular, spam becomes a serious problem of affecting the credibility and readability of Online Social Networks. Most existing studies took use of a set of features to identify spam, but without the consideration of the overlap and dependency among different features. In this study, we investigate the problem of spam detection by analyzing real spam dataset collections of Weibo and propose a novel hybrid model of spammer detection, called SDHM, which utilizing significant features, i.e. user behavior information, online social network attributes and text content characteristics, in an organic way. Experiments on real Weibo dataset demonstrate the power of the proposed hybrid model and the promising performance.}, 
keywords={behavioural sciences computing;social networking (online);text analysis;unsolicited e-mail;SDHM;Weibo;online social network attributes;real spam dataset collections;spammer detection;text content characteristics;user behavior information;Analytical models;Classification algorithms;Conferences;Feature extraction;Twitter;Unsolicited electronic mail;posting behavior;spammer detection;topic model}, 
doi={10.1109/ASONAM.2014.6921699}, 
month={Aug},}
@INPROCEEDINGS{7888201, 
author={L. Coviello and M. Franceschetti and M. García-Herranz and I. Rahwan}, 
booktitle={2016 Information Theory and Applications Workshop (ITA)}, 
title={Predicting and containing epidemic risk using friendship networks}, 
year={2016}, 
pages={1-7}, 
abstract={Physical encounter is the most common vehicle for the spread of infectious diseases, but detailed information about said encounters is often unavailable because expensive, unpractical to collect or privacy sensitive. The present work asks whether the friendship ties between the individuals in a social network can be used to successfully predict and contain epidemic risk. Using a dataset from a popular online review service, we build a time-varying network that is a proxy of physical encounter between users and a static network based on their reported friendship - the encounter network and the friendship network. Through computer simulation, we compare infection processes on the resulting networks and show that friendship provides a poor identification of the individuals at risk if the infection is driven by physical encounter. This result is not driven by the static nature of the friendship network opposed to the time-varying nature of the encounter network, as a static version of the encounter network provides more accurate prediction of risk than the friendship network. Despite this limit, the information enclosed in the friendship network can be leveraged for monitoring and containment of epidemics. In particular, we show that periodical and relatively infrequent monitoring of the infection on the encounter network allows to correct the predicted infection on the friendship network and to achieve satisfactory prediction accuracy. In addition, the friendship network contains valuable information to effectively contain epidemic outbreaks when a limited budget is available for immunization.}, 
keywords={epidemics;medical computing;social aspects of automation;computer simulation;dataset;epidemic outbreaks;epidemic risk;friendship networks;immunization;infection processes;online review service;privacy;social network;static network;time-varying network;Computational modeling;Infectious diseases;Measurement;Monitoring;Silicon;Social network services;Vehicle dynamics}, 
doi={10.1109/ITA.2016.7888201}, 
month={Jan},}
@INPROCEEDINGS{6890565, 
author={S. Zhao and H. Yao and Fanglin Wang and X. Jiang and Wei Zhang}, 
booktitle={2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
title={Emotion based image musicalization}, 
year={2014}, 
pages={1-6}, 
abstract={Playing appropriate music when watching images can make the images vivid and bring people into their intrinsic world. In this paper, we propose to musicalize images based on their emotions. Most of previous works on image emotion analysis mainly used elements-of-art based low-level visual features, which are vulnerable to the arrangements of elements. Here we propose to extract visual features, inspired by the concept of principles-of-art, to recognize image emotions. To enrich the descriptive power, a dimensional perspective is introduced to emotion modeling. Experiments on the IAPS dataset demonstrate the superiority of the proposed method in comparison to the state-of-the-art methods for emotion regression. The music in MST dataset with approximate emotions to the recognized image emotions is selected to musicalize these images. The user study results show its effectiveness and popularity of the image musicalization method.}, 
keywords={emotion recognition;feature extraction;music;IAPS dataset;MST dataset;dimensional perspective;elements-of-art based low-level visual features;emotion based image musicalization method;emotion modeling;emotion regression;image emotion analysis;image emotion recognition;image watching;visual feature extraction;Art;Emotion recognition;Feature extraction;Image color analysis;Image recognition;Standards;Visualization;Emotion recognition;dimensional model;elements and principles of art;image musicalization}, 
doi={10.1109/ICMEW.2014.6890565}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{7042501, 
author={S. K. Badam and J. Zhao and N. Elmqvist and D. S. Ebert}, 
booktitle={2014 IEEE Conference on Visual Analytics Science and Technology (VAST)}, 
title={TimeFork: Mixed-initiative time-series prediction}, 
year={2014}, 
pages={223-224}, 
abstract={We present TimeFork, an analytics technique for predicting the behavior of multivariate time-series data originating from modern disciplines such as economics (stock market) and meteorology (climate), with human-in-the-loop. We identify two types of machine-generated predictions for such datasets: temporal prediction that predicts the future of an attribute; and spatial prediction that predicts an attribute based on the other attributes in the dataset. Visual exploration of this prediction space, constituting of these predictions of different confidences, by chunking and chaining predictions over time promises accurate user-guided predictions. In order to utilize TimeFork technique, we created a visual analytics application for user-guided prediction over different time periods, thus allowing for visual exploration of time-series data.}, 
keywords={data analysis;data visualisation;time series;TimeFork;behavior prediction;chaining prediction;chunking prediction;confidence predictions;machine-generated predictions;mixed-initiative time-series prediction;multivariate time-series data;spatial prediction;temporal prediction;user-guided predictions;visual analytics application;visual exploration;Analytical models;Correlation;Data visualization;Prediction algorithms;Predictive models;Stock markets;Visualization}, 
doi={10.1109/VAST.2014.7042501}, 
month={Oct},}
@INPROCEEDINGS{6157865, 
author={Yajun Jiang}, 
booktitle={2011 IEEE 13th International Conference on Communication Technology}, 
title={Privacy-preserving set intersection in outsourcing environments}, 
year={2011}, 
pages={217-219}, 
abstract={We propose two privacy-preserving set intersection protocols in outsourcing environments. First, we propose a secure protocol (namely Protocol 1) for privacy-preserving set intersection using Mignotte's secret sharing scheme. The data owner outsources a dataset A to w third-party service providers by Mignotte's secret sharing scheme. Then, the user interacts with any k (k <; w) service providers to determine whether some elements of the user's dataset B belong to the dataset A without disclosing any useful information to all parties. Furthermore, we also construct a privacy-preserving cardinality computation protocol (namely Protocol 2) of set intersection which has no disclosure about the result of set intersection by improving Protocol 1. In the semi-honest model, we prove the security of these protocols.}, 
keywords={cryptographic protocols;data privacy;outsourcing;Mignotte secret sharing scheme;Protocol 1;Protocol 2;outsourcing environments;privacy-preserving cardinality computation protocol;privacy-preserving set intersection;secure protocol;semihonest model;third-party service providers;user dataset;Computational modeling;Cryptography;Outsourcing;Polynomials;Privacy;Protocols}, 
doi={10.1109/ICCT.2011.6157865}, 
month={Sept},}
@ARTICLE{4804392, 
author={D. Willkomm and S. Machiraju and J. Bolot and A. Wolisz}, 
journal={IEEE Communications Magazine}, 
title={Primary user behavior in cellular networks and implications for dynamic spectrum access}, 
year={2009}, 
volume={47}, 
number={3}, 
pages={88-95}, 
abstract={Dynamic spectrum access approaches, which propose to opportunistically use underutilized portions of licensed wireless spectrum such as cellular bands, are increasingly being seen as a way to alleviate spectrum scarcity. However, before DSA approaches can be enabled, it is important that we understand the dynamics of spectrum usage in licensed bands. Our focus in this article is the cellular band. Using a unique dataset collected inside a cellular network operator, we analyze the usage in cellular bands and discuss the implications of our results on enabling DSA in these bands. One of the key aspects of our dataset is its scale-it consists of data collected over three weeks at hundreds of base stations. We dissect this data along different dimensions to characterize if and when spectrum is available, develop models of primary usage, and understand the implications of these results on DSA techniques such as sensing.}, 
keywords={cellular radio;radio spectrum management;base stations;cellular network operator;dynamic spectrum access approaches;licensed wireless spectrum;spectrum scarcity;Base stations;Chromium;Cognitive radio;Femtocells;Interference;Land mobile radio cellular systems;Large-scale systems;Licenses;Resumes;TV}, 
doi={10.1109/MCOM.2009.4804392}, 
ISSN={0163-6804}, 
month={March},}
@INPROCEEDINGS{7330190, 
author={A. Gouta and D. Hausheer and A. M. Kermarrec and C. Koch and Y. Lelouedec and J. Rückert}, 
booktitle={2015 IEEE 23rd International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems}, 
title={CPSys: A System for Mobile Video Prefetching}, 
year={2015}, 
pages={188-197}, 
abstract={Online media services are reshaping the way video content is watched. People with similar interests tend to request same content. This provides enormous potential to predict which content users are interested in. Besides, mobile devices are commonly used to watch videos which popularity is largely driven by its social success. In this paper, we design CPSys a Central Predictor System to prefetch relevant videos for each user. To fine tune our prefetching system, we rely on a large dataset collected from a large mobile carrier in Europe. The rationale of our prefetching strategy is first to form a graph and build implicit or explicit ties between similar users. On top of this graph, we propose the Most Popular and Most Recent (MPMR) policy to predict relevant videos for each user. We show that CPSys can achieve high performance with respect to the correct prediction ratio and by significantly reducing the traffic overhead. We further show that CPSys outperforms other prefetching schemes that have been presented and studied in the state of the art. At the end, we provide a proof-of-concept implementation of our prefetching system.}, 
keywords={mobile computing;multimedia systems;storage management;telecommunication traffic;video signal processing;CPSys;Europe;MPMR policy;central predictor system;graph;mobile carrier;mobile devices;mobile video prefetching;most popular and most recent policy;online media services;prediction ratio;prefetching strategy;traffic overhead;video content;Analytical models;Facebook;Mobile communication;Mobile handsets;Prefetching;Streaming media;YouTube}, 
doi={10.1109/MASCOTS.2015.38}, 
ISSN={1526-7539}, 
month={Oct},}
@ARTICLE{7886274, 
author={A. Bartoli and A. De Lorenzo and E. Medvet and F. Tarlao}, 
journal={IEEE Transactions on Cybernetics}, 
title={Active Learning of Regular Expressions for Entity Extraction}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-14}, 
abstract={We consider the automatic synthesis of an entity extractor, in the form of a regular expression, from examples of the desired extractions in an unstructured text stream. This is a long-standing problem for which many different approaches have been proposed, which all require the preliminary construction of a large dataset fully annotated by the user. In this paper, we propose an active learning approach aimed at minimizing the user annotation effort: the user annotates only one desired extraction and then merely answers extraction queries generated by the system. During the learning process, the system digs into the input text for selecting the most appropriate extraction query to be submitted to the user in order to improve the current extractor. We construct candidate solutions with genetic programming (GP) and select queries with a form of querying-by-committee, i.e., based on a measure of disagreement within the best candidate solutions. All the components of our system are carefully tailored to the peculiarities of active learning with GP and of entity extraction from unstructured text. We evaluate our proposal in depth, on a number of challenging datasets and based on a realistic estimate of the user effort involved in answering each single query. The results demonstrate high accuracy with significant savings in terms of computational effort, annotated characters, and execution time over a state-of-the-art baseline.}, 
keywords={Cybernetics;Data models;Genetic programming;Learning systems;Proposals;Search problems;Uncertainty;Automatic programming;evolutionary computation;genetic programming (GP);inference mechanisms;man machine systems;semisupervised learning;text processing}, 
doi={10.1109/TCYB.2017.2680466}, 
ISSN={2168-2267}, 
month={},}
@INPROCEEDINGS{7817042, 
author={M. A. Bashar and Y. Li and Y. Gao}, 
booktitle={2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)}, 
title={A Framework for Automatic Personalised Ontology Learning}, 
year={2016}, 
pages={105-112}, 
abstract={Understanding or acquiring a user's information needs from their local information repository (e.g. a set of example-documents that are relevant to user information needs) is important in many applications. However, acquiring the user's information needs from the local information repository is very challenging. Personalised ontology is emerging as a powerful tool to acquire the information needs of users. However, its manual or semi-automatic construction is expensive and time-consuming. To address this problem, this paper proposes a model to automatically learn personalised ontology by labelling topic models with concepts, where the topic models are discovered from a user's local information repository. The proposed model is evaluated by comparing against ten baseline models on the standard dataset RCV1 and a large ontology LCSH. The results show that the model is effective and its performance is significantly improved.}, 
keywords={Internet;learning (artificial intelligence);ontologies (artificial intelligence);automatic personalised ontology learning;local information repository;topic models;Data models;Labeling;Manuals;Noise measurement;Ontologies;Semantics;Standards;Labelling Topic Models;Ontology Mining;Personalisation;User Information Needs;Web Intelligence}, 
doi={10.1109/WI.2016.0025}, 
month={Oct},}
@INPROCEEDINGS{6932876, 
author={F. Hadiji and R. Sifa and A. Drachen and C. Thurau and K. Kersting and C. Bauckhage}, 
booktitle={2014 IEEE Conference on Computational Intelligence and Games}, 
title={Predicting player churn in the wild}, 
year={2014}, 
pages={1-8}, 
abstract={Free-to-Play or “freemium” games represent a fundamental shift in the business models of the game industry, facilitated by the increasing use of online distribution platforms and the introduction of increasingly powerful mobile platforms. The ability of a game development company to analyze and derive insights from behavioral telemetry is crucial to the success of these games which rely on in-game purchases and in-game advertising to generate revenue, and for the company to remain competitive in a global marketplace. The ability to model, understand and predict future player behavior has a crucial value, allowing developers to obtain data-driven insights to inform design, development and marketing strategies. One of the key challenges is modeling and predicting player churn. This paper presents the first cross-game study of churn prediction in Free-to-Play games. Churn in games is discussed and thoroughly defined as a formal problem, aligning with industry standards. Furthermore, a range of features which are generic to games are defined and evaluated for their usefulness in predicting player churn, e.g. playtime, session length and session intervals. Using these behavioral features, combined with the individual retention model for each game in the dataset used, we develop a broadly applicable churn prediction model, which does not rely on game-design specific features. The presented classifiers are applied on a dataset covering five free-to-play games resulting in high accuracy churn prediction.}, 
keywords={advertising;computer games;mobile computing;public domain software;behavioral telemetry;business models;free-to-play games;freemium games;future player behavior prediction;game development company;game industry;game-design specific features;global marketplace;in-game advertising;in-game purchases;industry standards;marketing strategies;mobile platforms;online distribution platforms;player chum prediction;session intervals;session length;Data mining;Games;Mobile communication;behavior;behavior modeling;churn;churn prediction;free-to-play;freemium;game analytics;game data mining;games}, 
doi={10.1109/CIG.2014.6932876}, 
ISSN={2325-4270}, 
month={Aug},}
@INPROCEEDINGS{5584130, 
author={J. Andreu and P. Angelov}, 
booktitle={International Conference on Fuzzy Systems}, 
title={Forecasting time-series for NN GC1 using Evolving Takagi-Sugeno (eTS) Fuzzy Systems with on-line inputs selection}, 
year={2010}, 
pages={1-5}, 
abstract={In this paper we present results and algorithm used to predict 14 days horizon from a number of time series provided by the NN GC1 concerning transportation datasets [1]. Our approach is based on applying the well known Evolving Takagi-Sugeno (eTS) Fuzzy Systems [2-6] to self-learn from the time series. ETS are characterized by the fact that they self-learn and evolve the fuzzy rule-based system which, in fact, represents their structure from the data stream on-line and in real-time mode. That means we used all the data samples from the time series only once, at any instant in time we only used one single input vector (which consist of few data samples as described below) and we do not iterate or memorize the whole sequence. It should be emphasized that this is a huge practical advantage which, unfortunately cannot be compared directly to the other competitors in NN GC1 if only precision/error is taken as a criteria. It is also worth to require time for calculations and memory usage as well as iterations and computational complexity to be provided and compared to build a fuller picture of the advantages the proposed technique offers. Nevertheless, we offer a computationally light and easy to use approach which in addition does not require any user-or problem-specific thresholds or parameters to be specified. Additionally, this approach is flexible in terms not only of its structure (fuzzy rule based and automatic self-development), but also in terms of automatic input selection as will be described below.}, 
keywords={fuzzy set theory;fuzzy systems;knowledge based systems;time series;unsupervised learning;NN GC1;data sample;data stream;evolving Takagi Sugeno fuzzy system;forecasting time series;fuzzy rule based system;self learning;transportation dataset;Adaptation model;Artificial neural networks;Cybernetics;Estimation;Fuzzy systems;Takagi-Sugeno model;Time series analysis}, 
doi={10.1109/FUZZY.2010.5584130}, 
ISSN={1098-7584}, 
month={July},}
@ARTICLE{7305813, 
author={E. Smistad and F. Lindseth}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Real-Time Automatic Artery Segmentation, Reconstruction and Registration for Ultrasound-Guided Regional Anaesthesia of the Femoral Nerve}, 
year={2016}, 
volume={35}, 
number={3}, 
pages={752-761}, 
abstract={The goal is to create an assistant for ultrasound- guided femoral nerve block. By segmenting and visualizing the important structures such as the femoral artery, we hope to improve the success of these procedures. This article is the first step towards this goal and presents novel real-time methods for identifying and reconstructing the femoral artery, and registering a model of the surrounding anatomy to the ultrasound images. The femoral artery is modelled as an ellipse. The artery is first detected by a novel algorithm which initializes the artery tracking. This algorithm is completely automatic and requires no user interaction. Artery tracking is achieved with a Kalman filter. The 3D artery is reconstructed in real-time with a novel algorithm and a tracked ultrasound probe. A mesh model of the surrounding anatomy was created from a CT dataset. Registration of this model is achieved by landmark registration using the centerpoints from the artery tracking and the femoral artery centerline of the model. The artery detection method was able to automatically detect the femoral artery and initialize the tracking in all 48 ultrasound sequences. The tracking algorithm achieved an average dice similarity coefficient of 0.91, absolute distance of 0.33 mm, and Hausdorff distance 1.05 mm. The mean registration error was 2.7 mm, while the average maximum error was 12.4 mm. The average runtime was measured to be 38, 8, 46 and 0.2 milliseconds for the artery detection, tracking, reconstruction and registration methods respectively.}, 
keywords={biomedical ultrasonics;blood vessels;computerised tomography;image reconstruction;image registration;image segmentation;image sequences;medical image processing;CT dataset;Hausdorff distance;Kalman filter;average Dice similarity coefficient;distance 0.33 mm;distance 1.05 mm;real-time automatic femoral artery segmentation;time 0.2 ms;time 38 ms;time 46 ms;time 8 ms;ultrasound sequences;ultrasound-guided femoral nerve block;ultrasound-guided regional anaesthesia;Arteries;Image edge detection;Image segmentation;Probes;Real-time systems;Three-dimensional displays;Ultrasonic imaging;Artery segmentation;GPU;artery tracking;femoral nerve block;real-time;regional anaesthesia;ultrasound}, 
doi={10.1109/TMI.2015.2494160}, 
ISSN={0278-0062}, 
month={March},}
@INPROCEEDINGS{6675173, 
author={L. Liu and S. Ozer and K. Bemis and J. Takle and D. Silver}, 
booktitle={2013 IEEE Symposium on Large-Scale Data Analysis and Visualization (LDAV)}, 
title={An interactive method for activity detection visualization}, 
year={2013}, 
pages={129-130}, 
abstract={Visualizing each time step in an activity from a scientific dataset can aid in understanding the data and phenomena. In this work, we present a Graphical User Interface (GUI) that allows scientists to first graphically model an activity, then detect any activities that match the model, and finally visualize the detected activities in time varying scientific data sets. As a graphical and state based interactive approach, an activity detection framework is implemented by our GUI as a tool for modelling, hypothesis-testing and searching for interested activities from the phenomena evolution of the data set. We demonstrate here some features of our GUI: a histogram is used to visualize the number of activities detected as a function of time and to allow the user to focus on a moment in time; a table is used to give details about the activities and the features participating in them; and finally the user is given the ability to click on the screen to bring up 3D images of the overall activity sequence, single time steps of an activity, or individual feature in an activity. We present examples from applications to two different data sets.}, 
keywords={data visualisation;graphical user interfaces;interactive systems;3D image;GUI;activity detection visualization;graphical user interface;interactive method;state based interactive approach;time varying scientific data set;Graphical user interface;activity detection;data visualization;graph-based technique;interactive method}, 
doi={10.1109/LDAV.2013.6675173}, 
month={Oct},}
@INPROCEEDINGS{7841508, 
author={E. Ben Abdelkrim and M. A. Salahuddin and H. Elbiaze and R. Glitho}, 
booktitle={2016 IEEE Global Communications Conference (GLOBECOM)}, 
title={A Hybrid Regression Model for Video Popularity-Based Cache Replacement in Content Delivery Networks}, 
year={2016}, 
pages={1-7}, 
abstract={Content Delivery Networks (CDN) and their globally dispersed caches host a myriad of User Generated Videos (UGV) to meet end-user requests with quality of service. To efficiently utilize the limited storage of the caches, it is imperative to improve the hit ratio of UGVs. In contrast to the traditional static content, UGV popularity is highly dynamic and dependent on end-user behavior. Therefore, we devise a novel popularity prediction model for UGV, using a hybrid regression model. Our hybrid regression model dynamically adapts the popularity of UGV that is built from a historical training dataset. We reduce error in predicting popularity by up to 14%, when compared to pure offline and online approaches, with a small increase in the execution time and memory overhead. Our novel popularity prediction model accounts for end- user behavior by considering the end-user video watch time and the number of shares for the UGVs. To improve cache performance in CDN, we employ a cache replacement strategy that leverages our popularity prediction model to efficiently evict the less popular UGVs for more popular content. We compare our novel cache replacement strategy with the traditional and state-of-the-art cache replacement strategies and show an increase in the average hit ratio of up to 74% and 7%, respectively, for UGVs with shortterm popularity.}, 
keywords={Internet;regression analysis;cache performance;cache replacement;content delivery networks;end-user behavior;execution time;globally dispersed caches;historical training dataset;hybrid regression model;memory overhead;popularity prediction model;user generated videos;video popularity;Adaptation models;Analytical models;Correlation;Data models;Linear regression;Predictive models}, 
doi={10.1109/GLOCOM.2016.7841508}, 
month={Dec},}
@INPROCEEDINGS{5459924, 
author={H. Yang and J. Luo and M. Yin and Y. Liu}, 
booktitle={2010 Second International Workshop on Education Technology and Computer Science}, 
title={Automatically Detecting Personal Topics by Clustering Emails}, 
year={2010}, 
volume={3}, 
pages={91-94}, 
abstract={Emails play an important role in our daily life. It has been recognized that clustering emails into meaningful groups can greatly save cognitive load to process emails. Mailbox user becomes more and more concerned about how to organize and manage the emails as well as how to mine the meaningful data conveniently and effectively. This paper proposes a novel personal topics detection approach using clustering algorithm. First preprocess the emails and construct the improved email VSM(vector space model) to label the email combining the body and subject in a new method, then adopt the advanced k-means algorithm to cluster the emails and design a kernel-selected algorithm based on the lowest similarity, afterwards we get the appropriate keywords to label the topic of each cluster. Finally, experiments on 20Newsgruops email dataset show the validity of our approach and the experimental results also well match the labeled human clustering result.}, 
keywords={electronic mail;pattern clustering;automatic personal topic detection;clustering emails;email management;email organization;k-means algorithm;kernel-selected algorithm;vector space model;Algorithm design and analysis;Clustering algorithms;Computer science;Computer science education;Data mining;Educational technology;Humans;Information science;Natural languages;Speech recognition;Email VSM;email clustering;kernel-selected;topic detection}, 
doi={10.1109/ETCS.2010.238}, 
month={March},}
@INPROCEEDINGS{6365434, 
author={A. Drosou and N. Porfyriou and D. Tzovaras}, 
booktitle={2012 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)}, 
title={Enhancing 3D face recognition using soft biometrics}, 
year={2012}, 
pages={1-4}, 
abstract={This paper presents a novel probabilistic approach for augmenting the performance of a 3D face recognition system with information from continuous facial soft biometric traits. In particular, by estimating the distribution of the noise induced during the measurement of one or more soft biometric traits, the recognition score for a genuine user can be efficiently modelled as a conditional matching probability that takes into account both geometric and soft biometrics. Herein, the geometric traits are provided via a state-of-art 3D face recognition system, while the soft biometrics regard the distances between three facial nodal points (i.e. the eyes, the nose and the mouth). Experimental validation on a proprietary dataset of 54 subjects illustrates significant advances in both identification and authentication rates of the proposed method when compared to the 3D face recognition system.}, 
keywords={biometrics (access control);face recognition;statistical distributions;3D face recognition;authentication rates;conditional matching probability;facial nodal point;facial soft biometric trait;geometric biometric;image enhancement;noise distribution estimation;probabilistic approach;recognition score;Atmospheric measurements;Biometrics (access control);Clustering algorithms;Databases;Face;Face recognition;Particle measurements;3D face recognition;biometric recognition;soft biometrics}, 
doi={10.1109/3DTV.2012.6365434}, 
ISSN={2161-2021}, 
month={Oct},}
@INPROCEEDINGS{4538826, 
author={M. Anabuki and H. Ishii}, 
booktitle={2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}, 
title={AR-Jig: A Handheld Tangible User Interface for Modification of 3D Digital Form via 2D Physical Curve}, 
year={2007}, 
pages={55-66}, 
abstract={We introduce AR-Jig, a new handheld tangible user interface for 3D digital modeling in augmented reality (AR) space. AR-Jig has a pin array that displays a 2D physical curve coincident with a contour of a digitally displayed 3D form. It supports physical interaction with a portion of a 3D digital representation, allowing 3D forms to be directly touched and modified. Traditional tangible user interfaces physically embody all the data; in contrast, this project leaves the majority of the data in the digital domain but gives physicality to any portion of the larger digital dataset via a handheld tool. This tangible intersection enables the flexible manipulation of digital artifacts, both physically and virtually. Through an informal test by end-users and interviews with professionals, we confirmed the potential of the AR-Jig concept while identifying the improvements necessary to make AR-Jig a practical tool for 3D digital design.}, 
keywords={augmented reality;user interfaces;2D physical curve;3D digital form;3D digital representation;AR-Jig;augmented reality;digital artifacts;handheld tangible user interface;Augmented reality;Humans;Imaging phantoms;Laboratories;Pins;Prototypes;Space technology;Three dimensional displays;Two dimensional displays;User interfaces;actuated interface;augmented reality;digital modeling;handheld tool;pin array display}, 
doi={10.1109/ISMAR.2007.4538826}, 
month={Nov},}
@INPROCEEDINGS{6466249, 
author={K. Parameswaran}, 
booktitle={2013 International Conference on Computer Communication and Informatics}, 
title={Vector quantization, density estimation and outlier detection on cricket dataset}, 
year={2013}, 
pages={1-5}, 
abstract={This study aims to apply unsupervised machine learning algorithms on Cricket players' career statistics dataset. K-means clustering algorithm is used to find the natural grouping that exists within the cricket players using player's batting average, strike rate, bowling average, economy etc. as input features - in this case players are grouped into 3 groups. Further separate probability density models are fitted for batsmen, bowlers and all-rounding players using appropriate player's performance metrics as input features and using these models, outstanding players are identified. Similar method is used to identify match winning players, where the differences between player's performance metrics and team's average performance metrics are used as input features. The results obtained from this study seem to correlate with expert generated results where they used point based system to rank the players. This kind of statistical analysis of sports data plays a vital role in team planning and exploiting opponents' weakness.}, 
keywords={estimation theory;pattern clustering;probability;sport;statistical analysis;unsupervised learning;vector quantisation;Cricket player career statistics dataset;all-rounding players;batsmen;bowlers;density estimation;input features;k-means clustering algorithm;match winning player identification;opponent weakness exploitation;outlier detection;player natural grouping;player performance metrics;player ranking;point-based system;probability density models;sports data statistical analysis;team average performance metrics;team planning;unsupervised machine learning algorithms;vector quantization;Biological system modeling;Clustering algorithms;Computers;Engineering profession;Estimation;Informatics;Measurement;Cricket;Density estimation;K-means clustering;Outlier detection}, 
doi={10.1109/ICCCI.2013.6466249}, 
month={Jan},}
@INPROCEEDINGS{781685, 
author={J. F. Baldwin and C. Hill and T. P. Martin}, 
booktitle={18th International Conference of the North American Fuzzy Information Processing Society - NAFIPS (Cat. No.99TH8397)}, 
title={Induction of relational Fril rules}, 
year={1999}, 
pages={213-217}, 
abstract={We propose an, approach to extend inductive logic programming (ILP) to cater for uncertainties in the form of probabilities and fuzzy sets. A corresponding decision tree induction algorithm that induces Fril (a support logic programming language) classification. Rules involving both forms of uncertainties is also described. This algorithm iteratively builds decision trees where each decision tree consists of one branch. This branch is directly translated into Fril rules that explain a part of the problem. The work presented focuses on propositional representations for both the input data values and the learned models. The approach is illustrated on the Pima Indian dataset. Finally an overview of the current work is given which deals with improving the algorithm with a new method for the calculation of support pairs and also with a new, user-independent stopping criterion for adding literals to the body of a rule}, 
keywords={fuzzy logic;inductive logic programming;knowledge acquisition;uncertainty handling;Pima Indian dataset;decision tree induction algorithm;fuzzy sets;inductive logic programming;input data values;learned models;literals;logic programming language;probabilities;propositional representations;relational Fril rules;rule induction;uncertainties;user-independent stopping criterion;Classification tree analysis;Decision trees;Entropy;Fuzzy logic;Fuzzy sets;Iterative algorithms;Logic programming;Mathematics;Partitioning algorithms;Uncertainty}, 
doi={10.1109/NAFIPS.1999.781685}, 
month={Jul},}
@INPROCEEDINGS{7727726, 
author={I. Hossain and A. Khosravi and S. Nahavandhi}, 
booktitle={2016 International Joint Conference on Neural Networks (IJCNN)}, 
title={Active transfer learning and selective instance transfer with active learning for motor imagery based BCI}, 
year={2016}, 
pages={4048-4055}, 
abstract={Non-invasive EEG signal based brain computer interface (BCI) for motor imagery task - classification requires large number of subject specific training samples for each user session that reduces the user feasibility of BCI. A generalized classifier using few subject specific sample will ease the real world implementation of motor imagery based BCI. At first, this paper applies an improved active transfer learning (ATL) on motor imagery based BCI. Then, it proposes a noble method of transferring selective instances (selected by few new subject specific data) from other subjects to new subject combining with selecting most informative subject specific data determined by active learning. Experimental results on BCI competition IV 2B dataset show that improved ATL works well on six out of nine subjects and proposed SIITAL method overcomes ATL limitation for other subjects. This means, it can achieve similar or better accuracy with a lower quantity of subject specific training data. Thus, it reduces the calibration effort.}, 
keywords={brain-computer interfaces;electroencephalography;learning (artificial intelligence);medical signal processing;pattern classification;ATL;BCI user feasibility;SIITAL method;active transfer learning;brain computer interface;classifier;motor imagery task classification;noninvasive EEG signal;selective instance transfer;subject specific data;Brain modeling;Calibration;Electroencephalography;Electrooculography;Entropy;Labeling;Training}, 
doi={10.1109/IJCNN.2016.7727726}, 
month={July},}
@INPROCEEDINGS{6619281, 
author={I. Chakraborty and H. Cheng and O. Javed}, 
booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={3D Visual Proxemics: Recognizing Human Interactions in 3D from a Single Image}, 
year={2013}, 
pages={3406-3413}, 
abstract={We present a unified framework for detecting and classifying people interactions in unconstrained user generated images. Unlike previous approaches that directly map people/face locations in 2D image space into features for classification, we first estimate camera viewpoint and people positions in 3D space and then extract spatial configuration features from explicit 3D people positions. This approach has several advantages. First, it can accurately estimate relative distances and orientations between people in 3D. Second, it encodes spatial arrangements of people into a richer set of shape descriptors than afforded in 2D. Our 3D shape descriptors are invariant to camera pose variations often seen in web images and videos. The proposed approach also estimates camera pose and uses it to capture the intent of the photo. To achieve accurate 3D people layout estimation, we develop an algorithm that robustly fuses semantic constraints about human interpositions into a linear camera model. This enables our model to handle large variations in people size, heights (e.g. age) and poses. An accurate 3D layout also allows us to construct features informed by Proxemics that improves our semantic classification. To characterize the human interaction space, we introduce visual proxemes, a set of prototypical patterns that represent commonly occurring social interactions in events. We train a discriminative classifier that classifies 3D arrangements of people into visual proxemes and quantitatively evaluate the performance on a large, challenging dataset.}, 
keywords={feature extraction;human computer interaction;image classification;image sensors;object recognition;pose estimation;shape recognition;3D people layout estimation;3D shape descriptors;3D space;3D visual proxemics;camera pose variations;camera viewpoint estimation;discriminative classifier;human interaction recognition;people interaction classification;people interaction detection;people position estimation;relative distance estimation;relative orientation estimation;semantic classification;social interactions;spatial configuration feature extraction;unconstrained user generated images;Cameras;Face;Robustness;Shape;Three-dimensional displays;Videos;Visualization;3D people layout;RANSAC;Visual Proxemics;semantic constraints}, 
doi={10.1109/CVPR.2013.437}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{6921673, 
author={B. Fang and Q. Ye}, 
booktitle={2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)}, 
title={Competition between real-life friends and virtual-life friends}, 
year={2014}, 
pages={771-777}, 
abstract={With the rapid growth of social gaming market, it is of importance to investigate how to encourage players to contribute more economic value to the company. We analyzed a social gaming dataset from one of the social media leaders in China. The result suggests social influence from players' friends is one factor derive players to spend money. Also, it suggests social influence generated by friends known only in the game is statistically stronger than by friends known also in the real world.}, 
keywords={computer games;data analysis;marketing data processing;virtual reality;dataset;economic value;real world;real-life friends;social gaming market;social media leaders;virtual-life friends;Companies;Conferences;Data models;Economics;Games;Robustness;Social network services;social gaming;social influence;virtual-life}, 
doi={10.1109/ASONAM.2014.6921673}, 
month={Aug},}
@INPROCEEDINGS{6997053, 
author={Y. Luo and B. Xu and H. Cai and F. Bu}, 
booktitle={2014 Enterprise Systems Conference}, 
title={A Hybrid User Profile Model for Personalized Recommender System with Linked Open Data}, 
year={2014}, 
pages={243-248}, 
abstract={In order to better enhance user experience on the web, varies applications such as search engines have integrated with recommender systems. Users will get some relevant items recommended when browsing the result page. However, building such recommendation services needs a large amount of item information, which makes it hard to start a new recommender service. Due to the development of Semantic Web and Linked Data, a vast amount of RDF data can be accessed via the Internet and naturally used as a knowledge base for recommender systems. In this paper, we study modeling user profiles in semantic environment and building a personalized recommender system exclusively on Linked Open Data. First, we define two concepts related to user browsing history and propose a hybrid user profile model (Hay-UPM). Second, we design a generic and personalized recommender system to utilize the semantic information between the items and user profile model to make recommendations. Finally, we conduct our experiment on the movie dataset of DBpedia and MovieLens. The result shows Hy-UPMhas a better Mean Reciprocal Rank performance compared with other recommendation methods.}, 
keywords={human computer interaction;recommender systems;DBpedia;Hy-UPM;Linked Open Data;MovieLens;hybrid user profile model;mean reciprocal rank performance;personalized recommender system;semantic information;Data models;Films;History;Motion pictures;Ontologies;Recommender systems;Semantics;Linked Open Data;Recommender Algorithm;Recommender System;Semantic Web;User Profile Modeling}, 
doi={10.1109/ES.2014.16}, 
month={Aug},}
@INPROCEEDINGS{5328159, 
author={H. Zhu and Y. Luo and C. Weng and M. Li}, 
booktitle={2009 Fourth ChinaGrid Annual Conference}, 
title={A Collaborative Filtering Recommendation Model Using Polynomial Regression Approach}, 
year={2009}, 
pages={134-138}, 
abstract={In grid environment, collaborative filtering (CF) could be used for security recommendation when grid users face plenty of unknown security grid services. Also, CF recommender systems could be employed in the virtual machines managing platform to measure the creditability of each virtual machine. In this study, a polynomial regression based recommendation model on the basis of typical user-based CF is built to make security recommendation. In the model, a cluster of recommendation algorithms based on polynomial regression are derived according to various regression orders and dataset sizes. From our experiments, three significant conclusions are discovered in this model. Firstly, algorithms with lower regression orders make better predictions. Secondly, among algorithms with each fixed regression order, the best one satisfies that its dataset size is equal to its regression order in general. Thirdly, selecting appropriate regression order and dataset size could enhance recommendation quality.}, 
keywords={grid computing;polynomials;regression analysis;telecommunication security;virtual machines;collaborative filtering recommendation model;grid environment;polynomial regression approach;security recommendation;virtual machines managing platform;Clustering algorithms;Collaboration;Computer science;Computer security;Data security;Filtering;Polynomials;Recommender systems;Resource management;Virtual machining;collaborative filtering;polynomial regression;security recommendation}, 
doi={10.1109/ChinaGrid.2009.34}, 
ISSN={1949-131X}, 
month={Aug},}
@INPROCEEDINGS{6911837, 
author={L. Malott and S. Chellappan}, 
booktitle={2014 23rd International Conference on Computer Communication and Networks (ICCCN)}, 
title={Investigating the fractal nature of individual user netflow data}, 
year={2014}, 
pages={1-6}, 
abstract={Modeling and characterizing Internet traffic has been a widely studied problem since the conception of the Internet. The self-similar, bursty nature of the traffic has led to a number of conventional statistical models that unfortunately provide relatively weak modeling power. Recently, fractal analysis techniques have emerged to better characterize and model Internet traffic data. However, past research studies have focused on describing and quantifying the fractal nature of Internet traffic on user groups, instead of a single user. In this paper the authors investigate the issue of individual users exhibiting fractal (self-similar behavior) behavior across multiple application types. Using real Internet traffic traces (collected via Net-Flow logs) collected at a college campus for 30 days, our investigations reveal that in a number of application categories (http, chatting, p2p, email etc.) at least one user exhibits long-range correlations typical of fractal behavior. Of the 10 application groups, 7 had over 80% of users demonstrating self-similar behavior with 3 of those groups having > 98%. Potential benefits of our study in the realm of smart health and network security, by reducing the dimensionality of large Internet traffic datasets, are discussed.}, 
keywords={Big Data;Internet;computer network security;data reduction;fractals;statistical analysis;telecommunication traffic;Big-data;Internet traffic data modeling;Internet traffic dataset dimensionality reduction;Internet traffic fractal nature;fractal analysis techniques;fractal behavior;individual user netflow data;net-flow logs;network security;relatively weak modeling power;self similar behavior;smart health;statistical models;traffic bursty nature;Correlation;Electronic mail;Fluctuations;Fractals;Internet;Market research;Time series analysis}, 
doi={10.1109/ICCCN.2014.6911837}, 
ISSN={1095-2055}, 
month={Aug},}
@INPROCEEDINGS{7872772, 
author={M. Aprilianti and R. Mahendra and I. Budi}, 
booktitle={2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, 
title={Implementation of weighted parallel hybrid recommender systems for e-commerce in Indonesia}, 
year={2016}, 
pages={321-326}, 
abstract={This paper focus on building recommender system with weighted parallel hybrid method for e-commerce in Indonesia. The dataset was derived from one of the largest ecommerce company in Indonesia. The experiments used three sampling techniques, namely bootstrapping validation, timing series and systematic sampling. The best result of these experiments yields F1-measure of 9.99% which is achieved by the combination of user-based collaborative filtering approach and content-based filtering approach. Moreover, the value of evaluation metrics in this research is not much different from the previous research of recommender system. This indicates that recommender systems can be applied to e-commerce companies in Indonesia.}, 
keywords={collaborative filtering;electronic commerce;recommender systems;sampling methods;Indonesia;bootstrapping validation;collaborative filtering;content-based filtering;e-commerce;sampling technique;systematic sampling;timing series;weighted parallel hybrid recommender system;Collaboration;Companies;Computational modeling;Data models;Mathematical model;Recommender systems;Collaborative Filtering;Content-based Filtering;E-commerce;Recommender Systems;Weighted Parallel Hrbrid}, 
doi={10.1109/ICACSIS.2016.7872772}, 
month={Oct},}
@INPROCEEDINGS{6528706, 
author={Xiaoyuan Lu and Yidong Cui}, 
booktitle={2012 6th International Conference on New Trends in Information Science, Service Science and Data Mining (ISSDM2012)}, 
title={The study of micro-blog information diffusion model based on community structure detection}, 
year={2012}, 
pages={612-616}, 
abstract={As social network services become more popular and many commercial groups use micro blog as a media for information distribution, the analysis of the community structure in online social network specially the identification of significant nodes in information distribution is very important. In this paper the author investigates an algorithm that use the multi-dimension attributes of micro-blog user and the multiple iterations thought of page-rank to discover the center node of the community, and integrate multi-path to calculate the information diffusion distance from the center node to every node in the network, finally discover the communities in the network and construct the information diffusion model in a tree data structure. The algorithm is validated by applying it to a 16769 nodes dataset climbed from sina micro-blog user network and the result is as expected.}, 
keywords={data mining;social networking (online);tree data structures;center node discovery;community structure detection;information diffusion distance calculation;information diffusion model;information distribution media;microblog information diffusion model;microblog user;multidimension attributes;node identification;online social network;page-rank;sina microblog user network;social network services;tree data structure;center node identification;community discovery;information diffusion model;micro-blog}, 
month={Oct},}
@INPROCEEDINGS{1180909, 
author={T. Ohmori and Y. Tsutatani and M. Hoshi}, 
booktitle={First International Symposium on Cyber Worlds, 2002. Proceedings.}, 
title={A novel datacube model supporting interactive web-log mining}, 
year={2002}, 
pages={419-427}, 
abstract={Web-log mining is a technique to find "useful" information from access-log data. Typically, association rule mining is used to find frequent patterns (or sequence patterns) of visited pages from access logs and to build users' behavior models from those patterns. In this direction, there exists a difficulty that a human decision-maker must do such data mining process many times under different constraining conditions, different groups of pages, and different levels of abstraction. In order to support this process, this paper proposes a novel datacube model called itemset cube. This cube manages frequent itemsets under various conditions which are modeled by a n-dimensional space. An itemset cube is materialized, sliced, and rolled-up repeatedly in the same way as a traditional scalar datacube is done for interactive scalar-value analysis. Although this looks simple, fast execution of these operations on an itemset cube is difficult. It is because different cells in an itemset cube contain different numbers of records, but these cells must use the same threshold ratios in order to detect frequent itemsets of equal quality. In this paper, a datacube model for storing frequent itemsets is described, and then an efficient algorithm of associated operations is proposed. Its application to a real-life dataset is also demonstrated.}, 
keywords={Internet;data mining;data models;access-log data;association rule mining;data mining;datacube model;human decision-maker;interactive Web-log mining;itemset cube;n-dimensional space;Association rules;Data mining;Databases;Electronic mail;Engines;Humans;Information systems;Itemsets;Time factors;Web server}, 
doi={10.1109/CW.2002.1180909}, 
month={},}
@INPROCEEDINGS{6249939, 
author={M. S. Mushtaq and B. Augustin and A. Mellouk}, 
booktitle={2012 17th European Conference on Networks and Optical Communications}, 
title={Empirical study based on machine learning approach to assess the QoS/QoE correlation}, 
year={2012}, 
pages={1-7}, 
abstract={The appearance of new emerging multimedia services have created new challenges for cloud service providers, which have to react quickly to end-users experience and offer a better Quality of Service (QoS). Cloud service providers should use such an intelligent system that can classify, analyze, and adapt to the collected information in an efficient way to satisfy end-users' experience. This paper investigates how different factors contributing the Quality of Experience (QoE), in the context of video streaming delivery over cloud networks. Important parameters which influence the QoE are: network parameters, characteristics of videos, terminal characteristics and types of users' profiles. We describe different methods that are often used to collect QoE datasets in the form of a Mean Opinion Score (MOS). Machine Learning (ML) methods are then used to classify a preliminary QoE dataset collected using these methods. We evaluate six classifiers and determine the most suitable one for the task of QoS/QoE correlation.}, 
keywords={cloud computing;learning (artificial intelligence);multimedia communication;pattern classification;quality of service;video streaming;cloud network;cloud service provider;intelligent system;machine learning approach;mean opinion score;multimedia service;network parameter;preliminary QoE dataset classification;quality of experience correlation;quality of service correlation;terminal characteristics;video characteristics;video streaming delivery;Atmospheric measurements;Delay;Fires;Indexes;Particle measurements;Quality of service;Data classification models;Machine Learning;QoE;QoS}, 
doi={10.1109/NOC.2012.6249939}, 
month={June},}
@INPROCEEDINGS{7322473, 
author={A. Moran and V. Gadepally and M. Hubbell and J. Kepner}, 
booktitle={2015 IEEE High Performance Extreme Computing Conference (HPEC)}, 
title={Improving Big Data visual analytics with interactive virtual reality}, 
year={2015}, 
pages={1-6}, 
abstract={For decades, the growth and volume of digital data collection has made it challenging to digest large volumes of information and extract underlying structure. Coined `Big Data', massive amounts of information has quite often been gathered inconsistently (e.g from many sources, of various forms, at different rates, etc.). These factors impede the practices of not only processing data, but also analyzing and displaying it in an efficient manner to the user. Many efforts have been completed in the data mining and visual analytics community to create effective ways to further improve analysis and achieve the knowledge desired for better understanding. Our approach for improved big data visual analytics is two-fold, focusing on both visualization and interaction. Given geo-tagged information, we are exploring the benefits of visualizing datasets in the original geospatial domain by utilizing a virtual reality platform. After running proven analytics on the data, we intend to represent the information in a more realistic 3D setting, where analysts can achieve an enhanced situational awareness and rely on familiar perceptions to draw in-depth conclusions on the dataset. In addition, developing a human-computer interface that responds to natural user actions and inputs creates a more intuitive environment. Tasks can be performed to manipulate the dataset and allow users to dive deeper upon request, adhering to desired demands and intentions. Due to the volume and popularity of social media, we developed a 3D tool visualizing Twitter on MIT's campus for analysis. Utilizing emerging technologies of today to create a fully immersive tool that promotes visualization and interaction can help ease the process of understanding and representing big data.}, 
keywords={Big Data;data visualisation;human computer interaction;social networking (online);virtual reality;Big Data visual analytics;Twitter;data interaction;data processing;data visualization;digital data collection;geo-tagged information;human-computer interface;interactive virtual reality;social media;user actions;Big data;Data visualization;Games;Solid modeling;Three-dimensional displays;Twitter;Virtual reality;3D;Twitter;big data;data analysis;geospatial;situational awareness;user interaction;virtual reality;visual analytics;visualization}, 
doi={10.1109/HPEC.2015.7322473}, 
month={Sept},}
@INPROCEEDINGS{7307731, 
author={H. Hao and Z. Li and H. Yu}, 
booktitle={2015 International Symposium on Theoretical Aspects of Software Engineering}, 
title={An Effective Approach to Measuring and Assessing the Risk of Android Application}, 
year={2015}, 
pages={31-38}, 
abstract={As the most popular platform, Android dominates the mobile device market. In order to enrich the functions of the phone and facilitate the utilization of users, more and more Android applications have been developed. Unfortunately, a greatly increasing amount of malware targeting the Android platform mingle with the numerous benign applications and hide in almost every market, even the official market Google Play. Therefore, it is a pressing concern about how to measure and assess the risk of such apps. In this paper, we propose a novel approach to deal with this problem. First of all, through the empirical analysis with market-scale dataset, we verify the following fact: for a set of benign applications in the same category, the type and number of permissions they request are similar and consistent in general. Hence, for the benign applications in each category, we can construct a standard permission vector model, which can be used as a baseline to measure and assess the risk of applications in the category. For a downloaded app, we extract its requested permissions to form a permission vector, whose deviation from the baseline can be calculated by employing Euclidean distance and weighted Euclidean distance. The deviation can be used as metric to measure and assess the risk of the app. Finally, an experiment on real-world dataset, consisting of 7737 market apps and 1260 malware samples, is conducted to evaluate our method. The empirical result validates the effectiveness of our approach to help users understand the risk when they decide to install an app.}, 
keywords={Android (operating system);invasive software;risk management;vectors;Android application;Google Play;malware;risk assessment;standard permission vector model;weighted Euclidean distance;Androids;Feature extraction;Google;Humanoid robots;Malware;Smart phones;Standards;Android application;risk;smartphone}, 
doi={10.1109/TASE.2015.16}, 
month={Sept},}
@INPROCEEDINGS{5694057, 
author={G. Li and S. C. H. Hoi and K. Chang and R. Jain}, 
booktitle={2010 IEEE International Conference on Data Mining}, 
title={Micro-blogging Sentiment Detection by Collaborative Online Learning}, 
year={2010}, 
pages={893-898}, 
abstract={We study the online micro-blog sentiment detection problem, which aims to determine whether a micro-blog post expresses emotions. This problem is challenging because a micro-blog post is very short and individuals have distinct ways of expressing emotions. A single classification model trained on the entire corpus may fail to capture characteristics unique to each user. On the other hand, a personalized model for each user may be inaccurate due to the scarcity of training data, especially at the very beginning where users have just posted a few entries. To overcome these challenges, we propose learning a global model over all micro-bloggers, which is then leveraged to continuously refine the individual models through a collaborative online learning way. We evaluate our algorithm on a real-life micro-blog dataset collected from the popular micro-blog site - Twitter. Results show that our algorithm is effective and efficient for timely sentiment detection in real micro-blogging applications.}, 
keywords={Web sites;computer aided instruction;data mining;distance learning;groupware;pattern classification;Twitter;collaborative online learning;data mining;microblog post;microblog site;online microblog sentiment detection problem;real life microblog dataset;training data;classification;data mining;mining methods and algorithms}, 
doi={10.1109/ICDM.2010.139}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{6374816, 
author={A. L. D. Rossi and A. C. P. L. F. Carvalho and C. Soares}, 
booktitle={2012 Brazilian Symposium on Neural Networks}, 
title={Meta-Learning for Periodic Algorithm Selection in Time-Changing Data}, 
year={2012}, 
pages={7-12}, 
abstract={When users have to choose a learning algorithm to induce a model for a given dataset, a common practice is to select an algorithm whose bias suits the data distribution. In real-world applications that produce data continuously this distribution may change over time. Thus, a learning algorithm with the adequate bias for a dataset may become unsuitable for new data following a different distribution. In this paper we present a meta-learning approach for periodic algorithm selection when data distribution may change over time. This approach exploits the knowledge obtained from the induction of models for different data chunks to improve the general predictive performance. It periodically applies a meta-classifier to predict the most appropriate learning algorithm for new unlabeled data. Characteristics extracted from past and incoming data, together with the predictive performance from different models, constitute the meta-data, which is used to induce this meta-classifier. Experimental results using data of a travel time prediction problem show its ability to improve the general performance of the learning system. The proposed approach can be applied to other time-changing tasks, since it is domain independent.}, 
keywords={learning (artificial intelligence);meta data;pattern classification;data chunks;data distribution;general predictive performance;meta-classifier;meta-data;meta-learning algorithm;model induction;periodic algorithm selection;time-changing data;travel time prediction problem;Algorithm design and analysis;Data models;Heuristic algorithms;Prediction algorithms;Predictive models;Support vector machines;Training;Learning algorithm selection;Meta-learning;Time-changing data}, 
doi={10.1109/SBRN.2012.50}, 
ISSN={1522-4899}, 
month={Oct},}
@INPROCEEDINGS{7780416, 
author={N. Xu and B. Price and S. Cohen and J. Yang and T. Huang}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Deep Interactive Object Selection}, 
year={2016}, 
pages={373-381}, 
abstract={Interactive object selection is a very important research problem and has many applications. Previous algorithms require substantial user interactions to estimate the foreground and background distributions. In this paper, we present a novel deep-learning-based algorithm which has much better understanding of objectness and can reduce user interactions to just a few clicks. Our algorithm transforms user-provided positive and negative clicks into two Euclidean distance maps which are then concatenated with the RGB channels of images to compose (image, user interactions) pairs. We generate many of such pairs by combining several random sampling strategies to model users' click patterns and use them to finetune deep Fully Convolutional Networks (FCNs). Finally the output probability maps of our FCN-8s model is integrated with graph cut optimization to refine the boundary segments. Our model is trained on the PASCAL segmentation dataset and evaluated on other datasets with different object classes. Experimental results on both seen and unseen objects demonstrate that our algorithm has a good generalization ability and is superior to all existing interactive object selection approaches.}, 
keywords={generalisation (artificial intelligence);image colour analysis;image sampling;image segmentation;learning (artificial intelligence);probability;random processes;visual databases;Euclidean distance maps;FCN-8s model;PASCAL segmentation dataset;RGB image channels;background distribution estimation;boundary segments;deep-fully-convolutional network finetuning;deep-interactive object selection;deep-learning-based algorithm;foreground distribution estimation;generalization ability;graph cut optimization;object classes;output probability maps;random sampling strategies;seen objects;unseen objects;user click pattern modelling;user interaction reduction;user-provided negative clicks;user-provided positive clicks;Euclidean distance;Image segmentation;Machine learning;Manganese;Optimization;Semantics;Training}, 
doi={10.1109/CVPR.2016.47}, 
month={June},}
@ARTICLE{6930802, 
author={B. K. Samanthula and Y. Elmehdwi and W. Jiang}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={k-Nearest Neighbor Classification over Semantically Secure Encrypted Relational Data}, 
year={2015}, 
volume={27}, 
number={5}, 
pages={1261-1273}, 
abstract={Data Mining has wide applications in many areas such as banking, medicine, scientific research and among government agencies. Classification is one of the commonly used tasks in data mining applications. For the past decade, due to the rise of various privacy issues, many theoretical and practical solutions to the classification problem have been proposed under different security models. However, with the recent popularity of cloud computing, users now have the opportunity to outsource their data, in encrypted form, as well as the data mining tasks to the cloud. Since the data on the cloud is in encrypted form, existing privacy-preserving classification techniques are not applicable. In this paper, we focus on solving the classification problem over encrypted data. In particular, we propose a secure k-NN classifier over encrypted data in the cloud. The proposed protocol protects the confidentiality of data, privacy of user's input query, and hides the data access patterns. To the best of our knowledge, our work is the first to develop a secure k-NN classifier over encrypted data under the semi-honest model. Also, we empirically analyze the efficiency of our proposed protocol using a real-world dataset under different parameter settings.}, 
keywords={cloud computing;cryptography;data mining;data privacy;outsourcing;pattern classification;relational databases;cloud computing;data confidentiality;data mining applications;data outsourcing;encrypted relational data;k-nearest neighbor classification;privacy issues;privacy-preserving classification techniques;Data mining;Encryption;Protocols;Vectors;Zinc;Encryption;Outsourced Databases;Security;encryption;k-NN Classifier;k-NN classifier;outsourced databases}, 
doi={10.1109/TKDE.2014.2364027}, 
ISSN={1041-4347}, 
month={May},}
@INPROCEEDINGS{5360869, 
author={Peng Liu and Fang Liu and Yinan Dou and Zhenming Lei}, 
booktitle={2009 IEEE International Conference on Network Infrastructure and Digital Content}, 
title={Classification model of network users based on optimized LDA and entropy}, 
year={2009}, 
pages={149-154}, 
abstract={The classification of network users is very important in user behavior analysis. The algorithm which was based entropy and latent Dirichlet allocation (LDA) was used in this paper. It is important but difficult to select an appropriate number of topics for a specific dataset. Entropy was first used to solve the problem. A concept named difference-entropy was built to determine the number of topics. Experiments show that the proposed method can achieve performance matching the best of LDA without manually tuning the number of topics.}, 
keywords={Internet;behavioural sciences computing;entropy;optimisation;entropy;latent Dirichlet allocation;network users classification model;optimized LDA;user behavior analysis;Computer networks;Data processing;Entropy;Inference algorithms;Information analysis;Large-scale systems;Linear discriminant analysis;Machine learning;Machine learning algorithms;Network topology;LDA;classify;entropy;network users}, 
doi={10.1109/ICNIDC.2009.5360869}, 
ISSN={2374-0272}, 
month={Nov},}
@ARTICLE{7474338, 
author={F. Xu and Y. Li and M. Chen and S. Chen}, 
journal={IEEE Network}, 
title={Mobile cellular big data: linking cyberspace and the physical world with social ecology}, 
year={2016}, 
volume={30}, 
number={3}, 
pages={6-12}, 
abstract={Understanding mobile big data, inherent within large-scale cellular towers in the urban environment, is extremely valuable for service providers, mobile users, and government managers of the modern metropolis. By extracting and modeling the mobile cellular data associated with over 9600 cellular towers deployed in a metropolitan city of China, this article aims to link cyberspace and the physical world with social ecology via such big data. We first extract a human mobility and cellular traffic consumption trace from the dataset, and then investigate human behavior in cyberspace and the physical world. Our analysis reveals that human mobility and the consumed mobile traffic have strong correlations, and both have distinct periodical patterns in the time domain. In addition, both human mobility and mobile traffic consumption are linked with social ecology, which in turn helps us to better understand human behavior. We believe that the proposed big data processing and modeling methodology, combined with the empirical analysis on mobile traffic, human mobility, and social ecology, paves the way toward a deep understanding of human behaviors in a large-scale metropolis.}, 
keywords={Big Data;mobile computing;time-domain analysis;cellular traffic consumption;human mobility;large-scale cellular towers;mobile cellular Big Data;social ecology;time domain;Big data;Cellular networks;Cyberspace;Data visualization;Ecology;Mobile communication;Poles and towers;Urban areas}, 
doi={10.1109/MNET.2016.7474338}, 
ISSN={0890-8044}, 
month={May},}
@INPROCEEDINGS{6102478, 
author={J. Liu and E. T. Brown and R. Chang}, 
booktitle={2011 IEEE Conference on Visual Analytics Science and Technology (VAST)}, 
title={Find distance function, hide model inference}, 
year={2011}, 
pages={289-290}, 
abstract={Faced with a large, high-dimensional dataset, many turn to data analysis approaches that they understand less well than the domain of their data. An expert's knowledge can be leveraged into many types of analysis via a domain-specific distance function, but creating such a function is not intuitive to do by hand. We have created a system that shows an initial visualization, adapts to user feedback, and produces a distance function as a result. Specifically, we present a multidimensional scaling (MDS) visualization and an iterative feedback mechanism for a user to affect the distance function that informs the visualization without having to adjust the parameters of the visualization directly. An encouraging experimental result suggests that using this tool, data attributes with useless data are given low importance in the distance function.}, 
keywords={data analysis;data visualisation;data analysis approach;domain-specific distance function;iterative feedback mechanism;model inference;multidimensional scaling visualization;Analytical models;Computational modeling;Data models;Data visualization;Stress;Vectors;Visual analytics}, 
doi={10.1109/VAST.2011.6102478}, 
month={Oct},}
@INPROCEEDINGS{6921604, 
author={B. Yang and S. Manandhar}, 
booktitle={2014 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2014)}, 
title={Exploring user expertise and descriptive ability in community question answering}, 
year={2014}, 
pages={320-327}, 
abstract={The research on community question answering (CQA) has been paid increasing attention in recent years. In CQA, to reduce the number of unanswered questions and the time for askers to wait, it is very necessary to identify relevant experts or best answers for these questions. Generally, the experts' answers are more likely to be the best answers. Existing studies considered that user expertise is reflected by the voting scores of both answers and questions. However, voting scores of questions are not really related to user expertise. In this paper, we proposed a new probabilistic model to depict users' expertise based on answers and their descriptive ability based on questions. To exploit social information in CQA, the link analysis is also considered. Extensive experiments on the large Stack Overflow dataset demonstrate that our methods can achieve comparable or even better performance than the state-of-the-art models.}, 
keywords={probability;question answering (information retrieval);CQA;community question answering;descriptive ability;large Stack Overflow dataset;link analysis;probabilistic model;social information;user expertise;Communities;Conferences;Gaussian distribution;Knowledge discovery;Social network services;Training;Vocabulary}, 
doi={10.1109/ASONAM.2014.6921604}, 
month={Aug},}
@INPROCEEDINGS{7073079, 
author={M. U. Asad and N. Afroz and L. Dey and R. P. D. Nath and M. A. Azim}, 
booktitle={2014 17th International Conference on Computer and Information Technology (ICCIT)}, 
title={Introducing active learning on Text to Emotion Analyzer}, 
year={2014}, 
pages={35-40}, 
abstract={Now-a-days, online interpersonal communications have become more preferable than face-to-face interactions. However, emotions play a significant role in online communication. Automatic extraction of emotions from the text is a hot research issue because it minimizes the communication gap and misunderstanding between users. To become emotionally more intelligent, our previous text to emotion analyzing system should communicate with experts for suggestions of possible emotional state if it fails to analyze the text. In this research, we augment our previous system by introducing active learning approach which allows to query experts for emotional label of the given text. It makes our training dataset enriched. To build a classification model by analyzing the training dataset, we employ Naive Bayes classification technique. Our classifier updates the emotional database automatically. We also develop a prototype of our system named TEA: Text-to-Emotion-Analyzer. Our experiment and evaluation section exhibits satisfactory results in terms of recall-precision over our previous system as well as other method namely Vector Space Model (VSM).}, 
keywords={Bayes methods;database management systems;emotion recognition;learning (artificial intelligence);pattern classification;text analysis;Automatic emotions extraction;Naive Bayes classification technique;active learning approach;classification model;emotion analyzing system;emotional database;online interpersonal communications;text-to-emotion analyzer;training dataset;Computer architecture;Computers;Databases;Information technology;Speech;Vectors;XML;Affective Computing;Emotion Extraction;Intelligent Chat Messenger;Machine Learning;Sentiment Analysis;TEA: Text-to-Emotion-Analyzer}, 
doi={10.1109/ICCITechn.2014.7073079}, 
month={Dec},}
@INPROCEEDINGS{6483108, 
author={M. H. Aghdam and M. Analoui and P. Kabiri}, 
booktitle={6th International Symposium on Telecommunications (IST)}, 
title={Application of nonnegative matrix factorization in recommender systems}, 
year={2012}, 
pages={873-876}, 
abstract={Recommender systems actively collect various kinds of data in order to generate their recommendations. Collaborative filtering is based on collecting and analyzing information on users' preferences and estimating what users will like based on their similarity to other users. However, most of current collaborative filtering methods often suffer from two problems: sparsity and scalability. This paper proposes a framework for collaborative filtering by applying nonnegative matrix factorization, which alleviates the problems via matrix factorization. Experimental results on benchmark dataset are presented to show that our method is indeed more tolerant against both sparsity and scalability, and obtains good performance in the meanwhile.}, 
keywords={collaborative filtering;matrix decomposition;recommender systems;benchmark dataset;collaborative filtering;nonnegative matrix factorization;recommender systems;Collaboration;Computational modeling;Cost function;Recommender systems;Semantics;Vectors;collaborative filtering;matrix factorization;recommender sysem}, 
doi={10.1109/ISTEL.2012.6483108}, 
month={Nov},}
@INPROCEEDINGS{7820271, 
author={G. P. Gibilisco and M. Li and L. Zhang and D. Ardagna}, 
booktitle={2016 IEEE 9th International Conference on Cloud Computing (CLOUD)}, 
title={Stage Aware Performance Modeling of DAG Based in Memory Analytic Platforms}, 
year={2016}, 
pages={188-195}, 
abstract={Spark has grown both in popularity and complexity in recent years. In order to use available resources in an efficient way, users need to understand how the behavior of their applications is affected by the size of the datasets and various configuration settings. Indeed, Spark allows users to specify many configuration parameters and understanding the impact of these choices with respect to the application execution time is not easy. An accurate estimate of application execution time is important for cluster capacity planning and/or runtime scheduling. In this work we propose a gray-box approach to analyze the performance of Spark applications deployed in public cloud infrastructures. The approach is divided into two phases: during application profiling, the application is executed multiple times against different subsets of the input datasets to understand the effect of the data size on the execution time and its dependency on the main configuration parameters. Next, during the estimation phase, we use the data gathered in the first step to predict the execution time of the application, run against the entire dataset. The prediction approach builds several models in order to estimate separately the growth of the time required to execute each stage within the application. Finally, the DAG used by Spark to schedule the execution of stages is analyzed to aggregate the predictions of the stages execution times into the overall application execution time. Both phases are supported by our SLAP open source tool. Experimental results show that our model can effectively and accurately predict application execution time. The approach outperforms pure black-box polynomial regression methods obtaining 1-3% relative error.}, 
keywords={cloud computing;directed graphs;performance evaluation;processor scheduling;public domain software;storage management;DAG;SLAP open source tool;Spark applications;application execution time estimation;application profiling;cluster capacity planning;data size effect;directed acyclic graph;memory analytic platforms;public cloud infrastructures;runtime scheduling;stage aware performance modeling;Analytical models;Computational modeling;Data models;Estimation;Predictive models;Runtime;Sparks;Big Data;Performance models;Spark}, 
doi={10.1109/CLOUD.2016.0034}, 
month={June},}
@INPROCEEDINGS{7120634, 
author={X. Wang and Y. Jia and R. Chen and B. Zhou}, 
booktitle={2015 2nd International Conference on Information Science and Control Engineering}, 
title={Ranking User Tags in Micro-Blogging Website}, 
year={2015}, 
pages={400-403}, 
abstract={Users can annotate themselves using free tags in micro-blogging website such as Sina Weibo. The tags of a user demonstrate the characteristics of the user and are generally in a random order without any importance or relevance information. It limits the effectiveness of user tags in system recommendation and other applications. In this paper, we proposed a user tag ranking schema which is based on interactive relations between users. Influence strength between users is considered in our user tag ranking method. Relevance scores between tags and users are also utilized to rank user tags. Experiments are conducted on distributed processing framework Hadoop to process the very large Sina Weibo dataset which contains more than 140 million users. Experimental results show that our method outputs frequently used method and gives good performance.}, 
keywords={Web sites;data handling;parallel processing;recommender systems;Hadoop;Sina Weibo;distributed processing framework;microblogging Website;system recommendation;user tag ranking method;Collaboration;Distributed databases;Mathematical model;Media;Servers;Social network services;Tagging;Micro-blogging;Sina Weibo;Social Network;Tag Ranking}, 
doi={10.1109/ICISCE.2015.94}, 
month={April},}
@INPROCEEDINGS{6755917, 
author={H. Lu and G. Jiang and B. Yang and X. Xue}, 
booktitle={2013 IEEE International Conference on Computer Vision Workshops}, 
title={An Adaptive Query Prototype Modeling Method for Image Search Reranking}, 
year={2013}, 
pages={339-346}, 
abstract={As more and more images with user free tags are appearing on the Internet, image search reranking has received considerable attention to help users obtain images relevant to the query. In this paper, we propose to rerank the initial image search results using an adaptive online query modeling method based on local features. For a query and its initial rank list, we construct a visual word dictionary using the randomly selected images in the initial rank list. Then the top N images are incrementally used to evaluate the importance/score of the words for the query. The words with higher scores are selected to model the query. The relevance scores of the words to the query are also kept. Then the model is used to measure the relevance of each image in the image list to the query and rerank the image list according to the measured relevance. This method can obtain optimal local prototype for a query from the top N images (N is not large) without considering many images. Also, the relevances of the top N images to the query are not considered equally by weighting according to their positions in the initial image list. Using this model, the influences from some wrongly returned images at the top of the initial rank list can be filtered out. This method is also query independent and can be used to any other query. Experimental results on a large scale image dataset of Web Queries demonstrate the efficacy of the proposed method.}, 
keywords={Internet;image retrieval;Internet;Web queries;adaptive online query modeling method;adaptive query prototype modeling method;image list reranking;image search reranking;relevance measurement;user free tags;visual word dictionary;Adaptation models;Dictionaries;Internet;Prototypes;Search engines;Training;Visualization;Image search reranking;adaptive prototype modeling;object and scene}, 
doi={10.1109/ICCVW.2013.52}, 
month={Dec},}
@INPROCEEDINGS{6392598, 
author={D. Han and E. Stroulia}, 
booktitle={2012 IEEE 6th International Workshop on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems (MESOCA)}, 
title={A three-dimensional data model in HBase for large time-series dataset analysis}, 
year={2012}, 
pages={47-56}, 
abstract={In the transition of applications from the traditional enterprise infrastructures to cloud infrastructures, scalable database management system plays an important role in efficiently managing and analysing unprecedented massive amount of data. Compared to RDBMSs, NoSQL databases, are more attractive in addressing this challenge. However, it is not easy to manage data in NoSQL database effectively for non-expert users because of the rare data-organization support. A poor data organization may accidentally abuse the features of NoSQL database and achieve unsatisfactory performance. Therefore, a systematic method for NoSQL database data-schema design is a timely and important problem for researchers and practitioners. HBase, as a particular NoSQL database offering, relies (a) on HDFS, for its distributed and replicated storage, and (b) on coprocessors, for efficient parallel query processing. To harness the potential parallelism benefits, an appropriate partitioning of the data across the HBase storage is required. we investigate the effectiveness of the three-dimensional data model, which uses the “version” dimension of HBase to store the values of a data item over time. We have experimented and evaluated the performance impact of this type of data model with two data sets, of different sizes and different time lengths. For each of these data sets, we have compared the performance of several ad-hoc queries, implemented with HBase Coprocessors framework, across different data schemas, some of which (do not) use the third HBase dimension. The experiment results demonstrate improved performance with the data schemas that use the third dimension of HBase.}, 
keywords={coprocessors;data analysis;data models;distributed databases;file organisation;parallel processing;time series;HBase;HBase coprocessor framework;NoSQL database data-schema design;RDBMS;ad-hoc queries;cloud infrastructures;data analysis;data partitioning;data schemas;data-organization support;database management system;distributed storage;enterprise infrastructures;parallel query processing;replicated storage;three-dimensional data model;time-series dataset analysis;Conferences;Coprocessors;Data models;Databases;Maintenance engineering;Measurement;Organizations;Coprocessor;Data Model;Data Schema;HBase;Time-Series Dataset}, 
doi={10.1109/MESOCA.2012.6392598}, 
ISSN={2326-6910}, 
month={Sept},}
@INPROCEEDINGS{6449507, 
author={L. Xiong and Y. Xiang and Q. Zhang and L. Lin}, 
booktitle={2012 Third Global Congress on Intelligent Systems}, 
title={A Novel Nearest Neighborhood Algorithm for Recommender Systems}, 
year={2012}, 
pages={156-159}, 
abstract={Traditional k-nearest neighborhood (KNN) model is being widely used in the recommender systems. However, it behaves badly without enough history records for new users, called the cold starting problem. Both time and space complexity are huge for computing all pair wise similarities among items or users. A mixed neighborhood algorithm is proposed for treating new users and old users separately. For new users, this paper takes into account users' characteristics. For old users, combined with Singular Value Decomposition (SVD), we reduce the time and space complexity efficiently. Experiment on Movie Lens dataset shows that the proposed model can solve the cold starting problem in effect and remarkably improve the accuracy of traditional model and lower time consuming level.}, 
keywords={collaborative filtering;computational complexity;recommender systems;singular value decomposition;KNN model;MovieLens dataset;SVD;cold starting problem;k-nearest neighborhood model;mixed neighborhood algorithm;pairwise similarities;recommender systems;singular value decomposition;space complexity reduction;time complexity reduction;Accuracy;Collaboration;Computational modeling;Predictive models;Presses;Recommender systems;Training;K-means;Singular Value Decomposition;k-nearest Neighborhood;recommender system;similarity measure}, 
doi={10.1109/GCIS.2012.58}, 
ISSN={2155-6083}, 
month={Nov},}
@INPROCEEDINGS{7916789, 
author={J. Vijayaraj and R. Saravanan and P. Victer Paul and R. Raju}, 
booktitle={2016 Online International Conference on Green Engineering and Technologies (IC-GET)}, 
title={Hadoop security models - a study}, 
year={2016}, 
pages={1-5}, 
abstract={The new emerging technology to handle large number of dataset in an efficient manner is the big data which is used in various different platforms and domain to support several services and improve the system performance in a reliable manner. The main weakness in this domain is the security which can be easily destroyed or surpassed by the user. To enhance it in a protected way this paper provides a detailed survey about the various security mechanisms and methodologies used in the big data technique. These security measures should satisfy basic parameters such as Authentication, Authorization and confidentiality so that the system is stronger against different attacks and threats. Various different algorithms and technique can be proposed to improve the system protection and enables a reliable platform in the storage of datasets.}, 
keywords={Big Data;parallel processing;security of data;Big Data technique;Hadoop security models;authentication parameter;authorization parameter;confidentiality parameter;dataset handling;security mechanisms;Authentication;Big Data;Cloud computing;Data models;Encryption;Tools;Authentication;Authorization;Big data;Confidentiality;Dataset;Security}, 
doi={10.1109/GET.2016.7916789}, 
month={Nov},}
@INPROCEEDINGS{7562162, 
author={S. Faye and N. Louveton and G. Gheorghe and T. Engel}, 
booktitle={2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
title={Toward a characterization of human activities using smart devices: A micro/macro approach}, 
year={2016}, 
pages={676-681}, 
abstract={The emergence of new connected devices has opened up new opportunities and allowed to imagine concepts that bring computer sciences and social sciences closer together. In particular, today's increasingly sophisticated miniature sensors allow to track and understand human activities and behavior with a great precision. Taking different approaches and perspectives, we use in this paper smartwatches and smartglasses to explore these behaviors and show that these objects, considered by many as gadgets, have an important role to play in understanding the lives of individuals. The main objective of this work is to introduce two new scales of activity detection, which lacks a formal and consistent definition in the literature. First, we propose a model that precisely detects and interprets movements made by a person wearing smart devices. Then, we use this model to show different interactions between those micro-activities and bigger chunks of behaviors we call macro-activities. Using a new concept based on 3D visualization, we finally show that combining those two scales and using a limited dataset, it is possible to distinguish between different individuals when they are performing very similar activities. The findings of this study lead the way to enhanced user profiling.}, 
keywords={data visualisation;intelligent sensors;motion measurement;3D visualization;activity detection;human activity characterization;microactivity detection;smart device;smart glass;smart watch;user profiling;Accelerometers;Biomedical monitoring;Bluetooth;Gyroscopes;Sensors;Smart devices;Time series analysis;Activity and Mobility Detection;Mobile Computing;Sensing Systems;User Profiling}, 
doi={10.1109/INFCOMW.2016.7562162}, 
month={April},}
@INPROCEEDINGS{4063738, 
author={J. Li and Z. Chen and W. Xu}, 
booktitle={Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06)}, 
title={Bounded Support Vector Machines, Semidefinite}, 
year={2006}, 
pages={818-822}, 
abstract={Credit risk assessment is a basic and critical factor in credit risk management. In addition to conventional statistical method, neural network, decision tree and Support Vector Machine are the popular methods in this field in recent years. However, they all have weakness in two aspects: poor classification accuracy for unbalanced data and poor interpretability in real applications. A novel method, called Least Squares Support Feature Machine (LS-SFM), is proposed to reduce the misclassification cost and achieve an interpretable model by introducing the single feature kernel and sampling method. One important character of LS-SFM is that it can deliver the significance of each feature to users Our experiment on a real credit card dataset shows good performance. LS-SFM outperforms some well-known methods in several aspects.}, 
keywords={Costs;Decision trees;Kernel;Least squares methods;Neural networks;Risk management;Sampling methods;Statistical analysis;Support vector machine classification;Support vector machines;Credit Assessment;Feature Selection;Least Squares Support Feature Machine;Support Vector Machine}, 
doi={10.1109/ICDMW.2006.37}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{6831993, 
author={X. Wang and J. Liu and B. Cao and M. Tang}, 
booktitle={2013 IEEE 10th International Conference on High Performance Computing and Communications 2013 IEEE International Conference on Embedded and Ubiquitous Computing}, 
title={A Global Optimal Service Selection Approach Based on QoS and Load-Aware in Cloud Environment}, 
year={2013}, 
pages={762-768}, 
abstract={The global optimal Web service selection based on quality of service (QoS) in cloud environment has become a research focus when there are lots of the same or similar services. In this environment, it is possible that many service users request the same or similar services at the same time, which will result in users' unsatisfied requirement and services' load imbalance. The existing service selection approaches usually suppose that service's load capacity is infinite and user always select the service with the best expected QoS despite the amount of user requests. Therefore, it is very important problem how to get a tradeoff between the users' satisfied requirement and service load balance. To solve this problem, this paper presents a global optimal service selection approach based on QoS and load-aware in cloud environment. In this approach, we first build a user QoS utility model which describes the relationship between user's request and services' QoS, and design a service's load capacity model to achieve the load capacity of a service. Then, we use 0-1 integer programming to build a global optimal model based on QoS utility of users and services' load capacity, and provide the optimal service selection algorithm for users. Finally, by conducting large-scale experiments based on a Web service dataset, we show that our approach can effectively help users to select high qualified services while keeping load balance of services in cloud environment.}, 
keywords={Web services;cloud computing;integer programming;quality of service;resource allocation;0-1 integer programming;QoS utility model;Web service dataset;cloud environment;global optimal Web service selection;load-aware;optimal service selection algorithm;quality of service;service QoS;service load balance;service load capacity model;service user request;Cloud computing;Computational modeling;Heuristic algorithms;Linear programming;Load modeling;Quality of service;0-1 integer programming;QoS;Web service selection;cloud computing;load capacity}, 
doi={10.1109/HPCC.and.EUC.2013.111}, 
month={Nov},}
@INPROCEEDINGS{7366926, 
author={H. Li and T. Li and Y. Wang}, 
booktitle={2015 IEEE 12th International Conference on Mobile Ad Hoc and Sensor Systems}, 
title={Dynamic Participant Recruitment of Mobile Crowd Sensing for Heterogeneous Sensing Tasks}, 
year={2015}, 
pages={136-144}, 
abstract={With the rapid increasing of smart mobile devices and the advances of sensing technologies, mobile crowd sensing (MCS) becomes a new popular sensing paradigm, which enables a variety of large-scale sensing applications. One of the key challenges of large-scale mobile crowd sensing systems is how to effectively select appropriate participants from a huge user pool to perform various sensing tasks while satisfying certain constraints. This becomes more complex when the sensing tasks are dynamic (coming in real time) and heterogeneous (having different temporal and spacial requirements). In this paper, we consider such a dynamic participant recruitment problem with heterogeneous sensing tasks which aims to minimize the sensing cost while maintaining certain level of probabilistic coverage. Both offline and online algorithms are proposed to solve the challenging problem. Extensive simulations over a real-life mobile dataset confirm the efficiency of the proposed algorithms.}, 
keywords={mobile computing;dynamic participant recruitment problem;heterogeneous sensing tasks;mobile crowd sensing systems;Data models;Heuristic algorithms;Mobile communication;NP-hard problem;Probabilistic logic;Recruitment;Sensors;Coverage;Mobile Crowd Sensing;Optimization;Participant Recruitment}, 
doi={10.1109/MASS.2015.46}, 
month={Oct},}
@INPROCEEDINGS{6853766, 
author={C. Jiang and Y. Chen and K. J. R. Liu}, 
booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Modeling information diffusion dynamics over social networks}, 
year={2014}, 
pages={1095-1099}, 
abstract={Information diffusion over social networks becomes a hot topic recently. Most of the existing works are based on the machine learning method with social network structure analysis and empirical data mining. However, the results learned from some specific dataset may not apply to the future networks, since the social network structure is in a highly dynamic environment. Moreover, the dynamics of information diffusion are also heavily influenced by network users' decisions, actions and their socio-economic interactions, which is generally ignored by existing works. In this paper, we propose an evolutionary game theoretic framework to model the dynamic information diffusion process in social networks, which focuses on the users' behavior analysis from a microeconomics points of view. We also conduct experiments by using real-world Twitter information diffusion dataset, which shows that the proposed evolutionary game theoretic model is effective and practical in modeling the social network users' information diffusion dynamics.}, 
keywords={evolutionary computation;information dissemination;learning (artificial intelligence);social networking (online);socio-economic effects;evolutionary game theoretic framework;highly dynamic environment;information diffusion dynamics modeling;machine learning method;microeconomics view points;network user-decisions;real-world Twitter information diffusion dataset;social network structure analysis;socio-economic interactions;Diffusion processes;Game theory;Games;Sociology;Statistics;Twitter;Social networks;evolutionary game;game theory;information diffusion;information spreading}, 
doi={10.1109/ICASSP.2014.6853766}, 
ISSN={1520-6149}, 
month={May},}
@INPROCEEDINGS{7829813, 
author={Y. De Bock and A. Auquilla and K. Kellens and A. Nowé and J. R. Duflou}, 
booktitle={2016 Electronics Goes Green 2016+ (EGG)}, 
title={Intelligent occupancy-driven thermostat by dynamic user profiling}, 
year={2016}, 
pages={1-8}, 
abstract={Matching system functionality and user needs by learning from user behaviour enables a significant reduction in energy consumption. Habits and routine behaviour are exploited and captured in user profiles to automatically create customized heating schedules. However, over time the user conduct can change either gradually or abruptly and old occupancy patterns could become obsolete. Hence, a self-learning system should be able to cope with these changes and adapt the identified user profiles accordingly. An approach to track changing behaviour and update the corresponding user profiles, and hence heating schedules, is presented. The proposed strategy is evaluated by comparing prediction accuracy and potential energy savings to the case where learning is static and to incremental learning strategies. The results are illustrated by means of a real-life dataset of a single-user office.}, 
keywords={Adaptation models;Clustering algorithms;Heuristic algorithms;Schedules;Space heating;Thermostats}, 
doi={10.1109/EGG.2016.7829813}, 
month={Sept},}
@INPROCEEDINGS{6694005, 
author={A. Raza and L. F. Capretz and F. Ahmed}, 
booktitle={Eighth International Conference on Digital Information Management (ICDIM 2013)}, 
title={Maintenance support in open source software projects}, 
year={2013}, 
pages={391-395}, 
abstract={Easy and mostly free access to the Internet has resulted in the growing use of open source software (OSS). However, it is a common perception that closed proprietary software is still superior in areas such as software maintenance and management. The research model of this study establishes a relationship between maintenance issues (such as user requests and error handling) and support provided by open source software through project forums, mailing lists and trackers. To conduct this research, we have used a dataset consisting of 120 open source software projects, covering a wide range of categories. The results of the study show that project forums and mailing lists play a significant role in addressing user requests in open source software. However according to the empirical investigation, it has been explored that trackers are used as an effective medium for error reporting as well as user requests.}, 
keywords={project management;public domain software;software maintenance;software management;OSS;closed proprietary software;error handling;error reporting;free access;internet;mailing lists;maintenance support;open source software projects;project forums;software management;trackers;user requests;Correlation coefficient;Maintenance engineering;Open source software;Reliability;Software maintenance;Usability;Mailing Lists;Maintenance;Open Source Software (OSS);Project Forums;Trackers}, 
doi={10.1109/ICDIM.2013.6694005}, 
month={Sept},}
@INPROCEEDINGS{6132969, 
author={Y. Zhang and Z. Zheng and M. R. Lyu}, 
booktitle={2011 IEEE 22nd International Symposium on Software Reliability Engineering}, 
title={WSPred: A Time-Aware Personalized QoS Prediction Framework for Web Services}, 
year={2011}, 
pages={210-219}, 
abstract={The exponential growth of Web service makes building high-quality service-oriented applications an urgent and crucial research problem. User-side QoS evaluations of Web services are critical for selecting the optimal Web service from a set of functionally equivalent service candidates. Since QoS performance of Web services is highly related to the service status and network environments which are variable against time, service invocations are required at different instances during a long time interval for making accurate Web service QoS evaluation. However, invoking a huge number of Web services from user-side for quality evaluation purpose is time-consuming, resource-consuming, and sometimes even impractical (e.g., service invocations are charged by service providers). To address this critical challenge, this paper proposes a Web service QoS prediction framework, called WSPred, to provide time-aware personalized QoS value prediction service for different service users. WSPred requires no additional invocation of Web services. Based on the past Web service usage experience from different service users, WSPred builds feature models and employs these models to make personalized QoS prediction for different users. The extensive experimental results show the effectiveness and efficiency of WSPred. Moreover, we publicly release our real-world time-aware Web service QoS dataset for future research, which makes our experiments verifiable and reproducible.}, 
keywords={Web services;program verification;quality of service;WSPred;Web service QoS prediction framework;Web service usage experience;feature model;high quality service oriented application;quality evaluation;real world time aware Web service QoS dataset;service invocation;time aware personalized QoS prediction framework;Collaboration;Feature extraction;Monitoring;Quality of service;Tensile stress;Web services;QoS Prediction;Time-Aware;Web Service}, 
doi={10.1109/ISSRE.2011.17}, 
ISSN={1071-9458}, 
month={Nov},}
@INPROCEEDINGS{7444469, 
author={P. Orduña and L. Rodriguez-Gil and J. Garcia-Zubia and O. Dziabenko and I. Angulo and U. Hernandez and E. Azcuenaga}, 
booktitle={2016 13th International Conference on Remote Engineering and Virtual Instrumentation (REV)}, 
title={Classifying online laboratories: Reality, simulation, user perception and potential overlaps}, 
year={2016}, 
pages={224-230}, 
abstract={Students of technological fields must practice so as to properly learn a particular field. There are different ways to practice: hands-on-lab in a real environment or a mockup, datasets (and tools for analyzing these datasets), or simulations. Each solution provides different advantages and disadvantages. For example, students might not prefer simulations since they do not always provide accurate real values (and when testing in a real laboratory results differ and the engagement might be higher), but they might be more affordable than real laboratories (depending on the field, there might not be any other affordable solution than a simulation). Datasets of recorded measurements are an equidistant point, where costs are lower and data is real, but no interaction is performed by the users with the reality. When creating remote laboratories, a system that enables students access the final equipment is usually used, but this might not be the best option. Sometimes, every potential input could be recorded and used in the future as a dataset to let users access this laboratory in a scalable way, and hybrid solutions could also be achieved. The focus of this contribution is to classify online laboratories from this perspective.}, 
keywords={Internet;virtual instrumentation;hands-on-lab;online laboratories;potential overlaps;real environment;reality;simulation;user perception;Data models;Internet;Maintenance engineering;Physics;Remote laboratories;Robots}, 
doi={10.1109/REV.2016.7444469}, 
month={Feb},}
@INPROCEEDINGS{7230384, 
author={J. Huang and W. Zhou and H. Li and W. Li}, 
booktitle={2015 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP)}, 
title={Sign language recognition using real-sense}, 
year={2015}, 
pages={166-170}, 
abstract={Sign Language Recognition (SLR) targets on facilitating the communication between deaf-mute people and ordinary people. This task is very challenging due to the complexity and large variations in hand postures. Some methods require user wear sensor gloves which can detect the position and angle of finger articulations. Others use RGB-D camera like Kinect to track hands and rely on complex algorithms to segment hands from background. However, all these methods have its own disadvantages. Sensor-based methods are not natural as the user must wear cumbersome instruments while camera-based methods have to design extra algorithms to track and segment hands from complex background. To address these problems, we propose a novel method for SLR which involves the use of the Real-Sense. It is a camera device which can detect and track the location of hands in a natural way. More powerful, it provides the 3D coordinates of finger joints in real time. We build a deep neural network (DNN) based on Real-Sense to recognize different signs. The DNN takes the 3D coordinates of finger joints as input directly without using any handcrafted features. The reason is that DNN, as a deep model, is capable of learning suitable features for recognition from raw data. In experiment, to demonstrate the effectiveness of Real-Sense, we collect two datasets by Real-Sense and Kinect respectively, then build DNNs based on each dataset for recognition. To validate the powerfulness of DNN, we compare the performance of DNN and support vector machine (SVM) on the same dataset.}, 
keywords={cameras;data gloves;neural nets;object detection;object tracking;sign language recognition;3D coordinates;3D finger joint coordinates;DNN;Kinect;RGB-D camera;SLR;SVM;angle of finger articulations;deep neural network;hand tracking;position detection;real-sense;sensor gloves;sensor-based methods;sign language recognition;support vector machine;Assistive technology;Gesture recognition;Image recognition;Neural networks;Support vector machines;Three-dimensional displays;Training;Real-Sense;Sign language recognition;deep neural network}, 
doi={10.1109/ChinaSIP.2015.7230384}, 
month={July},}
@INPROCEEDINGS{7033684, 
author={T. Kandappu and A. Friedman and R. Boreli and V. Sivaraman}, 
booktitle={2014 IEEE 22nd International Symposium on Modelling, Analysis Simulation of Computer and Telecommunication Systems}, 
title={PrivacyCanary: Privacy-Aware Recommenders with Adaptive Input Obfuscation}, 
year={2014}, 
pages={453-462}, 
abstract={Recommender systems are widely used by online retailers to promote products and content that are most likely to be of interest to a specific customer. In such systems, users often implicitly or explicitly rate products they have consumed, and some form of collaborative filtering is used to find other users with similar tastes to whom the products can be recommended. While users can benefit from more targeted and relevant recommendations, they are also exposed to greater risks of privacy loss, which can lead to undesirable financial and social consequences. The use of obfuscation techniques to preserve the privacy of user ratings is well studied in the literature. However, works on obfuscation typically assume that all users uniformly apply the same level of obfuscation. In a heterogeneous environment, in which users adopt different levels of obfuscation based on their comfort level, the different levels of obfuscation may impact the users in the system in a different way. In this work we consider such a situation and make the following contributions: (a) using an offline dataset, we evaluate the privacy-utility trade-off in a system where a varying portion of users adopt the privacy preserving technique. Our study highlights the effects that each user's choices have, not only on their own experience but also on the utility that other users will gain from the system, and (b) we propose Privacy Canary, an interactive system that enables users to directly control the privacy-utility trade-off of the recommender system to achieve a desired accuracy while maximizing privacy protection, by probing the system via a private (i.e., undisclosed to the system) set of items. We evaluate the performance of our system with an off-line recommendations dataset, and show its effectiveness in balancing a target recommender accuracy with user privacy, compared to approaches that focus on a fixed privacy level.}, 
keywords={Internet;collaborative filtering;data privacy;recommender systems;retail data processing;PrivacyCanary;adaptive input obfuscation;collaborative filtering;financial consequences;obfuscation techniques;off-line recommendations dataset;online retailers;privacy loss risks;privacy preserving technique;privacy protection;privacy-aware recommender system;privacy-utility tradeoff;social consequences;user privacy;user rating privacy;Accuracy;Data privacy;Gaussian noise;Motion pictures;Privacy;Recommender systems;canary;obfuscation;recommender systems}, 
doi={10.1109/MASCOTS.2014.62}, 
ISSN={1526-7539}, 
month={Sept},}
@INPROCEEDINGS{6729494, 
author={B. T. Dai and H. W. Lauw}, 
booktitle={2013 IEEE 13th International Conference on Data Mining}, 
title={Modeling Preferences with Availability Constraints}, 
year={2013}, 
pages={101-110}, 
abstract={User preferences are commonly learned from historical data whereby users express preferences for items, e.g., through consumption of products or services. Most work assumes that a user is not constrained in their selection of items. This assumption does not take into account the availability constraint, whereby users could only access some items, but not others. For example, in subscription-based systems, we can observe only those historical preferences on subscribed (available) items. However, the objective is to predict preferences on unsubscribed (unavailable) items, which do not appear in the historical observations due to their (lack of) availability. To model preferences in a probabilistic manner and address the issue of availability constraint, we develop a graphical model, called Latent Transition Model (LTM) to discover users' latent interests. LTM is novel in incorporating transitions in interests when certain items are not available to the user. Experiments on a real-life implicit feedback dataset demonstrate that LTM is effective in discovering customers' latent interests, and it achieves significant improvements in prediction accuracy over baselines that do not model transitions.}, 
keywords={consumer behaviour;data handling;probability;psychology;LTM;availability constraint;customer latent interests;graphical model;latent transition model;real-life implicit feedback dataset;unavailable items;unsubscribed items;user latent interests;user preference modelling;Availability;Cable TV;Data models;Industries;Probability distribution;Watches;graphical model;latent interests;topic model;topic transition;user preferences}, 
doi={10.1109/ICDM.2013.41}, 
ISSN={1550-4786}, 
month={Dec},}
@ARTICLE{7120994, 
author={M. L. Wu and V. Popescu}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Multiperspective Focus #x002B;Context Visualization}, 
year={2016}, 
volume={22}, 
number={5}, 
pages={1555-1567}, 
abstract={Occlusions are a severe bottleneck for the visualization of large and complex datasets. Conventional images only show dataset elements to which there is a direct line of sight, which significantly limits the information bandwidth of the visualization. Multiperspective visualization is a powerful approach for alleviating occlusions to show more than what is visible from a single viewpoint. However, constructing and rendering multiperspective visualizations is challenging. We present a framework for designing multiperspective focus+context visualizations with great flexibility by manipulating the underlying camera model. The focus region viewpoint is adapted to alleviate occlusions. The framework supports multiperspective visualization in three scenarios. In a first scenario, the viewpoint is altered independently for individual image regions to avoid occlusions. In a second scenario, conventional input images are connected into a multiperspective image. In a third scenario, one or several data subsets of interest (i.e., targets) are visualized where they would be seen in the absence of occluders, as the user navigates or the targets move. The multiperspective images are rendered at interactive rates, leveraging the camera model's fast projection operation. We demonstrate the framework on terrain, urban, and molecular biology geometric datasets, as well as on volume rendered density datasets.}, 
keywords={cameras;data visualisation;image processing;rendering (computer graphics);camera model;camera model fast projection operation;complex dataset visualization;data subsets;image regions;information bandwidth;interactive rates;molecular biology geometric datasets;multiperspective focus+context visualization;multiperspective visualization rendering;occlusions;volume rendered density datasets;Cameras;Context;Data visualization;Image segmentation;Rendering (computer graphics);Solid modeling;Three-dimensional displays;Occlusion management;camera models;focus+context;interactive visualization;multiperspective visualization},
doi={10.1109/TVCG.2015.2443804}, 
ISSN={1077-2626}, 
month={May},}
@INPROCEEDINGS{7566711, 
author={N. Akkarapatty and N. S. Raj}, 
booktitle={2016 3rd International Conference on Signal Processing and Integrated Networks (SPIN)}, 
title={A Machine Learning approach for classification of sentence polarity}, 
year={2016}, 
pages={316-321}, 
abstract={Opinion Mining is the process used to determine the attitude/opinion/emotion expressed by a person about a particular topic. Analyzing opinions is an integral part for making decisions. In the era of web, if a person wants to buy a product, he will look into the reviews and comments given by the experienced users in web. But it seems to be a tedious task to read the entire reviews available in the web. Hence people are interested in checking whether the review recommends to buy a product or not. If lot of reviews recommends to buy the product, user reach at a conclusion to buy the product, otherwise not to buy the product. In this study, Machine Learning approach is applied to the TripAdvisor dataset in order to develop an efficient review classification. For this work to be carried out, style markers are applied to each of the reviews. In the next stage, significant style markers are recognized with the help of some suitable feature selection method. Thus the reviews can be identified by developing a classifier using the style markers that help to characterize nature of reviews as positive or negative.}, 
keywords={Internet;Web sites;data mining;feature selection;learning (artificial intelligence);pattern classification;sentiment analysis;TripAdvisor dataset;feature selection;machine learning;opinion mining;review classification;sentence polarity classification;Data mining;Feature extraction;Machine learning algorithms;Sentiment analysis;Signal processing;Syntactics;Tagging;Aspects;Classification;Opinion Mining;PoS tags;Sentiment Classification;ensemble model;machine learning}, 
doi={10.1109/SPIN.2016.7566711}, 
month={Feb},}
@INPROCEEDINGS{7225816, 
author={S. Formentin and A. G. Bianchessi and S. M. Savaresi}, 
booktitle={2015 IEEE Intelligent Vehicles Symposium (IV)}, 
title={On the prediction of future vehicle locations in free-floating car sharing systems}, 
year={2015}, 
pages={1006-1011}, 
abstract={The free-floating car sharing model is a recently introduced vehicle rental model, which allows customers to return the car anywhere within the operation area, without relying on depot stations. Driven by the flexibility of such a model, the popularity of car sharing has increased rapidly during the last years. However, some critical issues still arise when a user needs to make plans of vehicle usage, since no information is available on future vehicle locations. In this paper, the Vehicle Distance Prediction (VDP) approach is proposed, aimed to predict the distance of the nearest available vehicle at a given future instant. This technique shows great potential also for the service manager, e.g. vehicles could be moved in advance by the staff to balance the fleet distribution. The effectiveness of the proposed prediction approach is assessed on a real dataset taken from a car sharing service in Milan, Italy.}, 
keywords={automobiles;rental;travel industry;Italy;Milan;VDP approach;car sharing service;fleet distribution;free-floating car sharing system;future vehicle location prediction;nearest available vehicle distance prediction;rental customers;service management;vehicle rental model;vehicle usage;Cities and towns;Computational modeling;Computer architecture;Prediction algorithms;Predictive models;Time series analysis;Vehicles}, 
doi={10.1109/IVS.2015.7225816}, 
ISSN={1931-0587}, 
month={June},}
@INPROCEEDINGS{6883756, 
author={O. Shoukry and M. A. ElMohsen and J. Tadrous and H. E. Gamal and T. ElBatt and N. Wanas and Y. Elnakieb and M. Khairy}, 
booktitle={2014 IEEE International Conference on Communications (ICC)}, 
title={Proactive scheduling for content pre-fetching in mobile networks}, 
year={2014}, 
pages={2848-2854}, 
abstract={The global adoption of smart phones has raised major concerns about a potential surge in the wireless traffic due to the excessive demand on multimedia services. This ever increasing demand is projected to cause significant congestions and degrade the quality of service for network users. In this paper, we develop a proactive caching framework that utilizes the predictability of the mobile user behavior to offload predictable traffic through the WiFi networks ahead of time. First, we formulate the proactive scheduling problem with the objective of maximizing the user-content hit ratio subject to constrains stemming from the user behavioral models. Second, we propose a quadratic-complexity (in the number of slots per day) greedy, yet, high performance heuristic algorithm that pinpoints the best download slot for each content item to attain maximal hit ratio. We confirm the merits of the proposed scheme based on the traces of a real dataset leveraging a large number of smart phone users who consistently utilized our framework for two months.}, 
keywords={greedy algorithms;mobile radio;quality of service;scheduling;storage management;telecommunication traffic;wireless LAN;WiFi networks;content pre-fetching;high performance heuristic algorithm;maximal hit ratio;mobile networks;mobile user behavior predictability;multimedia services;offload predictable traffic;proactive caching framework;proactive scheduling problem;quadratic-complexity;quality of service;smart phones;wireless traffic;Batteries;Data models;IEEE 802.11 Standards;Mobile communication;Mobile computing;Smart phones;Wireless communication;Content pre-fetching;behavioral models;scheduling;smart phone user traces;traffic offloading}, 
doi={10.1109/ICC.2014.6883756}, 
ISSN={1550-3607}, 
month={June},}
@INPROCEEDINGS{6228179, 
author={U. Fischer and F. Rosenthal and W. Lehner}, 
booktitle={2012 IEEE 28th International Conference on Data Engineering}, 
title={F2DB: The Flash-Forward Database System}, 
year={2012}, 
pages={1245-1248}, 
abstract={Forecasts are important to decision-making and risk assessment in many domains. Since current database systems do not provide integrated support for forecasting, it is usually done outside the database system by specially trained experts using forecast models. However, integrating model-based forecasting as a first-class citizen inside a DBMS speeds up the forecasting process by avoiding exporting the data and by applying database-related optimizations like reusing created forecast models. It especially allows subsequent processing of forecast results inside the database. In this demo, we present our prototype F2DB based on PostgreSQL, which allows for transparent processing of forecast queries. Our system automatically takes care of model maintenance when the underlying dataset changes. In addition, we offer optimizations to save maintenance costs and increase accuracy by using derivation schemes for multidimensional data. Our approach reduces the required expert knowledge by enabling arbitrary users to apply forecasting in a declarative way.}, 
keywords={SQL;database management systems;decision making;query processing;risk management;DBMS;F2DB;PostgreSQL;database-related optimization;decision making;derivation scheme;flash-forward database system;maintenance cost;model maintenance;model-based forecasting;multidimensional data;risk assessment;transparent forecast query processing;Analytical models;Data models;Forecasting;Maintenance engineering;Mathematical model;Predictive models;Time series analysis}, 
doi={10.1109/ICDE.2012.117}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{6201202, 
author={N. Piedra and J. Chicaiza and J. López and E. Tovar and O. Martinez-Bonastre}, 
booktitle={Proceedings of the 2012 IEEE Global Engineering Education Conference (EDUCON)}, 
title={Combining Linked Data and mobiles devices to improve access to OCW}, 
year={2012}, 
pages={1-7}, 
abstract={As the Web becomes increasingly populated with diverse data, it continues evolving from a Web of linked documents to a Web Linked Data. This evolution allows relationships to be expressed between content in distributed data sets, enabling the way for integration of raw data from multiple knowledge domains and heterogeneous datasets. However, as the Web becomes increasingly populated with linked data, there is also a need to support users with applications to visualize or to explore available data for a specific task, especially in mobile applications. Examples of these tasks are consuming linked data, or creating new semantic relations between data entities, people and places. With the advent of mobiles, our views and behaviors about access/interaction the Internet are shifting. Specialized applications are available as an important alternative to replace a standard web browser for mobile access. In this article, authors show a three-tier client-server architecture using Android framework. This architecture relies on the RDF data from Linked OpenCourseWare Dataset. Additionally, authors show three proof-of-concept based on architecture which integrates into a common used interface for RDF data management on mobile devices.}, 
keywords={Internet;client-server systems;courseware;data integration;mobile computing;mobile radio;online front-ends;software architecture;Android framework;Internet;OCW;RDF data management;Web browser;Web linked data;World Wide Web;data entities;data integration;distributed data sets;heterogeneous dataset;knowledge domain;linked documents;linked opencourseware dataset;mobile access;mobile application;mobile device;semantic relations;three-tier client-server architecture;Data models;Educational institutions;Mobile communication;Mobile handsets;Resource description framework;Vocabulary;LinkedData;Mobil Architecture;Mobile;OCW;OER;OpenCourseWare;Triplestore}, 
doi={10.1109/EDUCON.2012.6201202}, 
ISSN={2165-9559}, 
month={April},}
@INPROCEEDINGS{6755145, 
author={H. K. Ali and D. I. G. Amalarethinam}, 
booktitle={2014 World Congress on Computing and Communication Technologies}, 
title={Activity Recognition with Fuzzy Finite Automata}, 
year={2014}, 
pages={222-227}, 
abstract={Offering context aware services to users is one of the main objectives of pervasive computing. A context aware system needs to know the activities being performed by the user. Deciding what a user is doing at a given time poses a number of challenges. One significant challenge is dealing with the variation in the number, order and duration of the constituent steps of an activity. There happens to be considerable variation in these parameters even if the same user is performing the same activity at different times. Though fuzzy finite automata have been used by researchers to overcome this challenge, manual construction of the automata for daily life activities becomes onerous. This paper illustrates how finite automata can be constructed automatically and fuzziness incorporated into it, to recognize user activities in a smart environment. The proposed method is tested with a publicly available dataset and is found to give promising results.}, 
keywords={finite automata;fuzzy set theory;ubiquitous computing;activity recognition;context aware services;daily life activities;fuzziness;fuzzy finite automata;pervasive computing;publicly available dataset;smart environment;Automata;Context-aware services;Hidden Markov models;Intelligent sensors;Time series analysis;Vectors;Activity Recognition;Fuzzy Automata;Pervasive Computing}, 
doi={10.1109/WCCCT.2014.34}, 
month={Feb},}
@ARTICLE{5466236, 
author={R. A. Negoescu and D. Gatica-Perez}, 
journal={IEEE Transactions on Multimedia}, 
title={Modeling Flickr Communities Through Probabilistic Topic-Based Analysis}, 
year={2010}, 
volume={12}, 
number={5}, 
pages={399-416}, 
abstract={With the increased presence of digital imaging devices, there also came an explosion in the amount of multimedia content available online. Users have transformed from passive consumers of media into content creators and have started organizing themselves in and around online communities. Flickr has more than 30 million users and over 3 billion photos, and many of them are tagged and public. One very important aspect in Flickr is the ability of users to organize in self-managed communities called groups. This paper examines an unexplored problem, which is jointly analyzing Flickr groups and users. We show that although users and groups are conceptually different, in practice they can be represented in a similar way via a bag-of-tags derived from their photos, which is amenable for probabilistic topic modeling. We then propose a probabilistic topic model representation learned in an unsupervised manner that allows the discovery of similar users and groups beyond direct tag-based strategies, and we demonstrate that higher-level information such as topics of interest are a viable alternative. On a dataset containing users of 10 000 Flickr groups and over 1 milion photos, we show how this common topic-based representation allows for a novel analysis of the groups-users Flickr ecosystem, which results into new insights about the structure of the entities in this social media source. We demonstrate novel practical applications of our topic-based representation, such as similarity-based exploration of entities, or single and multi-topic tag-based search, which address current limitations in the ways Flickr is used today.}, 
keywords={content management;multimedia computing;probability;social networking (online);Flickr communities modeling;bag-of-tags;digital imaging devices;multimedia content;probabilistic topic based analysis;Digital cameras;Digital images;Ecosystems;Explosions;Glass;Image analysis;Licenses;Organizing;Photography;Software libraries;Flickr;probabilistic topic models;social media}, 
doi={10.1109/TMM.2010.2050649}, 
ISSN={1520-9210}, 
month={Aug},}
@INPROCEEDINGS{7249265, 
author={A. Hess and I. Marsh and D. Gillblad}, 
booktitle={2015 IEEE International Conference on Communications (ICC)}, 
title={Exploring communication and mobility behavior of 3G network users and its temporal consistency}, 
year={2015}, 
pages={5916-5921}, 
abstract={Over the past decade, telecommunication network operators have more and more realized the added value of data analytics for their network deployment efficiency. Early studies targeted the global network perspective by localizing peak loads, both in terms of area and time period. Due to their higher granularity and information richness, current telecommunication datasets allow increasingly deeper insights into the network activities of the users. Existing network traffic classification studies tend to divide users into groups without considering the transitions between different groups caused by individual behavioral traits, which we expect to show observable regularities. Our approach defines a profiling model that characterizes the user behavior as well as its temporal dynamics from two perspectives: w.r.t. (i) the network load the users generate, and (ii) their mobility patterns. The model is evaluated with two unsupervised clustering algorithms of different complexity (namely, XMeans and EM) by means of a 3G trace dataset from a European operator.}, 
keywords={3G mobile communication;mobility management (mobile radio);telecommunication traffic;3G network users;EM complexity;European operator;XMeans complexity;global network;mobility behavior;mobility patterns;network deployment;network traffic classification;profiling model;telecommunication datasets;telecommunication network operators;temporal consistency;temporal dynamics;unsupervised clustering algorithms;Clustering algorithms;Computational modeling;Correlation;Feature extraction;Load modeling;Measurement;Mobile communication}, 
doi={10.1109/ICC.2015.7249265}, 
ISSN={1550-3607}, 
month={June},}
@INPROCEEDINGS{7168448, 
author={A. C. Enache and V. Sgârciu}, 
booktitle={2015 20th International Conference on Control Systems and Computer Science}, 
title={Anomaly Intrusions Detection Based on Support Vector Machines with an Improved Bat Algorithm}, 
year={2015}, 
pages={317-321}, 
abstract={The continuous proliferation of more complex and various security threats leads to the conclusion that new solutions are required. Intrusion Detection Systems can be a pertinent solution because they can deal with the large data volumes of logs gathered from the multitude of systems and can even identify new types of attacks if based on anomaly detection. In this paper we propose an IDS model which includes two stages: feature selection with information gain and detection with Support Vector Machines (SVM). A draw-back of SVM is that its performance results are influenced by its user input parameters. Therefore, in order to better the classifier we exploit the advantages of a recent Swarm Intelligence (SI) algorithm, the Bat Algorithm (BA), which we improve by enhancing its randomization with Lévy flights. We test our model for the NSL-KDD dataset and prove that it can outperform the original BA, ABC or the popular PSO.}, 
keywords={feature selection;optimisation;pattern classification;security of data;support vector machines;swarm intelligence;BA;IDS model;Lévy flights;NSL-KDD dataset;SI algorithm;SVM;anomaly intrusion detection systems;attack type identification;data volumes;feature selection;improved bat algorithm;information gain;support vector machines;swarm intelligence algorithm;Accuracy;Feature extraction;Intrusion detection;Kernel;Silicon;Support vector machines;Bat Algorithm;IDS;L¿¿vy flights;SVM}, 
doi={10.1109/CSCS.2015.12}, 
ISSN={2379-0474}, 
month={May},}
@INPROCEEDINGS{5360901, 
author={D. Guo and F. Lin and C. Chen}, 
booktitle={2009 IEEE International Conference on Network Infrastructure and Digital Content}, 
title={User behaviors in an Online Social Network}, 
year={2009}, 
pages={430-434}, 
abstract={In this paper, we focus on an online social network (OSN), namely Â¿kaixin-netÂ¿ (www.kaixin001.com) which is one of the biggest OSNs in China. Based on studying the most popular game `Buying-a-house', we investigate user behaviors. First, we made a model for Â¿Buying-a-houseÂ¿, then using the dataset that we crawled and traced (we crawled about one hundred and fifty user's profiles and traced forty users for about twenty days), we draw the following findings: (1) People who just join the game have much interest in this OSN, actively interact with his friends. (2) The interest goes down according to the time lapse for most of users, but there are also a few players whose interests never go down and enjoy this game very much. These can be interpreted as the population of kaixin-net is growing at the beginning state, but it will decline after the peak comes. So the organizers should think about how to slow down the peak's coming.}, 
keywords={computer games;social networking (online);computer game;kaixin-net;online social network;user behavior;economic;online games;social network;user behavior}, 
doi={10.1109/ICNIDC.2009.5360901}, 
ISSN={2374-0272}, 
month={Nov},}
@INPROCEEDINGS{7129551, 
author={J. Liu and K. Zhao and S. Khan and M. Cameron and R. Jurdak}, 
booktitle={2015 31st IEEE International Conference on Data Engineering Workshops}, 
title={Multi-scale population and mobility estimation with geo-tagged Tweets}, 
year={2015}, 
pages={83-86}, 
abstract={Recent outbreaks of Ebola and Dengue viruses have again elevated the significance of the capability to quickly predict disease spread in an emergent situation. However, existing approaches usually rely heavily on the time-consuming census processes, or the privacy-sensitive call logs, leading to their unresponsive nature when facing the abruptly changing dynamics in the event of an outbreak. In this paper we study the feasibility of using large-scale Twitter data as a proxy of human mobility to model and predict disease spread. We report that for Australia, Twitter users' distribution correlates well the census-based population distribution, and that the Twitter users' travel patterns appear to loosely follow the gravity law at multiple scales of geographic distances, i.e. national level, state level and metropolitan level. The radiation model is also evaluated on this dataset though it has shown inferior fitness as a result of Australia's sparse population and large landmass. The outcomes of the study form the cornerstones for future work towards a model-based, responsive prediction method from Twitter data for disease spread.}, 
keywords={data privacy;diseases;geographic information systems;microorganisms;mobile computing;social networking (online);Dengue virus;Ebola virus;Twitter users travel pattern;census-based population distribution;disease spread;geo-tagged Tweet;geographic distance;gravity law;human mobility;large-scale Twitter data;metropolitan level;mobility estimation;multiscale population;national level;privacy-sensitive call log;radiation model;state level;time-consuming census process;Australia;Data models;Diseases;Gravity;Sociology;Statistics;Twitter}, 
doi={10.1109/ICDEW.2015.7129551}, 
month={April},}
@INPROCEEDINGS{5980715, 
author={X. Sun and B. Yang and M. Attene and Q. Li and S. Jiang}, 
booktitle={2011 19th International Conference on Geoinformatics}, 
title={Automated abstraction of building models for 3D navigation on mobile devices}, 
year={2011}, 
pages={1-6}, 
abstract={Due to their limited expressive capabilities and computational resources, typical mobile devices are still not appropriate for real-time 3D navigation, in particular when widespread building models in the urban environments have a very detailed geometry. In order to better balance the capability of mobile devices and the perception demands of the users in 3D navigation, this paper proposes an automated algorithm to generate structure-preserving abstractions of building models for the purpose of suitable dataset preparation. Building models are segmented into different structural parts, and each structural part is approximated by a convex polyhedron, then the convex parts are combined together to achieve the ultimate abstraction. The robustness and efficiency of the algorithm is proved by extensive experiments.}, 
keywords={approximation theory;computerised navigation;mobile computing;solid modelling;structural engineering computing;3D building model;automated abstraction;convex polyhedron;dataset preparation;mobile devices;real-time 3D navigation;structural part approximation;structure-preserving abstraction;Algorithm design and analysis;Buildings;Clustering algorithms;Navigation;Shape;Solid modeling;Three dimensional displays;3D model simplification;3D navigation;geographic virtual environment;shape abstraction;structure analysis}, 
doi={10.1109/GeoInformatics.2011.5980715}, 
ISSN={2161-024X}, 
month={June},}
@INPROCEEDINGS{7025662, 
author={H. Kim and J. J. Thiagarajan and P. T. Bremer}, 
booktitle={2014 IEEE International Conference on Image Processing (ICIP)}, 
title={Image segmentation using consensus from hierarchical segmentation ensembles}, 
year={2014}, 
pages={3272-3276}, 
abstract={Unsupervised, automatic image segmentation without contextual knowledge, or user intervention is a challenging problem. The key to robust segmentation is an appropriate selection of local features and metrics. However, a single aggregation of the local features using a greedy merging order often results in incorrect segmentation. This paper presents an unsupervised approach, which uses the consensus inferred from hierarchical segmentation ensembles, for partitioning images into foreground and background regions. By exploring an expanded set of possible aggregations of the local features, the proposed method generates meaningful segmentations that are not often revealed when only the optimal hierarchy is considered. A graph cuts-based approach is employed to combine the consensus along with a foreground-background model estimate, obtained using the ensemble, for effective segmentation. Experiments with a standard dataset show promising results when compared to several existing methods including the state-of-the-art weak supervised techniques that use co-segmentation.}, 
keywords={feature extraction;image segmentation;background region;contextual knowledge;cosegmentation;feature selection;foreground region;graph cuts-based approach;greedy merging order;hierarchical segmentation ensemble consensus;image partitioning;image segmentation;user intervention;weak supervised technique;Accuracy;Estimation;Histograms;Image segmentation;Merging;Partitioning algorithms;Robustness;Unsupervised segmentation;consensus clustering;graph cuts;multiple hierarchies;superpixels}, 
doi={10.1109/ICIP.2014.7025662}, 
ISSN={1522-4880}, 
month={Oct},}
@INPROCEEDINGS{7912016, 
author={N. Goyal and S. K. Jain}, 
booktitle={2016 2nd International Conference on Applied and Theoretical Computing and Communication Technology (iCATccT)}, 
title={An efficient algorithm for mining top-rank-K frequent patterns from uncertain databases}, 
year={2016}, 
pages={324-328}, 
abstract={The analysis and management of uncertain data has gained a lot of importance in the past few years because of their importance in a wide variety of applications such as sensor network and privacy preserving data mining applications. Many algorithms have been proposed to mine the frequent pattern over uncertain database. However the existing algorithms for uncertain data generate a large no. of candidate patterns and required to define an appropriate user defined threshold which is a challenging task for users. In this paper, we propose a new algorithm known as UFAE (uncertain filtering and extending) algorithm to mine top-rank-k frequent itemset or patterns. Mining only top-rank-k frequent pattern greatly decrease the number of candidate pattern generated so reduce the mining time. Many algorithms exist to mine top-rank-k frequent itemset in case of precise data but none in case of uncertain database. Experiments are performed to evaluate the performance of the algorithm on various dataset.}, 
keywords={data analysis;data mining;UFAE algorithm;top-rank-k frequent itemset mining;top-rank-k frequent pattern mining;uncertain data analysis;uncertain databases;uncertain filtering and extending algorithm;Algorithm design and analysis;Data mining;Filtering;Itemsets;Mathematical model;Data mining;existential probability;expected support;frequent pattern;top-rank-k frequent pattern;uncertain database}, 
doi={10.1109/ICATCCT.2016.7912016}, 
month={July},}
@INPROCEEDINGS{6009391, 
author={Y. Jiang and J. Liu and M. Tang and X. Liu}, 
booktitle={2011 IEEE International Conference on Web Services}, 
title={An Effective Web Service Recommendation Method Based on Personalized Collaborative Filtering}, 
year={2011}, 
pages={211-218}, 
abstract={Collaborative filtering is one of widely used Web service recommendation techniques. There have been several methods of Web service selection and recommendation based on collaborative filtering, but seldom have they considered personalized influence of users and services. In this paper, we present an effective personalized collaborative filtering method for Web service recommendation. A key component of Web service recommendation techniques is computation of similarity measurement of Web services. Different from the Pearson Correlation Coefficient (PCC) similarity measurement, we take into account the personalized influence of services when computing similarity measurement between users and personalized influence of services. Based on the similarity measurement model of Web services, we develop an effective Personalized Hybrid Collaborative Filtering (PHCF) technique by integrating personalized user-based algorithm and personalized item-based algorithm. We conduct series of experiments based on real Web service QoS dataset WSRec [11] which contains more than 1.5 millions test results of 150 service users in different countries on 100 publicly available Web services located all over the world. Experimental results show that the method improves accuracy of recommendation of Web services significantly.}, 
keywords={Web services;groupware;information filtering;recommender systems;PHCF technique;Pearson correlation coefficient;Web service recommendation;Web service selection;personalized hybrid collaborative filtering;personalized item-based algorithm;personalized user-based algorithm;similarity measurement;Accuracy;Collaboration;Correlation;Filtering;Prediction algorithms;Quality of service;Web services;Web service;Web service recommendation;Web service similarity measurement;personalization;personalized collaborative filtering}, 
doi={10.1109/ICWS.2011.38}, 
month={July},}
@INPROCEEDINGS{5284908, 
author={Y. Zeng and Y. Yao and N. Zhong}, 
booktitle={2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology}, 
title={DBLP-SSE: A DBLP Search Support Engine}, 
year={2009}, 
volume={1}, 
pages={626-630}, 
abstract={A Search Support Engine (SSE) is implemented based on the basic principles of Information Retrieval Support Systems (IRSS) and Information Seeking Support Systems (ISSS). An SSE aims at meeting the diversity needs from different users, providing various supporting functionalities, tools, etc. for users to perform various tasks beyond the traditional search and browsing provided by current search engines. As an illustrative example, we developed a DBLP search support engine (DBLP-SSE), and we discuss some concrete supporting functionalities, namely, search refinement support, domain analysis support, etc. Each of the functionality focus on a unique perspective supporting users finding useful information and knowledge from the DBLP dataset. The search support engine can be considered as a step towards Knowledge Retrieval (KR) and Web Intelligence (WI).}, 
keywords={Computer science;Concrete;Conferences;Informatics;Information retrieval;Intelligent agent;Navigation;Search engines;Visualization;Web search;information retrieval support system;information seeking support system;knowledge retrieval;search support engine;user interest modeling}, 
doi={10.1109/WI-IAT.2009.364}, 
month={Sept},}
@INPROCEEDINGS{7794981, 
author={N. Tastevin and M. Bouet}, 
booktitle={2016 IEEE 27th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)}, 
title={Characterizing and modeling the distance of mobile calls: A metropolitan case study}, 
year={2016}, 
pages={1-6}, 
abstract={During the past few years, with the democratization of open data, mobile network operators facilitated the sharing of mobile datasets. Such data are particularly interesting to better understand the way mobile users communicate, move and consummate services, but also to rethink the design or the evolution of current infrastructures with respects to new information and communication technologies and continuous increase of mobile usage. In this paper, we analyze a mobile dataset from a telecommunication provider in the city of Milan, Italy, and characterize mobile users' behavior. We show that, on the scale of the city, the number of short distance calls is massively higher than the number of long distance calls. Based on these observations, we propose a realistic yet simple model to describe the distance distribution of the calls in such a metropolitan environment. Finally, we focus on a specific area of the city, the soccer stadium area, and show that local events provoke regular deviation to the observed behaviors on the scale of the city. The macroscopic model we propose can be used to generate mobile datasets for any large city and thus support further simulations.}, 
keywords={mobile radio;Italy;Milan city;communication technology;distance distribution;information technology;macroscopic model;metropolitan case study;mobile call distance;mobile datasets;mobile network operators;mobile user behavior;open data democratization;soccer stadium area;telecommunication provider;Business;Land mobile radio;Mathematical model;Mobile computing;Telecommunications;Urban areas}, 
doi={10.1109/PIMRC.2016.7794981}, 
month={Sept},}
@INPROCEEDINGS{6503157, 
author={Fei Hao and Min Chen and Chunsheng Zhu and M. Guizani}, 
booktitle={2012 IEEE Global Communications Conference (GLOBECOM)}, 
title={Discovering influential users in micro-blog marketing with influence maximization mechanism}, 
year={2012}, 
pages={470-474}, 
abstract={Micro-blog marketing has become a main business model for social networks nowadays. On social networking sites (e.g., Twitter), micro-blog marketing enables the advertisers to put ads to attract customers to buy their products. During this process, a rather key step for the success of advertisers is to conduct marketing researches to discover which micro-blog users are their potential customers who can greatly promote their products to other customers so that the advertising investment can be greatly reduced. This problem is considered as “influence maximization” issue. In this paper and in attempt to discover the influential users in micro-blog marketing, we try to analyze the influences of nodes in a micro-blog network and propose a Community Scale-Sensitive Maxdegree (CSSM) algorithm for maximizing the influences when placing ads. Experimental results on the very hot micro-blog service (i.e., Twitter dataset) demonstrate that our proposed CSSM algorithm significantly outperforms other related node selection strategies, in terms of the influence spread and time complexity.}, 
keywords={advertising;computational complexity;promotion (marketing);social networking (online);CSSM algorithm;advertising investment;community scale-sensitive maxdegree algorithm;influence maximization mechanism;influence spread;influential user discovery;micro-blog marketing;micro-blog service;node selection strategy;product promotion;social networking sites;time complexity;Micro-blog marketing;influence maximization;influence spread;social networks;time complexity}, 
doi={10.1109/GLOCOM.2012.6503157}, 
ISSN={1930-529X}, 
month={Dec},}
@INPROCEEDINGS{6274041, 
author={M. Li and J. Jin and J. Zhao and B. Xie}, 
booktitle={2012 IEEE Eighth World Congress on Services}, 
title={Propositional Logic-Based and Evidence-Rich Trustworthiness Evaluation for Web Services}, 
year={2012}, 
pages={125-132}, 
abstract={With more and more Web services available on the Internet, users reuse these services in their own applications. Because Web services are delivered by third parties and hosted on remote servers, it becomes a big problem to determine the trustworthiness of the Web services. Many Web services trustworthiness evaluation approaches have been proposed, however, the trustworthy evidences used in these approaches are limited and the methods proposed lack customizability and extensibility, which makes them difficult to apply. In this paper, we propose a lightweight Propositional Logic-based Web services trustworthiness evaluation method, which is customizable, extensible, and easy to apply in reality. First we collect comprehensive trustworthy evidences including both objective trustworthy evidences (e.g. QoS) and subjective evidences (e.g. reputation) from the Internet. Then we propose a Propositional Logic-based Web services trustworthiness evaluation model, which is customizable and extensible, to capture users' trustworthiness requirements. Finally, the trustworthiness of all Web services are evaluated and returned to the users via a Web services search engine. To validate the effectiveness of our approach, two experiments are conducted on a large-scale real-world dataset. The experimental results show that our method is easy to use and can effectively evaluate Web services trustworthiness, which helps users to reuse Web services.}, 
keywords={Web services;formal logic;search engines;security of data;Internet;Web services search engine;evidence-rich trustworthiness evaluation;objective trustworthy evidences;propositional logic-based trustworthiness evaluation;remote servers;subjective evidences;Availability;Compounds;Cows;Quality of service;Time factors;Web services;Propositional Logic;QoS;Web services;evaluation;reputation;trustworthiness}, 
doi={10.1109/SERVICES.2012.24}, 
ISSN={2378-3818}, 
month={June},}
@ARTICLE{7723822, 
author={X. Wei and H. Huang and L. Nie and H. Zhang and X. L. Mao and T. S. Chua}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={I Know What You Want to Express: Sentence Element Inference by Incorporating External Knowledge Base}, 
year={2017}, 
volume={29}, 
number={2}, 
pages={344-358}, 
abstract={Sentence auto-completion is an important feature that saves users many keystrokes in typing the entire sentence by providing suggestions as they type. Despite its value, the existing sentence auto-completion methods, such as query completion models, can hardly be applied to solving the object completion problem in sentences with the form of (subject, verb, object), due to the complex natural language description and the data deficiency problem. Towards this goal, we treat an SVO sentence as a three-element triple (subject, sentence pattern, object), and cast the sentence object completion problem as an element inference problem. These elements in all triples are encoded into a unified low-dimensional embedding space by our proposed TRANSFER model, which leverages the external knowledge base to strengthen the representation learning performance. With such representations, we can provide reliable candidates for the desired missing element by a linear model. Extensive experiments on a real-world dataset have well-validated our model. Meanwhile, we have successfully applied our proposed model to factoid question answering systems for answer candidate selection, which further demonstrates the applicability of the TRANSFER model.}, 
keywords={inference mechanisms;knowledge based systems;learning (artificial intelligence);natural language processing;question answering (information retrieval);SVO sentence;TRANSFER model;answer candidate selection;complex natural language description;data deficiency problem;element inference problem;external knowledge base;factoid question answering systems;linear model;query completion models;representation learning performance;sentence autocompletion methods;sentence element inference;sentence object completion problem;three-element triple;unified low-dimensional embedding space;Encyclopedias;Face;Internet;Knowledge based systems;Knowledge discovery;Natural languages;Semantics;Representation learning;external knowledge base;sentence modeling}, 
doi={10.1109/TKDE.2016.2622705}, 
ISSN={1041-4347}, 
month={Feb},}
@INPROCEEDINGS{7844449, 
author={Z. H. Wu and A. Liu and P. C. Zhou and Y. F. Su}, 
booktitle={2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={A Bayesian network based method for activity prediction in a smart home system}, 
year={2016}, 
pages={001496-001501}, 
abstract={A smart home system can provide better services to assist users if it knows what user activities will occur beforehand. Early research in activity prediction has indicated that the result of prediction is unique, but the accuracy remains unsatisfactory if only one result is considered. To solve this problem, this paper proposes a method of leveraging multiple models. In this work, we use a Bayesian network to build a model to predict which activity will happen, and then the predicted results go through a property filtering to get the final result. Due to the possibility that residents in a smart home may have different activity patterns, we have built a Bayesian network model to learn conditional probability of resident activities from the dataset of CASAS project. At last, we compare the results and show that our method has improved coverage and accuracy in activity prediction. This proposed method belongs to an ongoing project involving learning and control in a smart home system.}, 
keywords={Bayes methods;home automation;learning (artificial intelligence);probability;Bayesian network;CASAS project;activity prediction;resident activities;smart home system;Bayes methods;Conferences;Cybernetics;Data models;Prediction algorithms;Predictive models;Smart homes;Bayesian network;activity forecasting;activity prediction}, 
doi={10.1109/SMC.2016.7844449}, 
month={Oct},}
@INPROCEEDINGS{7880160, 
author={G. S. Njoo and C. H. Lai and K. W. Hsu}, 
booktitle={2016 Conference on Technologies and Applications of Artificial Intelligence (TAAI)}, 
title={Exploring multi-view learning for activity inferences on smartphones}, 
year={2016}, 
pages={212-219}, 
abstract={Inferring activities on smartphones is a challenging task. Prior works have elaborated on using sensory data from built-in hardware sensors in smartphones or taking advantage of location information to understand human activities. In this paper, we explore two types of data on smartphones to conduct activity inference: 1) Spatial-Temporal: reflecting daily routines from the combination of spatial and temporal patterns, 2) Application: perceiving specialized apps that assist the user's activities. We employ multi-view learning model to accommodate both types of data and use weighted linear kernel model to aggregate the views. Note that since resources of smartphones are limited, activity inference on smartphones should consider the constraints of resources, such as the storage, energy consumption, and computation power. Finally, we compare our proposed method with several classification methods on a real dataset to evaluate the effectiveness and performance of our method. The experimental results show that our approach outperforms other methods regarding the balance between accuracy, running time, and storage efficiency.}, 
keywords={inference mechanisms;learning (artificial intelligence);smart phones;activity inference;application data;built-in hardware sensors;classification methods;multiview learning model;sensory data;smart phones;spatial-temporal data;Computational modeling;Data models;Feature extraction;Hardware;Sensors;Smart phones;Software}, 
doi={10.1109/TAAI.2016.7880160}, 
month={Nov},}
@INPROCEEDINGS{6467454, 
author={C. Direkoğlu and N. E. O'Connor}, 
booktitle={2012 19th IEEE International Conference on Image Processing}, 
title={Team behavior analysis in sports using the Poisson equation}, 
year={2012}, 
pages={2693-2696}, 
abstract={We propose a novel physics-based model for analysing team players' positions and movements on a sports playing field. The goal is to detect for each frame the region with the highest population of a given team's players and the region towards which the team is moving as they press for territorial advancement, termed the region of intent. Given the positions of team players from a plan view of the playing field at any given time, we solve a particular Poisson equation to generate a smooth distribution. The proposed distribution provides the likelihood of a point to be occupied by players so that more highly populated regions can be detected by appropriate thresholding. Computing the proposed distribution for each frame provides a sequence of distributions, which we process to detect the region of intent at any time during the game. Our model is evaluated on a field hockey dataset, and results show that the proposed approach can provide effective features that could be used to generate team statistics useful for performance evaluation or broadcasting purposes.}, 
keywords={Poisson equation;sport;statistics;Poisson equation solution;broadcasting;field hockey dataset;highly populated regions;performance evaluation;physics-based model;region of intent;sports playing field;team behavior analysis;team player position analysis;team statistics;Cameras;Computer vision;Feature extraction;Games;Mathematical model;Poisson equations;Trajectory;Feature extraction;Poisson equation;computer vision;team sports;video analysis}, 
doi={10.1109/ICIP.2012.6467454}, 
ISSN={1522-4880}, 
month={Sept},}
@INPROCEEDINGS{7886009, 
author={P. Petsas and P. Kaimakis}, 
booktitle={2016 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)}, 
title={Soccer player tracking using particle filters}, 
year={2016}, 
pages={57-62}, 
abstract={We present a multi-target tracking system for estimating the position of multiple soccer players as they move around during a soccer game. Our system relies on silhouette observations recorded by a static camera, and utilises 3D models and particle filter methods to estimate athlete positions. Tracking parameters can be adapted in order to tip the scales towards precision or performance. We present the system's software architecture as a client-server application, and demonstrate results based on a simulated yet realistic dataset produced using the Unity3D game engine.}, 
keywords={image filtering;image motion analysis;object tracking;particle filtering (numerical methods);sport;target tracking;3D models;Unity3D game engine;athlete positions;client-server application;multitarget tracking system;particle filters;silhouette observations;soccer game;soccer player tracking;static camera;Cameras;Data models;Servers;Signal processing;Solid modeling;Three-dimensional displays;Trajectory;Soccer player tracking;athlete tracking;multi-target tracking;particle filters}, 
doi={10.1109/ISSPIT.2016.7886009}, 
month={Dec},}
@INPROCEEDINGS{7881458, 
author={M. Hassan and K. Assaleh and T. Shanableh}, 
booktitle={2016 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
title={User-Dependent Sign Language Recognition Using Motion Detection}, 
year={2016}, 
pages={852-856}, 
abstract={Sign language is the primary means of communication used by deaf people. Statistics show that around 360 million people around the world suffer from hearing loss. For those people, communication with hearing people is a tiring day to day process which may have an adverse effect on their lives. Sign Language Recognition (SLR) is relatively new area, it is not as mature as speech recognition for example. Moreover, Arabic Sign Language Recognition (ArSLR) did not receive much of attention until recent years. This paper presents a continuous sensor-based ArSLR system based on Hidden Markov Models (HMM) and a modified version of k-nearest neighbor (KNN). The proposed system is tested on two datasets. The first was collected using DG5-VHand data gloves and the second was collected using Polhemus G4 tracker. Each dataset was collected by a different signer. Both datasets consist of 40 Arabic sentences with 80-word perplexity. It is intended to make the collected datasets available for the research community. The proposed system provides an excellent performance of 97% sentence recognition rate.}, 
keywords={hidden Markov models;image motion analysis;learning (artificial intelligence);sign language recognition;Arabic sentences;Arabic sign language recognition;DG5-VHand data gloves;KNN;Polhemus G4 tracker;SLR;continuous sensor-based ArSLR system;hidden Markov models;k-nearest neighbor;motion detection;sentence recognition rate;user-dependent sign language recognition;word perplexity;Assistive technology;Feature extraction;Gesture recognition;Hidden Markov models;Sensors;Speech recognition;Standards;Arabic Sign Language Recognition;HMM;Modefied KNN;Motion detector}, 
doi={10.1109/CSCI.2016.0165}, 
month={Dec},}
@INPROCEEDINGS{937546, 
author={Y. Duan and H. Qin}, 
booktitle={Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001}, 
title={A novel modeling algorithm for shape recovery of unknown topology}, 
year={2001}, 
volume={1}, 
pages={402-409 vol.1}, 
abstract={This paper presents a novel modeling algorithm that is capable of simultaneously recovering correct shape geometry as well as its unknown topology from arbitrarily complicated datasets. Our algorithm starts from a simple seed model (of genus zero) that can be arbitrarily initiated by users within any dataset. The deformable behavior of our model is governed by a locally defined objective function associated with each vertex of the model. Through the numerical computation of function optimization, our algorithm can adaptively subdivide the model geometry, automatically detect self-collision of the model, properly modify its topology (because of the occurrence of self-collision), continuously evolve the model towards the object boundary, and reduce fitting error and improve fitting quality via global subdivision. Commonly used mesh optimization techniques are employed throughout the geometric deformation and topological variation in order to ensure the model both locally smooth and globally well conditioned. We have applied our algorithm to various real/synthetic range data as well as volumetric image data in order to empirically verify and validate its usefulness. Based on our experiments, the new modeling algorithm proves to be very powerful and extremely valuable for shape recovery in computer vision, reverse engineering in computer graphics, and iso-surface extraction in visualization}, 
keywords={computer graphics;computer vision;image reconstruction;complicated datasets;function optimization;mesh optimization;model geometry;modeling algorithm;recovering correct shape geometry;shape recovery;unknown topology;Computational geometry;Computer graphics;Computer vision;Data mining;Deformable models;Object detection;Reverse engineering;Shape;Solid modeling;Topology}, 
doi={10.1109/ICCV.2001.937546}, 
month={},}
@INPROCEEDINGS{6630420, 
author={Z. Wu and J. Zheng and S. Wang and H. Feng}, 
booktitle={2013 5th International Conference on Intelligent Networking and Collaborative Systems}, 
title={A Combined Predictor for Item-Based Collaborative Filtering}, 
year={2013}, 
pages={261-265}, 
abstract={Collaborative filtering is one of most important technologies in the field of recommender systems, the process of making predictions about user preferences for products or services by learning known user-item relationships. In this paper, slope one and item-based nearest neighbor collaborative filtering algorithms are analyzed on the Movie Lens dataset. In order to obtain better accuracy and rationality, a new combined approach is proposed that takes advantages of slope one and item-based nearest neighbor model. In addition, simple gradient descent and bias effects are used further to improve performance. Finally, some experiments are implemented on the dataset, and the experimental results show that the proposed final solution achieves great improvement of prediction accuracy when compared to the method of using slope one or item-based nearest neighbor model alone.}, 
keywords={collaborative filtering;learning (artificial intelligence);recommender systems;Movie Lens dataset;bias effects;combined predictor;gradient descent;item-based nearest neighbor collaborative filtering algorithms;recommender systems;slope one algorithm;user-item relationship learning;Accuracy;Collaboration;Computational modeling;Prediction algorithms;Predictive models;Recommender systems;collaborative filtering;item-based nearest neighbor;slope one}, 
doi={10.1109/INCoS.2013.46}, 
month={Sept},}
@INPROCEEDINGS{4270197, 
author={J. Philbin and O. Chum and M. Isard and J. Sivic and A. Zisserman}, 
booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Object retrieval with large vocabularies and fast spatial matching}, 
year={2007}, 
pages={1-8}, 
abstract={In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient spatial verification stage to re-rank the results returned from our bag-of-words model and show that this consistently improves search quality, though by less of a margin when the visual vocabulary is large. We view this work as a promising step towards much larger, "web-scale " image corpora.}, 
keywords={image matching;image retrieval;quantisation (signal);Web-scale image corpora;fast spatial matching;image-feature vocabulary;large vocabularies;large-scale object retrieval system;object retrieval;quantization method;query image;query object;query performance;scalable methods;spatial verification stage;Humans;Image retrieval;Information filtering;Information filters;Information retrieval;Large-scale systems;Quantization;Scalability;Silicon;Vocabulary}, 
doi={10.1109/CVPR.2007.383172}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7774674, 
author={D. Li and T. Salonidis and N. V. Desai and M. C. Chuah}, 
booktitle={2016 IEEE/ACM Symposium on Edge Computing (SEC)}, 
title={DeepCham: Collaborative Edge-Mediated Adaptive Deep Learning for Mobile Object Recognition}, 
year={2016}, 
pages={64-76}, 
abstract={Deep learning techniques achieve state-of-the-art performance on many computer vision related tasks, e.g. large-scale object recognition. In this paper we show that recognition accuracy degrades when used in daily mobile scenarios due to context variations caused by different locations, time of a day, etc. To solve this problem, we present DeepCham - the first adaptive mobile object recognition framework that allows deep learning techniques to be used successfully in mobile environments. Specifically, DeepCham is mediated by an edge master server which coordinates with participating mobile users to collaboratively train a domain-aware adaptation model which can yield much better object recognition accuracy when used together with a domain-constrained deep model. DeepCham generates high-quality domain-aware training instances for adaptation from in-situ mobile photos using two major steps: (i) a distributed algorithm which identifies qualifying images stored in each mobile device for training, (ii) a user labeling process for recognizable objects identified from qualifying images using suggestions automatically generated by a generic deep model. Using a newly collected dataset with smartphone images collected from different locations, time of a day, and device types, we show that DeepCham improves the object recognition accuracy by 150% when compared to that achieved merely using a generic deep model. In addition, we investigated how major design factors affect the performance of DeepCham. Finally, we demonstrate the feasibility of DeepCham using an implemented prototype.}, 
keywords={computer vision;learning (artificial intelligence);mobile computing;object recognition;smart phones;DeepCham;collaborative edge-mediated adaptive deep learning;computer vision related tasks;design factors;domain-aware adaptation model;domain-constrained deep model;edge master server;generic deep model;large-scale object recognition;mobile object recognition framework;mobile scenarios;participating mobile users;smartphone images;Adaptation models;Machine learning;Mobile communication;Mobile handsets;Object recognition;Training;Visualization;Adaptive Deep Learning;Crowd-sourcing;Edge Computing;Mobile Object Recognition}, 
doi={10.1109/SEC.2016.38}, 
month={Oct},}
@INPROCEEDINGS{7363864, 
author={J. W. Williams and P. Cuddihy and J. McHugh and K. S. Aggour and A. Menon and S. M. Gustafson and T. Healy}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={Semantics for Big Data access integration: Improving industrial equipment design through increased data usability}, 
year={2015}, 
pages={1103-1112}, 
abstract={With the advent of Big Data technologies, organizations can efficiently store and analyze more data than ever before. However, extracting maximal value from this data can be challenging for many reasons. For example, datasets are often not stored using human-understandable terms, making it difficult for a large set of users to benefit from them. Further, given that different types of data may be best stored using different technologies, datasets that are closely related may be stored separately with no explicit linkage. Finally, even within individual data stores, there are often inconsistencies in data representations, whether introduced over time or due to different data producers. These challenges are further compounded by frequent additions to the data, including new raw data as well as results produced by large-scale analytics. Thus, even within a single Big Data environment, it is often the case that multiple rich datasets exist without the means to access them in a unified and cohesive way, often leading to lost value. This paper describes the development of a Big Data management infrastructure with semantic technologies at its core to provide a unified data access layer and a consistent approach to analytic execution. Semantic technologies were used to create domain models describing mutually relevant datasets and the relationships between them, with a graphical user interface to transparently query across datasets using domain-model terms. This prototype system was built for GE Power & Water's Power Generation Products Engineering Division, which has produced over 50TB of gas turbine and component prototype test data to date. The system is expected to result in significant savings in productivity and expenditure.}, 
keywords={Big Data;data integration;design engineering;gas turbines;graphical user interfaces;production engineering computing;production equipment;Big Data access;Big Data integration;Big Data management infrastructure;Big Data technologies;GE Power and Water;Power Generation Products Engineering Division;data representation;data stores;data usability;dataset query;domain-model terms;gas turbine;graphical user interface;industrial equipment design;semantic technologies;Big data;Data models;Databases;Semantics;Time series analysis;Wind turbines;big data;data management;domain model;engineering;ontology;semantics;time series data}, 
doi={10.1109/BigData.2015.7363864}, 
month={Oct},}
@INPROCEEDINGS{5254660, 
author={U. Steinhoff and B. Schiele}, 
booktitle={2009 International Symposium on Wearable Computers}, 
title={An Exploration of Daily Routine Modeling Based on Bluetooth and GSM-Data}, 
year={2009}, 
pages={141-142}, 
abstract={We explore GSM & Bluetooth (BT) data to capture daily routines and social environments. With focus on the individual, non-networked user, showing the complementarity of these modalities. We present a realistic real-life dataset over 22 weeks and analyze probabilistic topic models to automatically discover typical situations of everyday life.}, 
keywords={Bluetooth;cellular radio;data analysis;pattern recognition;Bluetooth;GSM;daily routine modeling;social environment;Bluetooth;Data analysis;Data mining;GSM;Global Positioning System;Information analysis;Medical services;Mobile handsets;Portable computers;Wearable computers}, 
doi={10.1109/ISWC.2009.31}, 
ISSN={1550-4816}, 
month={Sept},}
@INPROCEEDINGS{6920709, 
author={S. Memar and L. S. Affendey and N. Mustapha and M. Ektefa}, 
booktitle={2013 13th International Conference on Intellient Systems Design and Applications}, 
title={Concept-based video retrieval model based on the combination of semantic similarity measures}, 
year={2013}, 
pages={64-68}, 
abstract={Multimedia Information Retrieval is one of the most challenging and novel issues. Search for knowledge in the form of video is the main focus of this study. In recent years, there has been a tremendous need to query and process large amount of data that cannot be easily described such as video data. There is a mismatch between the low-level interpretation of video frames and the way users express their information needs. This issue leads to the problem named semantic gap. To bridge semantic gap, concept-based video retrieval has been considered as a feasible alternative technique for video search. In order to retrieve a desirable video shot, a query should be defined based on users' needs. In spite of the fact that query can be on object, motion, texture, color and so on, queries which are expressed in terms of semantic concepts are more intuitive and realistic for end users. Therefore, a concept-based video retrieval model based on the combination of the knowledge-based and corpus-based semantic word similarity measures is proposed with respect to bridging semantic gap and supporting semantic queries. For evaluation purpose, TRECVID 2005 dataset is utilized as well.}, 
keywords={knowledge based systems;multimedia computing;semantic networks;video retrieval;video signal processing;concept-based video retrieval model;corpus-based semantic word similarity measures;knowledge-based semantic word similarity measures;multimedia information retrieval;semantic concepts;semantic gap;semantic queries;semantic similarity measures;user needs;video search;video shot retrieval;Atmospheric measurements;Bridges;Computational modeling;Educational institutions;Particle measurements;Semantics;World Wide Web;Concept retrieval;Search;Semantic knowledge;Video}, 
doi={10.1109/ISDA.2013.6920709}, 
ISSN={2164-7143}, 
month={Dec},}
@INPROCEEDINGS{6547440, 
author={A. Zaslavsky and P. P. Jayaraman and S. Krishnaswamy}, 
booktitle={2013 IEEE 29th International Conference on Data Engineering Workshops (ICDEW)}, 
title={ShareLikesCrowd: Mobile analytics for participatory sensing and crowd-sourcing applications}, 
year={2013}, 
pages={128-135}, 
abstract={Data and continuous data streams from mobile users/devices are becoming increasingly important for numerous applications including urban modelling, transportation, and more recently for mobile crowd-sensing to support citizen journalism and participatory sensing where sensor informatics and social networking meet. While significant efforts have focused towards the analysis of mobile user data, a key challenge that needs to be addressed in order to realize the full-potential is to address the scalability issues of real-time data collection and processing at run time. By scalability, we refer to both the challenges of data capture from a large number of users, as well as the issues of energy consumed on individual devices as a result of that capture. In this paper, we present mobile/on-board data stream mining as an effective approach to address the scalability issues of mobile data collection and run-time processing and as a significant component of mobile run-time analytics. We present experimental evaluation using the Nokia mobile data challenge open track dataset to show the significant energy and bandwidth savings that mobile data stream mining can achieve with no significant loss of useful information in this process.}, 
keywords={data mining;mobile computing;Nokia mobile data challenge open track dataset;ShareLikesCrowd;citizen journalism;continuous data streams;crowd-sourcing applications;data capture;experimental evaluation;mobile analytics;mobile crowd-sensing;mobile data collection;mobile data stream mining;mobile devices;mobile run-time analytics;mobile user data;mobile users;on-board data stream mining;participatory sensing;real-time data collection;real-time data processing;run-time processing;sensor informatics;social networking;transportation;urban modelling;Data analysis;Data collection;Data mining;Mobile communication;Mobile handsets;Sensors;Servers}, 
doi={10.1109/ICDEW.2013.6547440}, 
month={April},}
@INPROCEEDINGS{7536534, 
author={J. Wang and J. Tang and D. Yang and E. Wang and G. Xue}, 
booktitle={2016 IEEE 36th International Conference on Distributed Computing Systems (ICDCS)}, 
title={Quality-Aware and Fine-Grained Incentive Mechanisms for Mobile Crowdsensing}, 
year={2016}, 
pages={354-363}, 
abstract={Limited research efforts have been made for Mobile CrowdSensing (MCS) to address quality of the recruited crowd, i.e., quality of services/data each individual mobile user and the whole crowd are potentially capable of providing, which is the main focus of the paper. Moreover, to improve flexibility and effectiveness, we consider fine-grained MCS, in which each sensing task is divided into multiple subtasks and a mobile user may make contributions to multiple subtasks. In this paper, we first introduce mathematical models for characterizing the quality of a recruited crowd for different sensing applications. Based on these models, we present a novel auction formulation for quality-aware and fine-grained MCS, which minimizes the expected expenditure subject to the quality requirement of each subtask. Then we discuss how to achieve the optimal expected expenditure, and present a practical incentive mechanism to solve the auction problem, which is shown to have the desirable properties of truthfulness, individual rationality and computational efficiency. We conducted trace-driven simulation using the mobility dataset of San Francisco taxies. Extensive simulation results show the proposed incentive mechanism achieves noticeable expenditure savings compared to two well-designed baseline methods, and moreover, it produces close-to-optimal solutions.}, 
keywords={mobile computing;San Francisco taxies;auction formulation;auction problem;close-to-optimal solutions;expenditure savings;fine-grained MCS;fine-grained incentive mechanisms;mathematical models;mobile crowdsensing;mobile user;mobility dataset;quality of services;quality-aware MCS;quality-aware incentive mechanisms;trace-driven simulation;Computational modeling;Mathematical model;Mobile communication;Sensors;Smart phones;Sociology;Statistics;Auction;Incentive Mechanism;Mobile Crowdsensing;Quality of Crowd;Smartphones}, 
doi={10.1109/ICDCS.2016.30}, 
ISSN={1063-6927}, 
month={June},}
@INPROCEEDINGS{889960, 
author={M. Davies and C. J. S. daSilva}, 
booktitle={2000 First International Conference Advances in Medical Signal and Information Processing (IEE Conf. Publ. No. 476)}, 
title={An animated model of the knee joint}, 
year={2000}, 
pages={117-122}, 
abstract={We consider the development of an animated model of the human knee joint using the Visible Human dataset as the source of anatomical data. We describe this dataset and the techniques that were used to extract and render the appropriate anatomical details. We then examine the kinematics of the knee joint that the model illustrates and the way in which these were modelled. Finally, we describe the interface that was developed to allow users to interact with the model}, 
keywords={biomechanics;computer animation;medical image processing;rendering (computer graphics);user interfaces;visual databases;Visible Human dataset;anatomical data;animated model;human knee joint;knee kinematics;medical image processing;rendering;user interface}, 
doi={10.1049/cp:20000326}, 
ISSN={0537-9989}, 
month={},}
@INPROCEEDINGS{1013025, 
author={R. Vivanco and N. Pizzi}, 
booktitle={IEEE CCECE2002. Canadian Conference on Electrical and Computer Engineering. Conference Proceedings (Cat. No.02CH37373)}, 
title={Computational performance of Java and C++ in processing large biomedical datasets}, 
year={2002}, 
volume={2}, 
pages={691-696 vol.2}, 
abstract={In the analysis of image-based biomedical data, such as functional magnetic resonance imaging (fMRI) or magnetic resonance spectroscopy (fMRI), data sets from 40 to 90 MB are not uncommon, and some experimental fMRI paradigms require as much as 200 MB of storage. Software systems for the analysis of such data require a flexible data model, fast computational techniques and a graphical user interface. Object-oriented programming languages, such as Java and C++, facilitate software reuse and maintainability and provide the foundations for the development of complex data analysis applications. A is paper explores the advantages and disadvantages of using these two programming environments when processing large datasets.}, 
keywords={C++ language;Java;biomedical MRI;graphical user interfaces;magnetic resonance spectroscopy;medical image processing;40 to 200 MB;C++;GUI;Java;MRS;OO programming languages;complex data analysis applications;computational performance;fMRI;fast computational techniques;flexible data model;functional magnetic resonance imaging;graphical user interface;image-based biomedical data;large biomedical dataset processing;magnetic resonance spectroscopy;object-oriented programming languages;software maintainability;software reuse;Bioinformatics;Biomedical computing;Data analysis;Image analysis;Image storage;Java;Magnetic analysis;Magnetic resonance;Magnetic resonance imaging;Software maintenance}, 
doi={10.1109/CCECE.2002.1013025}, 
ISSN={0840-7789}, 
month={},}
@INPROCEEDINGS{6382872, 
author={W. Chen and S. Cheng and X. He and F. Jiang}, 
booktitle={2012 Second International Conference on Cloud and Green Computing}, 
title={InfluenceRank: An Efficient Social Influence Measurement for Millions of Users in Microblog}, 
year={2012}, 
pages={563-570}, 
abstract={Microblog, as one of the most popular social networking services, plays an increasingly significant role in communication and information propagation. Among the numerous studies on social network, one critical problem is identifying the influencers (or opinion leaders) and quantifying the influence strength of each individual effectively. This paper focuses on the problem of measuring users' influence in microblog network. We define respectively, the social influence in terms of the ability of interactivity driven and the breadth of information dissemination in global network. With the analysis of the characteristics of interactive behaviors and the way of information spread, the principal factors indicating influence are explored, which include the quantity and quality of followers, the quality of tweets, the ratio of retweeting and the similarity of users' interests. Although so many metrics have been taken into account to measure influence in proposed User Relative Influence Measure Model and User Network Global Influence Model, our Influence Rank Algorithm to implement the models is only O(e) on time complexity. Finally, the experimental evaluations and comparisons with related algorithms on million-user-level dataset demonstrate the efficiency and effectiveness of Influence Rank Algorithm.}, 
keywords={computational complexity;social networking (online);user interfaces;communication propagation;follower quality;follower quantity;individual influence strength;influence rank algorithm;information dissemination;information propagation;information spread;microblog;retweeting ratio;social influence measurement;social networking service;time complexity;tweet quality;user influence;user interest similarity;user network global influence model;user relative influence measure model;Algorithm design and analysis;Analytical models;Media;Social network services;Time measurement;measurement;microblog;social influence}, 
doi={10.1109/CGC.2012.31}, 
month={Nov},}
@INPROCEEDINGS{6551252, 
author={A. Fella and F. Bianchi and V. Ciaschini and M. Corvo and D. Delprete and A. Di Simone and G. Donvito and P. Franchini and F. Giacomini and A. Gianoli and A. Gianelle and S. Longo and S. Luiz and E. Luppi and M. Manzali and S. Pardi and A. Perez and M. Rama and G. Russo and B. Santeramo and R. Stroili and L. Tomassetti}, 
booktitle={2012 IEEE Nuclear Science Symposium and Medical Imaging Conference Record (NSS/MIC)}, 
title={A prototype suite for data-analysis management of the SuperB experiment}, 
year={2012}, 
pages={975-977}, 
abstract={The SuperB asymmetric e+e- collider and detector to be built at the newly founded Nicola Cabibbo Lab will provide a uniquely sensitive probe of New Physics in the flavor sector of the Standard Model. Studying minute effects in the heavy quark and heavy lepton sectors requires a data sample of 75ab-1 and a peak luminosity of 1036cm-2s-1. Providing a user-friendly solution to workload management in a distributed resource system is one of the key goal of a HEP community as SuperB experiment. Physicists involved in Monte Carlo simulation productions and in data analysis should be able to perform job management and basic data transfer operations limiting as possible training costs and maximizing flexibility in resource exploitation. The SuperB computing group adopted Ganga as the interface layer of such distributed analysis infrastructure. A SuperB-specific Ganga-plugin has been developed to accomplish experiment requirements as information system interface, use cases implementation, dataset management and job wrapper interactions. This work will present the Ganga plugin design and implementation, and its integration with the wider distributed system adopted by the collaboration.}, 
keywords={Monte Carlo methods;high energy physics instrumentation computing;prototypes;standard model;Ganga interface layer;Monte Carlo simulation;Nicola Cabibbo Lab;Standard Model;SuperB experiment;data analysis management;peak luminosity;positron+electron collider;prototype suite;resource exploitation;Distributed analysis facility;Distributed computing;Ganga;High Energy Physics;SuperB experiment}, 
doi={10.1109/NSSMIC.2012.6551252}, 
ISSN={1082-3654}, 
month={Oct},}
@INPROCEEDINGS{7472860, 
author={E. Ferreira and A. R. Masson and B. Jabaian and F. Lefèvre}, 
booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Adversarial Bandit for online interactive active learning of zero-shot spoken language understanding}, 
year={2016}, 
pages={6155-6159}, 
abstract={Many state-of-the-art solutions for the understanding of speech data have in common to be probabilistic and to rely on machine learning algorithms to train their models from large amount of data. The difficulty remains in the cost of collecting and annotating such data. Another point is the time for updating an existing model to a new domain. Recent works showed that a zero-shot learning method allows to bootstrap a model with good initial performance. To do so, this method relies on exploiting both a small-sized ontological description of the target domain and a generic word-embedding semantic space for generalization. Then, this framework has been extended to exploit user feedbacks to refine the zero-shot semantic parser parameters and increase its performance online. In this paper, we propose to drive this online adaptive process with a policy learnt using the Adversarial Bandit algorithm Exp3. We show, on the second Dialog State Tracking Challenge (DSTC2) datasets, that this proposition can optimally balance the cost of gathering valuable user feedbacks and the overall performance of the spoken language understanding module.}, 
keywords={learning (artificial intelligence);ontologies (artificial intelligence);speech processing;statistical analysis;DSTC2 dataset;adversarial bandit algorithm Exp3;generic word-embedding semantic space;machine learning algorithm;online adaptive process;online interactive active learning;second dialog state tracking challenge dataset;small-sized ontological description;zero-shot learning method;zero-shot semantic parser parameter;zero-shot spoken language understanding;Adaptation models;Cost function;Data collection;Databases;Learning systems;Loss measurement;Semantics;Spoken language understanding;bandit problem;online adaptation;out-of-domain training data;zero-shot learning}, 
doi={10.1109/ICASSP.2016.7472860}, 
month={March},}
@ARTICLE{7931647, 
author={C. Wang and A. Kalra and L. Zhou and C. Borcea and Y. Chen}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Probabilistic Models For Ad Viewability Prediction On The Web}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Online display advertising has becomes a billion-dollar industry, and it keeps growing. Advertisers attempt to send marketing messages to attract potential customers via graphic banner ads on publishers’ webpages. Advertisers are charged for each view of a page that delivers their display ads. However, recent studies have discovered that more than half of the ads are never shown on users' screens due to insufficient scrolling. Thus, advertisers waste a great amount of money on these ads that do not bring any return on investment. Given this situation, the Interactive Advertising Bureau calls for a shift toward charging by viewable impression, i.e., charge for ads that are viewed by users. With this new pricing model, it is helpful to predict the viewability of an ad. This paper proposes two probabilistic latent class models (PLC) that predict the viewability of any given scroll depth for a user-page pair. Using a real-life dataset from a large publisher, the experiments demonstrate that our models outperform comparison systems.}, 
keywords={Advertising;Data models;Layout;Predictive models;Pricing;Probabilistic logic;Real-time systems;Computational Advertising;User Behavior;Viewability Prediction}, 
doi={10.1109/TKDE.2017.2705688}, 
ISSN={1041-4347}, 
month={},}
@ARTICLE{5278049, 
author={S. Shirali-Shahreza and H. Abolhassani and M. H. Shirali-shahreza}, 
journal={IEEE Transactions on Consumer Electronics}, 
title={Fast and scalable system for automatic artist identification}, 
year={2009}, 
volume={55}, 
number={3}, 
pages={1731-1737}, 
abstract={Digital music technologies enable users to create and use large collections of music. One of the desirable features for users is the ability to automatically organize the collection and search in it. One of the operations that they need is automatic identification of tracks' artists. This operation can be used to automatically classify new added tracks to a collection. Additionally, the user can use this operation to identify the artist of an unknown track. The artist name of a track can help the user find similar music. In this paper, we introduce a fast and scalable system that can automatically identify the artist of music tracks. This system is creating a signature for each track that is a compact representation of the tracks. The tracks' signatures of an artist are then used to create a signature for that artist. A similarity measure is also defined to measure the distance or dissimilarity of two signatures. This similarity measure is based on graph matching. To identify the signature of an unknown track, the signature of that track is compared with the artists' signatures and the nearest artist is selected as the artist of the track. The accuracy of the system on the artist20 dataset is 71.5% which is better than previously reported results on this dataset. In comparison to other proposed methods, the artist model creation and model updates are faster and more scalable in our system. Additionally, the search time of our system is less than other systems and only depends on the number of artist.}, 
keywords={fingerprint identification;music;automatic artist identification;digital music technology;graph matching;scalable system;track classification;track signature;Computer science education;Digital recording;Educational technology;Fingerprint recognition;Instruments;Loudspeakers;Microcomputers;Mobile handsets;Music;Paper technology;Artist Identification;Audio Fingerprinting;Graph Matching;Music Processing}, 
doi={10.1109/TCE.2009.5278049}, 
ISSN={0098-3063}, 
month={August},}
@INPROCEEDINGS{6141398, 
author={V. Kumar and J. Sachdeva and I. Gupta and N. Khandelwal and C. K. Ahuja}, 
booktitle={2011 World Congress on Information and Communication Technologies}, 
title={Classification of brain tumors using PCA-ANN}, 
year={2011}, 
pages={1079-1083}, 
abstract={The present study is conducted to assist radiologists in marking tumor boundaries and in decision making process for multiclass classification of brain tumors. Primary brain tumors and secondary brain tumors along with normal regions are segmented by Gradient Vector Flow (GVF)-a boundary based technique. GVF is a user interactive model for extracting tumor boundaries. These segmented regions of interest (ROIs) are than classified by using Principal Component Analysis-Artificial Neural Network (PCA-ANN) approach. The study is performed on diversified dataset of 856 ROIs from 428 post contrast T1- weighted MR images of 55 patients. 218 texture and intensity features are extracted from ROIs. PCA is used for reduction of dimensionality of the feature space. Six classes which include primary tumors such as Astrocytoma (AS), Glioblastoma Multiforme (GBM), child tumor-Medulloblastoma (MED) and Meningioma (MEN), secondary tumor-Metastatic (MET) along with normal regions (NR) are discriminated using ANN. Test results show that the PCA-ANN approach has enhanced the overall accuracy of ANN from 72.97 % to 95.37%. The proposed method has delivered a high accuracy for each class: AS-90.74%, GBM-88.46%, MED-85.00%, MEN-90.70%, MET-96.67%and NR-93.78%. It is observed that PCA-ANN provides better results than the existing methods.}, 
keywords={biomedical MRI;feature extraction;gradient methods;image classification;image segmentation;medical image processing;neural nets;principal component analysis;radiology;tumours;GVF boundary based technique;PCA-ANN approach;artificial neural network;astrocytoma;brain tumor;child tumor-medulloblastoma;decision making process;dimensionality reduction;feature extraction;glioblastoma multiforme;gradient vector flow;intensity feature;meningioma;multiclass classification;principal component analysis;radiologist;secondary tumor-metastatic;segmented regions of interest;texture feature;tumor boundaries;user interactive model;weighted MR image;Accuracy;Artificial neural networks;Feature extraction;Gabor filters;Image segmentation;Principal component analysis;Tumors;Gradient Vector Flow (GVF);Principal component analysis (PCA);brain tumor classification;feature extraction;regions of interest (ROIs)}, 
doi={10.1109/WICT.2011.6141398}, 
month={Dec},}
@INPROCEEDINGS{7929939, 
author={H. Yin and L. Chen and W. Wang and X. Du and Q. V. H. Nguyen and X. Zhou}, 
booktitle={2017 IEEE 33rd International Conference on Data Engineering (ICDE)}, 
title={Mobi-SAGE: A Sparse Additive Generative Model for Mobile App Recommendation}, 
year={2017}, 
pages={75-78}, 
abstract={With the rapid prevalence of smart mobile devices and the dramatic proliferation of mobile applications (Apps), App recommendation becomes an emergent task that will benefit different stockholders of mobile App ecosystems. Unlike traditional items, Apps have privileges to access a user's sensitive resources (e.g., contacts, messages and locations) which may lead to security risk or privacy leak. Thus, users' choosing of Apps are influenced by not only their personal interests but also their privacy preferences. Moreover, user privacy preferences vary with App categories. In this paper, we propose a mobile sparse additive generative model (Mobi-SAGE) to recommend Apps by considering both user interests and category-aware user privacy preferences. We collected a real-world dataset from 360 App store - the biggest Android App platform in China, and conduct extensive experiments on it. The experimental results show that our Mobi-SAGE consistently and significantly outperforms the state-of-the-art approaches, which implies the importance of exploiting category-aware user privacy preferences.}, 
keywords={Additives;Data privacy;Mobile communication;Privacy;Smart phones;Visualization}, 
doi={10.1109/ICDE.2017.43}, 
month={April},}
@ARTICLE{7127030, 
author={M. Yan and J. Sang and C. Xu and M. S. Hossain}, 
journal={IEEE Transactions on Multimedia}, 
title={YouTube Video Promotion by Cross-Network Association: @Britney to Advertise Gangnam Style}, 
year={2015}, 
volume={17}, 
number={8}, 
pages={1248-1261}, 
abstract={The emergence and rapid proliferation of various social media networks have reshaped the way how video contents are generated, distributed, and consumed in traditional video sharing portals. Nowadays, online videos can be accessed from far beyond the internal mechanisms of the video sharing portals, such as internal search and front page highlight. Recent studies have found that external referrers, such as external search engines and other social media websites, arise to be the new and important portals to lead users to online videos. In this paper, we introduce a novel cross-network collaborative application to help drive the online traffic for given videos in the traditional video portal YouTube by leveraging the high propagation efficiency of the popular Twitter followees. Since YouTube videos and Twitter followees distribute on heterogeneous spaces, we present a cross-network association-based solution framework. In this framework, we first represent YouTube videos and Twitter followees in the corresponding topic spaces separately by employing generative topic models. Then, the cross-network topic spaces are associated from both semantic-based and network-based perspectives through the collective intelligence of the observed overlapped users. Based on the derived cross-network association, we finally match the query YouTube videos and candidate Twitter followees in the same topic space with a unified ranking method. The experiments on a real-world large-scale dataset of more than 2.2 million YouTube videos and 31.8 million tweets from 38,540 YouTube users and 39,400 Twitter users demonstrate the effectiveness and superiority of our solution in which network-based and semantic-based association are integrated.}, 
keywords={social networking (online);Twitter followees;Web sites;YouTube video promotion;collective intelligence;cross-network collaborative application;front page highlight;gangnam style;heterogeneous spaces;internal search;network-based perspectives;online traffic;semantic-based perspectives;social media networks;unified ranking method;video contents;video sharing portals;Media;Portals;Semantics;Twitter;YouTube;Cross-network analysis;social media;video promotion}, 
doi={10.1109/TMM.2015.2446949}, 
ISSN={1520-9210}, 
month={Aug},}
@INPROCEEDINGS{5734246, 
author={S. T. Kannan and K. Iyakutti}, 
booktitle={2009 5th IEEE GCC Conference Exhibition}, 
title={A clustered indexing method for optimizing the query for biological database}, 
year={2009}, 
pages={1-6}, 
abstract={Genome datasets have been growing exponentially in the past few years. With this rapid growth, genome datasets and the associated access structures have become too larger to fit in the main memory of a computer. It leads to a large number of disk accesses and slow response times occurred for queries. Here we should take all possible efforts to develop proper tools to access the data and mine them efficiently, otherwise, data mine will be wasted and leads to increase the search time and lack of efficiency. This paper describes a new architecture for the approximate matching of unstructured string data using clustering and indexes. Here we are using projected clustering algorithm, HARP for effective clustering. Because it heavily supports for clustering a high dimensional data. Some existing algorithms depend on some critical user parameters in determining the relevant attributes of each cluster. In case wrong parameter values are used, the clustering performance will be seriously degraded. The correct parameter values are rarely known in real datasets. However, it responds to the clustering status and adjusts the internal thresholds dynamically. The second component of the model, a new metric index, called M+Tree is used for very large dataset. Because it contains the key dimension feature, which effectively reduces the response time for similarity search. The main idea behind here is to make the fan-out of tree larger by partitioning a subspace further into two subspaces, called twin-nodes. By utilizing the twin-nodes, the filtering effectiveness can be doubled. In addition, for ensuring high space utilization, data will be reallocated dynamically between the twin nodes. The new method has been tested with both simulated and real expression data. The results show that it is able to uncover interesting patterns effectively. Based on these patterns, overlapping clusters can be discovered. It can be better understood that the expression levels at which each cluster of- genes co-expresses under different conditions.}, 
keywords={bioinformatics;data mining;data structures;database indexing;genomics;pattern clustering;query processing;very large databases;HARP;M+Tree;access structures;biological database query;clustered indexing method;data mining;genome datasets;projected clustering algorithm;search time;twin nodes;unstructured string data;very large dataset;Arrays;Clustering algorithms;Neurons;Partitioning algorithms;Power measurement;Prediction algorithms;Topology;Clustering;Indexing;Overlapping clusters;Query;Searching;bioinformatics;data mining}, 
doi={10.1109/IEEEGCC.2009.5734246}, 
month={March},}
@INPROCEEDINGS{7131733, 
author={A. L. Simeone and H. Gellerseny}, 
booktitle={2015 IEEE Symposium on 3D User Interfaces (3DUI)}, 
title={Comparing indirect and direct touch in a stereoscopic interaction task}, 
year={2015}, 
pages={105-108}, 
abstract={In this paper we studied the impact that the directedness of touch interaction has on a path following task performed on a stereoscopic display. The richness of direct touch interaction comes with the potential risk of occluding parts of the display area, in order to express one's interaction intent. In scenarios where attention to detail is of critical importance, such as browsing a 3D dataset or navigating a 3D environment, important details might be missed. We designed a user study in which participants were asked to move an object within a 3D environment while avoiding a set of static distractor objects. Participants used an indirect touch interaction technique on a tablet and a direct touch technique on the screen. Results of the study show that in the indirect touch condition, participants made 30% less collisions with the distractor objects.}, 
keywords={haptic interfaces;human computer interaction;touch sensitive screens;3D dataset browsing;3D environment navigation;critical importance;direct touch interaction;display area;distractor objects;indirect touch interaction technique;interaction intent;screens;static distractor objects;stereoscopic display;stereoscopic interaction task;tablets;Cameras;Performance evaluation;Solid modeling;Stereo image processing;Three-dimensional displays;User interfaces;Visualization}, 
doi={10.1109/3DUI.2015.7131733}, 
month={March},}
@INPROCEEDINGS{6374933, 
author={X. Chen and B. g. Cui and F. Zhang}, 
booktitle={2012 International Conference on Management of e-Commerce and e-Government}, 
title={A Method of XML Containing Data State in Emergency Plan Management Information System}, 
year={2012}, 
pages={319-322}, 
abstract={In Web application system, the response time is one of the evaluating indicators of system performance, which is effected by many factors both software and hardware. Firstly, we propose a novel model to formalize text emergency plan of different kinds of crisis and emergency handling into a structuring dataset. Based on the data structure of emergency plan during development, we propose a method containing data state in XML to reduce server response time, when users edit data item located on a few of Web pages at a time. We test the response time using the data state method in Emergency Plan Management Information System.}, 
keywords={Internet;XML;data structures;emergency management;information systems;Web application system;XML containing data state;data structure;emergency plan management;information system;text emergency plan;Accidents;Browsers;Databases;Servers;Time factors;Web pages;XML;Data state;Emergency Plan;Response time;XML}, 
doi={10.1109/ICMeCG.2012.83}, 
month={Oct},}
@INPROCEEDINGS{6596376, 
author={G. Delac and M. Silic and K. Vladimir}, 
booktitle={2013 36th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)}, 
title={Reliability sensitivity analysis for Yahoo! Pipes mashups}, 
year={2013}, 
pages={851-856}, 
abstract={Development of Web technologies has pushed forward a wide array of tools which can be used by the end-users to customize their own environment. Mashup development frameworks stand out as means to create Web applications by combining the existing content or functionality provided by various, disparate, providers on the Web. Like for all composite systems, reliability is one of the primary design concerns in mashup development. This is due to the fact that the content used in the mashup is often not under the supervision of the mashup developer and is rather deployed on the Web by a third-party provider. In this paper a reliability sensitivity analysis is conducted for the collected dataset of Yahoo! Pipes mashups. Reliability of each mashup in the dataset is estimated using a reliability model based on belief networks. A sensitivity analysis is conducted on the reliability models to ascertain which building components have the greatest impact on the overall reliability of the data set.}, 
keywords={Internet;sensitivity analysis;software reliability;user interfaces;Web technologies;Yahoo! Pipes mashups;mashup development;reliability sensitivity analysis;Analytical models;Buildings;Data models;Mashups;Reliability;Sensitivity analysis;Unified modeling language}, 
month={May},}
@ARTICLE{6197712, 
author={A. Merati and N. Poh and J. Kittler}, 
journal={IEEE Transactions on Information Forensics and Security}, 
title={User-Specific Cohort Selection and Score Normalization for Biometric Systems}, 
year={2012}, 
volume={7}, 
number={4}, 
pages={1270-1277}, 
abstract={An increasing body of evidence suggests that cohort-based score normalization can improve the performance of biometric authentication. This approach relies on the use of N cohort biometric templates, which can be computationally expensive. We contribute to the advancement of cohort score normalization in two ways. First, we show both theoretically and empirically that the most similar and the most dissimilar cohort templates to a target user contain discriminative information. We then investigate the extraction of this information using polynomial regression. Extensive evaluation on the face and fingerprint modalities in the Biosecure DS2 dataset indicates that the proposed method outperforms the state-of-the-art cohort score normalization methods, while reducing the computation cost by as much as half.}, 
keywords={face recognition;fingerprint identification;polynomials;regression analysis;Biosecure DS2 dataset;biometric authentication;biometric systems;cohort biometric templates;cohort-based score normalization;discriminative information;face modalities;fíngerprint modalities;polynomial regression;user-specific cohort selection;Authentication;Data mining;Indexes;Materials;Mathematical model;Polynomials;Support vector machines;Biometric authentication;cohort-based score normalization;discriminative cohort;ordered cohort selection}, 
doi={10.1109/TIFS.2012.2198469}, 
ISSN={1556-6013}, 
month={Aug},}
@ARTICLE{7776947, 
author={M. Alibeigi and M. N. Ahmadabadi and B. N. Araabi}, 
journal={IEEE Transactions on Robotics}, 
title={A Fast, Robust, and Incremental Model for Learning High-Level Concepts From Human Motions by Imitation}, 
year={2017}, 
volume={33}, 
number={1}, 
pages={153-168}, 
abstract={Social robots are becoming a companion in everyday life. To be well accepted by humans, they should efficiently understand meanings of their partners' motions and body language and respond accordingly. Learning concepts by imitation brings them this ability in a user-friendly way. This paper presents a fast and robust model for incremental learning of concepts by imitation (ILoCI). In ILoCI, observed multimodal spatiotemporal demonstrations are incrementally abstracted and generalized based on their perceptual and functional similarities during the imitation. Perceptually similar demonstrations are abstracted by a dynamic model of the mirror neuron system. The functional similarities of demonstrations are also learned through a limited number of interactions with the teacher. Incremental relearning of acquired concepts together through memory rehearsal enables the learner to gradually extract and utilize the common structural relations among demonstrations to expedite the learning process especially at the initial stages. Performance of ILoCI is assessed using a standard benchmark dataset and a human-robot interaction task in which a humanoid robot learns to abstract teacher's hand motions during imitation. Its performance is also evaluated on occluded observations that are probable in real environments. The results show efficiency of ILoCI in concept acquisition, recognition, prediction, and generation in addition to its robustness to occlusions and high variability in observations.}, 
keywords={human-robot interaction;humanoid robots;learning by example;ILoCI;high-level concept learning;human motion imitation;human-robot interaction;humanoid robot;incremental learning of concepts by imitation;mirror neuron system;Biological system modeling;Hidden Markov models;Mirrors;Neurons;Robot sensing systems;Robustness;Concepts;humanoid robots;imitation learning;social human–robot interaction}, 
doi={10.1109/TRO.2016.2623817}, 
ISSN={1552-3098}, 
month={Feb},}
@INPROCEEDINGS{1644308, 
author={S. Cohen and D. E. Guzman}, 
booktitle={18th International Conference on Scientific and Statistical Database Management (SSDBM'06)}, 
title={SQL.CT: Providing Data Management for Visual Exploration of CT Datasets}, 
year={2006}, 
pages={143-148}, 
abstract={We present the design and implementation of SQL.CT, a prototype for managing, querying, and visualizing 3D CT (computed tomography) datasets. The system is motivated by scientific studies of fossil records. Our prototype is built on top of SQL server 2005 and uses both Web service and service broker technologies. The desktop client utilizes commodity graphics cards to interactively render 3D volumes. This paper presents the user requirements, discusses the design of the data model and desktop client, and reports on preliminary performance measurements. Our experiments show that by pushing the spatial logic as low as possible into the DBMS, we can filter the data early. We observe some significant performance benefits from the reduced message traffic to and from the remote client}, 
keywords={Web services;computerised tomography;data models;data visualisation;medical computing;medical information systems;query processing;rendering (computer graphics);visual databases;3D computed tomography dataset;3D volume rendering;DBMS;SQL server 2005;SQL.CT;Web service;commodity graphics card;data model;fossil record;message traffic;service broker technology;spatial logic;visual exploration;Computed tomography;Computer graphics;Data models;Data visualization;Filters;Logic;Measurement;Prototypes;Traffic control;Web services}, 
doi={10.1109/SSDBM.2006.48}, 
ISSN={1551-6393}, 
month={},}
@INPROCEEDINGS{5234491, 
author={F. Ahmed and P. Campbell and A. Jaffar and L. F. Capretz}, 
booktitle={2009 2nd IEEE International Conference on Computer Science and Information Technology}, 
title={Managing support requests in open source software project: The role of online forums}, 
year={2009}, 
pages={590-594}, 
abstract={The use of free and open source software is gaining momentum due to the ever increasing availability and use of the Internet. Organizations are also now adopting open source software, despite some reservations in particular regarding the provision and availability of support. One of the greatest concerns about free and open source software is the availability of post release support and the handling of for support. A common belief is that there is no appropriate support available for this class of software, while an alternative argument is that due to the active involvement of Internet users in online forums, there is in fact a large resource available that communicates and manages the management of support requests. The research model of this empirical investigation establishes and studies the relationship between open source software support requests and online public forums. The results of this empirical study provide evidence about the realities of support that is present in open source software projects. We used a dataset consisting of 616 open source software projects covering a broad range of categories in this investigation. The results show that online forums play a significant role in managing support requests in open source software, thus becoming a major source of assistance in maintenance of the open source projects.}, 
keywords={Internet;project management;public domain software;software maintenance;Internet users;online forums;open source projects maintenance;open source software project;Collaborative software;Discussion forums;Internet;Open source software;Programming;Project management;Resource management;Software maintenance;Software quality;Technology management;Online Fourms;Open Source Software;Support Requests},
doi={10.1109/ICCSIT.2009.5234491}, 
month={Aug},}
@INPROCEEDINGS{6478225, 
author={A. Ibrahim and H. Jin and A. A. Yassin and D. Zou}, 
booktitle={2012 IEEE Asia-Pacific Services Computing Conference}, 
title={Secure Rank-Ordered Search of Multi-keyword Trapdoor over Encrypted Cloud Data}, 
year={2012}, 
pages={263-270}, 
abstract={Advances in cloud computing and Internet technologies have pushed more and more data owners to outsource their data to remote cloud servers to enjoy with huge data management services in an efficient cost. However, despite its technical advances, cloud computing introduces many new security challenges that need to be addressed well. This is because, data owners, under such new setting, loss the control over their sensitive data. To keep the confidentiality of their sensitive data, data owners usually outsource the encrypted format of their data to the untrusted cloud servers. Several approaches have been provided to enable searching the encrypted data. However, the majority of these approaches are limited to handle either a single keyword search or a Boolean search but not a multikeyword ranked search, a more efficient model to retrieve the top documents corresponding to the provided keywords. In this paper, we propose a secure multi-keyword ranked search scheme over the encrypted cloud data. Such scheme allows an authorized user to retrieve the most relevant documents in a descending order, while preserving the privacy of his search request and the contents of documents he retrieved. To do so, data owner builds his searchable index, and associates with each term document with a relevance score, which facilitates document ranking. The proposed scheme uses two distinct cloud servers, one for storing the secure index, while the other is used to store the encrypted document collection. Such new setting prevents leaking the search result, i.e. the document identifiers, to the adversary cloud servers. We have conducted several empirical analyses on a real dataset to demonstrate the performance of our proposed scheme.}, 
keywords={authorisation;cloud computing;cryptography;data privacy;document handling;Boolean search;Internet technology;cloud computing;data management service;data owner;document identifier;document ranking;empirical analysis;encrypted cloud data;multikeyword ranked search;multikeyword trapdoor;privacy preservation;search request;secure rank-ordered search;sensitive data confidentiality;sensitive data control;single keyword search;user authorization;Cloud computing;Encryption;Indexes;Privacy;Servers;Bloom filter;Cloud computing;Paillier encryption;information retrieval;searchable encryption}, 
doi={10.1109/APSCC.2012.59}, 
month={Dec},}
@INPROCEEDINGS{6556655, 
author={M. van de Giessen and Q. Tao and R. J. van der Geest and B. P. F. Lelieveldt}, 
booktitle={2013 IEEE 10th International Symposium on Biomedical Imaging}, 
title={Model-based alignment of Look-Locker MRI sequences for calibrated myocardical scar tissue quantification}, 
year={2013}, 
pages={1038-1041}, 
abstract={The characterization of myocardial scar tissue in Late Gadolinium Enhancement (LGE) MRI volumes is hampered by the nonquantitative nature of MRI image intensities. Using the widely available Look-Locker (LL) sequence that images the heart using different inversion times a T1 map can be created per patient to calibrate the LGE datasets. However, due to the nature of the LL acquisition, the myocardium is imaged at different phases of the cardiac cycle, resulting in deformations between slices of the LL stack and preventing accurate T1 map estimates. In this paper a method is proposed for the non-rigid alignment of the LL stack that uses a model of the exponential contrast development throughout the LL stack to concurrently align all LL stack slices. The model based alignment is shown to be more robust than a pairwise mutual information based alignment. More importantly, correlations between the relaxivity (R1) map and the LGE intensities (needed for the LGE calibration) are higher using the proposed alignment than when using manual annotations. The model based alignment thereby allows the use of the LL sequence for LGE calibration without manually annotating the (typically) 33 slices in this sequence. After alignment, the myocardium only needs to be annotated in the LGE slice. The latter is also needed for non-calibrated scar quantification and thus requires no additional user effort.}, 
keywords={biomedical MRI;cardiology;image registration;medical image processing;LGE MRI volumes;LGE dataset calibration;LGE intensities;Look-Locker MRI sequence;Look-Locker acquisition;T1 map;calibrated myocardical scar tissue quantification;cardiac cycle phases;exponential contrast model;heart;inversion time;late gadolinium enhancement MRI;model based alignment;myocardial scar tissue characterization;myocardium imaging;nonquantitative MRI image intensities;nonrigid LL stack alignment;relaxivity map;Calibration;Correlation;Magnetic resonance imaging;Manuals;Myocardium;Robustness;T1 mapping;calibration;late gadolinium enhanced MR;motion correction;myocardial scar;non-rigid registration}, 
doi={10.1109/ISBI.2013.6556655}, 
ISSN={1945-7928}, 
month={April},}
@INPROCEEDINGS{7389698, 
author={C. H. Hsieh and C. M. Lai and C. H. Mao and T. C. Kao and K. C. Lee}, 
booktitle={2015 International Carnahan Conference on Security Technology (ICCST)}, 
title={AD2: Anomaly detection on active directory log data for insider threat monitoring}, 
year={2015}, 
pages={287-292}, 
abstract={What you see is not definitely believable is not a rare case in the cyber security monitoring. However, due to various tricks of camouflages, such as packing or virutal private network (VPN), detecting "advanced persistent threat"(APT) by only signature based malware detection system becomes more and more intractable. On the other hand, by carefully modeling users' subsequent behaviors of daily routines, probability for one account to generate certain operations can be estimated and used in anomaly detection. To the best of our knowledge so far, a novel behavioral analytic framework, which is dedicated to analyze Active Directory domain service logs and to monitor potential inside threat, is now first proposed in this project. Experiments on real dataset not only show that the proposed idea indeed explores a new feasible direction for cyber security monitoring, but also gives a guideline on how to deploy this framework to various environments.}, 
keywords={behavioural sciences computing;invasive software;learning (artificial intelligence);AD2;active directory domain service log;active directory log data;behavioral analytic framework;cyber security monitoring;insider threat monitoring;machine learning;malware detection system;probability;Computational modeling;Computer security;Data models;Hidden Markov models;Markov processes;Monitoring;Organizations;Active Directory Log Analysis;Advanced Persistent Threat;Anomaly Detection;Behavioral Modeling;Machine Learning}, 
doi={10.1109/CCST.2015.7389698}, 
month={Sept},}
@ARTICLE{942595, 
author={J. X. Chen and H. Wechsler and J. M. Pullen and Ying Zhu and E. B. MacMahon}, 
journal={IEEE Transactions on Biomedical Engineering}, 
title={Knee surgery assistance: patient model construction, motion simulation, and biomechanical visualization}, 
year={2001}, 
volume={48}, 
number={9}, 
pages={1042-1052}, 
abstract={The authors present a new system that integrates computer graphics, physics-based modeling, and interactive visualization to assist knee study and surgical operation. First, they discuss generating patient-specific three-dimensional (3-D) knee models from patient's magnetic resonance images (MRIs). The 3-D model is obtained by deforming a reference model to match the MRI dataset. Second, the authors present simulating knee motion that visualizes patient-specific motion data on the patient-specific knee model. Third, the authors introduce visualizing biomechanical information on a patient-specific model. The focus is on visualizing contact area, contact forces, and menisci deformation. Traditional methods have difficulty in visualizing knee contact area without using invasive methods. The approach presented here provides an alternative of visualizing the knee contact area and forces without any risk to the patient. Finally, a virtual surgery can be performed. The constructed 3-D knee model is the basis of motion simulation, biomechanical visualization, and virtual surgery. Knee motion simulation determines the knee rotation angles as well as knee contact points. These parameters are used to solve the biomechanical model. The authors' results integrate 3-D construction, motion simulation, and biomechanical visualization into one system. Overall, the methodologies here are useful elements for future virtual medical systems where all the components of visualization, automated model generation and surgery simulation come together.}, 
keywords={biomechanics;biomedical MRI;orthopaedics;physiological models;surgery;biomechanical visualization;interactive visualization;invasive methods;knee surgery assistance;menisci deformation;motion simulation;patient model construction;physics-based modeling;virtual surgery;Biomedical imaging;Computational modeling;Computer graphics;Data visualization;Deformable models;Knee;Magnetic resonance;Magnetic resonance imaging;Medical simulation;Surgery;Biomechanics;Computer Graphics;Computer Simulation;Humans;Imaging, Three-Dimensional;Knee;Magnetic Resonance Imaging;Therapy, Computer-Assisted;User-Computer Interface}, 
doi={10.1109/10.942595}, 
ISSN={0018-9294}, 
month={Sept},}
@INPROCEEDINGS{6305693, 
author={D. Xu and Z. Wang and Y. Zhang and P. Zong}, 
booktitle={2012 4th International Conference on Intelligent Human-Machine Systems and Cybernetics}, 
title={A Collaborative Tag Recommendation Based on User Profile}, 
year={2012}, 
volume={1}, 
pages={331-334}, 
abstract={With the increasing popularity of social tagging, services that assist the user in the task of tagging, such as tag recommenders, are more and more required. As we all known, crucial to the performance of a recommendation system is the accuracy of the user profiles used to represent the interests of the users. We propose a tag recommendation based on user profile which represents user preferences by taking user's ranking pairwise tag preferences. Although the dataset is obtained indirectly, the experiments show that the tag recommendation based on the proposed user profile has outperformed the baseline tag recommendation.}, 
keywords={collaborative filtering;recommender systems;social networking (online);user modelling;collaborative tag recommendation;pairwise tag preference ranking;recommendation system performance;social tagging;user preference representation;user profile;Collaboration;Communities;Educational institutions;Learning systems;Recommender systems;Tagging;Vectors;personalized tag rcommendation;user prferences;user profile}, 
doi={10.1109/IHMSC.2012.89}, 
month={Aug},}
@INPROCEEDINGS{6335202, 
author={A. Bellaachia and D. Alathel}, 
booktitle={2012 6th IEEE International Conference Intelligent Systems}, 
title={Trust-based Ant Recommender (T-BAR)}, 
year={2012}, 
pages={130-135}, 
abstract={Recommender Systems suggest to users items that may be of interest to them. Collaborative filtering recommender systems suggest the items based on the item ratings provided by similar users in the network. Trust-based recommender systems utilize an explicitly issued trust between users to increase the accuracy of the recommendations. In this paper, we propose a bio-inspired algorithm, called Trust-based Ant Recommender (T-BAR), to further increase the accuracy and the coverage of the recommendations in trust-based networks. T-BAR uses the Ant Colony System computational model to imitate the behavior of ants during their search for a good food source. T-BAR's advantage over other known algorithms is that it considers all the target item ratings along the paths rather than just using the ratings found at the end of each path. The Epinions.com dataset was used for the empirical evaluation of Trust-based Ant Recommender and proved its success by drastically improving the coverage of the recommendations while maintaining a reasonable level of accuracy of the results. T-BAR outperforms the basic CF algorithm that uses the Pearson Similarity and Massa's MoleTrust (MT) by achieving a balanced trade-off between accuracy and coverage.}, 
keywords={ant colony optimisation;collaborative filtering;recommender systems;security of data;CF algorithm;Epinions.com dataset;Massa MoleTrust;Pearson similarity;T-BAR;ant behavior imitation;ant colony system computational model;bio-inspired algorithm;collaborative filtering recommender system;good food source searching;item rating;recommendation accuracy;trust-based ant recommender;trust-based network;user explicitly issued trust;Accuracy;Heuristic algorithms;Measurement;Motion pictures;Prediction algorithms;Recommender systems;Ant colony optimization;Ant colony system;Artificial agents;Bio-inspired algorithm;Recommender systems;Trust}, 
doi={10.1109/IS.2012.6335202}, 
ISSN={1541-1672}, 
month={Sept},}
@ARTICLE{6392444, 
author={M. Khabbaz and K. Kianmehr and R. Alhajj}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 
title={Employing Structural and Textual Feature Extraction for Semistructured Document Classification}, 
year={2012}, 
volume={42}, 
number={6}, 
pages={1566-1578}, 
abstract={This paper addresses XML document classification by considering both structural and content-based features of the documents. This approach leads to better constructing a set of informative feature vectors that represents both structural and textual aspects of XML documents. For this purpose, we integrate soft clustering of words and feature reduction into the process. To extract structural information, we employ an existing frequent tree-mining algorithm combined with an information gain filter to retrieve the most informative substructures from XML documents. However, for extracting content information, we propose soft clustering of words using each cluster as a textual feature. We have conducted extensive experiments on a benchmark dataset, namely 20NewsGroups, and an XML documents dataset given in LOGML that describes the web-server logs of user sessions. With regards to the classifier built only using our textual features, the results show that it outperforms a naive support-vector-machine (SVM)-based classifier, as well as an information retrieval classifier (IRC). We further demonstrate the effectiveness of incorporating both structural and content information into the process of learning, by comparing our classifier model and several XML document classifiers. In particular, by applying SVM and decision tree algorithms using our feature vector representation of XML documents dataset, we have achieved 85.79% and 87.04% classification accuracy, respectively, which are higher than accuracy achieved by XRules, a well-known structural-based XML document classifier.}, 
keywords={XML;data mining;feature extraction;information retrieval;support vector machines;IRC;SVM;XML document classification;feature reduction;information gain filter;information retrieval classifier;informative feature vectors;informative substructures;learning process;semistructured document classification;soft clustering;structural feature extraction;support vector machine;textual feature extraction;tree mining algorithm;web server;words reduction;Clustering algorithms;Data mining;Feature extraction;Indexes;Vectors;XML;Document classification;XML documents;feature reduction;soft clustering;structural information}, 
doi={10.1109/TSMCC.2012.2208102}, 
ISSN={1094-6977}, 
month={Nov},}
@INPROCEEDINGS{7180121, 
author={L. Ponzanelli and A. Mocci and M. Lanza}, 
booktitle={2015 IEEE/ACM 12th Working Conference on Mining Software Repositories}, 
title={StORMeD: Stack Overflow Ready Made Data}, 
year={2015}, 
pages={474-477}, 
abstract={Stack Overflow is the de facto Question and Answer (Q&A) website for developers, and it has been used in many approaches by software engineering researchers to mine useful data. However, the contents of a Stack Overflow discussion are inherently heterogeneous, mixing natural language, source code, stack traces and configuration files in XML or JSON format. We constructed a full island grammar capable of modeling the set of 700,000 Stack Overflow discussions talking about Java, building a heterogeneous abstract syntax tree (H-AST) of each post (question, answer or comment) in a discussion. The resulting dataset models every Stack Overflow discussion, providing a full H-AST for each type of structured fragment (i.e., JSON, XML, Java, Stack traces), and complementing this information with a set of basic meta-information like term frequency to enable natural language analyses. Our dataset allows the end-user to perform combined analyses of the Stack Overflow by visiting the H-AST of a discussion.}, 
keywords={Java;Web sites;XML;question answering (information retrieval);software engineering;JSON format;StORMeD;XML;configuration files;heterogeneous abstract syntax tree;natural language;question and answer Website;software engineering researchers;source code;stack overflow ready made data;stack traces;term frequency;Data mining;Data models;Grammar;Java;Natural languages;Software;XML;h-ast;island parsing;unstructured data}, 
doi={10.1109/MSR.2015.67}, 
ISSN={2160-1852}, 
month={May},}
@INPROCEEDINGS{6240449, 
author={F. Beuvens and J. Vanderdonckt}, 
booktitle={2012 Sixth International Conference on Research Challenges in Information Science (RCIS)}, 
title={UsiGesture: An environment for integrating pen-based interaction in user interface development}, 
year={2012}, 
pages={1-12}, 
abstract={Several algorithms have been developed for pen-based gesture recognition. Yet, their integration in streamline engineering of interactive systems is bound to several shortcomings: they are hard to compare to each other, determining which one is the most suitable in which situation is a research problem, their performance largely vary depending on contextual parameters that are hard to predict, their fine-tuning in a real interactive application is a challenge. In order to address these shortcomings, we developed UsiGesture, an engineering method and a software support platform that accommodates multiple algorithms for pen-based gesture recognition in order to integrate them in a straightforward way into interactive computing systems. The method is aimed at providing designers and developers with support for the following steps: defining a dataset that is appropriate for an interactive system (e.g., made of commands, symbols, characters), determining the most suitable gesture recognition algorithm depending on contextual variables (e.g., user, platform, environment), fine-tuning the parameters of this algorithm by multi-criteria optimization (e.g., system speed and recognition rate vs. human distinguishability and perception), and incorporation of the fine-tuned algorithm in an integrated development environment for engineering interactive systems.}, 
keywords={gesture recognition;light pens;optimisation;software tools;user interface management systems;UsiGesture;contextual variables;engineering interactive systems;interactive computing systems;multicriteria optimization;parameter fine-tuning;pen-based gesture recognition;pen-based interaction;software support platform;user interface development;Algorithm design and analysis;Databases;Gesture recognition;Machine learning algorithms;Shape;Training;User interfaces;Gesture recognition;pen-based interaction;user interface modeling Introduction}, 
doi={10.1109/RCIS.2012.6240449}, 
ISSN={2151-1349}, 
month={May},}
@INPROCEEDINGS{6019834, 
author={M. Liu and X. Jiang}, 
booktitle={2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)}, 
title={Modeling user and item biases with Gaussian distribution for collaborative filtering}, 
year={2011}, 
volume={3}, 
pages={2070-2073}, 
abstract={The collaborative filtering approach to recommender systems focuses on learning predictive models of user preferences, interests and behavior from community data, that is, the behavior of other available users. Matrix Factorization (MF) based approaches have been proven to be efficient collaborative filtering algorithm for rating-based recommender systems. But existing MF algorithms have several disadvantages, including ignoring the distribution of the ubiquitous user and item biases. In this work we present an improved probabilistic matrix factorization (IPMF) algorithm and its graphical model. We analyzed the statistical pattern of user and item biases in the MovieLens dataset. The user and item biases are normally distributed. The improved model takes user and item preference biases into account, thereby building a more accurate model. Further accuracy improvements are achieved by extending this model with nonnegative user feature vectors. We evaluated these methods on the MovieLens dataset, and we show that our experimental results are better than those previously reported on this dataset.}, 
keywords={Gaussian distribution;information filtering;matrix decomposition;normal distribution;recommender systems;statistical analysis;user modelling;Gaussian distribution;IPMF algorithm;MovieLens dataset;collaborative filtering;graphical model;improved probabilistic matrix factorization;item biases;learning predictive models;normal distribution;rating based recommender systems;statistical pattern analysis;user interest;Accuracy;Collaboration;Graphical models;Motion pictures;Prediction algorithms;Probabilistic logic;Recommender systems}, 
doi={10.1109/FSKD.2011.6019834}, 
month={July},}
@ARTICLE{799947, 
author={I. Myrtveit and E. Stensrud}, 
journal={IEEE Transactions on Software Engineering}, 
title={A controlled experiment to assess the benefits of estimating with analogy and regression models}, 
year={1999}, 
volume={25}, 
number={4}, 
pages={510-525}, 
abstract={To have general validity, empirical results must converge. To be credible, an experimental science must understand the limitations and be able to explain the disagreements of empirical results. We describe an experiment to replicate previous studies which claim that estimation by analogy outperforms regression models. In the experiment, 68 experienced practitioners each estimated a project from a dataset of 48 industrial COTS projects. We applied two treatments, an analogy tool and a regression model, and we used the estimating performance when aided by the historical data as the control. We found that our results do not converge with previous results. The reason is that previous studies have used other datasets and partially different data analysis methods, and last but not least, the tools have been validated in isolation from the tool users. This implies that the results are sensitive to the experimental design: the characteristics of the dataset, the norms for removing outliers and other data points from the original dataset, the test metrics, significance levels, and the use of human subjects and their level of expertise. Thus, neither our results nor previous results are robust enough to claim any general validity.}, 
keywords={project management;software cost estimation;statistical analysis;analogy tool;commercial off-the-shelf software projects;controlled experiment;data points;dataset;empirical results;enterprise resource planning;estimating performance;estimation by analogy;experienced practitioners;experimental design;experimental science;historical data;human performance;human subjects;industrial COTS projects;multivariate regression analysis;outliers;partially different data analysis methods;regression model;regression models;significance levels;software cost estimation;test metrics;tool users;Convergence;Costs;Data analysis;Design for experiments;Enterprise resource planning;Humans;Physics;Robustness;Software performance;Testing}, 
doi={10.1109/32.799947}, 
ISSN={0098-5589}, 
month={July},}
@INPROCEEDINGS{7195597, 
author={A. Abdullah and X. Li}, 
booktitle={2015 IEEE International Conference on Web Services}, 
title={An Integrated-Model QoS-Based Graph for Web Service Recommendation}, 
year={2015}, 
pages={416-423}, 
abstract={Web services (WS) are integrated software components that facilitate interoperable machine-to-machine interaction over a network. In the era of Web 2.0, companies worldwide are actively deploying Web services within their business environments. As a result, designing effective Web service recommendation mechanisms based on Quality of Service (QoS) is attracting more attention. However, traditional Neighborhood-based Collaborative Filtering (CF) models fail to capture the actual relationships between users or services due to data sparsity. On the other hand, Random Walk (RW) algorithm, which has been categorized as a sparsity-tolerant recommendation approach, suffers from poor performance in terms of recommendation accuracy. In this paper, we aim at designing a recommendation model that achieves high recommendation accuracy over the transitional RW based model. First, we propose an Integrated-Model QoS-based Graph (IMQG), in which users and services represent the nodes while weighted QoS magnitudes and User/Service similarity measurements serve as the edges. We use Jaccard coefficient in several variants to separately compute similarities of both Users and Services. Then, Top-k Random Walk algorithm is applied to generate final recommendation list to active users. Finally, to demonstrate the effectiveness of our model, comprehensive experiments are conducted on a real-world QoS dataset. Analysis of the results shows high improvement in recommendation accuracy with more tolerance to data sparsity.}, 
keywords={Web services;graph theory;integrated software;open systems;quality of service;random processes;recommender systems;IMQG;Jaccard coefficient;WS;Web 2.0;Web service deployment;Web service recommendation;active users;business environments;data sparsity;graph edges;graph nodes;integrated software components;integrated-model QoS-based graph;interoperable machine-to-machine interaction;quality-of-service;real-world QoS dataset;recommendation list;top-k random walk algorithm;user/service similarity measurements;weighted QoS magnitudes;Accuracy;Bipartite graph;Collaboration;Computational modeling;Quality of service;Time factors;Web services;Jaccard coefficient;QoS;Random Walk;Web service}, 
doi={10.1109/ICWS.2015.62}, 
month={June},}
@INPROCEEDINGS{1214955, 
author={Kesheng Wu and W. Koegler and J. Chen and A. Shoshani}, 
booktitle={15th International Conference on Scientific and Statistical Database Management, 2003.}, 
title={Using bitmap index for interactive exploration of large datasets}, 
year={2003}, 
pages={65-74}, 
abstract={Many scientific applications generate large spatio-temporal datasets. A common way of exploring these datasets is to identify and track regions of interest. Usually these regions are defined as contiguous sets of points whose attributes satisfy some user defined conditions, e.g. high temperature regions in a combustion simulation. At each time step, the regions of interest may be identified by first searching for all points that satisfy the conditions and then grouping the points into connected regions. To speed up this process, the searching step may use a tree-based indexing scheme, such as a KD-tree or an Octree. However, these indices are efficient only if the searches are limited to one or a small number of selected attributes. Scientific datasets often contain hundreds of attributes and scientists frequently study these attributes in complex combinations, e.g. finding regions of high temperature and low pressure. Bitmap indexing is an efficient method for searching on multiple criteria simultaneously. We apply a bitmap compression scheme to reduce the size of the indices. In addition, we show that the compressed bitmaps can be used efficiently to perform the region growing and the region tracking operations. Analyses show that our approach scales well and our tests on two datasets from simulation of the autoignition process show impressive performance.}, 
keywords={content-based retrieval;database indexing;image coding;spatial data structures;temporal databases;tree data structures;tree searching;visual databases;Octree;bitmap compression;bitmap index;combustion simulation;data analysis;data exploration;data tracking;interactive exploration;isocontouring algorithm;spatio-temporal dataset;tree-based indexing;Analytical models;Combustion;Data visualization;Energy management;Indexing;Laboratories;Performance analysis;Spatiotemporal phenomena;Temperature;Testing}, 
doi={10.1109/SSDM.2003.1214955}, 
ISSN={1099-3371}, 
month={July},}
@INPROCEEDINGS{5164457, 
author={J. C. Ma and C. A. Karl and A. Dyukov}, 
booktitle={2009 IEEE Intelligent Vehicles Symposium}, 
title={Intelligent Access Program: System architecture}, 
year={2009}, 
pages={1227-1231}, 
abstract={The Intelligent Access Program (IAP) provides heavy vehicles with improved access to the Australian road network by utilization of GPS technology to remotely monitor heavy vehicles against access conditions imposed on them by Jurisdictions. The paper provides a background to this program and describes the IAP system architecture including data flow, the electronic data interchange model and work flows of associated with the definition of access conditions and the use of a common map dataset.}, 
keywords={data flow computing;electronic data interchange;road vehicles;traffic information systems;Australian road network;GPS technology;access conditions;common map dataset;data flow;electronic data interchange model;heavy vehicles;intelligent access program;jurisdictions;system architecture;Australia;Condition monitoring;Global Positioning System;Intelligent networks;Intelligent systems;Intelligent vehicles;Road safety;Road vehicles;Telematics;Vehicle safety;IAC;IAM;IAP;data flow;key players;system architecture}, 
doi={10.1109/IVS.2009.5164457}, 
ISSN={1931-0587}, 
month={June},}
@ARTICLE{7483548, 
author={J. C. Gomez and T. Tommasi and S. Zoghbi and M. F. Moens}, 
journal={IEEE Latin America Transactions}, 
title={What Would They Say? Predicting User #039;s Comments in Pinterest}, 
year={2016}, 
volume={14}, 
number={4}, 
pages={2013-2019}, 
abstract={When we refer to an image that attracts our attention, it is natural to mention not only what is literally depicted in the image, but also the sentiments, thoughts and opinions that it invokes in ourselves. In this work we deviate from the standard mainstream tasks of associating tags or keywords to an image, or generating content image descriptions, and we introduce the novel task of automatically generate user comments for an image. We present a new dataset collected from the social media Pinterest and we propose a strategy based on building joint textual and visual user models, tailored to the specificity of the mentioned task. We conduct an extensive experimental analysis of our approach on both qualitative and quantitative terms, which allows to assess the value of the proposed approach and shows its encouraging results against several existing image-to-text methods.}, 
keywords={image processing;social networking (online);image descriptions;image-to-text methods;social media Pinterest;standard mainstream;user comment prediction;visual user models;Buildings;Facebook;Media;Pins;Standards;User-generated content;Visualization;Deep-Learning Representation;Multimodal Clustering;Pinterest;Social Media;User Generated Content}, 
doi={10.1109/TLA.2016.7483548}, 
ISSN={1548-0992}, 
month={April},}
@INPROCEEDINGS{7797030, 
author={V. John and A. Boyali and S. Mita and M. Imanishi and N. Sanma}, 
booktitle={2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA)}, 
title={Deep Learning-Based Fast Hand Gesture Recognition Using Representative Frames}, 
year={2016}, 
pages={1-8}, 
abstract={In this paper, we propose a vision-based hand gesture recognition system for intelligent vehicles. Vision-based gesture recognition systems are employed in automotive user interfaces to increase the driver comfort without compromising their safety. In our algorithm, the long-term recurrent convolution network is used to classify the video sequences of hand gestures. In the standard long-term recurrent convolution network-based action classifier, multiple frames sampled from the video sequence are given as an input to the network, to perform classification. However, the use of multiple frames increases the computational complexity, apart from reducing the classification accuracy of the classifier. We propose to address these issues by extracting a fewer representative frames from the video sequence, and inputting them to the long-term recurrent convolution network. To extract the representative frames, we propose to use novel tiled image patterns and tiled binary pattern within a semantic segmentation- based deep learning framework, the deconvolutional neural network. The novel tiled image patterns contain multiple non-overlapping blocks and represent the entire gesture-based video sequence within a single tiled image. These image patterns represent the input to the deconvolution network and are generated from the video sequence. The novel tiled binary pattern also contain multiple non-overlapping blocks and represent the representative frames of the video sequence. These binary patterns represent the output of the deconvolution network. The training binary patterns are generated from the training video sequences using the dictionary learning and sparse modeling framework. We validate our proposed algorithm on the public Cambridge gesture recognition dataset. A comparative analysis is performed with baseline algorithms and an improved classification accuracy is observed. We also perform a detailed parametric analysis of the proposed algorithm. We report a gesture cl- ssification accuracy of 91% and report a near real-time computational complexity of $110$~ms per video sequence.}, 
keywords={computer vision;driver information systems;gesture recognition;image segmentation;image sequences;learning (artificial intelligence);recurrent neural nets;video signal processing;automotive user interfaces;deep learning-based fast hand gesture recognition;dictionary learning;driver comfort;hand gestures;intelligent vehicles;long-term recurrent convolution network;parametric analysis;public Cambridge gesture recognition dataset;representative frames;semantic segmentation;sparse modeling framework;video sequences;vision-based hand gesture recognition system;Feature extraction;Gesture recognition;Machine learning;Neural networks;Prediction algorithms;Semantics;Video sequences}, 
doi={10.1109/DICTA.2016.7797030}, 
month={Nov},}
@INPROCEEDINGS{7559606, 
author={L. Yao and F. Fan and Y. Feng and D. Zhao}, 
booktitle={2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)}, 
title={Leveraging tweet ranking in an optimization framework for tweet timeline generation}, 
year={2016}, 
pages={245-246}, 
abstract={When users search in Twitter, they are overloaded with a mass of microblog posts every time, which are not particularly informative and lack of meaningful organization. Therefore, it is helpful to produce a summarized tweet timeline about the topic. The tweet timeline generation is such a task aiming at selecting a small set of representative tweets to generate meaningful timeline. In this paper, we introduce an optimization framework to jointly model the relevance, novelty and coverage of the tweet timeline, including effective tweet ranking algorithm. Extensive experiments on the public TREC 2014 dataset demonstrate our method can achieve very competitive results against the state-of-art TTG systems.}, 
keywords={social networking (online);TTG systems;Twitter;effective tweet ranking algorithm;microblog posts;optimization framework;tweet timeline generation;Clustering algorithms;Fans;Measurement;Optimization;Organizations;Semantics;Twitter;Tweet timeline generation;optimization framework;tweet ranking}, 
month={June},}
@INPROCEEDINGS{7022726, 
author={C. M. Chen and H. P. Chen and M. F. Tsai and Y. H. Yang}, 
booktitle={2014 IEEE International Conference on Data Mining Workshop}, 
title={Leverage Item Popularity and Recommendation Quality via Cost-Sensitive Factorization Machines}, 
year={2014}, 
pages={1158-1162}, 
abstract={The accuracy of recommendation trends to be worse towards the long tail of the popularity distribution of items, but items in the long tail are generally considered to be valuable as they occupy a majority part of entire data. In this paper, we develop an instance-level cost-sensitive Factorization Machine (FM) to tackle the problem. The new algorithm allows the FM model to automatically leverage the trade-off between item popularity and recommendation quality. Specifically, by adding a cost criterion to the loss function, the FM model is now able to discriminate the relative importance of popularity from massive data. In addition, we convert several well-known functions into the popularity weighting functions, thereby demonstrating that the proposed method can fit the model parameters to various kinds of measurements. In the experiments, we assess the performance on a real-world music dataset which is collected from an online music streaming service, KKBOX. The dataset contains 1,800,000 listening records that cover 5,000 users and 30,000 songs. The results show that, the proposed method not only keeps the performance as primitive model but also avoids retrieving too much popular music in the top recommendations.}, 
keywords={music;recommender systems;FM model;KKBOX;cost criterion;instance-level cost-sensitive factorization machine;item popularity distribution;long tail;loss function;online music streaming service;popularity weighting functions;real-world music dataset;recommendation quality;recommendation trend accuracy;Accuracy;Frequency modulation;Learning systems;Music;Prediction algorithms;Recommender systems;Standards;Long tail;Recommender System}, 
doi={10.1109/ICDMW.2014.62}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{7022674, 
author={G. Cai and R. Lv and H. Wu and X. Hu}, 
booktitle={2014 IEEE International Conference on Data Mining Workshop}, 
title={An Improved Collaborative Method for Recommendation and Rating Prediction}, 
year={2014}, 
pages={781-788}, 
abstract={User-Item matrix (UI matrix) has been widely used in recommendation systems for data representation. However, as the amount of users and items increases, UI matrix becomes very sparse, which leads to unsatisfactory performance in traditional recommendation algorithms. To address this problem, in this paper, a rating prediction method with low sensitivity to sparse datasets is proposed. This method incorporates tag information and factor analysis approach that has been successfully applied in various areas, to discover the most similar top-N users based on the similarity of users' inner idiosyncrasies. Based on the most similar top-N users discovered, an improved collaborative filtering method is designed for rating prediction and recommendation. Extensive experiments have been done for comparing the proposed method with traditional collaborative filtering and the matrix factorization methods. The results demonstrate that our proposed method can achieve better accuracy, and it is less sensitive to sparseness of datasets.}, 
keywords={collaborative filtering;data structures;matrix algebra;recommender systems;UI matrix;collaborative method;data representation;factor analysis;rating prediction;recommendation system;tag information;user-item matrix;Collaboration;Data models;Fitting;Motion pictures;Prediction algorithms;Sparse matrices;Vectors;dynamic dataset;factor analysis;low sensitivity to sparseness;rating prediction;tag system}, 
doi={10.1109/ICDMW.2014.60}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{4427589, 
author={T. Ishikawa and P. Klaisubun and M. Honma}, 
booktitle={2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops}, 
title={Navigation Efficiency of Social Bookmarking Service}, 
year={2007}, 
pages={280-283}, 
abstract={The paper describes an experiment on the navigation efficiency of social bookmarking service for information discovery. The purpose is to evaluate the effects of navigation information that can be used in choosing users' libraries, which includes the number of bookmarks in each library, the ratio of user's bookmarks in a bookmark collection, and the ratio of bookmarks with certain tags in each library. The experiment was performed by computer simulation for an example real dataset. The navigation efficiency was evaluated by the average number of bookmarks browsed to find a randomly selected target bookmark for each randomly selected tag. The result shows that all these information effect on the navigation efficiency with statistical significance.}, 
keywords={information retrieval;library automation;bookmark collection;information discovery;navigation efficiency;social bookmarking service;Computational modeling;Computer simulation;Conferences;Displays;Information resources;Intelligent agent;Libraries;Navigation;Paper technology;Web pages;social bookmarkingsocial navigationnavigation efficiencycomputer simulation}, 
doi={10.1109/WI-IATW.2007.62}, 
month={Nov},}
@ARTICLE{6654121, 
author={B. J. Santoso and G. M. Chiu}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Close Dominance Graph: An Efficient Framework for Answering Continuous Top- $k$ Dominating Queries}, 
year={2014}, 
volume={26}, 
number={8}, 
pages={1853-1865}, 
abstract={There are two preference-based queries commonly used in database systems: (1) top-k query and (2) skyline query. By combining the ranking rule used in top-(k) query and the notion of dominance relationships utilized in the skyline query, a top-(k) dominating query emerges, providing a new perspective on data processing. This query returns the (k) records with the highest domination scores from the dataset. However, the processing of the top-(k) dominating query is complex when the dataset operates under a streaming model. With new data being continuously generated while stale data being removed from the database, a continuous top-(k) dominating query (cTKDQ) requires that updated results can be returned to users at any time. This work explores the cTKDQ problem and proposes a unique indexing structure, called a Close Dominance Graph (CDG), to support the processing of a cTKDQ. The CDG provides comprehensive information regarding the dominance relationship between records, which is vital in answering a cTKDQ with a limited search space. The update process for a cTKDQ is then converted to a simple update affecting a small portion of the CDG. Experimental results show that this scheme is able to offer much better performance when compared with existing solutions.}, 
keywords={graph theory;indexing;query processing;CDG structure;cTKDQ problem;close dominance graph;continuous top-k dominating queries;data processing;database systems;dominance relationships notion;indexing structure;preference-based queries;query answering;ranking rule;skyline query;top-k query;Algorithm design and analysis;Computational modeling;Data models;Data structures;Indexing;Query processing;Anchor set;close dominance graph;continuous query;dominance relationship;streaming model;top-k dominating query}, 
doi={10.1109/TKDE.2013.172}, 
ISSN={1041-4347}, 
month={Aug},}
@INPROCEEDINGS{7324650, 
author={A. Davoudi and M. Chatterjee}, 
booktitle={2015 36th IEEE Sarnoff Symposium}, 
title={Product rating prediction using centrality measures in social networks}, 
year={2015}, 
pages={94-98}, 
abstract={Online recommendation systems provide useful information to users on various products and also allow the users to rate the products. However, they do not usually consider the fact that users trust their connections more than others and that the trusts vary from connection to connection i.e., we value the opinions of our connections differently. Moreover, the importance of connections' opinion changes over time. Thus, there is a need to consider the evolving trust relationships among users. In this work, we use both the user's social connections and non-connections to predict how a user would rate a particular product. We argue that we not only trust our connections more but also the trust varies over time, which we capture using a time-dependent trust matrix. We use the degree and eigen-vector centrality measures in conjunction with the user-item rating matrix to find how the social connections impact how one rates a product. To test the validity of the proposed framework, we use Epinions dataset which provides the ratings for products and trust matrix over 11 time periods. We show the accuracy our predictive model using the mean absolute error.}, 
keywords={matrix algebra;recommender systems;social networking (online);trusted computing;Epinions dataset;centrality measures;mean absolute error;online recommendation systems;product rating prediction;social networks;time-dependent trust matrix;trust relationships;user-item rating matrix;Accuracy;Collaboration;Predictive models;Probability density function;Recommender systems;Social factors;Social network services;Social networks;cluster coefficients;hubs;power-law;recommendations}, 
doi={10.1109/SARNOF.2015.7324650}, 
month={Sept},}
@INBOOK{7111497, 
author={Alain Abran}, 
booktitle={Software Project Estimation:The Fundamentals for Providing High Quality Information to Decision Makers}, 
title={Verification of the Dataset Used to Build the Models}, 
year={2015}, 
pages={288-}, 
abstract={This chapter focuses on the verification of the inputs to productivity models based on statistical techniques. It present a number of aspects that must be analyzed to determine the relevance and quality of the inputs to productivity models, that is, on both the independent and dependent variables used to build the productivity models. The chapter discusses the graphical analysis of the inputs and analyzes the distribution of the input variables. One-dimensional graphical analysis typically provides the user with an intuitive feel about the data collected, one data field at a time. The chapter introduces two-dimensional graphical analysis of the input variables. Multidimensional graphical analysis will typically provide an intuitive feel for the relationships between the dependent variable and independent variable. The chapter discusses issues related to the use of size inputs derived from conversion formulas.}, 
doi={10.1002/9781118959312.ch5}, 
publisher={Wiley-IEEE Press}, 
isbn={9781118959312}, 
url={http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7111497},}
@INPROCEEDINGS{6883933, 
author={J. Zou and F. Fekri}, 
booktitle={2014 IEEE International Conference on Communications (ICC)}, 
title={On top-N recommendation using implicit user preference propagation over social networks}, 
year={2014}, 
pages={3919-3924}, 
abstract={Social recommender systems exploit the historic user data as well as user relations in the social networks to make recommendations. However, users are increasingly concerned with their online privacy, and hence, they are not willing to reveal their personal data to the general public. In this paper, we propose a social recommendation algorithm for top-N recommendation using only implicit user preference data. In particular, we model users' consumption behavior in the social network with Bayesian networks, using which we can infer the probabilities for items to be selected by each user. We develop an Expectation Propagation (EP) message-passing algorithm to perform approximate inference efficiently in the constructed Bayesian network. The original proposed algorithm is a central scheme, in which the user data are collected and processed by a central authority. However, it can be easily adapted for a distributed implementation, where users only exchange messages with their directly connected friends in the social network. This helps further protect user privacy, as users do not release any data to the public. We evaluate the proposed algorithm on the Epinions dataset, and compare it with other existing social recommendation algorithms. The results show its superior top-N recommendation performance in terms of recall.}, 
keywords={Bayes methods;data protection;message passing;probability;recommender systems;social networking (online);Bayesian networks;EP message-passing algorithm;Epinions dataset;central authority;expectation propagation message-passing algorithm;historic user data;implicit user preference propagation;online privacy;social networks;social recommendation algorithm;social recommender systems;top-N recommendation;user consumption behavior;user data collection;user privacy protection;user relations;Approximation algorithms;Bayes methods;Collaboration;Inference algorithms;Probabilistic logic;Recommender systems;Social network services}, 
doi={10.1109/ICC.2014.6883933}, 
ISSN={1550-3607}, 
month={June},}
@INPROCEEDINGS{4511561, 
author={D. H. Kim and H. P. In}, 
booktitle={2008 International Conference on Information Security and Assurance (isa 2008)}, 
title={Cyber Criminal Activity Analysis Models using Markov Chain for Digital Forensics}, 
year={2008}, 
pages={193-198}, 
abstract={Recognizing links between offender patterns is one of the most crucial skills of an investigator. Early recognition of similar patterns can lead to focusing resources, improving clearance rates, and ultimately saving lives in terms of digital forensics. In this paper we propose a forensics methodology using Markov chain during a given time interval for tracking and predicting the degree of criminal activity as it evolves over time. In other words, we describe intrusion scenario, and classify profiling of user's behavior by prior probability based Markov chain. Also, we apply the noise page elimination algorithm (NPEA) to reduce an error of probability prediction. Finally, we have experiment our model on dataset and have analysis their accuracy by Monte Carlo simulation.}, 
keywords={Markov processes;computer crime;pattern recognition;Markov chain;Monte Carlo simulation;cyber criminal activity analysis;digital forensics;noise page elimination algorithm;pattern recognition;probability prediction;user behavior profiling;Bayesian methods;Digital forensics;Hidden Markov models;Inference algorithms;Information analysis;Information security;Pattern recognition;Probability;Sockets;Web pages;Data Mining;Digital Forensics;Markov Chian;Monte Carlo Simulation;Noise Page Elimination Algorithm}, 
doi={10.1109/ISA.2008.90}, 
month={April},}
@ARTICLE{4579344, 
author={J. Ren and R. V. Patel and K. A. McIsaac and G. Guiraudon and T. M. Peters}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Dynamic 3-D Virtual Fixtures for Minimally Invasive Beating Heart Procedures}, 
year={2008}, 
volume={27}, 
number={8}, 
pages={1061-1070}, 
abstract={Two-dimensional or 3-D visual guidance is often used for minimally invasive cardiac surgery and diagnosis. This visual guidance suffers from several drawbacks such as limited field of view, loss of signal from time to time, and in some cases, difficulty of interpretation. These limitations become more evident in beating-heart procedures when the surgeon has to perform a surgical procedure in the presence of heart motion. In this paper, we propose dynamic 3-D virtual fixtures (DVFs) to augment the visual guidance system with haptic feedback, to provide the surgeon with more helpful guidance by constraining the surgeon's hand motions thereby protecting sensitive structures. DVFs can be generated from preoperative dynamic magnetic resonance (MR) or computed tomograph (CT) images and then mapped to the patient during surgery. We have validated the feasibility of the proposed method on several simulated surgical tasks using a volunteer's cardiac image dataset. Validation results show that the integration of visual and haptic guidance can permit a user to perform surgical tasks more easily and with reduced error rate. We believe this is the first work presented in the field of virtual fixtures that explicitly considers heart motion.}, 
keywords={cardiology;medical robotics;patient diagnosis;surgery;virtual reality;beating-heart procedures;cardiac image dataset;computed tomograph images;dynamic virtual fixtures;haptic feedback;heart motion;minimally invasive cardiac diagnosis;minimally invasive cardiac surgery;preoperative dynamic magnetic resonance images;robot-assisted surgery;simulated surgical tasks;three-dimensional virtual fixtures;visual guidance;Computational modeling;Computed tomography;Error analysis;Feedback;Fixtures;Haptic interfaces;Heart;Magnetic resonance;Minimally invasive surgery;Surge protection;Beating heart surgery;dynamic virtual fixtures;haptic feedback;minimally invasive robot-assisted surgery;Cardiovascular Surgical Procedures;Coronary Artery Bypass, Off-Pump;Heart;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Surgery, Computer-Assisted;Surgical Procedures, Minimally Invasive}, 
doi={10.1109/TMI.2008.917246}, 
ISSN={0278-0062}, 
month={Aug},}
@ARTICLE{7889041, 
author={W. T. Sun and T. H. Chao and Y. H. Kuo and W. Hsu}, 
journal={IEEE Transactions on Multimedia}, 
title={Photo Filter Recommendation by Category-Aware Aesthetic Learning}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Nowadays, social media has become a popular platform for the public to share photos. To make photos more visually appealing, users usually apply filters on their photos without domain knowledge. However, due to the growing number of filter types, it becomes a major issue for users to choose the best filter type. For this purpose, filter recommendation for photo aesthetics takes an important role in image quality ranking problems. In these years, several works have declared that Convolutional Neural Networks (CNNs) outperform traditional methods in image aesthetic categorization, which classifies images into high or low quality. Most of them do not consider the effect on filtered images; hence, we propose a novel image aesthetic learning for filter recommendation. Instead of binarizing image quality, we adjust the state-of-the-art CNN architectures and design a pairwise loss function to learn the embedded aesthetic responses in hidden layers for filtered images. Based on our pilot study, we observe image categories (e.g., portrait, landscape, food) will affect user preference on filter selection. We further integrate category classification into our proposed aesthetic-oriented models. To the best of our knowledge, there is no public dataset for aesthetic judgment with filtered images. We create a new dataset called Filter Aesthetic Comparison Dataset (FACD). It contains 28,160 filtered images based on the AVA dataset and 42,240 reliable image pairs with aesthetic annotations using Amazon Mechanical Turk. It is the first dataset containing filtered images and user preference labels. We conduct experiments on the collected FACD for filter recommendation, and the results show that our proposed category-aware aesthetic learning outperforms aesthetic classification methods (e.g., 12% relative improvement).}, 
keywords={Electronic mail;Feature extraction;Image color analysis;Image quality;Neural networks;Object detection;Social network services;Aesthetic;Convolutional Neural Network;Filter Recommendation;Image Quality;Pairwise Comparison}, 
doi={10.1109/TMM.2017.2688929}, 
ISSN={1520-9210}, 
month={},}
@INPROCEEDINGS{7450540, 
author={X. Fan and Y. Hu and J. Li and C. Wang}, 
booktitle={2015 International Conference on Cloud Computing and Big Data (CCBD)}, 
title={Context-Aware Ubiquitous Web Services Recommendation Based on User Location Update}, 
year={2015}, 
pages={111-118}, 
abstract={In this paper, we propose a novel ubiquitous Web service recommendation approach to context-aware recommendation based on user location update (CASR-ULU). First, we model the influence of user location update based on user preference expansion. Second, we perform the context-aware similarity mining for updated location. Third, we predict the Quality of Service by Bayesian inference, and thus recommend the ideal Web service for the specific user subsequently. Furthermore, a calendar Android mobile application is implemented to testify the CASR-ULU algorithm in a ubiquitous environment. Finally, we evaluate the CASR-ULU method on WS-Dream dataset with evaluation matrices such as RMSE and MAE. Experimental results show that our method achieves competitive recommendation performance in a ubiquitous environment, compared to several state-of-the-art methods.}, 
keywords={Bayes methods;Web services;matrix algebra;mobile computing;quality of service;recommender systems;Bayesian inference;CASR-ULU;WS-Dream dataset;calendar Android mobile application;context-aware similarity mining;context-aware ubiquitous Web services recommendation;evaluation matrices;quality of service;user location update;user preference expansion;Automobiles;Bayes methods;Context;Meteorology;Quality of service;Recommender systems;Web services;QoS;Web services;context awareness;recommender system;user location update}, 
doi={10.1109/CCBD.2015.20}, 
month={Nov},}
@INPROCEEDINGS{6820611, 
author={J. Lee and J. Lim and W. Cho and H. K. Kim}, 
booktitle={2013 12th Annual Workshop on Network and Systems Support for Games (NetGames)}, 
title={I know what the BOTs did yesterday: Full action sequence analysis using Na #x00EF;ve Bayesian algorithm}, 
year={2013}, 
pages={1-2}, 
abstract={A game BOT is a major threat in the online game industry. There have been many efforts to distinguish game BOT users from normal users. Several studies have proposed BOT detection models based on the analysis of users' in-game action sequence data. These studies indicated that the analysis of users' in-game actions is effective to detect BOTs. However, they do not use sufficiently large data sets to train and test their algorithms. In this paper, we have proposed a BOT detection model that uses users' in-game action sequence data obtained with the aid of big data analysis environments. We did empirical analysis of the dataset of “Blade and Soul”, the third largest MMORPG in Korea. The result shows that a large amount of sequence data leads to high accuracy.}, 
keywords={Bayes methods;Big Data;computer games;data analysis;security of data;Big Data analysis environments;Blade and Soul;Korea;MMORPG;game BOT detection model;online game industry;user in-game action sequence data analysis;Algorithm design and analysis;Bayes methods;Data mining;Games;Servers;Training;BOT detection;Naïve Bayesian classifier;online game security;sequence data}, 
doi={10.1109/NetGames.2013.6820611}, 
ISSN={2156-8138}, 
month={Dec},}
@INPROCEEDINGS{6048448, 
author={Z. Saeed and A. Sadaf and S. Muhammad}, 
booktitle={2011 7th International Conference on Emerging Technologies}, 
title={Activity-based correlation of personal documents and their visualization using association rule mining}, 
year={2011}, 
pages={1-7}, 
abstract={It is a common observation nowadays that the personal information of user is difficult to manage, the material which is copied by the users to their personal system are often forgotten by the users. So when they require their information it becomes very difficult to find the relevant information from huge repository. We have introduced a method using which the activities of user for reading documents are captured from running process list and managed in a dataset along with accessing time, then frequent item set and associated weights are calculated for each document with other using Apriori Algorithm and confidence measure in conjunction with combined access time. When user searches a document, the document list appears using any conventional model of retrieval, we have used primary metadata including title, author, type for document searching. Beside this, a visual interface is designed to display the list correlated document on the basis of users activities may help them to indentify documents according to their past activities.}, 
keywords={data mining;data visualisation;document handling;information retrieval;meta data;personal information systems;user interfaces;a priori algorithm;access time;activity-based correlation;association rule mining;confidence measures;document reading;document searching;frequent item set;information repository;information retrieval;personal document correlation;personal document visualization;primary metadata;title;user personal information management;visual interface;Association rules;Correlation;Information management;Prototypes;Visualization;Weight measurement;Association Rule Mining;Correlation of documents;Personal Information Management}, 
doi={10.1109/ICET.2011.6048448}, 
month={Sept},}
@ARTICLE{7299630, 
author={D. Yuan and L. Cui and W. Li and X. Liu and Y. Yang}, 
journal={IEEE Transactions on Cloud Computing}, 
title={An Algorithm for Finding the Minimum Cost of Storing and Regenerating Datasets in Multiple Clouds}, 
year={2015}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={The proliferation of cloud computing allows users to flexibly store, re-compute or transfer large generated datasets with multiple cloud service providers. However, due to the pay-as-you-go model, the total cost of using cloud services depends on the consumption of storage, computation and bandwidth resources which are three key factors for the cost of IaaS-based cloud resources. In order to reduce the total cost for data, given cloud service providers with different pricing models on their resources, users can flexibly choose a cloud service to store a generated dataset, or delete it and choose a cloud service to regenerate it whenever reused. However, finding the minimum cost is a complicated yet unsolved problem. In this paper, we propose a novel algorithm that can calculate the minimum cost for storing and regenerating datasets in clouds, i.e. whether datasets should be stored or deleted, and furthermore where to store or to regenerate whenever they are reused. This minimum cost also achieves the best trade-off among computation, storage and bandwidth costs in multiple clouds. Comprehensive analysis and rigid theorems guarantee the theoretical soundness of the paper, and general (random) simulations conducted with popular cloud service providers’ pricing models demonstrate the excellent performance of our approach.}, 
keywords={Algorithm design and analysis;Bandwidth;Benchmark testing;Cloud computing;Computational modeling;Data models;Finite element analysis;Cloud Computing;Data Storage and Regeneration;Minimum Cost}, 
doi={10.1109/TCC.2015.2491920}, 
ISSN={2168-7161}, 
month={},}
@INPROCEEDINGS{4649688, 
author={S. Kapoor and C. F. Quo and A. H. Merrill and M. D. Wang}, 
booktitle={2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 
title={An interactive visualization tool and data model for experimental design in systems biology}, 
year={2008}, 
pages={2423-2426}, 
abstract={Experimental design is important, but is often under-supported, in systems biology research. To improve experimental design, we extend the visualization of complex sphingolipid pathways to study biosynthetic origin in SphinGOMAP. We use the ganglio-series sphingolipid dataset as a test bed and the Java Universal Network / Graph Framework (JUNG) visualization toolkit. The result is an interactive visualization tool and data model for experimental design in lipid systems biology research. We improve the current SphinGOMAP in terms of interactive visualization by allowing (i) choice of four different network layouts, (ii) dynamic addition / deletion of on-screen molecules and (iii) mouse-over to reveal detailed molecule data. Future work will focus on integrating various lipid-relevant data systematically i.e. SphinGOMAP biosynthetic data, Lipid Bank molecular data (Japan) and Lipid MAPS metabolic pathway data (USA). We aim to build a comprehensive and interactive communication platform to improve experimental design for scientists globally in high-throughput lipid systems biology research.}, 
keywords={Algorithm design and analysis;Bioinformatics;Biology computing;Biomedical engineering;Cancer;Data models;Data visualization;Design for experiments;Lipidomics;Systems biology;Experimental design;graph layout algorithms;interactive visualization;sphingolipid pathways;Algorithms;Computational Biology;Computer Graphics;Computer Simulation;Database Management Systems;Databases, Factual;Gene Expression Profiling;Humans;Lipids;Metabolic Networks and Pathways;Reproducibility of Results;Sphingolipids;Systems Biology;Systems Integration;User-Computer Interface}, 
doi={10.1109/IEMBS.2008.4649688}, 
ISSN={1094-687X}, 
month={Aug},}
@INPROCEEDINGS{6736722, 
author={J. Unnikrishnan and F. M. Naini}, 
booktitle={2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
title={De-anonymizing private data by matching statistics}, 
year={2013}, 
pages={1616-1623}, 
abstract={Recent research has illustrated privacy breaches that can be effected on an anonymized dataset by an attacker who has access to auxiliary information about the users. Most of these attack strategies rely on the uniqueness of specific aspects of the users' data - e.g., observing a mobile user at just a few points on the time-location space are sufficient to uniquely identify him/her from an anonymized set of users. In this work, we consider de-anonymization attacks on anonymized summary statistics in the form of histograms. Such summary statistics are useful for many applications that do not need knowledge about exact user behavior. We consider an attacker who has access to an anonymized set of histograms of K users' data and an independent set of data belonging to the same users. Modeling the users' data as i.i.d., we study the composite hypothesis testing problem of identifying the correct matching between the anonymized histograms from the first set and the user data from the second. We propose a Generalized Likelihood Ratio Test as a solution to this problem and show that the solution can be identified using a minimum weight matching algorithm on an K × K complete bipartite weighted graph. We show that a variant of this solution is asymptotically optimal as the data lengths are increased.We apply the algorithm on mobility traces of over 1000 users on EPFL campus collected during two weeks and show that up to 70% of the users can be correctly matched. These results show that anonymized summary statistics of mobility traces themselves contain a significant amount of information that can be used to uniquely identify users by an attacker who has access to auxiliary information about the statistics.}, 
keywords={data privacy;graph theory;mobile computing;statistical testing;EPFL campus;anonymized histogram set access;anonymized summary statistics;anonymized user data set;asymptotic optimality;auxiliary information access;complete bipartite weighted graph;composite hypothesis testing problem;data lengths;generalized likelihood ratio test;histograms;independent data set;minimum weight matching algorithm;mobile user;mobility traces;privacy breach;private data de-anonymization attack strategies;time-location space;user data modeling;Accuracy;Bipartite graph;Databases;Histograms;Testing;Trajectory;Wireless communication}, 
doi={10.1109/Allerton.2013.6736722}, 
month={Oct},}
@ARTICLE{7105404, 
author={R. Donida Labati and A. Genovese and V. Piuri and F. Scotti}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
title={Toward Unconstrained Fingerprint Recognition: A Fully Touchless 3-D System Based on Two Views on the Move}, 
year={2016}, 
volume={46}, 
number={2}, 
pages={202-219}, 
abstract={Touchless fingerprint recognition systems do not require contact of the finger with any acquisition surface and thus provide an increased level of hygiene, usability, and user acceptability of fingerprint-based biometric technologies. The most accurate touchless approaches compute 3-D models of the fingertip. However, a relevant drawback of these systems is that they usually require constrained and highly cooperative acquisition methods. We present a novel, fully touchless fingerprint recognition system based on the computation of 3-D models. It adopts an innovative and less-constrained acquisition setup compared with other previously reported 3-D systems, does not require contact with any surface or a finger placement guide, and simultaneously captures multiple images while the finger is moving. To compensate for possible differences in finger placement, we propose novel algorithms for computing 3-D models of the shape of a finger. Moreover, we present a new matching strategy based on the computation of multiple touch-compatible images. We evaluated different aspects of the biometric system: acceptability, usability, recognition performance, robustness to environmental conditions and finger misplacements, and compatibility and interoperability with touch-based technologies. The proposed system proved to be more acceptable and usable than touch-based techniques. Moreover, the system displayed satisfactory accuracy, achieving an equal error rate of 0.06% on a dataset of 2368 samples acquired in a single session and 0.22% on a dataset of 2368 samples acquired over the course of one year. The system was also robust to environmental conditions and to a wide range of finger rotations. The compatibility and interoperability with touch-based technologies was greater or comparable to those reported in public tests using commercial touchless devices.}, 
keywords={fingerprint identification;image matching;open systems;biometric system;commercial touchless devices;finger placement guide;finger rotations;fully touchless 3D system;interoperability;matching strategy;multiple touch-compatible images;touch-based technologies;touchless fingerprint recognition systems;unconstrained fingerprint recognition;Accuracy;Biological system modeling;Biometrics (access control);Cameras;Computational modeling;Shape;Solid modeling;3-D reconstruction;biometrics;contactless;fingerprint;less-constrained;on the move;touchless}, 
doi={10.1109/TSMC.2015.2423252}, 
ISSN={2168-2216}, 
month={Feb},}
@INPROCEEDINGS{6089628, 
author={C. Birtolo and D. Ronca and R. Armenise and M. Ascione}, 
booktitle={2011 Third World Congress on Nature and Biologically Inspired Computing}, 
title={Personalized suggestions by means of Collaborative Filtering: A comparison of two different model-based techniques}, 
year={2011}, 
pages={444-450}, 
abstract={Recommendation systems are commonly used for suggesting products or services. Among different existing techniques, Model-Based Collaborative Filtering (MBCF) approaches have been proven to address scalability and cold-starting problems that often arise. In this paper we investigate two MBCF algorithms: Self-Organizing Maps (SOM) for Collaborative Filtering and Item-based Fuzzy Clustering Collaborative Filtering (IFCCF). These two techniques have been selected because preliminary results have proven that when applied to the clustering of users or items the quality of the recommendation system increases with respect to the k-means. Within recommendation systems, no comparison of these two techniques exists. Therefore, our experimentation is aimed at comparing these two techniques by means of MovieLens and Jester dataset in order to provide a guideline for their implementation in the e-Commerce domain.}, 
keywords={collaborative filtering;electronic commerce;fuzzy set theory;pattern clustering;recommender systems;self-organising feature maps;Jester dataset;MovieLens dataset;e-commerce domain;item based fuzzy clustering collaborative filtering;model based collaborative filtering;personalized suggestions;recommendation systems;self organizing maps;Clustering algorithms;Collaboration;Filtering;Motion pictures;Neurons;Prediction algorithms;Vectors;Collaborative Filtering;Fuzzy Clustering;Model-based CF;Recommendation System;Self-Organizing Map}, 
doi={10.1109/NaBIC.2011.6089628}, 
month={Oct},}
@ARTICLE{1356025, 
author={U. Garain and B. B. Chaudhuri}, 
journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
title={Recognition of online handwritten mathematical expressions}, 
year={2004}, 
volume={34}, 
number={6}, 
pages={2366-2376}, 
abstract={This paper aims at automatic understanding of online handwritten mathematical expressions (MEs) written on an electronic tablet. The proposed technique involves two major stages: symbol recognition and structural analysis. Combination of two different classifiers have been used to achieve high accuracy for the recognition of symbols. Several online and offline features are used in the structural analysis phase to identify the spatial relationships among symbols. A context-free grammar has been designed to convert the input expressions into their corresponding TEX strings which are subsequently converted into MathML format. Contextual information has been used to correct several structure interpretation errors. A new method for evaluating performance of the proposed system has been formulated. Experiments on a dataset of considerable size strongly support the feasibility of the proposed system.}, 
keywords={context-free grammars;handwritten character recognition;mathematical analysis;pattern classification;performance evaluation;MathML format;context-free grammar;contextual information;mathematical expressions;multiple-classifier system;online handwritten character recognition;performance evaluation;structural analysis;structure interpretation errors;symbol recognition;Character recognition;Colon;Computer vision;Equations;Error correction;Handwriting recognition;Object recognition;Pattern recognition;Shape;Two dimensional displays;Interpretation of two-dimensional structures;mathematical expression (ME);multiple-classifier system;online character recognition;performance evaluation;Algorithms;Artificial Intelligence;Automatic Data Processing;Computer Graphics;Documentation;Handwriting;Image Enhancement;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Markov Chains;Mathematics;Models, Statistical;Numerical Analysis, Computer-Assisted;Pattern Recognition, Automated;Reading;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;User-Computer Interface}, 
doi={10.1109/TSMCB.2004.836817}, 
ISSN={1083-4419}, 
month={Dec},}
@INPROCEEDINGS{7495766, 
author={B. Büyüksaraç and M. M. Bulut and G. B. Akar}, 
booktitle={2016 24th Signal Processing and Communication Application Conference (SIU)}, 
title={Sign language recognition by image analysis}, 
year={2016}, 
pages={417-420}, 
abstract={The Sign Language Recognition (SLR) Problem is a highly important research topic, because of its ability to increase the interaction between the people who are hearing-impaired or impediment in speech. We propose a simple but robust system. The proposed system consists of three main steps. First we apply segmentation to the face and hand region by using Fuzzy C-Means Clustering (FCM) and Thresholding. FCM is a clustering technique which employs fuzzy partitioning, in an iterative algorithm. After the face and hands are segmented, the feature vectors are extracted. The feature vectors are chosen among the low level features such as the bounding ellipse, bounding box, and center of mass coordinates, since they are known to be more robust to segmentation errors due to low resolution images. In total there are 23 features for each hand. After the feature vectors are extracted, they are used for recognition with discrete Hidden Markov Model (HMM). Recognition stage is composed of two stages, namely training and classification. The Baum Welch algorithm is used for HMM training. In classification part the likelihood of each HMM is calculated and the HMM with the highest likelihood is chosen. In order to measure the success rate of the system, the eNTERFACE dataset is used. In this dataset 8 different American Sign Language example classified and in user independent case, is shown to be working with 94.19% accuracy.}, 
keywords={face recognition;feature extraction;hidden Markov models;image segmentation;iterative methods;pattern clustering;sign language recognition;American sign language;Baum Welch algorithm;FCM;HMM training;clustering technique;discrete hidden Markov model;eNTERFACE dataset;face segmentation;feature vector extraction;fuzzy C-means clustering;fuzzy partitioning;hand region;hand segmentation;image analysis;image thresholding;iterative algorithm;sign language recognition;Assistive technology;Feature extraction;Gesture recognition;Hidden Markov models;Markov processes;Object segmentation;Baum Welch;Discrete Hidden Markov Model;Fuzzy C-Means Clustering;Machine Learning;Machine Vision;Sign Language Recognition}, 
doi={10.1109/SIU.2016.7495766}, 
month={May},}
@INPROCEEDINGS{7840789, 
author={A. Jabakji and H. Dag}, 
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
title={Improving item-based recommendation accuracy with user's preferences on Apache Mahout}, 
year={2016}, 
pages={1742-1749}, 
abstract={Recommendation systems play a critical role in the Information Science application domain, especially in ecommerce ecosystems. In almost all recommender systems, statistical methods and machine learning techniques are used to recommend items to the users. Although the user-based collaborative filtering approaches have been applied successfully in many different domains, some serious challenges remain especially in regards to large e-commerce sites, for recommender systems need to manage millions of users and millions of catalog products. In particular, the need to scan a vast number of potential neighbors makes it very hard to compute predictions. Many researchers have been trying to come up with solutions like using neighborhood-based collaborative filtering algorithms, model-based collaborative filtering algorithms, and text mining algorithms. Others have proposed new methods or have built various architectures/frameworks. In this paper, we proposed a new data model based on users' preferences to improve item-based recommendation accuracy by using the Apache Mahout library. We also present details of the implementation of this model on a dataset taken from Amazon. Our experimental results indicate that the proposed model can achieve appreciable improvements in terms of recommendation quality.}, 
keywords={collaborative filtering;data mining;electronic commerce;recommender systems;text analysis;Amazon;Apache Mahout library;catalog products;e-commerce ecosystems;e-commerce sites;information science application domain;item-based recommendation accuracy improvement;machine learning;model-based collaborative filtering algorithms;neighborhood-based collaborative filtering algorithms;recommendation quality;statistical methods;text mining;user preferences;Business;Collaboration;Data models;Euclidean distance;Prediction algorithms;Recommender systems;Collaboration Filtering;Mahout;Mean Absolute Error (MAE);Recommendation Systems}, 
doi={10.1109/BigData.2016.7840789}, 
month={Dec},}
@INPROCEEDINGS{5284289, 
author={M. De Choudhury and H. Sundaram and A. John and D. D. Seligmann}, 
booktitle={2009 International Conference on Computational Science and Engineering}, 
title={Social Synchrony: Predicting Mimicry of User Actions in Online Social Media}, 
year={2009}, 
volume={4}, 
pages={151-158}, 
abstract={We propose a computational framework to predict synchrony of action in online social media. Synchrony is a temporal social network phenomenon in which a large number of users are observed to mimic a certain action over a period of time with sustained participation from early users. Understanding social synchrony can be helpful in identifying suitable time periods of viral marketing. Our method consists of two parts - the learning framework and the evolution framework. In the learning framework, we develop a DBN based representation that includes an understanding of user context to predict the probability of user actions over a set of time slices into the future. In the evolution framework, we evolve the social network and the user models over a set of future time slices to predict social synchrony. Extensive experiments on a large dataset crawled from the popular social media site Digg (comprising ~7 M diggs) show that our model yields low error (15.2 plusmn 4.3%) in predicting user actions during periods with and without synchrony. Comparison with baseline methods indicates that our method shows significant improvement in predicting user actions.}, 
keywords={Bayes methods;evolutionary computation;social networking (online);social sciences computing;DBN based representation;dynamic Bayesian network;evolution framework;learning framework;mimicry prediction;online social media;social network;social synchrony;Art;Evolution (biology);Facebook;Intelligent networks;International collaboration;Online Communities/Technical Collaboration;Oscillators;Predictive models;Resource management;Social network services;Digg;cascades;social media;social networks;social synchrony;user actions}, 
doi={10.1109/CSE.2009.439}, 
month={Aug},}
@INPROCEEDINGS{7838052, 
author={C. Fuchs and C. Voigt and O. Baldizan and G. Groh}, 
booktitle={2016 Third European Network Intelligence Conference (ENIC)}, 
title={Explicit and Latent Topic Representations of Information Spaces in Social Information Retrieval}, 
year={2016}, 
pages={106-112}, 
abstract={We evaluate the suitability of latent and explicit semantic spaces of documents for Information Retrieval (IR) tasks using a dataset obtained from the Q&A community Stackexchange. In addition, the ability of the latent semantic spaces to reconstruct human relevance judgments is explored. The latent semantic spaces are generated with Latent Dirichlet Allocation (LDA), while explicit semantic spaces are modeled using Explicit Semantic Analyis (ESA). In the first part of the experiment, a series of ad-hoc information retrieval tasks is performed, interpreting closeness in the semantic and explicit spaces as a criterion for relevance. In the second part, it is investigated whether the latent semantic representation allows to infer user defined quality assessments of answers. The findings suggest that the semantic spaces show a correlation between query and relevant information items, however, both algorithms are outperformed by a simple Vector Space Model using TF-IDF. In addition, no significant correlation between the user defined order of relevant answers to a question and the similarity-based order (using closeness in the latent semantic space as similarity function) could be demonstrated.}, 
keywords={query processing;question answering (information retrieval);social networking (online);ESA;IR tasks;LDA;Stackexchange Q&A community;ad-hoc information retrieval tasks;explicit semantic analysis;explicit semantic space;explicit topic representation;human relevance judgment reconstruction;information items;information space;latent Dirichlet allocation;latent semantic representation;latent semantic space;latent topic representation;query processing;similarity-based order;social information retrieval;Electronic publishing;Encyclopedias;Information retrieval;Internet;Quality assessment;Semantics;Social Information Retrieval}, 
doi={10.1109/ENIC.2016.023}, 
month={Sept},}
@INPROCEEDINGS{5772280, 
author={E. E. S. de Lima and B. Feijó and A. L. Furtado and C. T. Pozzer and A. E. M. Ciarlini}, 
booktitle={2010 Brazilian Symposium on Games and Digital Entertainment}, 
title={Director of Photography and Music Director for Interactive Storytelling}, 
year={2010}, 
pages={129-137}, 
abstract={The way emotions are expressed in a film has great impact on the viewer's understanding of the narrative. Over the years, filmmakers developed several techniques to enhance the perception of these emotions, such as the photography and the audio editing of the scenes. This paper proposes two cinematography-inspired autonomous agents designed to better express the emotional aspects of interactive storytelling environments. Both the director of photography and music director use support vector machines trained with a cinematography-knowledge dataset to create and manipulate the audio and visual parameters of a runtime dramatization engine, thus increasing the immersion of the viewers in the story.}, 
keywords={audio signal processing;cinematography;music;software agents;support vector machines;audio and visual parameters;audio editing;cinematography-inspired autonomous agents;cinematography-knowledge dataset;emotional aspects;emotions;filmmakers;interactive storytelling environments;music director;photography;runtime dramatization engine;support vector machines;Cameras;Cinematography;Films;Games;Mood;Support vector machines;Cinematography;Interactive Storytelling;Support Vector Machine}, 
doi={10.1109/SBGAMES.2010.13}, 
ISSN={2159-6654}, 
month={Nov},}
@INPROCEEDINGS{4938623, 
author={Su Chen and Tiejian Luo and Wei Liu and Yanxiang Xu}, 
booktitle={2009 IEEE Symposium on Computational Intelligence and Data Mining}, 
title={Collaborative filtering with fine-grained trust metric}, 
year={2009}, 
pages={9-16}, 
abstract={Similarity-based collaborative filtering systems are vulnerable to the data sparsity, cold-start, and robustness problems. Computational trust models are promising alternative solutions to alleviate these problems by replacing similarity metric with trust metric. However, they often have some shortages that rely on users' explicit trust statements. A fine-grained model computing trust from user ratings is more reasonable and gets more nonintrusive for average users. We propose a novel trust-based recommendation model for this purpose. Experiments on a large real dataset show that the proposed model has better performance in terms of MAE, coverage, and F-metric than the conventional collaborative filtering model.}, 
keywords={groupware;information filtering;security of data;computational trust models;data sparsity;fine-grained trust metric;similarity-based collaborative filtering systems;user ratings;Books;Clustering algorithms;Collaboration
@INPROCEEDINGS{6996138, 
author={Yazhini R. and Vishnu Raja P.}, 
booktitle={2014 International Conference on Recent Trends in Information Technology}, 
title={Automatic summarizer for mobile devices using sentence ranking measure}, 
year={2014}, 
pages={1-6}, 
abstract={The modern digital world is immersed in tons of information in the form of electronic documents. To overcome the consequences of system failure, the fault tolerance system uses redundant information too. While surfing in the mobile devices, since they have small display screen. The mobile users prefer to analyse the summarized report, if it is relevant to their requirement, the user may observe it deeper. However, it is difficult to manually summarize the large documents of text. Sentence level clustering can be employed to perform automatic summarization task. The proposed work computes the centroid sentence of each cluster to automatically generate the summary which in turn reduces the manual processing. The integration of automatic summarization process to mobile devices is deployed android mobile application development interface. Experimental evaluation on the sample dataset shows that the clustering algorithm is more effective in retrieval of summarized content in mobile devices.}, 
keywords={application program interfaces;pattern clustering;smart phones;software fault tolerance;text analysis;Android mobile application development interface;document text summarization;electronic documents;fault tolerance system;mobile devices;sentence level clustering;sentence ranking measure;Algorithm design and analysis;Clustering algorithms;Data mining;Equations;Mathematical model;Mobile communication;Mobile handsets;Fuzzy clustering;mobile devices;sentence ranking}, 
doi={10.1109/ICRTIT.2014.6996138}, 
month={April},}
@INPROCEEDINGS{4271981, 
author={P. Buono and C. Plaisant and A. Simeone and A. Aris and G. Shmueli and W. Jank}, 
booktitle={Information Visualization, 2007. IV '07. 11th International Conference}, 
title={Similarity-Based Forecasting with Simultaneous Previews: A River Plot Interface for Time Series Forecasting}, 
year={2007}, 
pages={191-196}, 
abstract={Time-series forecasting has a large number of applications. Users with a partial time series for auctions, new stock offerings, or industrial processes desire estimates of the future behavior. We present a data driven forecasting method and interface called similarity-based forecasting (SBF). A pattern matching search in an historical time series dataset produces a subset of curves similar to the partial time series. The forecast is displayed graphically as a river plot showing statistical information about the SBF subset. A forecasting preview interface allows users to interactively explore alternative pattern matching parameters and see multiple forecasts simultaneously. User testing with 8 users demonstrated advantages and led to improvements.}, 
keywords={data visualisation;graphical user interfaces;time series;data driven forecasting method;forecasting preview interface;historical time series dataset;new stock offerings;partial time series;pattern matching search;river plot interface;similarity-based forecasting;time series forecasting;Data visualization;Economic forecasting;Laboratories;Pattern matching;Predictive models;Rivers;Smoothing methods;Technological innovation;Testing;Weather forecasting}, 
doi={10.1109/IV.2007.101}, 
ISSN={1550-6037}, 
month={July},}
@ARTICLE{4118179, 
author={H. Greenspan and A. T. Pinhas}, 
journal={IEEE Transactions on Information Technology in Biomedicine}, 
title={Medical Image Categorization and Retrieval for PACS Using the GMM-KL Framework}, 
year={2007}, 
volume={11}, 
number={2}, 
pages={190-202}, 
abstract={This paper presents an image representation and matching framework for image categorization in medical image archives. Categorization enables one to determine automatically, based on the image content, the examined body region and imaging modality. It is a basic step in content-based image retrieval (CBIR) systems, the goal of which is to augment text-based search with visual information analysis. CBIR systems are currently being integrated with picture archiving and communication systems for increasing the overall search capabilities and tools available to radiologists. The proposed methodology is comprised of a continuous and probabilistic image representation scheme using Gaussian mixture modeling (GMM) along with information-theoretic image matching via the Kullback-Leibler (KL) measure. The GMM-KL framework is used for matching and categorizing X-ray images by body regions. A multidimensional feature space is used to represent the image input, including intensity, texture, and spatial information. Unsupervised clustering via the GMM is used to extract coherent regions in feature space that are then used in the matching process. A dominant characteristic of the radiological images is their poor contrast and large intensity variations. This presents a challenge to matching among the images, and is handled via an illumination-invariant representation. The GMM-KL framework is evaluated for image categorization and image retrieval on a dataset of 1500 radiological images. A classification rate of 97.5% was achieved. The classification results compare favorably with reported global and local representation schemes. Precision versus recall curves indicate a strong retrieval result as compared with other state-of-the-art retrieval techniques. Finally, category models are learned and results are presented for comparing images to learned category models}, 
keywords={PACS;content-based retrieval;diagnostic radiography;feature extraction;image classification;image matching;image representation;image retrieval;image segmentation;image texture;medical image processing;statistical analysis;GMM-KL framework;Gaussian mixture modeling;Kullback-Leibler measure;PACS;X-ray image analysis;X-ray images;content-based image retrieval system;feature extraction;global representation schemes;illumination-invariant representation;image classification;image content;image intensity;image representation;image texture;imaging modality;information-theoretic image matching;local representation schemes;medical image archives;medical image categorization;multidimensional feature space;picture archiving and communication systems;radiological images;spatial information;statistical medical image modeling;text-based search;unsupervised clustering;visual information analysis;Biomedical imaging;Body regions;Content based retrieval;Image matching;Image representation;Image retrieval;Information analysis;Information retrieval;Picture archiving and communication systems;X-ray imaging;Content-based image retrieval (CBIR);X-ray image analysis;image matching;medical content retrieval;medical image categorization;picture archiving and communication systems (PACS);statistical medical image modeling;Algorithms;Artificial Intelligence;Database Management Systems;Information Storage and Retrieval;Pattern Recognition, Automated;Radiographic Image Interpretation, Computer-Assisted;Radiology Information Systems;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;User-Computer Interface}, 
doi={10.1109/TITB.2006.874191}, 
ISSN={1089-7771}, 
month={March},}
@INPROCEEDINGS{7483278, 
author={C. Wansing and O. Banos and P. Gloesekoetter and H. Pomares and I. Rojas}, 
booktitle={2015 Third World Conference on Complex Systems (WCCS)}, 
title={Development of a platform for the exchange of bio-datasets with integrated opportunities for artificial intelligence using MatLab}, 
year={2015}, 
pages={1-6}, 
abstract={This paper deals with the issue of automating the process of machine learning and analyzing bio-datasets. For this a user-friendly website has been developed for the interaction with the researchers. On this website it is possible to upload datasets and to share them, if desired, with other scientists. The uploaded data can also be analyzed by various methods and functions. The signals inside these datasets can also be visualized. Furthermore, several algorithms have been implemented to create machine learning models with the uploaded data. Based on these generated models new data can be classified or calculated. For all these applications the simplest possible handling was implemented to make the website available to all interested researchers.}, 
keywords={Web sites;data visualisation;health care;learning (artificial intelligence);medical administrative data processing;pattern classification;Matlab;artificial intelligence;bio-dataset exchange;data calculation;data classification;data handling;dataset visualization;health care;machine learning;user-friendly Web site;Computer languages;Databases;MATLAB;Operating systems;Security;Web servers;M-health;machine learning;website}, 
doi={10.1109/ICoCS.2015.7483278}, 
month={Nov},}
@INPROCEEDINGS{5193736, 
author={K. Liu and K. K. Lai and S. M. Guu}, 
booktitle={2009 International Joint Conference on Computational Sciences and Optimization}, 
title={A Fuzzy Markov Model for Consumer Credit Behavior Dynamics}, 
year={2009}, 
volume={1}, 
pages={460-462}, 
abstract={A heterogeneous fuzzy Markov model is built for modeling and forecasting credit behavior dynamics of credit card users in this paper. This model brings together a fuzzy rule-based inference system and the Markov chain. The credit migration rate matrix of the fuzzy Markov chain is obtained by the inference system based on reasonable setting of rules for each consumer, and updated at each time interval, a process which results in heterogeneity across consumers and over time, thereby imparting more flexibility and accuracy to the model to reflect the reality. Credit status evolution process is predicted for a simulated dataset of consumer credit behaviors.}, 
keywords={Markov processes;consumer behaviour;credit transactions;forecasting theory;fuzzy reasoning;fuzzy set theory;knowledge based systems;consumer credit behavior dynamics;credit behavior dynamics forecasting;credit behavior dynamics modeling;credit card user;credit migration rate matrix;credit status evolution process;fuzzy rule-based inference system;heterogeneous fuzzy Markov model;Conference management;Consumer behavior;Credit cards;Fuzzy logic;Fuzzy sets;Fuzzy systems;History;Predictive models;Probability distribution;Uncertainty;Credit Behavior Dynamics;Fuzzy Rule-based Inference;Markov Chain}, 
doi={10.1109/CSO.2009.296}, 
month={April},}
@INPROCEEDINGS{6916879, 
author={X. W. Ruan and S. C. Lee and W. C. Peng}, 
booktitle={2014 IEEE 15th International Conference on Mobile Data Management}, 
title={Exploring Location-Related Data on Smart Phones for Activity Inference}, 
year={2014}, 
volume={2}, 
pages={73-78}, 
abstract={In this paper, we propose a framework to infer different people's activity from the view of both the geographical habit and temporal habit of user. Such a personal activity inference framework is a crucial prerequisite for intelligent user experience, and power management of smart phones. By analyzing the real activity log data, we extract 3 kinds of features: 1) The geographical feature captures the user's activity preference of places, 2) The temporal feature records the routine habit of user's activity, 3) The semantic feature obtained from location-based social network can be used as an activity reference of public opinion for each location. Finally, we hybrid the features to build a Semantic-based Activity Inference Model (SAIM). To evaluate our proposed framework SAIM, we compared it with the state-of-art methods over a real dataset. The experimental results show that our framework could accurately inference user's activity and each feature of the three has different inferring ability for different user.}, 
keywords={feature extraction;geography;mobile computing;power aware computing;smart phones;social networking (online);SAIM;geographical feature;geographical habit;intelligent user experience;location-based social network;location-related data exploration;personal activity inference framework;public opinion activity reference;semantic feature;semantic-based activity inference model;smart phone power management;temporal feature;user activity place preference;user activity routine habit recording;user temporal habit;Data mining;Data models;Entropy;Feature extraction;Global Positioning System;Semantics;Support vector machines;Activity Inference;Location;Mobile}, 
doi={10.1109/MDM.2014.71}, 
ISSN={1551-6245}, 
month={July},}
@ARTICLE{6797266, 
author={R. Kalawsky and G. Simpkin}, 
journal={Presence}, 
title={Automating the Display of Third Person/Stealth Views of Virtual Environments}, 
year={2006}, 
volume={15}, 
number={6}, 
pages={717-739}, 
abstract={In order to gain a greater insight into the relationships that exist between entities in three-dimensional (3D) datasets, the scientific, engineering, and arts communities are increasingly using interactive visualization and virtual reality (VR) techniques. They have realized that interactively visualizing 3D datasets from different viewpoints makes it possible to achieve a better understanding of the underlying dataset structure. Viewpoints can be either static or dynamic as in an interactive fly-through. However, unskilled users often select flight paths (or viewing situations) that cause nauseous effects that detrimentally distract the user from the task at hand. Interactions between multiple users or virtual agents in a virtual environment present further challenges because it is necessary for the user to monitor multiple activities concurrently. If the user has to make decisions based on what is taking place in a complex virtual environment, then it is very important that correct and appropriate viewpoints are maintained. For example, flight simulator debriefing tools require first and third person viewing so that the actions that have taken place can be understood. In these situations there is a need to select multiple viewpoints for each participant. Consequently (because of the high cognitive load), maintaining control over a number of different viewpoints is very challenging. Within this paper the authors describe the real-time automatic display controller they have developed for third person/stealth views of a multi-participant virtual environment—where it is important for users to gain a global and localized understanding of the tasks being performed. They discuss important cinematic conventions and how these are set in the context of characterizing a 3D communication medium, as well as determining their role for optimal viewing parameters. The real time automatic display controller is of particular benefit to applications such as scientific visua- ization, flight simulation, engineering/architectural modeling, scene of accident reconstruction/analysis, and other complex human-system behavior analysis applications.}, 
doi={10.1162/pres.15.6.717}, 
ISSN={1054-7460}, 
month={Dec},}
@INPROCEEDINGS{7878248, 
author={H. Liu and J. Huang and C. An and X. Fu}, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={Answer Quality Prediction Joint Textual and Non-Textual Features}, 
year={2016}, 
pages={144-148}, 
abstract={Community question answering (CQA) is a popular online service for people to ask and answer questions. But along with the increasing of user generating contents, the quality of answers provided by different users varies widely. So the quality of the answer caused wide attention. In this paper, we propose an answer quality prediction model to evaluate the answer quality considering both aspects of textual and non-textual features. We firstly employ Bidirectional long Short-Term Memory (BLSTM) based RNN model to evaluate textual quality of the answers. And we extract 11 features of the answers to evaluate the non-textual quality of answers. Finally, we jointly consider the score of answers textual and non-textual qualities. We evaluate our model in a benchmark dataset and the experimental results show that our model outperforms other existing approaches.}, 
keywords={question answering (information retrieval);recurrent neural nets;text analysis;BLSTM;CQA;RNN model;answer quality prediction;answer quality prediction model;benchmark dataset;bidirectional long short-term memory;community question answering;nontextual features;online service;textual features;user generating contents;Computers;Feature extraction;Knowledge discovery;Logic gates;Predictive models;Semantics;Syntactics;Bidirectional long short term memory;answer quality prediction;community question answering}, 
doi={10.1109/WISA.2016.38}, 
month={Sept},}
@INPROCEEDINGS{6409252, 
author={X. Xing and W. Zhang and Z. Jia and X. Zhang}, 
booktitle={2012 World Congress on Information and Communication Technologies}, 
title={Learning to recommend top-k items in online social networks}, 
year={2012}, 
pages={1171-1176}, 
abstract={In this paper, we propose SIR, a Social Item Recommendation model based on latent variable model and neighborhood model which effectively models the user interest similarities and social relationships in online social networks. We develop the learning algorithm for the parameter estimates of SIR. Furthermore, we construct an extended SIR model (SIR+) by taking the social interaction features into account to improve the performance of top-A item recommendation. The experiments on a real dataset from Sina Weibo, one of the most popular social network sites (SNS) in China, demonstrate that both SIR and SIR+ outperform the traditional collaborative filtering methods, and SIR+ achieves a better performance than SIR.}, 
keywords={collaborative filtering;learning (artificial intelligence);parameter estimation;recommender systems;social networking (online);China;SIR+ model;SNS;Sina Weibo;collaborative filtering methods;extended SIR model;latent variable model;learning algorithm;neighborhood model;online social networks;parameter estimation;social interaction features;social item recommendation model;social relationships;top-A item recommendation;top-k item recommendation;user interest similarity;Communications technology;Decision support systems;collaborative filtering;latent variable model;neighborhood model;recommender system;social recommendation}, 
doi={10.1109/WICT.2012.6409252}, 
month={Oct},}
@ARTICLE{7513405, 
author={R. de Souza Baptista and A. Padilha Lanari B/actue{o} and M. Hayashibe}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={Automatic Human Movement Assessment with Switching Linear Dynamic System: Motion Segmentation and Motor Performance}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Performance assessment of human movement is critical in diagnosis and motor-control rehabilitation. Recent developments in portable sensor technology enable clinicians to measure spatiotemporal aspects to aid in the neurological assessment. However the extraction of quantitative information from such measurements is usually done manually through visual inspection. This paper presents a novel framework for automatic human movement assessment that executes segmentation and motor performance parameter extraction in time-series of measurements from a sequence of human movements. We use the elements of a Switching Linear Dynamic System model as building blocks to translate formal definitions and procedures from human movement analysis. Our approach provides a method for users with no expertise in signal processing to create models for movements using labeled dataset and latter use it for automatic assessment. We validated our framework on preliminary tests involving six healthy adult subjects that executed common movements in functional tests and rehabilitation exercise sessions, such as sit-to-stand and lateral elevation of the arms and five elderly subjects, two of which with limited mobility, that executed the sit-to-stand movement. The proposed method worked on random motion sequences for the dual purpose of movement segmentation (accuracy of 72-100%) and motor performance assessment (mean error of 0-12%).}, 
keywords={Hidden Markov models;Motion segmentation;Parameter extraction;State-space methods;Superluminescent diodes;Switches;Time series analysis;human movement assessment;motor performance;segmentation;switching linear dynamic systems}, 
doi={10.1109/TNSRE.2016.2591783}, 
ISSN={1534-4320}, 
month={},}
@INPROCEEDINGS{7321229, 
author={A. Guimarães and A. P. C. D. Silva and J. M. Almeida}, 
booktitle={2015 Second European Network Intelligence Conference}, 
title={On the Dynamics of Topic-Based Communities in Online Knowledge-Sharing Networks}, 
year={2015}, 
pages={1-8}, 
abstract={Online knowledge-sharing networks, such as wikis and Question-Answering (Q&A) portals, offer a rich environment where people can collaborate and share knowledge on specific topics pertaining to their interests and expertise. The continued collaboration of groups of users who share interests in the same topics essentially make up communities, which are here referred to as topic-based communities. Analyzing the dynamics of such communities is key to understanding network processes such as the flow of users and information in the network. In this paper, we study how users relate to topic-based communities and how such relationship shapes long-term community dynamics. Using a large dataset collected from Stack Overflow, a popular programming oriented Q&A site, we investigate several factors related to the evolution of communities in the site, such as user participation and the importance of revisits for community sustainability. Moreover, we develop a model to describe community evolution based on user activity which incorporates key aspects of community dynamics, including member revisits and the flow of users across related communities.}, 
keywords={portals;social networking (online);Q&A portals;StackOverflow;Wikis;community evolution;long-term community dynamics;online knowledge-sharing networks;programming oriented Q&A site;question-answering portals;topic-based community dynamics;Collaboration;Databases;Electric shock;Information services;Knowledge engineering;Programming;Rails;community dynamics;epidemic model;knowledge sharing;question-answering systems;topic-based communities}, 
doi={10.1109/ENIC.2015.9}, 
month={Sept},}
@INPROCEEDINGS{5995783, 
author={S. B. Rahayu and S. A. Noah}, 
booktitle={2011 International Conference on Semantic Technology and Information Retrieval}, 
title={Annotated document: Scoring and ranking method}, 
year={2011}, 
pages={167-170}, 
abstract={Semantic annotation represents a metadata of the document based on domain ontology. The purpose of this paper is to present semantic similarity document annotation ranking framework given a user's query. The framework features related concepts inclusion and applies appropriate weighting functions. Our aim is to rank and score semantic document annotation based on document richness. We also compare our approach with other methods using a research prototype retrieval engine, PicoDoc. The system framework of PicoDoc is based on OCAS2008 ontology. In this experiment, we are using a real-life dataset from news article corpus from ABC and BBC. The experiment shows promising results in retrieving related information using the proposed framework.}, 
keywords={document handling;meta data;ontologies (artificial intelligence);query processing;OCAS2008 ontology;PicoDoc retrieval engine;domain ontology;information retrieval;metadata;research prototype retrieval engine;scoring method;semantic similarity document annotation ranking framework;user query;Equations;Knowledge based systems;Mathematical model;Ontologies;Portals;Semantic Web;Semantics;concept spreading;relevance}, 
doi={10.1109/STAIR.2011.5995783}, 
ISSN={2166-0697}, 
month={June},}
@INPROCEEDINGS{7000751, 
author={H. R. HakimDavoodi and M. M. Homayounpour}, 
booktitle={Telecommunications (IST), 2014 7th International Symposium on}, 
title={Optimizing spatio-spectral filters by motor imagery pattern quantification in self-paced Brain Computer Interface}, 
year={2014}, 
pages={481-486}, 
abstract={Analyzing ongoing brain activities and distinguishing no control (NC) state of users are the most challenging parts in the self-paced BCI. Many spatial filters such as Common Spatial Pattern and its other developed versions have been proposed to differentiate among specific motor imagery (MI) activities, but they didn't result in significant achievements in the self-paced BCI. To overcome these drawbacks, this paper proposes a new spatio-spatial filter optimization method by a novel quantification measure for motor imagery patterns. Maximizing the correlation between the linear mixtures of motor cortex channels and motor imagery patterns is used as goal function for genetic optimization algorithm. No sensitivity to initial value is the significant property of this evolutionary algorithm. The most important consequence of this method is increasing the resolution of motor imagery pattern and also improving the motor imagery detection rate in self-paced BCI. Our approach was validated on self-paced dataset 1 of the BCI Competition IV and was compared to different spatial filters including CSP, TRCSP, WTRCSP, and Laplacian filter. The proposed method achieved the highest Area under ROC among the other methods.}, 
keywords={Brain modeling;Electroencephalography;Hidden Markov models;Mathematical model;Spatial filters;Synchronous motors;Training;Brain Computer Interface;Common Spatial Pattern;ERD;ERS;Motor imagery;Spatio-Spectral Filtering}, 
doi={10.1109/ISTEL.2014.7000751}, 
month={Sept},}
@INPROCEEDINGS{1368988, 
author={A. Colapicchioni}, 
booktitle={IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium}, 
title={KES: knowledge enabled services for better EO information use}, 
year={2004}, 
volume={1}, 
pages={179}, 
abstract={Modem imaging sensors, especially those aboard satellites, continuously deliver enormous amounts of data. These represent typical cases, where users need automated tools to discover, explore and understand the contents of large image databases. The state-of-the-art image catalogues allow only queries based on data like geographical position, acquisition date, sensor type, etc. and not on image content. Once the search data have been defined, these catalogues show the associated image to the user, leaving the interpretation task to his specific knowledge. The call up a synergy between stochastic modeling, knowledge discovery, semantic representation, up to build a collaborative environment permitting to share the knowledge between heterogeneous user communities. In a previous project (Knowledge Driven Information Mining in Remote Sensing Image Archives - KIM) methods to associate concepts to images were developed for the ESA (European Space Agency). The consortium was composed by ACS (Advanced Computer Systems. Rome). DLR (German Aerospace Center) and ETH (Swiss Federal Institute of Technology. Zurich). This result has been achieved by extracting primitive image features (i.e.: texture, geometrical shapes, spectral information), and by providing the user with a simple, graphic interface to define weighted combinations of these features and to associate concepts (labels) to them through positive and negative examples. The weighted combinations of primitive image features and the associated semantic labels can then be applied to the entire dataset. and not only to the image from which they were defined}, 
keywords={data acquisition;data mining;geophysical signal processing;information retrieval;remote sensing;semantic networks;ACS;Advanced Computer Systems;DLR;EO information;ESA;ETH;Earth observation satellite;European Space Agency;German Aerospace Center;KES;KIM method;Knowledge Driven Information Mining;Remote Sensing Image Archives;Rome;Swiss Federal Institute of Technology;Zurich;acquisition date;geographical position;geometrical shape;graphic interface;knowledge discovery;knowledge enabled services;modem imaging sensor;primitive image feature;semantic representation;sensor type;spectral information;stochastic modeling;texture;Collaboration;Data mining;Feature extraction;Image databases;Image sensors;Modems;Remote sensing;Satellites;Space technology;Stochastic processes}, 
doi={10.1109/IGARSS.2004.1368988}, 
month={Sept},}
@ARTICLE{1522628, 
author={C. Maire and M. Datcu}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={Earth observation image and DEM information aggregation for realistic 3-D visualization of natural landscapes}, 
year={2005}, 
volume={43}, 
number={11}, 
pages={2676-2683}, 
abstract={This paper presents a system to integrate digital elevation model (DEM) enhancement and Earth Observation (EO) image analysis for realistic three-dimensional (3-D) rendering applications. There is an increasing interest in interferometric synthetic aperture radar (InSAR) data, principally due to the availability of the nearly global Shuttle Radar Topography Mission coverage. To remove artifacts and noise from an InSAR DEM, a nonstationary Bayesian filtering is applied that preserves structural information. Land-cover or man-made structures are easily recognized in an optical image. The corresponding geometry encapsulated in the DEM differs from our implicit perception and generally leads to unrealistic 3-D rendering. To improve this, DEM regularization is achieved using only the visualization dataset (optical image and DEM). It consists of extracting relevant information from the optical image and integrate them in the filtered DEM. To gather image information, an object-based description of large optical EO images is obtained in two stages 1) an image is segmented to create a partition of regions and 2) a novel dynamical algorithm is proposed to extract the regions and encode them in a tree structure. Regions are modeled by objects primitives stored in a database. Spatial relationships between regions are reflected by the presented tree of regions. Using the object-based description generation, structures to be integrated into the DEM are interactively selected and classified among a set of user-thematic. Each thematic is associated with a corresponding elevation modeling and enables to estimate the region's 3-D structure. The proposed object line processing provides more realistic 3-D visualizations.}, 
keywords={data visualisation;filtering theory;remote sensing by radar;synthetic aperture radar;terrain mapping;topography (Earth);3D visualization;DEM regularization;Earth Observation image analysis;Shuttle Radar Topography Mission;data merging;digital elevation model;geometry extraction;interferometric SAR;natural landscapes;nonstationary Bayesian filtering;object modeling;optical image;synthetic aperture radar;topology extraction;tree data structure;Adaptive optics;Earth;Information filtering;Information filters;Optical filters;Optical interferometry;Optical noise;Rendering (computer graphics);Synthetic aperture radar interferometry;Visualization;Bayesian filter;data merging;geometry and topology extraction;interferometric synthetic aperture radar (InSAR) digital elevation model (DEM);object modeling}, 
doi={10.1109/TGRS.2005.857322}, 
ISSN={0196-2892}, 
month={Nov},}
@INPROCEEDINGS{7340814, 
author={Xiaojiang Lei and X. Qian}, 
booktitle={2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)}, 
title={Rating prediction via exploring service reputation}, 
year={2015}, 
pages={1-6}, 
abstract={With the explosion of e-commerce, it presents a great opportunity for people to share their consumption experience in review websites. However, at the same time we face the information overloading problem. How to mine valuable information from these reviews and make an accurate recommendation is crucial for us. Traditional recommender systems (RS) consider many factors, such as product category, geographic location, user's purchase records, and the other social network factors. In this paper, we firstly propose a social user's reviews sentiment measurement approach and calculate each user's sentiment score on items/services. Secondly, we consider service reputation, which reflects the customers' comprehensive evaluation. At last, we fuse service reputation factor into our recommender system to make an accurate rating prediction, which is based on probabilistic matrix factorization. We conduct a series of experiments on Yelp dataset, and experimental results show the proposed approach outperforms the existing RS approaches.}, 
keywords={information dissemination;matrix decomposition;probability;recommender systems;Website review;consumption experience;geographic location;information overloading problem;probabilistic matrix factorization;product category;rating prediction;recommender systems;sentiment measurement;service reputation;Dictionaries;Feature extraction;Filtering;Matrix decomposition;Predictive models;Probabilistic logic;Social network services}, 
doi={10.1109/MMSP.2015.7340814}, 
month={Oct},}
@INPROCEEDINGS{7353717, 
author={K. S. Narayan and P. Abbeel}, 
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={Optimized color models for high-quality 3D scanning}, 
year={2015}, 
pages={2503-2510}, 
abstract={We consider the problem of estimating high-quality color models of 3D meshes, given a collection of RGB images of the original object. Applications of a database of high-quality colored meshes include object recognition in robot vision, virtual reality, graphics, and online shopping. Most modern approaches that color a 3D object model from a collection of RGB images face problems in (1) producing realistic colors for non-Lambertian surfaces and (2) seamlessly integrating colors from multiple views. Our approach efficiently solves a non-linear least squares optimization problem to jointly estimate the RGB camera poses and color model. We discover that incorporating 2D texture cues, vertex color smoothing, and texture-adaptive camera viewpoint selection into the optimization problem produces qualitatively more coherent color models than those produced by competing methods. We further introduce practical strategies to accelerate optimization. We provide extensive empirical results on the BigBIRD dataset [15], [21]: results from a user study with 133 participants indicate that on all 16 objects considered, our method outperforms competing approaches. Our code is available for download online at http://rll.berkeley.edu/iros2015colormodels.}, 
keywords={image colour analysis;image texture;least squares approximations;optimisation;2D texture cues;3D meshes;BigBIRD dataset;RGB camera pose estimation;RGB images;color models;high-quality 3D scanning;nonlinear least squares optimization problem;texture-adaptive camera viewpoint selection;vertex color smoothing;Cameras;Image color analysis;Image reconstruction;Optimization;Smoothing methods;Solid modeling;Three-dimensional displays}, 
doi={10.1109/IROS.2015.7353717}, 
month={Sept},}
@INPROCEEDINGS{7872753, 
author={Kurniawati and A. Syauqi}, 
booktitle={2016 International Conference on Advanced Computer Science and Information Systems (ICACSIS)}, 
title={Term weighting based class indexes using space density for Al-Qur'an relevant meaning ranking}, 
year={2016}, 
pages={460-463}, 
abstract={Nowadays information retrieval based on specific queries is already used in computer system. One of the popular methods is document ranking using Vector Space Model (SVM) based on TF.IDF term-weighting. In this paper TF.IDF.ICSδF term-weighting based class-indexing is proposed, afterward comparing its effectiveness to TF.IDF and TF.IDF.ICF term weighting. Each method is investigated through Al-Qur'an dataset. Al-Qur'an consist many verses, each verse of the Al-Qur'an is a single document which is ranked based on user query. The experimental show that the proposed method can be implemented on document ranking and the performance is better than previous methods with accurate value 93%.}, 
keywords={document handling;indexing;query processing;vectors;Al-Qur'an relevant meaning ranking;SVM;TF.IDF.ICSδF term-weighting based class-indexing;computer system queries;document ranking;information retrieval;space density;term weighting based class indexes;user query;vector space model;Computers;Indexing;Information retrieval;Integrated circuits;Mathematical model;Testing;ICF;ICSδF;TF.IDF;class indexing;document ranking;term weighting}, 
doi={10.1109/ICACSIS.2016.7872753}, 
month={Oct},}
@INPROCEEDINGS{5190132, 
author={Y. Liu and M. Liu and X. Chen and L. Xiang and Q. Yang}, 
booktitle={2009 International Conference on Information Technology and Computer Science}, 
title={Automatic Tag Recommendation for Weblogs}, 
year={2009}, 
volume={1}, 
pages={546-549}, 
abstract={There have been many researches on how to recommend tags for weblogs. In this paper, we propose a novel automatic tag recommendation algorithm, which can be used in the large-scale and real-time data process effectively and efficiently. Most existing researches on tag suggestion focus on firstly mining the relationship between testing and training data and then assigning the top ranked tags of the most related training data to the testing object. However, they ignore the internal relationship between tags and weblogs. According to our research, more than 43% tags, which have been labeled by weblog users, have actually been used in the body of the text. At the meanwhile, the term frequency distribution, the paragraph frequency distribution and the first occurrence position of tags are very different from the ones of non-tags in the text. In this paper, the tags of a weblog are assigned in two steps. First of all, some probability distributions of the word attributes are trained by the labeled training weblogs, and some keywords of a testing weblog are extracted as one part of the tags based on the probability distributions. Then the other part of the tags are obtained from the first part ones with the help of Latent Semantic Indexing (LSI) model. Experiments on a large-scale tagging dataset of weblogs 12 show that the average tagging time for a new weblog is less than 0.02 seconds, and over 74% testing weblogs are correctly labeled with the top 15 tags.}, 
keywords={Web sites;data mining;identification technology;indexing;Weblogs;automatic tag recommendation;data mining;large-scale data process;latent semantic indexing;real-time data process;Frequency;Indexing;Internet;Large scale integration;Large-scale systems;Probability distribution;Tagging;Testing;Training data;Web pages;data mining;recommendation system}, 
doi={10.1109/ITCS.2009.263}, 
month={July},}
@INPROCEEDINGS{7837939, 
author={C. Lesaege and F. Schnitzler and A. Lambert and J. R. Vigouroux}, 
booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
title={Time-Aware User Identification with Topic Models}, 
year={2016}, 
pages={997-1002}, 
abstract={Accounts are often shared by multiple users, each of them having different item consumption and temporal habits. Identifying of the active user can lead to improvements in a variety of services by switching from account personalized services to user personalized services. To do so, we develop a topic model extending the Latent Dirichlet Allocation using a hidden variable representing the active user and assuming consumption times to be generated by latent time topics. We create a new dataset of composite accounts from real users to test the identification capabilities of our model. We show that our model is able to learn temporal patterns from the whole set of accounts and infer the active user using both the consumption time and the consumed item.}, 
keywords={data handling;active user identification;composite accounts;item consumption;latent Dirichlet allocation;temporal habits;time-aware user identification;topic models;Biological system modeling;History;Motion pictures;Parameter estimation;Resource management;TV;Twitter;IP-TV recommendation;Shared accounts;User identification;User profile decomposition}, 
doi={10.1109/ICDM.2016.0126}, 
month={Dec},}
@ARTICLE{6517218, 
author={C. Yi and Y. Tian and A. Arditi}, 
journal={IEEE/ASME Transactions on Mechatronics}, 
title={Portable Camera-Based Assistive Text and Product Label Reading From Hand-Held Objects for Blind Persons}, 
year={2014}, 
volume={19}, 
number={3}, 
pages={808-817}, 
abstract={We propose a camera-based assistive text reading framework to help blind persons read text labels and product packaging from hand-held objects in their daily lives. To isolate the object from cluttered backgrounds or other surrounding objects in the camera view, we first propose an efficient and effective motion-based method to define a region of interest (ROI) in the video by asking the user to shake the object. This method extracts moving object region by a mixture-of-Gaussians-based background subtraction method. In the extracted ROI, text localization and recognition are conducted to acquire text information. To automatically localize the text regions from the object ROI, we propose a novel text localization algorithm by learning gradient features of stroke orientations and distributions of edge pixels in an Adaboost model. Text characters in the localized text regions are then binarized and recognized by off-the-shelf optical character recognition software. The recognized text codes are output to blind users in speech. Performance of the proposed text localization algorithm is quantitatively evaluated on ICDAR-2003 and ICDAR-2011 Robust Reading Datasets. Experimental results demonstrate that our algorithm achieves the state of the arts. The proof-of-concept prototype is also evaluated on a dataset collected using ten blind persons to evaluate the effectiveness of the system's hardware. We explore user interface issues and assess robustness of the algorithm in extracting and reading text from different objects with complex backgrounds.}, 
keywords={Gaussian processes;edge detection;feature extraction;handicapped aids;human computer interaction;image motion analysis;learning (artificial intelligence);optical character recognition;text detection;user interfaces;video cameras;video signal processing;Adaboost model;ICDAR-2003 robust reading datasets;ICDAR-2011 robust reading datasets;ROI;blind persons;camera-based assistive text reading framework;edge pixel distributions;gradient feature learning;handheld objects;mixture-of-Gaussians-based background subtraction method;motion-based method;moving object region extraction;off-the-shelf optical character recognition software;portable camera-based assistive text;product label reading;product packaging;recognized text codes;region of interest;stroke orientations;text characters;text information;text labels;text localization algorithm;text recognition;user interface;Assistive devices;blindness;distribution of edge pixels;hand-held objects;optical character recognition (OCR);stroke orientation;text reading;text region localization}, 
doi={10.1109/TMECH.2013.2261083}, 
ISSN={1083-4435}, 
month={June},}
@INPROCEEDINGS{5136775, 
author={S. H. Park and H. J. Lee and S. P. Han and D. H. Lee}, 
booktitle={2009 International Conference on Advanced Information Networking and Applications Workshops}, 
title={User Age Profile Assessment Using SMS Network Neighbors' Age Profiles}, 
year={2009}, 
pages={960-965}, 
abstract={Customer profile data used in information systems such as recommender systems and collaborative customer relationship management system should be reliable. However, it is hard to maintain high quality of customer profile data because profile information is usually self-reported by users who do not always want to throw their profiles to the company. This paper presents a study of user profile reliability assessment using homophily in a large-scale mobile SMS (short messaging service) data. Our research provides a simple statistical method to find out users' true profiles based on profile information of users' neighbors in social network. Our dataset contains randomly selected 117,333 user data from a larg Korean mobile company, including users' demographic profiles and their text communication histories. Using the text network data, we construct social network. Results show that our method efficiently identifies users with great discrepancy between reported age and actual age. In particular, the prediction accuracy for a user's actual age by our method is 94.4% which is very high compared to 86.5%, the second best accuracy by the simple relational inference approach. The results imply that our age profile assessment model can verify whether a user's age profile is reliable or not and can be applied in practical use.}, 
keywords={electronic messaging;information filters;mobile communication;SMS network;collaborative customer relationship management system;customer profile data;information systems;large-scale mobile SMS data;recommender systems;relational inference approach;short messaging service;social network;user age profile assessment;Collaboration;Customer profiles;Customer relationship management;Large-scale systems;Maintenance;Management information systems;Message service;Recommender systems;Social network services;Statistical analysis;SMS;homophily;mobile communication networks;relational inference;uncertain data}, 
doi={10.1109/WAINA.2009.136}, 
month={May},}
@INPROCEEDINGS{6185492, 
author={E. Hoseini and S. Hashemi and A. Hamzeh}, 
booktitle={2012 26th International Conference on Advanced Information Networking and Applications Workshops}, 
title={Link Prediction in Social Network Using Co-clustering Based Approach}, 
year={2012}, 
pages={795-800}, 
abstract={This paper introduces an approach to derive whether an individual is related to an item or not. In our approach, the well-known DBLP dataset is used and we try to find some skills that are related to an author that we were not aware of before. To realize our objective, we cluster authors and skills using Spectral Graph Clustering algorithm, then simultaneously obtain user and movie clusters via Bipartite Graph (Bigraph) Spectral Co-clustering approach, and then generate predictions based on the outputs of clustering and co-clustering steps. Accordingly, we utilize clustering and co-clustering advantages to predict the probability of link existing between an author and a skill. Experimental results on DBLP dataset show that our approach works well in the specified task.}, 
keywords={graph theory;pattern clustering;probability;spectral analysis;DBLP dataset;bipartite graph;link prediction probability;social network;spectral co-clustering approach;spectral graph clustering algorithm;Algorithm design and analysis;Bipartite graph;Clustering algorithms;Equations;Mathematical model;Partitioning algorithms;Prediction algorithms;Bigraph Spectral Co-clustering;Link prediction;Spectral Graph clustering}, 
doi={10.1109/WAINA.2012.189}, 
month={March},}
@INPROCEEDINGS{7736913, 
author={S. Kirchgasser and A. Uhl}, 
booktitle={2016 International Conference of the Biometrics Special Interest Group (BIOSIG)}, 
title={Biometric Menagerie in Time-Span Separated Fingerprint Data}, 
year={2016}, 
pages={1-7}, 
abstract={Multiple factors are influencing the performance of fingerprint recognition systems. Some of those depend on the used recognition implementations or datasets, while other factors like fingerprint ageing can be difficult to isolate. The aim of this research is the consideration of user related characteristics which have been introduced as the so called "Doddington's Zoo" to describe possible present template ageing influence. Certain user dependent weaknesses could be influenced by fingerprint ageing such that those system vulnerabilities are amplified or even attenuated. To investigate this aspect, the users in the databases (including a time separation of 4 years) are labelled according to the main model provided by the menagerie concept. The analysis of the single categories revealed that the animal groups are not extended (labelled users in the older datasets are not the same as in the newer ones) regardless which dataset and recognition system is considered.}, 
keywords={ageing;fingerprint identification;Doddington zoo;animal groups;biometric menagerie;datasets;fingerprint ageing;fingerprint recognition systems;recognition implementations;template ageing influence;time-span separated fingerprint data;user dependent weaknesses;Aging;Animals;Correlation;Image resolution;NIST;Sensors;Software}, 
doi={10.1109/BIOSIG.2016.7736913}, 
month={Sept},}
@INPROCEEDINGS{7396778, 
author={J. Subercaze and C. Gravier and F. Laforest}, 
booktitle={2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
title={Mining User-Generated Comments}, 
year={2015}, 
volume={1}, 
pages={45-52}, 
abstract={Social-media websites, such as newspapers, blogs, and forums, are the main places of generation and exchange of user-generated comments. These comments are viable sources for opinion mining, descriptive annotations and information extraction. User-generated comments are formatted using a HTML template, they are therefore entwined with the other information in the HTML document. Their unsupervised extraction is thus a taxing issue - even greater when considering the extraction of nested answers by different users. This paper presents a novel technique (CommentsMiner) for unsupervised users comments extraction. Our approach uses both the theoretical framework of frequent subtree mining and data extraction techniques. We demonstrate that the comment mining task can be modelled as a constrained closed induced subtree mining problem followed by a learning-to-rank problem. Our experimental evaluations show that CommentsMiner solves the plain comments and nested comments extraction problems for 84% of a representative and accessible dataset, while outperforming existing baselines techniques.}, 
keywords={data mining;hypermedia markup languages;learning (artificial intelligence);social networking (online);CommentsMiner;HTML document;HTML template;blogs;comment mining task;constrained closed induced subtree mining problem;data extraction;descriptive annotations;forums;frequent subtree mining;information extraction;learning-to-rank problem;nested answer extraction;nested comments extraction;newspapers;opinion mining;plain comment extraction;social-media Web sites;theoretical framework;unsupervised extraction;unsupervised users comments extraction;user-generated comment exchange;user-generated comment generation;user-generated comment mining;user-generated comments;Companies;Data mining;Databases;Feature extraction;HTML;User-generated content;Vegetation}, 
doi={10.1109/WI-IAT.2015.138}, 
month={Dec},}
@ARTICLE{6678350, 
author={O. Khalid and M. U. S. Khan and S. U. Khan and A. Y. Zomaya}, 
journal={IEEE Transactions on Services Computing}, 
title={OmniSuggest: A Ubiquitous Cloud-Based Context-Aware Recommendation System for Mobile Social Networks}, 
year={2014}, 
volume={7}, 
number={3}, 
pages={401-414}, 
abstract={The evolution of mobile social networks and the availability of online check-in services, such as Foursquare and Gowalla, have initiated a new wave of research in the area of venue recommendation systems. Such systems recommend places to users closely related to their preferences. Although venue recommendation systems have been studied in recent literature, the existing approaches, mostly based on collaborative filtering, suffer from various issues, such as: 1) data sparseness, 2) cold start, and 3) scalability. Moreover, many existing schemes are limited in functionality, as the generated recommendations do not consider group of “friends” type situations. Furthermore, the traditional systems do not take into account the effect of real-time physical factors (e.g., distance from venue, traffic, and weather conditions) on recommendations. To address the aforementioned issues, this paper proposes a novel cloud-based recommendation framework OmniSuggest that utilizes: 1) Ant colony algorithms, 2) social filtering, and 3) hub and authority scores, to generate optimal venue recommendations. Unlike existing work, our approach suggests venues at a finer granularity for an individual or a “group” of friends with similar interest. Comprehensive experiments are conducted with a large-scale real dataset collected from Foursquare. The results confirm that our method offers more effective recommendations than many state of the art schemes.}, 
keywords={ant colony optimisation;cloud computing;information filtering;mobile computing;social networking (online);Foursquare;Gowalla;OmniSuggest system;ant colony algorithms;authority score;hub score;mobile social networks;social filtering;ubiquitous cloud-based context-aware recommendation system;venue recommendation systems;Collaboration;Data models;Filtering;Mobile communication;Real-time systems;Scalability;Social network services;Recommendation framework;cloud-framework;group recommendation;mobile social networks}, 
doi={10.1109/TSC.2013.53}, 
ISSN={1939-1374}, 
month={July},}
@INPROCEEDINGS{7813380, 
author={N. El Din Elmadany and Y. He and L. Guan}, 
booktitle={2016 IEEE 18th International Workshop on Multimedia Signal Processing (MMSP)}, 
title={Human gesture recognition via bag of angles for 3D virtual city planning in CAVE environment}, 
year={2016}, 
pages={1-5}, 
abstract={Cave Automatic Virtual Environment (CAVE) provides an immersive virtual environment for 3D city planning. However, the user in the cave has to wear wearable markers for the interaction with the 3D models. To develop a natural interaction, depth camera like Kinect has to be considered. In this paper, we propose a new skeleton joint representation called Bag of Angles (BoA) for human gesture recognition. We evaluated our proposed BoA representation on two dataset UTD-MHAD and UTD-MHAD-KinectV2. The evaluation results demonstrated that the proposed BoA representation can achieve a higher recognition accuracy compared to the other existing representation methods.}, 
keywords={data analysis;gesture recognition;virtual reality;3D models;3D virtual city planning;BoA representation;CAVE environment;UTD-MHAD dataset;UTD-MHAD-KinectV2 dataset;bag of angles;cave automatic virtual environment;depth camera;human gesture recognition;immersive virtual enviroment;natural interaction;recognition accuracy;skeleton joint representation;Encoding;Histograms;Sensors;Skeleton;Three-dimensional displays;Trajectory;Urban areas}, 
doi={10.1109/MMSP.2016.7813380}, 
month={Sept},}
@INPROCEEDINGS{5680852, 
author={Shuang Song and Li Yu and Xiaoping Yang}, 
booktitle={2010 IEEE International Conference on Intelligent Systems and Knowledge Engineering}, 
title={LDA-based user interests discovery in collaborative tagging system}, 
year={2010}, 
pages={338-343}, 
abstract={The success and popularity of collaborative tagging systems, such as delicious, Flickr, Last.fm, has increasingly centered on. Users of these websites can easily tag their interested WebPages, photos and music with their preferred words. Subsequently, the extensive tagging data attract many researchers to mine useful information from these. In this paper, we propose a novel user interests quantified approach based on user-generated tags. Moreover, by means of the generative probabilistic model Latent Dirichlet Allocation (LDA), we acquire the interests for each user. Experimenting with the dataset provided within the ECML PKDD Discovery Challenge 2009, our method makes better performance.}, 
keywords={Web sites;data mining;groupware;identification technology;user interfaces;Flickr;Latent Dirichlet allocation;WebPages;collaborative tagging system;data tagging;del.ici.ous;generative probabilistic model;last.fm;music tagging;photo tagging;user-generated tags;Collaborative tagging system;Interests discovery;LDA;User modeling}, 
doi={10.1109/ISKE.2010.5680852}, 
month={Nov},}
@ARTICLE{4130381, 
author={L. Xie and Z. Q. Liu}, 
journal={IEEE Transactions on Multimedia}, 
title={Realistic Mouth-Synching for Speech-Driven Talking Face Using Articulatory Modelling}, 
year={2007}, 
volume={9}, 
number={3}, 
pages={500-510}, 
abstract={This paper presents an articulatory modelling approach to convert acoustic speech into realistic mouth animation. We directly model the movements of articulators, such as lips, tongue, and teeth, using a dynamic Bayesian network (DBN)-based audio-visual articulatory model (AVAM). A multiple-stream structure with a shared articulator layer is adopted in the model to synchronously associate the two building blocks of speech, i.e., audio and video. This model not only describes the synchronization between visual articulatory movements and audio speech, but also reflects the linguistic fact that different articulators evolve asynchronously. We also present a Baum-Welch DBN inversion (DBNI) algorithm to generate optimal facial parameters from audio given the trained AVAM under maximum likelihood (ML) criterion. Extensive objective and subjective evaluations on the JEWEL audio-visual dataset demonstrate that compared with phonemic HMM approaches, facial parameters estimated by our approach follow the true parameters more accurately, and the synthesized facial animation sequences are so lively that 38% of them are undistinguishable}, 
keywords={belief networks;computer animation;learning (artificial intelligence);maximum likelihood estimation;speech-based user interfaces;Baum-Welch DBN inversion algorithm;acoustic speech;articulatory modelling;audio-visual articulatory model;dynamic Bayesian network;facial animation;maximum likelihood criterion;mouth-synching;phonemic HMM approach;realistic mouth animation;Articulatory model;Baum–Welch DBN inversion (DBNI);dynamic Bayesian networks (DBNs);facial animation;mouth-synching;talking face}, 
doi={10.1109/TMM.2006.888009}, 
ISSN={1520-9210}, 
month={April},}
@INPROCEEDINGS{6495803, 
author={T. Miu and P. Missier}, 
booktitle={2012 SC Companion: High Performance Computing, Networking Storage and Analysis}, 
title={Predicting the Execution Time of Workflow Activities Based on Their Input Features}, 
year={2012}, 
pages={64-72}, 
abstract={The ability to accurately estimate the execution time of computationally expensive e-science algorithms enables better scheduling of workflows that incorporate those algorithms as their building blocks, and may give users an insight into the expected cost of workflow execution on cloud resources. When a large history of past runs can be observed, crude estimates such as the average execution time can easily be provided. We make the hypothesis that, for some algorithms, better estimates can be obtained by using the histories to learn regression models that predict execution time based on selected features of their inputs. We refer to this property as input predictability of algorithms. We are motivated by e-science workflows that involve repetitive training of multiple learning models. Thus, we verify our hypothesis on the specific case of the C4.5 decision tree builder, a well-known learning method whose training execution time is indeed sensitive to the specific input dataset, but in non-obvious ways. We use the case study to demonstrate a method for assessing input predictability. While this yields promising results, we also find that its more general applicability involves a trade off between the black-box nature of the algorithms under analysis, and the need for expert insight into relevant features of their inputs.}, 
keywords={cloud computing;computer aided instruction;decision trees;natural sciences computing;regression analysis;workflow management software;C4.5 decision tree;cloud resources;computationally expensive e-science algorithms;e-science predictability;e-science workflows;learning models;regression models;training execution time;workflow activity execution time prediction}, 
doi={10.1109/SC.Companion.2012.21}, 
month={Nov},}
@INPROCEEDINGS{7538621, 
author={Zhipeng Wang and Jichang Zhao and Ke Xu}, 
booktitle={2016 13th International Conference on Service Systems and Service Management (ICSSSM)}, 
title={Emotion-based Independent Cascade model for information propagation in online social media}, 
year={2016}, 
pages={1-6}, 
abstract={Messages in online social media not only deliver factual signals but also sentiments of massive users. Moreover, emotions, especially the negative ones, can promote the propagation of information that carrying them. However, conventional information diffusion models like Independent Cascade ignore the influence of emotions. With the fact that emotions may lead to wider information propagation, we propose an emotion-based independent cascade model to describe the process of information spread in the circumstance of emotion contagion in online social media. In our model, a message is depicted as a distribution of five emotions and each emotion has a propagation probability between different users. Thus the propagation probability of a message is the weighted average of different emotions. In addition, we introduce an improved method to estimate the parameters of the proposed model. Experimental results on a realistic Weibo data set demonstrate that our model is much better in predicting the real-world information propagation than existing baselines. Our model could shed lights on the employment of emotion contagion in viral marketing of online social media.}, 
keywords={information management;parameter estimation;probability;social networking (online);Weibo dataset;emotion contagion;emotion-based independent cascade model;information diffusion models;information propagation;information spread process;online social media;parameter estimation;propagation probability;viral marketing;Lead;Microwave integrated circuits;Training}, 
doi={10.1109/ICSSSM.2016.7538621}, 
month={June},}
@INPROCEEDINGS{7474069, 
author={A. M. Alkalbani and A. M. Ghamry and F. K. Hussain and O. K. Hussain}, 
booktitle={2016 IEEE 30th International Conference on Advanced Information Networking and Applications (AINA)}, 
title={Sentiment Analysis and Classification for Software as a Service Reviews}, 
year={2016}, 
pages={53-58}, 
abstract={With the rapid growth of cloud services, there has been a significant increase in the number of online consumer reviews and opinions on these services on different social media platforms. These reviews are a source of valuable information in regard to cloud market position and cloud consumer satisfaction. This study explores cloud consumers' reviews that reflect the user's experience with Software as a Service (SaaS) applications. The reviews were collected from different web portals, and around 4000 online reviews were analysed using sentiment analysis to identify the polarity of each review, that is, whether the sentiment being expressed is positive, negative, or neutral. Also, this research develops a model for predicting the sentiment of Software as a Service consumers' reviews using a supervised learning machine called a support vector machine (SVM). The sentiment results show that 62% of the reviews are positive which indicates that consumers are most likely satisfied with SaaS services. The results show that the prediction accuracy of the SVM-based Binary Occurrence approach (3-fold crossvalidation testing) is 92.30%, indicating it performs better in determining sentiment compared with other approaches (Term Occurrences, TFIDF). This work also provides valuable insight into online SaaS reviews and offers the research community the first SaaS polarity dataset.}, 
keywords={classification;cloud computing;consumer behaviour;customer satisfaction;learning (artificial intelligence);portals;sentiment analysis;social networking (online);support vector machines;SVM;SaaS;Web portals;binary occurrence approach;cloud consumer satisfaction;cloud market position;cloud services;online consumer reviews;sentiment analysis;sentiment classification;social media;software as a service;supervised learning machine;support vector machine;Cloud computing;Data mining;Portals;Sentiment analysis;Software as a service;Support vector machines;SaaS polarity dataset;SaaS reviews;sentiment analysis;sentiment classification;supervised machine learning}, 
doi={10.1109/AINA.2016.148}, 
ISSN={1550-445X}, 
month={March},}
@INPROCEEDINGS{7322459, 
author={V. Gadepally and J. Kepner}, 
booktitle={2015 IEEE High Performance Extreme Computing Conference (HPEC)}, 
title={Using a Power Law distribution to describe big data}, 
year={2015}, 
pages={1-5}, 
abstract={The gap between data production and user ability to access, compute and produce meaningful results calls for tools that address the challenges associated with big data volume, velocity and variety. One of the key hurdles is the inability to methodically remove expected or uninteresting elements from large data sets. This difficulty often wastes valuable researcher and computational time by expending resources on uninteresting parts of data. Social sensors, or sensors which produce data based on human activity, such as Wikipedia, Twitter, and Facebook have an underlying structure which can be thought of as having a Power Law distribution. Such a distribution implies that few nodes generate large amounts of data. In this article, we propose a technique to take an arbitrary dataset and compute a power law distributed background model that bases its parameters on observed statistics. This model can be used to determine the suitability of using a power law or automatically identify high degree nodes for filtering and can be scaled to work with big data.}, 
keywords={Big Data;Facebook;Twitter;Wikipedia;big data;data production;power law distribution;social sensors;Big data;Data models;Distributed databases;Matrix converters;Media;Signal processing;Twitter;Big Data;Power Law;Signal Processing}, 
doi={10.1109/HPEC.2015.7322459}, 
month={Sept},}
@INPROCEEDINGS{5466911, 
author={C. Y. Ting and K. C. Khor and S. Phon-Amnuaisuk}, 
booktitle={2010 International Conference on Information Retrieval Knowledge Management (CAMP)}, 
title={Feature extraction and model construction for predicting scientific inquiry skills acquisition}, 
year={2010}, 
pages={240-244}, 
abstract={Assessing scientific inquiry skills in INQPRO, a scientific inquiry learning environment developed in this research work, presents two major challenges: (i) identifying a set of important features from a series of student interactions for assessment of scientific inquiry skills is difficult. Such difficulty stemmed not only because there exists ways a student interacts with the scientific inquiry learning environment, but more challengingly defining the causal dependencies between the extracted features is not a trivial task; (ii) constructing a classification model from large number of features and can handle uncertainty in assessing scientific inquiry skills is not a trivial task. To overcome these challenges, feature selection approach was firstly employed, using the preprocessed dataset from interaction logs of 130 students. A Bayesian Network was subsequently constructed to handle the uncertainty inherent in assessing scientific inquiry skills. Both quantitative and qualitative portions of the Bayesian Network were elicited from a domain expert. Empirical study concluded that (i) expert elicited features outperformed features selected by feature selection algorithms; (ii) Machine-learned Bayesian Network can better encode knowledge about patterns of scientific inquiry skills acquisition as compared to NaiÂ¿ve Bayesian Network.}, 
keywords={belief networks;computer aided instruction;feature extraction;graphical user interfaces;pattern classification;INQPRO learning environment;classification model;feature extraction;machine-learned Bayesian network;model construction;naive Bayesian network;scientific inquiry skills acquisition;Bayesian methods;Feature extraction;Graphical user interfaces;Humans;Inference mechanisms;Information technology;Predictive models;Uncertainty;Bayesian Networks;Feature Selection;Scientific Inquiry Skills;Student Modeling}, 
doi={10.1109/INFRKM.2010.5466911}, 
month={March},}
@INPROCEEDINGS{7817053, 
author={K. Inuzuka and T. Hayashi and T. Takagi}, 
booktitle={2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)}, 
title={Recommendation System Based on Prediction of User Preference Changes}, 
year={2016}, 
pages={192-199}, 
abstract={Time always exists in our lives and time data can easily be collected in a variety of applications. For example, when you purchase items online or click on an ad, the time at which you chose the item or clicked the ad is recorded. The analysis of time information can therefore be applied in various areas. It is important to note that user preferences change over time. For example, a person who watched animated TV shows in childhood will most likely switch to watching the news in adulthood. It is effective to incorporate such changes into recommender systems. In this paper, we propose an approach that predicts user preferences with consideration of preference changes by learning the order of purchase history in a recommender system. Our approach is composed of three steps. First, we obtain user features based on matrix factorization and purchasing time. Next, we use a Kalman filter to predict user preference vectors from user features. Finally, we generate a recommendation list, at which time we propose two types of recommendation methods using the predicted vectors. We then show through experiments using a real-world dataset that our approach outperforms competitive methods such as the first order Markov model.}, 
keywords={Markov processes;matrix decomposition;recommender systems;first order Markov model;matrix factorization;purchasing time;recommendation system;recommender systems;user preference changes;Automobiles;Context;Hidden Markov models;History;Kalman filters;Recommender systems;Time series analysis;Kalman Filter;Matrix Factorization;Time-Aware Recommendation}, 
doi={10.1109/WI.2016.0036}, 
month={Oct},}
@INPROCEEDINGS{6576551, 
author={N. Pappas and A. Popescu-Belis}, 
booktitle={2013 11th International Workshop on Content-Based Multimedia Indexing (CBMI)}, 
title={Combining content with user preferences for TED lecture recommendation}, 
year={2013}, 
pages={47-52}, 
abstract={T
@INPROCEEDINGS{7799643, 
author={H. Liu and J. Huang and C. An and X. Fu}, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={Program Committee}, 
year={2016}, 
pages={xiii-xiv}, 
abstract={Community question answering (CQA) is a popular online service for people to ask and answer questions. But along with the increasing of user generating contents, the quality of answers provided by different users varies widely. So the quality of the answer caused wide attention. In this paper, we propose an answer quality prediction model to evaluate the answer quality considering both aspects of textual and non-textual features. We firstly employ Bidirectional long Short-Term Memory (BLSTM) based RNN model to evaluate textual quality of the answers. And we extract 11 features of the answers to evaluate the non-textual quality of answers. Finally, we jointly consider the score of answers textual and non-textual qualities. We evaluate our model in a benchmark dataset and the experimental results show that our model outperforms other existing approaches.}, 
keywords={Bidirectional long short term memory;answer quality prediction;community question answering}, 
doi={10.1109/WISA.2016.9}, 
month={Sept},}
@ARTICLE{4757270, 
author={L. Yang and O. Tuzel and W. Chen and P. Meer and G. Salaru and L. A. Goodell and D. J. Foran}, 
journal={IEEE Transactions on Information Technology in Biomedicine}, 
title={PathMiner: A Web-Based Tool for Computer-Assisted Diagnostics in Pathology}, 
year={2009}, 
volume={13}, 
number={3}, 
pages={291-299}, 
abstract={Large-scale, multisite collaboration has become indispensable for a wide range of research and clinical activities that rely on the capacity of individuals to dynamically acquire, share, and assess images and correlated data. In this paper, we report the development of a Web-based system, PathMiner , for interactive telemedicine, intelligent archiving, and automated decision support in pathology. The PathMiner system supports network-based submission of queries and can automatically locate and retrieve digitized pathology specimens along with correlated molecular studies of cases from ldquoground-truthrdquo databases that exhibit spectral and spatial profiles consistent with a given query image. The statistically most probable diagnosis is provided to the individual who is seeking decision support. To test the system under real-case scenarios, a pipeline infrastructure was developed and a network-based test laboratory was established at strategic sites at the University of Medicine and Dentistry of New Jersey-Robert Wood Johnson Medical School, Robert Wood Johnson University Hospital, the University of Pennsylvania School of Medicine, Hospital of the University of Pennsylvania, The Cancer Institute of New Jersey, and Rutgers University. The average five-class classification accuracy of the system was 93.18% based on a tenfold cross validation on a close dataset containing 3691 imaged specimens. We also conducted prospective performance studies with the PathMiner system in real applications in which the specimens exhibited large variations in staining characters compared with the training data. The average five-class classification accuracy in this open-set experiment was 87.22%. We also provide the comparative results with the previous literature and the PathMiner system shows superior performance.}, 
keywords={Internet;decision support systems;information retrieval systems;patient diagnosis;pipeline processing;telemedicine;Hospital of the University of Pennsylvania;PathMiner;Robert Wood Johnson Medical School;Robert Wood Johnson University Hospital;Rutgers University;The Cancer Institute of New Jersey;University of Medicine and Dentistry of New Jersey;University of Pennsylvania School of Medicine;Web based computer assisted diagnostics;automated decision support;digitized pathology specimens;ground truth databases;intelligent archiving;interactive telemedicine;network based query submission;network based test laboratory;pipeline infrastructure;Classification;computer-aided diagnostics;content-based image retrieval;segmentation;Algorithms;Artificial Intelligence;Blood Cells;Computer Communication Networks;Diagnosis, Computer-Assisted;Humans;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Internet;Models, Statistical;Reproducibility of Results;User-Computer Interface}, 
doi={10.1109/TITB.2008.2008801}, 
ISSN={1089-7771}, 
month={May},}
@ARTICLE{7784781, 
author={M. Tkachenko and H. W. Lauw}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Comparative Relation Generative Model}, 
year={2017}, 
volume={29}, 
number={4}, 
pages={771-783}, 
abstract={Online reviews are important decision aids to consumers. Other than helping users to evaluate individual products, reviews also support comparison shopping by comparing two (or more) products based on a specific aspect. However, making a comparison across two different reviews, written by different authors, is not always equitable due to the different standards and preferences of authors. Therefore, we focus on comparative sentences, whereby two products are compared directly by a review author within a sentence. We study the problem of comparative relation mining. Given a set of comparative sentences, each relating a pair of entities, our objective is three-fold: to interpret the comparative direction in each sentence, to identify the aspect of each sentence, and to determine the relative merits of each entity with respect to that aspect. This requires mining comparative relations at two levels of resolution: at the sentence level, and at the entity level. Our insight is that there is a significant synergy between the two levels. We propose a generative model for comparative text, which jointly models comparative directions at the sentence level, and ranking at the entity level. This model is tested comprehensively on Amazon reviews dataset with good empirical outperformance over pipelined baselines.}, 
keywords={data mining;Amazon reviews dataset;comparative relation generative model;comparative relation mining problem;entity level;sentence level;Context;Digital cameras;Earth Observing System;Image quality;Information systems;Roads;Generative model;comparative sentences;comparison mining}, 
doi={10.1109/TKDE.2016.2640281}, 
ISSN={1041-4347}, 
month={April},}
@INPROCEEDINGS{7307808, 
author={H. Li and S. Jin and S. LI}, 
booktitle={2015 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery}, 
title={A Hybrid Model for Experts Finding in Community Question Answering}, 
year={2015}, 
pages={176-185}, 
abstract={As a means to share knowledge, the community question answering (CQA) service provides users a chance to obtain or provide help by raising or answering questions. After a question is posted, the system must find an appropriate individual to answer this question. Several approaches have recently been proposed to find experts in CQA. In this paper, a new method to find experts in CQA is proposed by considering user post contents, answer votes, ratio of best answers, and user relation. The votes are used in post relation analysis to calculate user authority. The user's knowledge score can be calculated through topic analysis. Considering that a question usually includes many trivial words, an accurate distribution is nearly impossible to obtain with LDA. To solve this problem, vocabulary is extended by including the link information shown in a question, the top 10 relevant words from Wikipedia are provided for each tag. Tag-LDA models the user topic distribution and predicts the topic distribution of new questions. An experiment is conducted on Stack Overflow dataset, which is the world's largest computer programming CQA site. Experimental results showed approximately 2.97% to 7.79% performance improvement in nDCG@N metrics.}, 
keywords={question answering (information retrieval);text analysis;CQA service;Stack Overflow dataset;Tag-LDA model;Wikipedia;answer votes;community question answering service;computer programming CQA site;expert finding;knowledge sharing;link information;nDCG@N metrics;post relation analysis;topic analysis;user authority;user knowledge score;user post content;user relation;user topic distribution;vocabulary;Computers;Encyclopedias;Internet;Java;Knowledge discovery;Semantics;Web pages;Community question answering;Expert finding;Link analysis;Tag semantic enrichment}, 
doi={10.1109/CyberC.2015.87}, 
month={Sept},}
@INPROCEEDINGS{6091898, 
author={R. Corralejo and R. Hornero and D. Álvarez}, 
booktitle={2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 
title={Feature selection using a genetic algorithm in a motor imagery-based Brain Computer Interface}, 
year={2011}, 
pages={7703-7706}, 
abstract={This study performed an analysis of several feature extraction methods and a genetic algorithm applied to a motor imagery-based Brain Computer Interface (BCI) system. Several features can be extracted from EEG signals to be used for classification in BCIs. However, it is necessary to select a small group of relevant features because the use of irrelevant features deteriorates the performance of the classifier. This study proposes a genetic algorithm (GA) as feature selection method. It was applied to the dataset IIb of the BCI Competition IV achieving a kappa coefficient of 0.613. The use of a GA improves the classification results using extracted features separately (kappa coefficient of 0.336) and the winner competition results (kappa coefficient of 0.600). These preliminary results demonstrated that the proposed methodology could be useful to control motor imagery-based BCI applications.}, 
keywords={brain-computer interfaces;electroencephalography;feature extraction;genetic algorithms;medical signal processing;signal classification;EEG signals;brain computer interface;feature extraction;feature selection;genetic algorithm;kappa coefficient;motor imagery;signal classification;Brain models;Discrete wavelet transforms;Electroencephalography;Feature extraction;Genetic algorithms;Rhythm;Algorithms;Brain;Electroencephalography;Humans;Imagery (Psychotherapy);Motor Cortex;User-Computer Interface}, 
doi={10.1109/IEMBS.2011.6091898}, 
ISSN={1094-687X}, 
month={Aug},}
@INPROCEEDINGS{6651107, 
author={M. Hereld and T. Malik and V. Vishwanath}, 
booktitle={2013 IEEE International Symposium on Parallel Distributed Processing, Workshops and Phd Forum}, 
title={Proactive Support for Large-Scale Data Exploration}, 
year={2013}, 
pages={2025-2034}, 
abstract={Computational science is generating increasingly unwieldy datasets created by complex and high-resolution simulations of physical, social, and economic systems. Traditional post processing of such large datasets requires high bandwidth to large storage resources. In situ processing approaches can reduce I/O requirements but steal processing cycles from the simulation and forsake interactive data exploration. The Fusion project aims to develop a new approach for exploring large-scale scientific datasets wherein the system actively assists the user in the data exploration process. A key component of the system is a software assistant that evaluates the stated and implied analysis goals of the scientist, observes the environment, models and proposes actions to be taken, and orchestrates the generation of analysis and visualization products for the user. These products are managed and made available to the scientist through an interactive space consisting of a database and a visual interface. The scientist sifts through and explores the available analysis products while indicating preferences that are translated into goals, completing the feedback loop that steers the assistant in its future actions.}, 
keywords={data analysis;data visualisation;interactive systems;natural sciences computing;user interfaces;Fusion project;I/O requirement reduction;computational science;data analysis;database;economic system;feedback loop;high-resolution simulation;interactive data exploration;interactive space;large dataset processing;large-scale data exploration;large-scale scientific dataset exploration;physical system;proactive support;processing cycle;social system;software assistant;storage resources;visual interface;visualization products;Analytical models;Data handling;Data models;Data visualization;Databases;Information management;Visualization;adaptive systems;data visualization;human computer interaction;intelligent systems;supercomputers}, 
doi={10.1109/IPDPSW.2013.48}, 
month={May},}
@INPROCEEDINGS{6883388, 
author={Y. Meng and W. Li and L. F. Kwok}, 
booktitle={2014 IEEE International Conference on Communications (ICC)}, 
title={Enhancing email classification using data reduction and disagreement-based semi-supervised learning}, 
year={2014}, 
pages={622-627}, 
abstract={Email classification is an important topic in literature attempting to correctly classify user emails and filter out spam emails. In this paper, we identify some challenges regarding this topic and propose an effective email classification model based on both data reduction and disagreement-based semi-supervised learning. In particular, the main objective of the data reduction is to select an optimum collection of email features and reduce the pointless data, while the objective of the disagreement-based approach is to enhance the accuracy of detecting spam emails by utilizing unlabeled data automatically. In the evaluation, we explore the performance of our proposed email classification model using two public datasets and a private dataset. The experimental results demonstrate that our proposed model can overall enhance the performance of email classification through improving detection accuracy and reducing false rates.}, 
keywords={electronic mail;learning (artificial intelligence);pattern classification;data reduction;disagreement-based semisupervised learning;electronic mail;email classification;private dataset;public dataset;spam email;user email;Accuracy;Data models;Electronic mail;Semisupervised learning;Support vector machines;Training;Vegetation;Data Reduction;Disagreement-based Semi-Supervised Learning;Email Classification;Machine Learning}, 
doi={10.1109/ICC.2014.6883388}, 
ISSN={1550-3607}, 
month={June},}
@INPROCEEDINGS{962156, 
author={A. Roullier-Callaghan}, 
booktitle={6th IEEE High Frequency Postgraduate Colloquium (Cat. No.01TH8574)}, 
title={A radio coverage and planning tool}, 
year={2001}, 
pages={35-40}, 
abstract={With the increased utilisation of mobile communications, and the speed necessary to keep radio systems efficient, a 'RAdioWave Coverage And Planning Tool', RAWCAPT, is proposed. This tool could prove not only cost efficient, but also time efficient for users needing to ascertain the optimal position for a transmitter-receiver radio systems for any urban terrain. The tool utilises a Lidar dataset as the input, due to fact that Lidar data is reasonably cheap compared to other forms of data. On completion of the tool, the full process of radio path planning will be contained within a solitary software application}, 
keywords={radio networks;radiocommunication;radiowave propagation;telecommunication computing;telecommunication network planning;Lidar dataset input;RAWCAPT;mobile communications;propagation modeling;radio coverage tool;radio path planning;radio planning tool;radio systems;software application;urban radio networks;Application software;Cost function;Laser radar;Mobile communication;Path planning;Radio transmitters;Radiowave propagation;Receivers;Software tools;Tiles}, 
doi={10.1109/HFPSC.2001.962156}, 
month={},}
@INPROCEEDINGS{6888896, 
author={X. Yuan and X. Wang and C. Wang and A. Squicciarini and K. Ren}, 
booktitle={2014 IEEE 34th International Conference on Distributed Computing Systems}, 
title={Enabling Privacy-Preserving Image-Centric Social Discovery}, 
year={2014}, 
pages={198-207}, 
abstract={The increasing popularity of images at social media sites is posing new opportunities for social discovery applications, i.e., suggesting new friends and discovering new social groups with similar interests via exploring images. To effectively handle the explosive growth of images involved in social discovery, one common trend for many emerging social media sites is to leverage the commercial public cloud as their robust backend data center. While extremely convenient, directly exposing content-rich images and the related social discovery results to the public cloud also raises new acute privacy concerns. In light of the observation, in this paper we propose a privacy-preserving social discovery service architecture based on encrypted images. As the core of such social discovery is to compare and quantify similar images, we first adopt the effective Bag-of-Words model to extract the "visual similarity content" of users' images into image profile vectors, and then model the problem as similarity retrieval of encrypted high-dimensional image profiles. To support fast and scalable similarity search over hundreds of thousands of encrypted images, we propose a secure and efficient indexing structure. The resulting design enables social media sites to obtain secure, practical, and accurate social discovery from the public cloud, without disclosing the encrypted image content. We formally prove the security and discuss further extensions on user image update and the compatibility with existing image sharing social functionalities. Extensive experiments on a large Flickr image dataset demonstrate the practical performance of the proposed design. Our qualitative social discovery results show consistency with human perception.}, 
keywords={cryptography;data privacy;image retrieval;service-oriented architecture;social networking (online);vectors;Flickr image dataset;backend data center;bag-of-words model;commercial public cloud;encrypted high-dimensional image profile similarity retrieval;encrypted images;image profile vectors;image sharing social functionalities;privacy-preserving image-centric social discovery service architecture;secure indexing structure;social media sites;visual similarity content extraction;Cloud computing;Cryptography;Indexes;Media;Vectors;Visualization;Cloud computing;Image;Privacy-preserving;Social discovery}, 
doi={10.1109/ICDCS.2014.28}, 
ISSN={1063-6927}, 
month={June},}
@INPROCEEDINGS{5656507, 
author={M. T. Islam and K. M. Nahiduzzaman and Y. P. Why and G. Ashraf}, 
booktitle={2010 International Conference on Cyberworlds}, 
title={Learning Character Design from Experts and Laymen}, 
year={2010}, 
pages={134-141}, 
abstract={The use of pose and proportion to represent character traits is well established in art and psychology literature. However, there are no Golden Rules that quantify a generic design template for stylized character figure drawing. Given the wide variety of drawing styles and a large feature dimension space, it is a significant challenge to extract this information automatically from existing cartoon art. This paper outlines a game-inspired methodology for systematically collecting layman perception feedback, given a set of carefully chosen trait labels and character silhouette images. The rated labels were clustered and then mapped to the pose and proportion parameters of characters in the dataset. The trained model can be used to classify new drawings, providing valuable insight to artists who want to experiment with different poses and proportions in the draft stage. The proposed methodology was implemented as follows: 1) Over 200 full-body, front-facing character images were manually annotated to calculate pose and proportion, 2) A simplified silhouette was generated from the annotations to avoid copyright infringements and prevent users from identifying the source of our experimental figures, 3) An online casual role-playing puzzle game was developed to let players choose meaningful tags (role, physicality and personality) for characters, where tags and silhouettes received equitable exposure, 4) Analysis on the generated data was done both in stereotype label space as well as character shape space, 5) Label filtering and clustering enabled dimension reduction of the large description space, and subsequently, a select set of design features were mapped to these clusters to train a neural network classifier. The mapping between the collected perception and shape data give us quantitative and qualitative insight into character design. It opens up applications for creative reuse of (and deviation from) existing character designs.}, 
keywords={computer animation;computer games;learning (artificial intelligence);neural nets;pattern classification;pattern clustering;cartoon art;character design learning;character shape space;character silhouette images;character traits representation;drawing styles;experts;game-inspired methodology;label filtering;large feature dimension space;layman perception feedback;neural network classifier;role-playing puzzle game;stereotype label space;Art;Book reviews;Educational institutions;Games;Psychology;Shape;Visualization;perception games;shape learning;shape psychology}, 
doi={10.1109/CW.2010.66}, 
month={Oct},}
@ARTICLE{4376180, 
author={T. McGraw and M. Nadar}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Stochastic DT-MRI Connectivity Mapping on the GPU}, 
year={2007}, 
volume={13}, 
number={6}, 
pages={1504-1511}, 
abstract={We present a method for stochastic fiber tract mapping from diffusion tensor MRI (DT-MRI) implemented on graphics hardware. From the simulated fibers we compute a connectivity map that gives an indication of the probability that two points in the dataset are connected by a neuronal fiber path. A Bayesian formulation of the fiber model is given and it is shown that the inversion method can be used to construct plausible connectivity. An implementation of this fiber model on the graphics processing unit (GPU) is presented. Since the fiber paths can be stochastically generated independently of one another, the algorithm is highly parallelizable. This allows us to exploit the data-parallel nature of the GPU fragment processors. We also present a framework for the connectivity computation on the GPU. Our implementation allows the user to interactively select regions of interest and observe the evolving connectivity results during computation. Results are presented from the stochastic generation of over 250,000 fiber steps per iteration at interactive frame rates on consumer-grade graphics hardware.}, 
keywords={Bayes methods;biodiffusion;biomedical MRI;brain;computer graphic equipment;data visualisation;medical image processing;neurophysiology;probability;stochastic processes;tensors;Bayesian formulation;GPU;brain;data visualization;diffusion tensor;graphics hardware;graphics processing unit;inversion method;magnetic resonance imaging;neuronal fiber path;parallel algorithm;probability;stochastic DT-MRI connectivity mapping;stochastic fiber tract mapping;Anisotropic magnetoresistance;Diffusion tensor imaging;Ellipsoids;Graphics;Hardware;Injuries;Magnetic resonance imaging;Stochastic processes;Tensile stress;Visualization;diffusion tensor;magnetic resonance imaging;stochastic tractography;Algorithms;Brain;Computer Graphics;Computer Simulation;Diffusion Magnetic Resonance Imaging;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Models, Biological;Models, Neurological;Models, Statistical;Nerve Fibers, Myelinated;Neural Pathways;Numerical Analysis, Computer-Assisted;Reproducibility of Results;Sensitivity and Specificity;Stochastic Processes;User-Computer Interface}, 
doi={10.1109/TVCG.2007.70597}, 
ISSN={1077-2626}, 
month={Nov},}
@INPROCEEDINGS{7529566, 
author={H. Ali and R. Moawad and A. A. F. Hosni}, 
booktitle={2016 IEEE International Conference on Cloud Computing and Big Data Analysis (ICCCBDA)}, 
title={A cloud interoperability broker (CIB) for data migration in SaaS}, 
year={2016}, 
pages={250-256}, 
abstract={Cloud computing is becoming increasingly popular. Information technology market leaders, e.g., Microsoft, Google, and Amazon, are extensively shifting toward cloud-based solutions. However, there is isolation in the cloud implementations provided by the cloud vendors. Limited interoperability can cause one user to adhere to a single cloud provider; thus, a required migration of an application or data from one cloud provider to another may necessitate a significant effort and/or full-cycle redevelopment to fit the new provider's standards and implementation. The ability to move from one cloud vendor to another would be a step toward advancing cloud computing interoperability and increasing customer trust. This study proposes a cloud broker solution to fill the interoperability gap between different software-as-a-service providers. The proposed cloud broker was implemented and tested on a real enterprise application dataset. The migration process was completed and it worked correctly, according to a specified mapping model.}, 
keywords={business data processing;cloud computing;open systems;SaaS;cloud computing;cloud interoperability broker;cloud provider;cloud vendors;cloud-based solutions;customer trust;data migration;enterprise application dataset;full-cycle redevelopment;software-as-a-service providers;Computational modeling;Europe;Facsimile;Interoperability;Mediation;Sugar;Wide area networks;SaaS broker;cloud computing;interoperability}, 
doi={10.1109/ICCCBDA.2016.7529566}, 
month={July},}
@ARTICLE{7312964, 
author={O. Y. Al-Jarrah and O. Alhussein and P. D. Yoo and S. Muhaidat and K. Taha and K. Kim}, 
journal={IEEE Transactions on Cybernetics}, 
title={Data Randomization and Cluster-Based Partitioning for Botnet Intrusion Detection}, 
year={2016}, 
volume={46}, 
number={8}, 
pages={1796-1806}, 
abstract={Botnets, which consist of remotely controlled compromised machines called bots, provide a distributed platform for several threats against cyber world entities and enterprises. Intrusion detection system (IDS) provides an efficient countermeasure against botnets. It continually monitors and analyzes network traffic for potential vulnerabilities and possible existence of active attacks. A payload-inspection-based IDS (PI-IDS) identifies active intrusion attempts by inspecting transmission control protocol and user datagram protocol packet's payload and comparing it with previously seen attacks signatures. However, the PI-IDS abilities to detect intrusions might be incapacitated by packet encryption. Traffic-based IDS (T-IDS) alleviates the shortcomings of PI-IDS, as it does not inspect packet payload; however, it analyzes packet header to identify intrusions. As the network's traffic grows rapidly, not only the detection-rate is critical, but also the efficiency and the scalability of IDS become more significant. In this paper, we propose a state-of-the-art T-IDS built on a novel randomized data partitioned learning model (RDPLM), relying on a compact network feature set and feature selection techniques, simplified subspacing and a multiple randomized meta-learning technique. The proposed model has achieved 99.984% accuracy and 21.38 s training time on a well-known benchmark botnet dataset. Experiment results demonstrate that the proposed methodology outperforms other well-known machine-learning models used in the same detection task, namely, sequential minimal optimization, deep neural network, C4.5, reduced error pruning tree, and randomTree.}, 
keywords={computer network security;digital signatures;learning (artificial intelligence);pattern clustering;telecommunication traffic;transport protocols;PI-IDS;RDPLM;T-IDS;active attacks;active intrusion;attack signatures;benchmark botnet dataset;botnet intrusion detection;cluster-based partitioning;compact network feature set;critical detection-rate;distributed platform;feature selection techniques;intrusion detection system;multiple randomized meta-learning technique;network traffic;packet encryption;packet header analysis;payload-inspection-based IDS;randomized data partitioned learning model;remotely-controlled compromised machines;subspacing technique;traffic-based IDS;transmission control protocol;user datagram protocol packet payload;Accuracy;Clustering algorithms;Computational modeling;Data models;Feature extraction;Intrusion detection;Partitioning algorithms;Botnet intrusion detection;efficient learning;ensembles;feature selection;machine-learning (ML)}, 
doi={10.1109/TCYB.2015.2490802}, 
ISSN={2168-2267}, 
month={Aug},}
@INPROCEEDINGS{7504835, 
author={E. Armenio and M. Ben Meftah and M. F. Bruno and D. De Padova and F. De Pascalis and F. De Serio and A. Di Bernardino and M. Mossa and G. Leuzzi and P. Monti}, 
booktitle={2016 IEEE Workshop on Environmental, Energy, and Structural Monitoring Systems (EESMS)}, 
title={Semi enclosed basin monitoring and analysis of meteo, wave, tide and current data: Sea monitoring}, 
year={2016}, 
pages={1-6}, 
abstract={The present paper aims to show and discuss the long term and continuous recordings of both meteorological and hydrodynamic data collected in a semi enclosed sea. The site in question is composed by the Mar Grande and Mar Piccolo basins (Southern Italy), which are mutually connected. In turn, the Mar Grande is joined to the Ionian Sea by means of two openings. Therefore, the system shows features typical of a lagunar environment, which is also affected by coastal heavy industry and anthropic pressure, thus being highly vulnerable. A monitoring of its hydrodynamics could be useful, allowing both to check the real-time status of the basin and promptly intervene when accidents occur and to create a dataset necessary to calibrate and validate modelling systems providing forecasts. To this, in the framework of the Italian flagship Project RITMARE, a meteo-oceanographic station, a wave-current meter and a tide gauge have been installed in the area, since December 2013. In detail, measurements of wind, waves, tides and current profiles, are acquired on site with different sampling frequencies and are transmitted on a web cloud by a router 3G, where they are stored, thus being available for download by remote users. The data acquisition and processing is managed by the research group of the Department of Civil, Environmental, Building, Engineering and Chemistry (Technical University of Bari). All the acquired data are archived in monthly time-series, examined and discussed. The analysis of currents is made in two different measuring stations for the whole year 2015, as well as the analysis of wave data. On the contrary, tide data have been assessing only recently, since August 2015. Comparisons with available recordings of the year 2014 are examined. Also spatial and temporal correlations of both waves and currents are discussed. Finally, tidal trends are shown, consistent with current inversions.}, 
keywords={data acquisition;oceanographic regions;tides;AD 2013 12;AD 2015;Department of Civil Environmental Building Engineering and Chemistry;Ionian Sea;Italian flagship Project RITMARE;Mar Grande basin;Mar Piccolo basin;Southern Italy;anthropic pressure;basin real-time status;coastal heavy industry;current inversions;current profile measurement;data acquisition;data processing;hydrodynamic data;hydrodynamics monitoring;lagunar environment;meteooceanographic station;meteorological data analysis;router 3G;sampling frequencies;semienclosed basin monitoring;semienclosed sea;spatial correlation;temporal correlation;tidal trends;tide analysis;tide data;tide gauge;tide measurement;time-series;wave data analysis;wave measurement;wave-current meter;web cloud;wind measurement;Acoustic beams;Acoustic measurements;Acoustics;Current measurement;Monitoring;Sea measurements;Tides;annual wave distribution;annual wind distribution;current and tidal level interaction;environmental monitoring}, 
doi={10.1109/EESMS.2016.7504835}, 
month={June},}
@INPROCEEDINGS{6247686, 
author={M. Serra and O. Penacchio and R. Benavente and M. Vanrell}, 
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Names and shades of color for intrinsic image estimation}, 
year={2012}, 
pages={278-285}, 
abstract={In the last years, intrinsic image decomposition has gained attention. Most of the state-of-the-art methods are based on the assumption that reflectance changes come along with strong image edges. Recently, user intervention in the recovery problem has proved to be a remarkable source of improvement. In this paper, we propose a novel approach that aims to overcome the shortcomings of pure edge-based methods by introducing strong surface descriptors, such as the color-name descriptor which introduces high-level considerations resembling top-down intervention. We also use a second surface descriptor, termed color-shade, which allows us to include physical considerations derived from the image formation model capturing gradual color surface variations. Both color cues are combined by means of a Markov Random Field. The method is quantitatively tested on the MIT ground truth dataset using different error metrics, achieving state-of-the-art performance.}, 
keywords={Markov processes;edge detection;image colour analysis;MIT ground truth dataset;Markov random field;color cues;color names;color shades;color-name descriptor;color-shade;error metrics;gradual color surface variations;high-level considerations;image formation model;intrinsic image decomposition;intrinsic image estimation;pure edge-based methods;reflectance changes;strong image edges;strong surface descriptors;top-down intervention;Coherence;Geometry;Image color analysis;Image edge detection;Labeling;Lighting;Vectors}, 
doi={10.1109/CVPR.2012.6247686}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{4462449, 
author={M. S. Renno and Y. Shang and J. Sweeney and O. Dossel}, 
booktitle={2006 International Conference of the IEEE Engineering in Medicine and Biology Society}, 
title={Segmentation of 4D Cardiac Images: Investigation on Statistical Shape Models}, 
year={2006}, 
pages={3086-3089}, 
abstract={The purpose of this research was two-fold: (1) to investigate the properties of statistical shape models constructed from manually segmented cardiac ventricular chambers to confirm the validity of an automatic 4-dimensional (4D) segmentation model that uses gradient vector flow (GVF) images of the original data and (2) to develop software to further automate the steps necessary in active shape model (ASM) training. These goals were achieved by first constructing ASMs from manually segmented ventricular models by allowing the user to cite entire datasets for processing using a GVF-based landmarking procedure and principal component analysis (PCA) to construct the statistical shape model. The statistical shape model of one dataset was used to regulate the segmentation of another dataset according to its GVF, and these results were then analyzed and found to accurately represent the original cardiac data when compared to the manual segmentation results as the golden standard}, 
keywords={biomedical MRI;cardiology;image reconstruction;image segmentation;medical image processing;principal component analysis;4D cardiac image segmentation;MRI;PCA;active shape model training;cardiac ventricular chambers;gradient vector flow images;landmarking procedure;principal component analysis;statistical shape model;Active shape model;Biomedical engineering;Biomedical imaging;Heart;Humans;Image converters;Image reconstruction;Image segmentation;Lattices;Principal component analysis;Biomedical Engineering;Heart;Humans;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Models, Anatomic;Models, Cardiovascular;Models, Statistical;Principal Component Analysis;Software}, 
doi={10.1109/IEMBS.2006.259289}, 
ISSN={1557-170X}, 
month={Aug},}
@INPROCEEDINGS{7098259, 
author={C. Morales and S. Moral}, 
booktitle={2015 IEEE Twelfth International Symposium on Autonomous Decentralized Systems}, 
title={Discretization of Simulated Flight Parameters for Estimation of Situational Awareness Using Dynamic Bayesian Networks}, 
year={2015}, 
pages={196-201}, 
abstract={In the context of a PhD thesis on data mining, we have implemented a simulation environment that collects data for measurements of certain aspects of the Situational Awareness (SA) of a pilot using Bayesian networks (BN). The tool is based on a web application that emulates an Electronic Flight Bag (EFB) and is connected to a flight simulator, providing the user with basic autopilot controls and a customizable interface to access aeronautical information. Relevant data concerning to actions of the pilot, information queries and flight parameters are stored in a database. The use of System Wide Information Management (SWIM) technologies is specially applicable to this research because they provide a robust and powerful approach to the exploration of data relationships. But before analyzing the probabilistic dependencies of the dataset collected during the simulation, it is necessary to study how variables are adapted to the requirements of a Dynamic BN (DBN). This paper briefly presents some SA rating techniques and our approach to achieve a relevant measurement that relies on cockpit information management and DBN, focusing on the first experiment performed with the simulation environment, that analyzes the influence of different discretization criteria on the scores obtained by DBN that learn variable dependencies from data.}, 
keywords={Internet;aerospace computing;aerospace simulation;belief networks;data mining;probability;query processing;DBN;EFB;SA;SWIM technology;Web application;data mining;dynamic Bayesian network;electronic flight bag;flight parameter database;flight parameter simulation;information query;probabilistic dependency analysis;situational awareness estimation;system wide information management;Aircraft;Bayes methods;Computational modeling;Data models;Information management;Measurement;Probabilistic logic;AIXM;BN;DBN;EFB;FIXM;SA;SWIM;bayesian networks;discretization;electronic flight bag;flight simulator;information management;situational awareness}, 
doi={10.1109/ISADS.2015.48}, 
ISSN={1541-0056}, 
month={March},}
@INPROCEEDINGS{858277, 
author={J. D. Hippie and D. J. Daugherty}, 
booktitle={IGARSS 2000. IEEE 2000 International Geoscience and Remote Sensing Symposium. Taking the Pulse of the Planet: The Role of Remote Sensing in Managing the Environment. Proceedings (Cat. No.00CH37120)}, 
title={Urban validation site for testing impervious surface models derived from remotely sensed imagery}, 
year={2000}, 
volume={5}, 
pages={2074-2076 vol.5}, 
abstract={Accurate quantification of impervious surfaces is a necessary input in a variety of urban applications including hydrologic and hydraulic models and landscape change. The purpose is to assess the performance and effectiveness of multiple sensor platforms for the delineation of impervious surface in an urban setting using multifarious classification strategies. Data acquired from airborne and satellite based sensors are used, along with a variety of classification and data fusion strategies, to gather a cost versus reliability measurement for each of the systems. A framework is presented to aid users in selecting the appropriate dataset and methodology for their specific situational needs. The impervious surface generation models are applied to remotely sensed data collected over the Springfield Urban Validation Site (UVS), an approximately 1-km N-S by 4-km E-W urban corridor within the City of Springfield, Missouri. The site is highly documented with respect to position and composition of structures and land covers and consists of varying aged residential developments, commercial, institutional, parks and open space, and light industrial land uses. The models and comparisons developed here can be reliably used to estimate the costs and spatial variability of different methods of impervious surface generation from various imagery inputs, aiding urban planners and managers in the assessment of the errors and biases of various impervious surface generation strategies}, 
keywords={geophysical signal processing;geophysical techniques;hydrological techniques;remote sensing;sensor fusion;terrain mapping;Missouri;Springfield;USA;United States;city;hydraulic model;hydrology;impervious surface;impervious surface model;industrial land use;land cover;land surface;landscape;measurement technique;multifarious classification;multiple sensor;open space;park;remote sensing;runoff;sensor fusion;terrain mapping;town;urban site;validation site;Cities and towns;Costs;Fusion power generation;Land surface;Radiometry;Satellites;Sensor fusion;Sensor systems;Testing;Vegetation mapping},
doi={10.1109/IGARSS.2000.858277}, 
month={},}
@ARTICLE{6634158, 
author={E. Packer and P. Bak and M. Nikkilä and V. Polishchuk and H. J. Ship}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Visual Analytics for Spatial Clustering: Using a Heuristic Approach for Guided Exploration}, 
year={2013}, 
volume={19}, 
number={12}, 
pages={2179-2188}, 
abstract={We propose a novel approach of distance-based spatial clustering and contribute a heuristic computation of input parameters for guiding users in the search of interesting cluster constellations. We thereby combine computational geometry with interactive visualization into one coherent framework. Our approach entails displaying the results of the heuristics to users, as shown in Figure 1, providing a setting from which to start the exploration and data analysis. Addition interaction capabilities are available containing visual feedback for exploring further clustering options and is able to cope with noise in the data. We evaluate, and show the benefits of our approach on a sophisticated artificial dataset and demonstrate its usefulness on real-world data.}, 
keywords={computational geometry;data analysis;data visualisation;pattern clustering;computational geometry;data analysis;distance-based spatial clustering;guided exploration;heuristic approach;interactive visualization;visual analytics;visual feedback;Clustering algorithms;Data visualization;Heuristic algorithms;Image color analysis;Noise measurement;Shape analysis;Visual analytics;Clustering algorithms;Data visualization;Heuristic algorithms;Heuristic-based spatial clustering;Image color analysis;Noise measurement;Shape analysis;Visual analytics;iInteractive visual clustering;k-order a-(alpha)-shapes;Algorithms;Computer Graphics;Computer Simulation;Image Interpretation, Computer-Assisted;Models, Theoretical;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface}, 
doi={10.1109/TVCG.2013.224}, 
ISSN={1077-2626}, 
month={Dec},}
@INPROCEEDINGS{7022698, 
author={M. Yang and K. Chen and Z. Miao and X. Yang}, 
booktitle={2014 IEEE International Conference on Data Mining Workshop}, 
title={Cost-Effective User Monitoring for Popularity Prediction of Online User-Generated Content}, 
year={2014}, 
pages={944-951}, 
abstract={In this paper, we study on the popularity prediction of online user-generated contents, where high quality predictions give us much more flexibility and preparing time in deploying limited resources (such as advertising budget, monitoring capacity) into more popular contents. However the high retrieval cost of data used in prediction is a big challenge due to the large amount of users and contents involved. We propose a notion that higher popularity user-generated contents can be predicted by concentrating on fewer but informative users, as we notice the fact that contents generated by those users tend to become popular while that which are generated by the rest users do not. We develop a cost-effective popularity prediction framework to fulfil online prediction. It contains 3 modules: (a) online data retrieving, (b) informative users selection and (c) popularity prediction. A hybrid user selection algorithm and several popularity prediction algorithms/improvements are presented, and their performance are evaluated and compared using (a) the selected users' generated data and (b) all users' generated data, retrieved from Sina Weibo Micro blogger. The best prediction algorithm reaches a 78% accuracy at the time of 24 hours after publishing time when level width Nl equals 500. And the best combination of prediction and selection algorithms performs only about 7% worse on dataset of 2000 users than on dataset of all users (about 4.46 million).}, 
keywords={information retrieval;monitoring;publishing;social networking (online);user interfaces;Sina Weibo micro blogger;cost-effective popularity prediction framework;cost-effective user monitoσring;high quality predictions;hybrid user selection algorithm;informative users selection;online data retrieving;online user-generated contents;popularity prediction algorithms;popularity user-generated contents;publishing time;Accuracy;Internet;Monitoring;Prediction algorithms;Predictive models;User-generated content;Videos;cost-effective;online user-generated content;popularity prediction;user selection}, 
doi={10.1109/ICDMW.2014.72}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{7871016, 
author={P. Mehetrey and B. Shahriari and M. Moh}, 
booktitle={2016 International Conference on Collaboration Technologies and Systems (CTS)}, 
title={Collaborative Ensemble-Learning Based Intrusion Detection Systems for Clouds}, 
year={2016}, 
pages={404-411}, 
abstract={Cloud computation has become prominent with seemingly unlimited amount of storage and computation available to users. Yet, security is a major issue that hampers the growth of cloud. In this research we investigate a collaborative Intrusion Detection System (IDS) based on the ensemble learning method. It uses weak classifiers, and allows the use of untapped resources of cloud to detect various types of attacks on the cloud system. In the proposed system, tasks are distributed among available virtual machines (VM), individual results are then merged for the final adaptation of the learning model. Performance evaluation is carried out using decision trees and using fuzzy classifiers, on KDD99, one of the largest datasets for IDS. Segmentation of the dataset is done in order to mimic the behavior of real-time data traffic occurred in a real cloud environment. The experimental results show that the proposed approach reduces the execution time with improved accuracy, and is fault-tolerant when handling VM failures. The system is a proof-of-concept model for a scalable, cloud-based distributed system that is able to explore untapped resources, and may be used as a base model for a real-time hierarchical IDS.}, 
keywords={cloud computing;decision trees;fault tolerant computing;groupware;learning (artificial intelligence);pattern classification;security of data;virtual machines;IDS;KDD99;VM failures;cloud-based distributed system;collaborative ensemble-learning;dataset segmentation;decision trees;fault-tolerance;fuzzy classifiers;intrusion detection systems;virtual machines;Bagging;Cloud computing;Collaboration;Decision trees;Intrusion detection;Learning systems;Training;Decision Tree;Intrusion detection;anomaly detection;collaborative systems;ensemble learning;fuzzy classifier}, 
doi={10.1109/CTS.2016.0078}, 
month={Oct},}
@INPROCEEDINGS{7511224, 
author={L. Amour and S. Sami and M. S. Mushtaq and S. Hoceini and A. Mellouk}, 
booktitle={2016 IEEE International Conference on Communications (ICC)}, 
title={Perceived video quality evaluation based on interactive/repulsive relation between the QoE IFs}, 
year={2016}, 
pages={1-7}, 
abstract={The user satisfaction measurement has gained high attention from Network Operators (NOs) and Service Providers (SPs) because their businesses are highly dependent on the user's satisfaction. Generally, the traditional strategies to measure the user's perception are based on Quality of Service (QoS), which is not sufficient to reflect the real user's perceived quality. Therefore, NOs and SPs start to develop new strategies based on the Quality of Experience (QoE) metric to analyze the relationship between the user's satisfaction and influence factors (QoE IFs). In this paper, a new method to build a predictive model to estimate user's satisfaction in terms of Mean Opinion Score (MOS) is proposed. The proposed method uses the dataset collected using the controlled testbed based on the YouTube video service. In the proposed model, the correlation matrix is used to develop a new heuristic method that used back-jumping technique to select the most beneficial factors to predict the optimal user's satisfaction.}, 
keywords={correlation methods;image processing;matrix algebra;quality of experience;MOS;NO;QoE IF;QoS;SP;YouTube video service;back-jumping technique;correlation matrix;dataset collection;heuristic method;interactive-repulsive relation;mean opinion score;network operator;perceived video quality evaluation;predictive model;quality of experience;quality of service;service provider;user perception measurement;user satisfaction measurement;Correlation;Crowdsourcing;Estimation;Predictive models;Quality of service;Streaming media;YouTube;Crowdsourcing;Mean Opinion Score (MOS);Pearson correlation;Quality of Experience (QoE);YouTube video}, 
doi={10.1109/ICC.2016.7511224}, 
month={May},}
@INPROCEEDINGS{6842284, 
author={A. Carrio and C. Fu and J. Pestana and P. Campoy}, 
booktitle={2014 International Conference on Unmanned Aircraft Systems (ICUAS)}, 
title={A ground-truth video dataset for the development and evaluation of vision-based Sense-and-Avoid systems}, 
year={2014}, 
pages={441-446}, 
abstract={The importance of vision-based systems for Sense-and-Avoid is increasing nowadays as remotely piloted and autonomous UAVs become part of the non-segregated airspace. The development and evaluation of these systems demand flight scenario images which are expensive and risky to obtain. Currently Augmented Reality techniques allow the compositing of real flight scenario images with 3D aircraft models to produce useful realistic images for system development and benchmarking purposes at a much lower cost and risk. With the techniques presented in this paper, 3D aircraft models are positioned firstly in a simulated 3D scene with controlled illumination and rendering parameters. Realistic simulated images are then obtained using an image processing algorithm which fuses the images obtained from the 3D scene with images from real UAV flights taking into account on board camera vibrations. Since the intruder and camera poses are user-defined, ground truth data is available. These ground truth annotations allow to develop and quantitatively evaluate aircraft detection and tracking algorithms. This paper presents the software developed to create a public dataset of 24 videos together with their annotations and some tracking application results.}, 
keywords={aerospace computing;augmented reality;autonomous aerial vehicles;computer vision;control engineering computing;object detection;object tracking;rendering (computer graphics);video signal processing;3D aircraft models;3D scene;UAV flights;aircraft detection;augmented reality techniques;autonomous UAV;ground truth annotations;ground truth data;ground-truth video dataset;illumination;image processing algorithm;nonsegregated airspace;real flight scenario images;realistic simulated images;remotely piloted UAV;rendering parameters;system development;system evaluation;tracking algorithms;vision-based sense-and-avoid systems;Aircraft;Atmospheric modeling;Cameras;Software;Solid modeling;Three-dimensional displays;Vibrations}, 
doi={10.1109/ICUAS.2014.6842284}, 
month={May},}
@ARTICLE{5477191, 
author={A. A. Shah and G. Folino and N. Krasnogor}, 
journal={IEEE Transactions on NanoBioscience}, 
title={Toward High-Throughput, Multicriteria Protein-Structure Comparison and Analysis}, 
year={2010}, 
volume={9}, 
number={2}, 
pages={144-155}, 
abstract={Protein-structure comparison (PSC) is an essential component of biomedical research as it impacts on, e.g., drug design, molecular docking, protein folding and structure prediction algorithms as well as being essential to the assessment of these predictions. Each of these applications, as well as many others where molecular comparison plays an important role, requires a different notion of similarity that naturally lead to the multicriteria PSC (MC-PSC) problem. Protein (Structure) Comparison, Knowledge, Similarity, and Information (ProCKSI) (www.procksi.org) provides algorithmic solutions for the MC-PSC problem by means of an enhanced structural comparison that relies on the principled application of information fusion to similarity assessments derived from multiple comparison methods. Current MC-PSC works well for moderately sized datasets and it is time consuming as it provides public service to multiple users. Many of the structural bioinformatics applications mentioned above would benefit from the ability to perform, for a dedicated user, thousands or tens of thousands of comparisons through multiple methods in real time, a capacity beyond our current technology. In this paper, we take a key step into that direction by means of a high-throughput distributed reimplementation of ProCKSI for very large datasets. The core of the proposed framework lies in the design of an innovative distributed algorithm that runs on each compute node in a cluster/grid environment to perform structure comparison of a given subset of input structures using some of the most popular PSC methods [e.g., universal similarity metric (USM), maximum contact map overlap (MaxCMO), fast alignment and search tool (FAST), distance alignment (DaliLite), combinatorial extension (CE), template modeling alignment (TMAlign)]. We follow this with a procedure of distributed consensus building. Thus, the new algorithms proposed here achieve ProCKSI's similarity assessment quality but with a fraction of- - the time required by it. Our results show that the proposed distributed method can be used efficiently to compare: 1) a particular protein against a very large protein structures dataset (target-against-all comparison), and 2) a particular very large-scale dataset against itself or against another very large-scale dataset (all-against-all comparison). We conclude the paper by enumerating some of the outstanding challenges for real-time MC-PSC.}, 
keywords={biochemistry;bioinformatics;message passing;molecular biophysics;proteins;real-time systems;MC-PSC problem;algorithmic solutions;combinatorial extension;maximum contact map overlap;molecular docking;multicriteria protein-structure;protein folding;protein-structure comparison;structural bioinformatics;template modeling alignment;Alignment;comparison;grid;message passing interface (MPI);multicriteria;protein structure;real time;very large-scale datasets;Algorithms;Cluster Analysis;Databases, Protein;High-Throughput Screening Assays;Protein Conformation;Proteins;Proteomics;Sequence Alignment;Sequence Analysis, Protein;Software}, 
doi={10.1109/TNB.2010.2043851}, 
ISSN={1536-1241}, 
month={June},}
@INPROCEEDINGS{6928892, 
author={W. Zhang and H. Sun and X. Liu and X. Guo}, 
booktitle={2014 IEEE International Conference on Web Services}, 
title={Incorporating Invocation Time in Predicting Web Service QoS via Triadic Factorization}, 
year={2014}, 
pages={145-152}, 
abstract={With the development of Service-Oriented technologies, the amount of Web services grows rapidly. QoS-Aware Web service recommendation can help service users to design more efficient service-oriented systems. However, existing methods assume the QoS information for service users are all known and accurate, but in real case, there are always many missing QoS values in history records, which increase the difficulty of the missing QoS value prediction. By considering the user-service-time three dimension context information, we study a Temporal QoS-Aware Web Service Prediction Framework which aims to recommend best candidates to service user's requirements and meanwhile improve the QoS prediction accuracy. One major challenge is that how to deal with the high dimension, sparse QoS value data. Tensor which is known as multi-way array provides a natural representation for such QoS value data. Therefore, we formalize this problem as a tensor factorization model and propose a Tucker Decomposition (TD) algorithm which is able to deal with the triadic relations of user-service-time model. Extensive experiments are conducted based on our real-world QoS dataset collected on Planet-Lab, comprised of service invocation response-time values from 408 users on 5,473 Web services at 56 time periods. Comprehensive empirical studies demonstrate that our approach is more accuracy than other approaches and achieves 100X to 1000X memory space reduction.}, 
keywords={Web services;quality of service;service-oriented architecture;tensors;Planet-Lab;QoS information;QoS-aware Web service recommendation;TD algorithm;Tucker decomposition algorithm;invocation response-time values;multiway array;service-oriented systems;service-oriented technologies;tensor factorization model;triadic factorization;user-service-time;Accuracy;Matrix decomposition;Quality of service;Tensile stress;Time factors;Vectors;Web services;Collaborative Filtering;QoS Prediction;Tensor factorization;Web service recommendation}, 
doi={10.1109/ICWS.2014.32}, 
month={June},}
@INPROCEEDINGS{4028500, 
author={D. q. Zheng and T. j. Zhao and F. Yu and S. Li and H. Yu}, 
booktitle={2006 International Conference on Machine Learning and Cybernetics}, 
title={Research on Chinese Information Retrieval Based on a Hybrid Language Modeling}, 
year={2006}, 
pages={2586-2591}, 
abstract={For information retrieval, users hope to acquire more relevant information from the top indexing documents. In this paper, a combination of ontology with statistical method is presented to retrieval initial document set and improves the precision of top N ranking documents by re-ranking document set. The experiment with NTCIR-3 Chinese CLIR dataset shows the proposed method improved the precision of information retrieval}, 
keywords={document handling;indexing;information retrieval;natural languages;ontologies (artificial intelligence);statistical analysis;Chinese information retrieval;document indexing;hybrid language modeling;ontology combination;statistical method;Business;Cybernetics;Dictionaries;Frequency;Indexing;Information retrieval;Laboratories;Machine learning;Natural language processing;Natural languages;Ontologies;Speech processing;Statistical analysis;Tagging;Ontology;information retrieval;knowledge acquisition;linguistic Ontology knowledge;statistical method}, 
doi={10.1109/ICMLC.2006.258854}, 
ISSN={2160-133X}, 
month={Aug},}
@ARTICLE{6634112, 
author={M. Bögl and W. Aigner and P. Filzmoser and T. Lammarsch and S. Miksch and A. Rind}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Visual Analytics for Model Selection in Time Series Analysis}, 
year={2013}, 
volume={19}, 
number={12}, 
pages={2237-2246}, 
abstract={Model selection in time series analysis is a challenging task for domain experts in many application areas such as epidemiology, economy, or environmental sciences. The methodology used for this task demands a close combination of human judgement and automated computation. However, statistical software tools do not adequately support this combination through interactive visual interfaces. We propose a Visual Analytics process to guide domain experts in this task. For this purpose, we developed the TiMoVA prototype that implements this process based on user stories and iterative expert feedback on user experience. The prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics. The insights from the experts' feedback and the usage scenarios show that TiMoVA is able to support domain experts in model selection tasks through interactive visual interfaces with short feedback cycles.}, 
keywords={data analysis;data visualisation;iterative methods;mathematics computing;statistical analysis;time series;TiMoVA prototype;automated computation;domain experts;epidemiology;human judgement;interactive visual interfaces;iterative expert feedback;model selection tasks;statistical software tools;time series analysis;user experience;user stories;visual analytics process;Analytical models;Autoregressive processes;Data models;Mathematical model;Time series analysis;Analytical models;Autoregressive processes;Data models;Mathematical model;Time series analysis;Visual analytics;coordinated &amp;amp; multiple views;model selection;time series analysis;visual interaction;Algorithms;Computer Graphics;Computer Simulation;Data Interpretation, Statistical;Decision Support Techniques;Models, Statistical;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface}, 
doi={10.1109/TVCG.2013.222}, 
ISSN={1077-2626}, 
month={Dec},}
@INPROCEEDINGS{7275645, 
author={D. Kamalraj and B. Balamurugan and S. Jegadeeswari and M. Sugumaran}, 
booktitle={2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
title={Shamir's key based confidentiality on cloud data storage}, 
year={2015}, 
pages={418-423}, 
abstract={Confidentiality is used on accessing the set of cloud database information with high security level. The conventional encryption and decryption mechanism for the privacy maintenance in cloud zone acquire additional processing time. Hence, the cloud service did not explore effective confidentiality on information retrieval process. A new key distribution scheme for efficient privacy preserving query plans on the cloud data achieves a higher percentage of confidentiality by residing the cloud data with polynomial interpolation. To ensure high confidentiality and providing privacy to the cloud users the Shamir's Key Distribution based Confidentiality (SKDC) scheme is employed. SKDC scheme generates a polynomial of degree with the secret as the first coefficient and the remaining coefficients picked up at random to improve the privacy preserving level on the cloud infrastructure. Shamir's Key Distribution supports batch auditing where multiple user requests for data auditing is held concurrently at a higher confidentiality rate. SKDC scheme handles query processing using the matrix-structure form. An Experimental evaluation is performed with Amazon Simple Storage Service dataset to evaluate the confidentiality and privacy performance with Shamir's Key Distribution based Confidentiality (SKDC) scheme is compared with two well-known privacy schemes such as Trusted Third Party Model (TTPM) and Trusted Hardware Based Database. As a result, the SKDC scheme increases confidentiality level of the user by 22 % as compared to existing TTPM method.}, 
keywords={cloud computing;cryptography;data privacy;information retrieval;interpolation;storage management;Amazon simple storage service dataset;SKDC scheme;Shamir's key based confidentiality;Shamir's key distribution based confidentiality scheme;cloud data storage;cloud database information;cloud infrastructure;data auditing;decryption mechanism;encryption mechanism;information retrieval process;key distribution scheme;matrix-structure form;polynomial interpolation;privacy maintenance;privacy preserving query plans;query processing;Cloud computing;Data privacy;Interpolation;Memory;Polynomials;Privacy;Query processing;Cloud Computing;Confidentiality;SKDC;privacy preserving and polynomial interpolation}, 
doi={10.1109/ICACCI.2015.7275645}, 
month={Aug},}
@INPROCEEDINGS{6883032, 
author={Y. Yin and R. Davis}, 
booktitle={2014 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, 
title={Real-time continuous gesture recognition for natural human-computer interaction}, 
year={2014}, 
pages={113-120}, 
abstract={Our real-time continuous gesture recognition system addresses problems that have previously been neglected: handling both gestures that are characterized by distinct paths and gestures characterized by distinct hand poses; and determining how and when the system should respond to gestures. Our probabilistic recognition framework based on hidden Markov models (HMMs) unifies the recognition of the two forms of gestures. Using information from the hidden states in the HMM, we can identify different gesture phases: the pre-stroke, the nucleus and the post-stroke phases. This allows the system to respond appropriately to both gestures that require a discrete response and those needing a continuous response. Our system is extensible: in only a few minutes, users can define their own gestures by giving a few examples rather than writing code. We also collected a new gesture dataset that contains the two forms of gestures, and propose a new hybrid performance metric for evaluating gesture recognition methods for real-time interaction.}, 
keywords={Gesture recognition;Hidden Markov models;Real-time systems;Sensors;Shape;Smoothing methods;Vectors;Gesture recognition;hidden Markov models;real-time}, 
doi={10.1109/VLHCC.2014.6883032}, 
ISSN={1943-6092}, 
month={July},}
@INPROCEEDINGS{6121364, 
author={Q. Wang and Q. Deng}, 
booktitle={2011 IEEE 17th International Conference on Parallel and Distributed Systems}, 
title={Catching Preference Drift with Initiators in Social Network}, 
year={2011}, 
pages={829-834}, 
abstract={User's preference drift over time gets it difficult to make accurate recommendation. A recommender system ignoring the fact always recommends similar items which were loved previously by users while users' preference have changed and new items appear. It has been proven empirically that traditional algorithms handling the concept drift problem which simply takes time into account is not appropriate, so a model considering both the static and dynamic preference well is the key to catch users' preference drift. We put forward an original model which explores the behavior of influential people, the initiators who initiate trends in social network, to handle this problem. Compared to traditional collaborative filtering approaches and time weighted approaches, empirical study on lastfm dataset has shown that our model improves the accuracy of the recommendation.}, 
keywords={recommender systems;social networking (online);catching preference drift;collaborative filtering;recommender system;social network;users preference drift;Accuracy;Collaboration;Lead;Markov processes;Prediction algorithms;Recommender systems;Social network services;collaborative filtering;initiator;preference drift;recommender system}, 
doi={10.1109/ICPADS.2011.39}, 
ISSN={1521-9097}, 
month={Dec},}
@INPROCEEDINGS{7760618, 
author={S. Adalbjörnsson and J. Swärd and M. Ö. Berg and S. Vang Andersen and A. Jakobsson}, 
booktitle={2016 24th European Signal Processing Conference (EUSIPCO)}, 
title={Conjugate priors for Gaussian emission plsa recommender systems}, 
year={2016}, 
pages={2096-2100}, 
abstract={Collaborative filtering for recommender systems seeks to learn and predict user preferences for a collection of items by identifying similarities between users on the basis of their past interest or interaction with the items in question. In this work, we present a conjugate prior regularized extension of Hofmann's Gaussian emission probabilistic latent semantic analysis model, able to overcome the over-fitting problem restricting the performance of the earlier formulation. Furthermore, in experiments using the EachMovie and MovieLens data sets, it is shown that the proposed regularized model achieves significantly improved prediction accuracy of user preferences as compared to the latent semantic analysis model without priors.}, 
keywords={collaborative filtering;recommender systems;EachMovie dataset;Gaussian emission PLSA recommender system;Hofmann Gaussian emission probabilistic latent semantic analysis model;MovieLens dataset;collaborative filtering;prior conjugation;user preference prediction;Collaboration;Data models;Europe;Probabilistic logic;Recommender systems;Signal processing;Signal processing algorithms;Recommender systems;collaborative filtering;probabilistic matrix factorization}, 
doi={10.1109/EUSIPCO.2016.7760618}, 
month={Aug},}
@INPROCEEDINGS{5694099, 
author={Q. Yang and J. Fan and J. Wang and L. Zhou}, 
booktitle={2010 IEEE International Conference on Data Mining}, 
title={Personalizing Web Page Recommendation via Collaborative Filtering and Topic-Aware Markov Model}, 
year={2010}, 
pages={1145-1150}, 
abstract={Web-page recommendation is to predict the next request of pages that Web users are potentially interested in when surfing the Web. This technique can guide Web users to find more useful pages without asking for them explicitly and has attracted much attention in the community of Web mining. However, few studies on Web page recommendation consider personalization, which is an indispensable feature to meet various preferences of users. In this paper, we propose a personalized Web page recommendation model called PIGEON (abbr. for PersonalIzed web paGe rEcommendatiON) via collaborative filtering and a topic-aware Markov model. We propose a graph-based iteration algorithm to discover users' interested topics, based on which user similarities are measured. To recommend topically coherent pages, we propose a topic-aware Markov model to learn users' navigation patterns which capture both temporal and topical relevance of pages. A thorough experimental evaluation conducted on a large real dataset demonstrates PIGEON's effectiveness and efficiency.}, 
keywords={Internet;Markov processes;graph theory;groupware;information filtering;iterative methods;recommender systems;PIGEON;collaborative filtering;graph based iteration algorithm;personalized Web page recommendation model;requested Web pages prediction;topic aware Markov model;Collaborative Filtering;Markov model;Personalized Recommendation;Web Page Clustering}, 
doi={10.1109/ICDM.2010.28}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{1048137, 
author={T. Choudhury and J. M. Rehg and V. Pavlovic and A. Pentland}, 
booktitle={Object recognition supported by user interaction for service robots}, 
title={Boosting and structure learning in dynamic Bayesian networks for audio-visual speaker detection}, 
year={2002}, 
volume={3}, 
pages={789-794 vol.3}, 
abstract={Bayesian networks are an attractive modeling tool for human sensing, as they combine an intuitive graphical representation with efficient algorithms for inference and learning. Earlier work has demonstrated that boosted parameter learning could be used to improve the performance of Bayesian network, classifiers for complex multi-modal inference problems such as speaker detection. In speaker detection, the goal is to use video and audio cites to infer when a person is speaking to a user interface. In this paper we introduce a new boosted structure learning algorithm based on AdaBoost. Given labeled data, our algorithm modifies both the network structure and parameters so as to improve classification accuracy. We compare its performance to both standard structure learning and boosted parameter learning on a fixed structure. We present results for speaker detection and for the UCI "chess" dataset.}, 
keywords={belief networks;image recognition;inference mechanisms;speaker recognition;AdaBoost;audio-visual speaker detection;boosted parameter learning;dynamic Bayesian networks;inference;modeling tool;structure learning;Bayesian methods;Boosting;Computer networks;Educational institutions;Intelligent networks;Laboratories;Lips;Petroleum;Speech;Testing}, 
doi={10.1109/ICPR.2002.1048137}, 
ISSN={1051-4651}, 
month={},}
@INPROCEEDINGS{7349759, 
author={H. W. Samuel and M. Y. Kim and S. Prabhakar and M. Shazan and M. Jabbar}, 
booktitle={2015 International Conference on Healthcare Informatics}, 
title={Golden Retriever: Question Retrieval System}, 
year={2015}, 
pages={519-520}, 
abstract={Duplicate questions get posted on Q&A online forums because users may not be aware of similar questions. Our proposed system, Golden Retriever, can recommend existing questions that are semantically related to incoming questions. Compared with other existing techniques such as Latent Semantic Indexing, Language Model and Semantic Similarity, our approach shows good results for the ICHI Healthcare Data Analytics Challenge dataset using normalized TF-IDF, relevance heuristics, and semantic relatedness.}, 
keywords={data analysis;health care;medical administrative data processing;question answering (information retrieval);recommender systems;Golden Retriever;ICHI Healthcare Data Analytics Challenge dataset;Q&A online forums;normalized TF-IDF;question recommendation;question retrieval system;relevance heuristics;semantic relatedness;Data analysis;Frequency measurement;Indexing;Medical services;Semantics;Thesauri;community question answering;question retrieval;semantic relatedness}, 
doi={10.1109/ICHI.2015.98}, 
month={Oct},}
@INPROCEEDINGS{1625097, 
author={M. Saban and A. Altinok and A. Peck and C. Kenney and S. Feinstein and L. Wilson and K. Rose and B. S. Manjunath}, 
booktitle={3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.}, 
title={Automated tracking and modeling of microtubule dynamics}, 
year={2006}, 
pages={1032-1035}, 
abstract={The method of microtubule tracking and dynamics analysis, presented here, improves upon the current means of manual and automated quantification of microtubule behavior. Key contributions are increasing accuracy and data volume, eliminating user bias and providing advanced analysis tools for the discovery of temporal patterns in cellular processes. By tracking the entire length of each resolvable microtubule, as opposed to only the tip, it is possible to boost dynamics studies with positional information that is virtually impossible to collect manually. We demonstrate the method on the analysis of a microtubule dataset, which was manually tracked and analyzed in the study of betaIII-tubulin isoform. Our results show that automated recognition of temporal patterns in cellular processes offers a highly promising potential}, 
keywords={biomedical optical imaging;cellular biophysics;medical image processing;molecular biophysics;pattern recognition;proteins;automated microtubule tracking;automated temporal pattern recognition;betaIII-tubulin isoform;cellular processes;microtubule dynamics;Biological system modeling;Biology computing;Cells (biology);Fluorescence;Informatics;Interference constraints;Machine learning;Microscopy;Proteins;Videos}, 
doi={10.1109/ISBI.2006.1625097}, 
ISSN={1945-7928}, 
month={April},}
@INPROCEEDINGS{5160938, 
author={H. H. Su and M. Billingsley and A. D. George}, 
booktitle={2009 IEEE International Symposium on Parallel Distributed Processing}, 
title={A generalized, distributed analysis system for optimization of Parallel Applications}, 
year={2009}, 
pages={1-8}, 
abstract={Developing a high performance parallel application is difficult. An application must often be analyzed and optimized by the programmer before reaching an acceptable level of performance. Performance tools that collect and visualize performance data can reduce the effort needed by the user in the nontrivial optimization process. However, as the size of the performance dataset grows, it becomes nearly impossible for the user to manually examine the data and find performance issues. To address this problem, we have developed a new analysis system to automatically detect, diagnose, and possibly resolve bottlenecks. In this paper, we present the architecture and the distributed, peer-to-peer processing mechanism of a programming model-independent analysis system, which includes a range of useful analyses such as scalability analysis and common-bottleneck detection. We then describe the details of an initial sequential implementation of the system that has been integrated into our parallel performance wizard (PPW) tool. Finally, we provide correctness and performance results for this initial version and demonstrate the effectiveness of the system through two case studies.}, 
keywords={optimisation;peer-to-peer computing;software tools;common-bottleneck detection;distributed analysis system;nontrivial optimization process;parallel applications;parallel performance wizard tool;peer-to-peer processing mechanism;programming model-independent analysis system;scalability analysis;Analytical models;Computational modeling;Concurrent computing;Data visualization;Distributed computing;High performance computing;Parallel programming;Peer to peer computing;Performance analysis;Programming profession}, 
doi={10.1109/IPDPS.2009.5160938}, 
ISSN={1530-2075}, 
month={May},}
@INPROCEEDINGS{4053171, 
author={S. t. Wu and Y. Li and Y. Xu}, 
booktitle={Sixth International Conference on Data Mining (ICDM'06)}, 
title={Deploying Approaches for Pattern Refinement in Text Mining}, 
year={2006}, 
pages={1157-1161}, 
abstract={Text mining is the technique that helps users find useful information from a large amount of digital text documents on the Web or databases. Instead of the keyword-based approach which is typically used in this field, the pattern-based model containing frequent sequential patterns is employed to perform the same concept of tasks. However, how to effectively use these discovered patterns is still a big challenge. In this study, we propose two approaches based on the use of pattern deploying strategies. The performance of the pattern deploying algorithms for text mining is investigated on the Reuters dataset RCVI and the results show that the effectiveness is improved by using our proposed pattern refinement approaches.}, 
keywords={Internet;data mining;text analysis;Reuters dataset;World Wide Web;databases;digital text documents;frequent sequential patterns;pattern deploying algorithms;pattern refinement;pattern-based model;text mining;Australia;Data communication;Data mining;Databases;Frequency;Indexing;Information retrieval;Software engineering;Text categorization;Text mining}, 
doi={10.1109/ICDM.2006.50}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{6754003, 
author={A. Saluja and M. Pakdaman and D. Piao and A. P. Parikh}, 
booktitle={2013 IEEE 13th International Conference on Data Mining Workshops}, 
title={Infinite Mixed Membership Matrix Factorization}, 
year={2013}, 
pages={800-807}, 
abstract={Rating and recommendation systems have become a popular application area for applying a suite of machine learning techniques. Current approaches rely primarily on probabilistic interpretations and extensions of matrix factorization, which factorizes a user-item ratings matrix into latent user and item vectors. Most of these methods fail to model significant variations in item ratings from otherwise similar users, a phenomenon known as the "Napoleon Dynamite'' effect. Recent efforts have addressed this problem by adding a contextual bias term to the rating, which captures the mood under which a user rates an item or the context in which an item is rated by a user. In this work, we extend this model in a nonparametric sense by learning the optimal number of moods or contexts from the data, and derive Gibbs sampling inference procedures for our model. We evaluate our approach on the Movie Lens 1M dataset, and show significant improvements over the optimal parametric baseline, more than twice the improvements previously encountered for this task. We also extract and evaluate a DBLP dataset, wherein we predict the number of papers co-authored by two authors, and present improvements over the parametric baseline on this alternative domain as well.}, 
keywords={inference mechanisms;learning (artificial intelligence);matrix decomposition;recommender systems;sampling methods;DBLP dataset;Gibbs sampling inference procedures;Movie Lens 1M dataset;Napoleon dynamite effect;contextual bias term;infinite mixed membership matrix fa
@INPROCEEDINGS{7870917, 
author={Y. N. Babuji and K. Chard and A. Gerow and E. Duede}, 
booktitle={2016 IEEE 12th International Conference on e-Science (e-Science)}, 
title={A secure data enclave and analytics platform for social scientists}, 
year={2016}, 
pages={337-342}, 
abstract={Data-driven research is increasingly ubiquitous and data itself is a defining asset for researchers, particularly in the computational social sciences and humanities. Entire careers and research communities are built around valuable, proprietary or sensitive datasets. However, many existing computation resources fail to support secure and cost-effective storage of data while also enabling secure and flexible analysis of the data. To address these needs we present CLOUD KOTTA, a cloud-based architecture for the secure management and analysis of social science data. CLOUD KOTTA leverages reliable, secure, and scalable cloud resources to deliver capabilities to users, and removes the need for users to manage complicated infrastructure. CLOUD KOTTA implements automated, cost-aware models for efficiently provisioning tiered storage and automatically scaled compute resources. CLOUD KOTTA has been used in production for several months and currently manages approximately 10TB of data and has been used to process more than 5TB of data with over 75,000 CPU hours. It has been used for a broad variety of text analysis workflows, matrix factorization, and various machine learning algorithms, and more broadly, it supports fast, secure and cost-effective research.}, 
keywords={cloud computing;data analysis;learning (artificial intelligence);matrix decomposition;resource allocation;security of data;social sciences computing;storage management;text analysis;CLOUD KOTTA;cloud-based architecture;computational social sciences;cost-aware model;cost-effective data storage;cost-effective research;data analytics;data-driven research;humanities;machine learning algorithm;matrix factorization;proprietary dataset;scalable cloud resources;secure data enclave;secure data management;sensitive dataset;social science data analysis;text analysis workflow;tiered storage;Cloud computing;Computational modeling;Computer architecture;Data models;Databases;Production;Reliability}, 
doi={10.1109/eScience.2016.7870917}, 
month={Oct},}
@INPROCEEDINGS{1221015, 
author={Shu-Ching Chen and Keqi Zhang and Min Chen}, 
booktitle={Multimedia and Expo, 2003. ICME '03. Proceedings. 2003 International Conference on}, 
title={A real-time 3D animation environment for storm surge}, 
year={2003}, 
volume={1}, 
pages={I-705-8 vol.1}, 
abstract={This paper describes an approach to develop a high performance animation environment for storm surge. The system provides the capability to simulate the storm surge effects in the physical world by (1) modeling a region using the airborne light detection and ranging (LIDAR) data, USGS orthophotos, RLG road data and photos; (2) animating the storm impact by using the features of this model; and (3) providing the capability for users to explore the animation environment. We present our system by modeling the dataset collected from Ft. Lauderdale, a region in South Florida, USA.}, 
keywords={atmospheric techniques;computer animation;geophysics computing;optical radar;radar computing;storms;Ft. Lauderdale;LIDAR data;South Florida;USA;airborne light detection and ranging;real-time 3D animation environment;storm surge;Animation;Architecture;Buildings;Filters;Laser radar;Power system modeling;Roads;Storms;Surges;Vegetation mapping}, 
doi={10.1109/ICME.2003.1221015}, 
month={July},}
@INPROCEEDINGS{5569294, 
author={C. Huang and J. Yin}, 
booktitle={2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery}, 
title={Effective association clusters filtering to cold-start recommendations}, 
year={2010}, 
volume={5}, 
pages={2461-2464}, 
abstract={In this paper, we focus on how to overcome cold-start problem in the traditional research of recommendations system(RS). The popular technique of RS is collaborative filtering(CF). While in real online RS, CF can't practically solve cold-start problem for the sparsity ratings dataset. In this paper, we propose a novel efficiently association clusters filtering(ACF) algorithm. Considering hybrid approaches, using clustering and also filtering to relieve cold-start problem. ACF algorithm establishes clusters models based on the ratings matrix. We assume the users in the same cluster, they will have the same interests. On the other hand, different users in different clusters present they will have less common interests. The more users ratings for some item in the cluster, can delegate the opinion of the cluster. So we can use the opinion of the cluster to predict the unkowned ratings. Throught the experiments, our method can enlarge the prediction scope and improve the accuracy.}, 
keywords={information filtering;recommender systems;association cluster filtering;cold-start problem;cold-start recommendation system;collaborative filtering;prediction scope;sparsity rating dataset;Algorithm design and analysis;Clustering algorithms;Collaboration;Correlation;Filtering;Filtering algorithms;Prediction algorithms;Association clusters filtering;Cold-start;Collaborative Filtering;Recommendation System;component}, 
doi={10.1109/FSKD.2010.5569294}, 
month={Aug},}
@INPROCEEDINGS{6783842, 
author={J. C. Ying and B. N. Shi and V. S. Tseng and H. W. Tsai and K. H. Cheng and S. C. Lin}, 
booktitle={2013 Conference on Technologies and Applications of Artificial Intelligence}, 
title={Preference-Aware Community Detection for Item Recommendation}, 
year={2013}, 
pages={49-54}, 
abstract={In recent years, researches on recommendation systems based on social information have attracted a lot of attentions. Although a number of social-based recommendation techniques have been proposed in the literature, most of their concepts are only based on the individual or friends' rating behaviors. It leads to the problem that the recommended item list is usually constrained within the users' or friends' living area. Furthermore, since context-aware and environmental information changes quickly, especially in social networks, how to select appropriate relevant users from such kind of heterogeneous social structure to facilitate the social-based recommendation is also a critical and challenging issue. In this paper, we propose a novel approach named Preference-aware Community-based Recommendation System (PCRS) that integrates Preference-aware Community Detection (PCD) for recommending items to users based on the user preferences and social network structure simultaneously. The core idea of PCRS is to build a community-based collaborating filtering model in the user-to-item matrix, so as to support the estimation of users' rating for each item. Based on the social network data, we detect communities through users' Social Factor and Individual Preference for our community-based collaborating filtering model. To our best knowledge, this is the first work on community-based collaborating filtering model that considers both social factor and individual preference in social network data, simultaneously. Through comprehensive experimental evaluations on a real dataset from Go Walla, the proposed PCRS is shown to deliver excellent performance.}, 
keywords={collaborative filtering;data mining;environmental factors;recommender systems;social networking (online);Gowalla;PCD;PCRS;community-based collaborating filtering model;context-aware information;environmental information;friend rating behaviors;heterogeneous social structure;individual rating behaviors;item recommendation system;preference-aware community detection;preference-aware community-based recommendation system;recommended item list;social information;social network data;social network structure;social-based recommendation techniques;user preferences;user rating estimation;user social factor;user-to-item matrix;Collaboration;Communities;Data mining;Filtering;Mathematical model;Social factors;Social network services;Community Detection;Data Mining;Recommendation System;Social Network;User Preference Mining}, 
doi={10.1109/TAAI.2013.23}, 
ISSN={2376-6816}, 
month={Dec},}
@INPROCEEDINGS{6633642, 
author={E. Laufer and R. C. Ferrari and L. Yao and O. Delalleau and Y. Bengio}, 
booktitle={2013 IEEE Conference on Computational Inteligence in Games (CIG)}, 
title={Stacked calibration of off-policy policy evaluation for video game matchmaking}, 
year={2013}, 
pages={1-8}, 
abstract={We consider an industrial strength application of recommendation systems for video-game matchmaking in which off-policy policy evaluation is important but where standard approaches can hardly be applied. The objective of the policy is to sequentially form teams of players from those waiting to be matched, in such a way as to produce well-balanced matches. Unfortunately, the available training data comes from a policy that is not known perfectly and that is not stochastic, making it impossible to use methods based on importance weights. Furthermore, we observe that when the estimated reward function and the policy are obtained by training from the same off-policy dataset, the policy evaluation using the estimated reward function is biased. We present a simple calibration procedure that is similar to stacked regression and that removes most of the bias, in the experiments we performed. Data collected during beta tests of Ghost Recon Online, a first person shooter from Ubisoft, were used for the experiments.}, 
keywords={calibration;computer games;pattern matching;recommender systems;regression analysis;Ghost Recon Online;Ubisoft;beta tests;industrial strength application;off-policy policy evaluation;recommendation systems;reward function;stacked calibration procedure;stacked regression;training data;video game matchmaking;Context;Data models;Games;Predictive models;Standards;Training;Vegetation}, 
doi={10.1109/CIG.2013.6633642}, 
ISSN={2325-4270}, 
month={Aug},}
@INPROCEEDINGS{7847983, 
author={W. Y. Quck and D. Y. Huang and W. Lin and H. Li and M. Dong}, 
booktitle={2016 IEEE Region 10 Conference (TENCON)}, 
title={Mobile acoustic Emotion Recognition}, 
year={2016}, 
pages={170-174}, 
abstract={The applicability of Emotion Recognition using machines spans on or many industries. It is due to this thai much resource has been put into researching and developing algorithms that can perform such task more accurately and efficiently. However, despite all the work done and its uses, such technology is not readily available to non-professionals. Most applications in the market tend to be complex. In this paper, we document the development of EmoRec. an application made for iOS that strive to serve this purpose. It achieves this through two folds. Firstly, it is presents an intuitive interface that a layperson can use to predict his or her emotions through an audio input easily. Also, it allows users to record and upload their audio data and labels which can be used for future work in the field of Emotion Recognition. As with the algorithm used for EmoRec. deep learning algorithms work best with a huge dataset. Hence, this function will also improve the result of these algorithms. This paper will detail the process of development from designing to optimization and testing, and end off with a discussion of the result and future work that could be done.}, 
keywords={acoustic signal processing;audio signal processing;emotion recognition;graphical user interfaces;iOS (operating system);learning (artificial intelligence);mobile computing;EmoRec;audio data recording;audio data upload;audio input;audio labels;deep learning algorithms;iOS;mobile acoustic emotion recognition;Algorithm design and analysis;Data models;Emotion recognition;Feature extraction;Prediction algorithms;Predictive models;Training}, 
doi={10.1109/TENCON.2016.7847983}, 
month={Nov},}
@INPROCEEDINGS{6377772, 
author={C. J. Wang and H. S. Huang and H. H. Chen}, 
booktitle={2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={Automatic construction of an evaluation dataset from wisdom of the crowds for information retrieval applications}, 
year={2012}, 
pages={490-495}, 
abstract={A benchmark evaluation dataset which reflects users' search behaviors in the real world is indispensable for evaluating the performance of information retrieval applications. A typical evaluation dataset consists of a document set, a topic set and relevance judgments. Manual preparation of an evaluation dataset needs much human cost, and human-made topics may not fully capture users' real search needs. This paper aims at automatically constructing an evaluation dataset from wisdom of the crowds in search query logs for information retrieval applications. We begin with collecting documents of clicked documents in search query logs, selecting suitable queries in terms of topics, sampling documents from the document collection for each query and estimating the multi-level relevance of document samples based on click count, normalized count and average count functions. The machine-made evaluation dataset is trained and tested by three learning to rank algorithms, including linear regression, SVMRank and FRank. We compare their performance on a testing collection MQ2007 of LETOR which is a well-known human-made benchmark dataset for learning to rank. The experimental results show that the performance tendency is similar by using machine-made and human-made evaluation datasets. That demonstrates our proposed models can construct an evaluation dataset with similar quality of human-made.}, 
keywords={document handling;information retrieval;learning (artificial intelligence);regression analysis;sampling methods;support vector machines;user interfaces;FRank algorithm;SVM algorithm;average count function;click count function;document collection;document sampling;document set;human-made evaluation dataset;information retrieval application;learning-to-rank algorithm;linear regression algorithm;machine-made evaluation dataset;normalized count function;relevance judgment;search query log;support vector machines;topic set;user search behavior;user search need;Humans;Information retrieval;Linear regression;Measurement;Predictive models;Testing;Training;evaluation dataset construction;retrieval evaluation;search query logs analysis}, 
doi={10.1109/ICSMC.2012.6377772}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{7881496, 
author={S. M. Hussein}, 
booktitle={2016 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
title={Performance Evaluation of Intrusion Detection System Using Anomaly and Signature Based Algorithms to Reduction False Alarm Rate and Detect Unknown Attacks}, 
year={2016}, 
pages={1064-1069}, 
abstract={With the rapidly growth of technology, used Internet become an important part in human life it used in many sectors of society, communicating over global network, sending or receiving sensitive data is risk because different techniques are used by attackers to intercept and exposed data. As a result, strong security technique is required to guaranteeing the user data. Many methods proposed to improve security issues. Intrusion detection system (IDS) is used to observe unwanted action on network systems and individual computers. However due to opposite effect of using IDS, using individual methods of IDS only misuse or anomaly attacks can be detected. This paper proposed a model that integrates both approaches signature and anomaly based of IDS to reduce obtained alerts and detects new attacks. False alarm rate, accuracy, and detect attacks are the parameters used to evaluate effectiveness of hybrid IDS in addition Knowledge Discovery Data Mining (KDD) CUP 99 dataset and Waikato Environment for Knowledge Analysis (WEKA) program has been used for testing the proposed hybrid IDS.}, 
keywords={security of data;IDS;Internet;KDD CUP 99 dataset;WEKA program;Waikato Environment for Knowledge Analysis;anomaly based algorithms;false alarm rate reduction;intrusion detection system;knowledge discovery data mining;signature based algorithms;Computers;Data mining;Engines;Intrusion detection;Neural networks;Telecommunication traffic;Baysian Networks;IDS;K-Means;anomaly based detection;hybrid;naïve bayes;signature based detection;snort}, 
doi={10.1109/CSCI.2016.0203}, 
month={Dec},}
@INPROCEEDINGS{6217424, 
author={Y. Su and G. Agrawal}, 
booktitle={2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012)}, 
title={Supporting User-Defined Subsetting and Aggregation over Parallel NetCDF Datasets}, 
year={2012}, 
pages={212-219}, 
abstract={While dissemination of scientific data is becoming crucial for facilitating scientific discoveries, a key challenge being faced by these efforts is that the dataset sizes continue to grow rapidly. Coupled with the fact that wide area data transfer bandwidths and disk retrieval speeds are growing at a much slower pace, it is becoming extremely hard for scientists to download, manage, and process scientific datasets. We have developed a light-weight data management tool, which allows server-side sub setting and aggregation on scientific datasets stored in a native format. While our approach is more general, this paper describes an implementation specific to NetCDF, which is one of the most popular scientific data formats. To support a variety of queries efficiently, our tool generates code for pre-filtering and post-filtering, and parallelize selection and aggregation queries efficiently using novel algorithms. We have extensively evaluated our implementation and compared its performance and functionality against Open DAP. We demonstrate that even for sub setting queries that are directly supported in Open DAP, the sequential performance of our system is better. In addition, our system is capable of supporting a larger variety of queries, scaling performance by parallelizing the queries, and reducing wide area data transfers through server-side data aggregation.}, 
keywords={electronic data interchange;file servers;information dissemination;program compilers;query processing;OPeNDAP;aggregation queries;code generation tool;data transfer bandwidths;disk retrieval speeds;light-weight data management tool;parallel NetCDF datasets;post-filtering;pre-filtering;scientific data dissemination;scientific discoveries;server-side data aggregation;server-side subsetting;user-defined aggregation;user-defined subsetting;Arrays;Atmospheric modeling;Data mining;Data models;Databases;Layout;Parallel processing;Data-intensive computing;Scientific databases}, 
doi={10.1109/CCGrid.2012.45}, 
month={May},}
@ARTICLE{5492194, 
author={P. C. van Oorschot and A. Salehi-Abari and J. Thorpe}, 
journal={IEEE Transactions on Information Forensics and Security}, 
title={Purely Automated Attacks on PassPoints-Style Graphical Passwords}, 
year={2010}, 
volume={5}, 
number={3}, 
pages={393-405}, 
abstract={We introduce and evaluate various methods for purely automated attacks against PassPoints-style graphical passwords. For generating these attacks, we introduce a graph-based algorithm to efficiently create dictionaries based on heuristics such as click-order patterns (e.g., five points all along a line). Some of our methods combine click-order heuristics with focus-of-attention scan-paths generated from a computational model of visual attention, yielding significantly better automated attacks than previous work. One resulting automated attack finds 7%-16% of passwords for two representative images using dictionaries of approximately 226 entries (where the full password space is 243). Relaxing click-order patterns substantially increased the attack efficacy albeit with larger dictionaries of approximately 235 entries, allowing attacks that guessed 48%-54% of passwords (compared to previous results of 1% and 9% on the same dataset for two images with 235 guesses). These latter attacks are independent of focus-of-attention models, and are based on image-independent guessing patterns. Our results show that automated attacks, which are easier to arrange than human-seeded attacks and are more scalable to systems that use multiple images, require serious consideration when deploying basic PassPoints-style graphical passwords.}, 
keywords={computer graphics;image processing;security of data;PassPoints-style graphical password;automated attack;click-order heuristics;click-order pattern;computational model;dictionaries;focus-of-attention model;focus-of-attention scan-path;graph-based algorithm;human-seeded attack;image-independent guessing pattern;password space;Computational modeling;Computer security;Dictionaries;Focusing;Graphical user interfaces;Human factors;Image processing;Machine vision;Permission;Proposals;Algorithms;computer security;graphical user interfaces;human factors;image processing;machine vision}, 
doi={10.1109/TIFS.2010.2053706}, 
ISSN={1556-6013}, 
month={Sept},}
@INPROCEEDINGS{6544850, 
author={K. Hu and W. Hsu and M. L. Lee}, 
booktitle={2013 IEEE 29th International Conference on Data Engineering (ICDE)}, 
title={Utilizing users' tipping points in E-commerce Recommender systems}, 
year={2013}, 
pages={494-504}, 
abstract={Existing recommendation algorithms assume that users make their purchase decisions solely based on individual preferences, without regard to the type of users nor the products' maturity stages. Yet, extensive studies have shown that there are two types of users: innovators and imitators. Innovators tend to make purchase decisions based solely on their own preferences; whereas imitators' purchase decisions are often influenced by a product's stage of maturity. In this paper, we propose a framework that seamlessly incorporates the type of user and product maturity into existing recommendation algorithms. We apply Bass model to classify each user as either an innovator or imitator according to his/her previous purchase behavior. In addition, we introduce the concept of tipping point of a user. This tipping point refers to the point on the product maturity curve beyond which the user is likely to be more receptive to purchasing the product. We refine two widely-adopted recommendation algorithms to incorporate the effect of product maturity in relation to the user type. Experiment results on a real-world dataset obtained from an E-commerce website show that the proposed approach outperforms existing algorithms.}, 
keywords={Web sites;electronic commerce;purchasing;recommender systems;bass model;e-commerce website;imitators;innovators;product maturity curve;purchase decisions;real-world dataset;recommender systems;user tipping points;user type;Arrays;Collaboration;Computational modeling;Educational institutions;History;Predictive models;Recommender systems}, 
doi={10.1109/ICDE.2013.6544850}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{1532817, 
author={C. D. Correa and D. Silver}, 
booktitle={VIS 05. IEEE Visualization, 2005.}, 
title={Dataset traversal with motion-controlled transfer functions}, 
year={2005}, 
pages={359-366}, 
abstract={In this paper, we describe a methodology and implementation for interactive dataset traversal using motion-controlled transfer functions. Dataset traversal here refers lo the process of translating a transfer function along a specific path. In scientific visualization, it is often necessary to manipulate transfer functions in order to visualize datasets more effectively. This manipulation of transfer functions is usually performed globally, i.e., a new transfer function is applied to the entire dataset. Our approach allows one to locally manipulate transfer functions while controling its movement along a traversal path. The method we propose allows the user to select a traversal path within the dataset, based on the shape of the volumetric model and manipulate a transfer function along this path. Examples of dataset traversal include the animation of transfer functions along a pre-defined path, the simulation of flow in vascular structures, and the visualization of convoluted shapes. For example, this type of traversal is often used in medical illustration to highlight flow in blood vessels. We present an interactive implementation of our method using graphics hardware, based on the decomposition of the volume. We show examples of our approach using a variety of volumetric datasets, and we also demonstrate that with our novel decomposition, the rendering process is faster.}, 
keywords={blood vessels;computer animation;data visualisation;haemorheology;image motion analysis;medical image processing;rendering (computer graphics);transfer functions;visual databases;blood vessels;dataset traversal;graphics hardware;interactive dataset traversal;medical illustration;motion-controlled transfer function;rendering process;scientific visualization;transfer function animation;vascular structure;volumetric datasets;volumetric model;Animation;Biomedical imaging;Blood vessels;Computer graphics;Medical simulation;Rendering (computer graphics);Shape;Silver;Transfer functions;Visualization}, 
doi={10.1109/VISUAL.2005.1532817}, 
month={Oct},}
@ARTICLE{7522083, 
author={S. Liang and F. Cai and Z. Ren and M. de Rijke}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Efficient Structured Learning for Personalized Diversification}, 
year={2016}, 
volume={28}, 
number={11}, 
pages={2958-2973}, 
abstract={This paper is concerned with the problem of personalized diversification of search results, with the goal of enhancing the performance of both plain diversification and plain personalization algorithms. In previous work, the problem has mainly been tackled by means of unsupervised learning. To further enhance the performance, we propose a supervised learning strategy. Specifically, we set up a structured learning framework for conducting supervised personalized diversification, in which we add features extracted directly from tokens of documents and those utilized by unsupervised personalized diversification algorithms, and, importantly, those generated from our proposed user-interest latent Dirichlet topic model. We also define two constraints in our structured learning framework to ensure that search results are both diversified and consistent with a user's interest. To further boost the efficiency of training, we propose a fast training framework for our proposed method by adding additional multiple highly violated but also diversified constraints at every training iteration of the cutting-plane algorithm. We conduct experiments on an open dataset and find that our supervised learning strategy outperforms unsupervised personalized diversification methods as well as other plain personalization and plain diversification methods. Our fast training framework significantly saves training time while it maintains almost the same performance.}, 
keywords={document handling;learning (artificial intelligence);optimisation;query processing;search engines;cutting-plane algorithm;document tokens;performance enhancement;plain diversification algorithm;plain personalization algorithm;structured learning;supervised personalized diversification;training framework;training time;unsupervised personalized diversification algorithm;user-interest latent Dirichlet topic model;Adaptation models;Feature extraction;Search problems;Standards;Supervised learning;Training;Web search;Personalization;ad hoc retrieval;diversity;structured SVMs}, 
doi={10.1109/TKDE.2016.2594064}, 
ISSN={1041-4347}, 
month={Nov},}
@INPROCEEDINGS{5452756, 
author={D. Thau and S. Bowers and B. Ludäscher}, 
booktitle={2010 IEEE 26th International Conference on Data Engineering Workshops (ICDEW 2010)}, 
title={Towards best-effort merge of taxonomically organized data}, 
year={2010}, 
pages={151-154}, 
abstract={We consider the task of merging datasets that have been organized using different, but aligned taxonomies. We assume such a merge is intended to create a single dataset that unambiguously describes the information in the source datasets using the alignment. We also assume that the merged result should reflect the observations of the datasets as specifically as possible. Typically, there will be no single merge result that is both unambiguous and maximally specific. In this case, a user may be provided with a set of possible merged datasets. If the user requires a single dataset, that dataset loses specificity. Here we examine whether the data exchange setting can provide a way to derive a ¿¿best-effort¿¿ merge. We find that the data exchange setting might be a good candidate for providing the merge, but further research is needed.}, 
keywords={electronic data interchange;best-effort merge;data exchange setting;taxonomically organized data;Biodiversity;Biological system modeling;Casting;Computer science;Databases;Environmental factors;Evolution (biology);Instruction sets;Merging;Taxonomy}, 
doi={10.1109/ICDEW.2010.5452756}, 
month={March},}
@INPROCEEDINGS{5591295, 
author={K. Farrahi and D. Gatica-Perez}, 
booktitle={2010 IEEE Second International Conference on Social Computing}, 
title={Mining Human Location-Routines Using a Multi-Level Approach to Topic Modeling}, 
year={2010}, 
pages={446-451}, 
abstract={In this work we address the problem of modeling varying time duration sequences for large-scale human routine discovery from cellphone sensor data using a multi-level approach to probabilistic topic models. We use an unsupervised learning approach that discovers human routines of varying durations ranging from half-hourly to several hours. Our methodology can handle large sequence lengths based on a principled procedure to deal with potentially large routine-vocabulary sizes, and can be applied to rather naive initial vocabularies to discover meaningful location-routines. We successfully apply the model to a large, real-life dataset, consisting of 97 cellphone users and 16 months of their location patterns, to discover routines with varying time durations.}, 
keywords={data mining;probability;social sciences computing;unsupervised learning;cellphone sensor data;data mining;human location-routines;multi-level approach;probabilistic topic models;unsupervised learning approach;Data models;Humans;Markov processes;Mobile handsets;Probabilistic logic;Visualization;Vocabulary}, 
doi={10.1109/SocialCom.2010.71}, 
month={Aug},}
@INPROCEEDINGS{6266230, 
author={K. Bamba and R. Ohbuchi}, 
booktitle={2012 IEEE International Conference on Multimedia and Expo Workshops}, 
title={Supervised, Geometry-Aware Segmentation of 3D Mesh Models}, 
year={2012}, 
pages={49-54}, 
abstract={Segmentation of 3D model models has applications, e.g., in mesh editing and 3D model retrieval. Unsupervised, automatic segmentation of 3D models can be useful. However, some applications require user-guided, interactive segmentation that captures user intention. This paper presents a supervised, local-geometry aware segmentation algorithm for 3D mesh models. The algorithm segments manifold meshes based on interactive guidance from users. The method casts user-guided mesh segmentation as a semi-supervised learning problem that propagates segmentation labels given to a subset of faces to the unlabeled faces of a 3D model. The proposed algorithm employs Zhou's Manifold Ranking [18] algorithm, which takes both local and global consistency in high-dimensional feature space for the label propagation. Evaluation using a 3D model segmentation benchmark dataset has shown that the method is effective, although achieving interactivity for a large and complex mesh requires some work.}, 
keywords={geometry;image segmentation;mesh generation;3D mesh models;3D model retrieval;Zhou manifold ranking;automatic segmentation;global consistency;high-dimensional feature space;interactive segmentation;label propagation;local consistency;local-geometry aware segmentation algorithm;mesh editing;semi supervised learning problem;user-guided;Computational efficiency;Computational modeling;Histograms;Image segmentation;Manifolds;Solid modeling;Vectors;Geometric modeling;computer graphics;diffusion distance;manifold ranking}, 
doi={10.1109/ICMEW.2012.16}, 
month={July},}
@INPROCEEDINGS{6569071, 
author={F. K. Chou and M. F. Chiang and W. C. Peng}, 
booktitle={2013 IEEE 14th International Conference on Mobile Data Management}, 
title={A Temporal Probabilistic Model for Dynamic Circle Recommendation in Mobile Applications}, 
year={2013}, 
volume={2}, 
pages={98-103}, 
abstract={This paper presents a novel framework for dynamic circle recommendation for a query user at a given time point from historical communication logs. We identify the fundamental factors that govern interactions and aim to automatically form friend circles for scenarios, such as, who should I share the photo with in the early morning? Whose post should be listed on top of my Facebook Wall feed at night? We develop a temporal probabilistic model that not only captures temporal tendencies between the query user and each friend candidate but also blends frequency and recency into circle formation. Experimental results on Enron dataset and Call Detail Records prove the effectiveness of dynamic circle formation with proposed temporal probabilistic model.}, 
keywords={mobile computing;query processing;recommender systems;social networking (online);temporal logic;Enron dataset;Facebook wall;call detail records;communication logs;dynamic circle recommendation;friend candidate;friend circles;mobile applications;query user;temporal probabilistic model;temporal tendencies;Entropy;Equations;Feeds;Mathematical model;Postal services;Probabilistic logic;Receivers;Dynamic Circle;Mobile Social Network;Time-Dependency}, 
doi={10.1109/MDM.2013.75}, 
ISSN={1551-6245}, 
month={June},}
@INPROCEEDINGS{6970182, 
author={C. M. Gonano and F. Tomasi and F. Mambelli and F. Vitali and S. Peroni}, 
booktitle={IEEE/ACM Joint Conference on Digital Libraries}, 
title={Zeri e LODE. Extracting the Zeri photo archive to linked open data: formalizing the conceptual model}, 
year={2014}, 
pages={289-298}, 
abstract={This paper presents the first steps of a project to convert the notable Italian “Zeri photo archive” to a linked and open dataset. The full project entails the analysis of the records' description model (Scheda F) in order to define a suitable ontology by exploring existing data models, the creation of the RDF triple store, the creation of links to the cloud, and the definition of the user interface for browsing the linked open dataset. This paper presents and discusses the conceptual modeling of the data stored in the Zeri archival database.}, 
keywords={information retrieval systems;ontologies (artificial intelligence);user interfaces;visual databases;Zeri archival database;Zeri e LODE;Zeri photo archive;browsing;description model;linked open data;ontology;user interface;Art;Cultural differences;Data models;Databases;Libraries;Ontologies;Standards;CIDOC-CRM;FABIO;FEO;FRBR;OWL 2 DL;PROV-O;RDF;Scheda F}, 
doi={10.1109/JCDL.2014.6970182}, 
month={Sept},}
@INPROCEEDINGS{6011843, 
author={P. Sinha and R. Jain}, 
booktitle={2011 IEEE International Conference on Multimedia and Expo}, 
title={Extractive summarization of personal photos from life events}, 
year={2011}, 
pages={1-6}, 
abstract={Manually sifting through large collections of personal photos shot at various life events is both tedious and inefficient. In this paper, we propose a photo summarization system which creates a representative subset summary by extracting photos from a larger set shot at an event (e.g., in a trip, birthday, etc). We define three properties that are necessary to generate an effective summary: relevance, diversity and coverage. We propose methods to compute them using multimodal content and context data. The objective for automatic photo summarization is formulated as an optimization of these properties. We discuss algorithms that solve the problem efficiently. A dataset of 7,700 photos from personal life events is created with user-generated ground truth summary. We also propose objective metrics to evaluate summaries automatically. Evaluations using both objective metrics and user feedback show our models can generate summaries which are much better than baselines.}, 
doi={10.1109/ICME.2011.6011843}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{7004444, 
author={T. Wu and S. H. Yu and W. Liao and C. S. Chang}, 
booktitle={2014 IEEE International Conference on Big Data (Big Data)}, 
title={Temporal bipartite projection and link prediction for online social networks}, 
year={2014}, 
pages={52-59}, 
abstract={In user-item networks, the link prediction problem has received considerable attentions and has many applications (e.g., recommender systems, ranking item popularity) in recent years. Many previous works commonly fail to utilize the dynamic nature of the networks. This paper focuses on dealing with the temporal information and proposes an algorithm to cope with the link prediction problem on bipartite networks. We describe a temporal bipartite projection method that yields a projected item graph, called the temporal projection graph (TPG). Based on the TPG, we propose a scoring function called STEP (Score for TEmporal Prediction) for each user-item pair. STEP leverages the historical behaviors of individual users and the social aggregated behaviors learned from the TPG for the link prediction problem. Furthermore, we use TPG and PageRank to rank the popularity of items. To validate our algorithms, we perform various experiments by using the DBLP author-conference dataset, the Flickr dataset and the Delicious dataset. We show that our results of the link prediction problem for new links are substantially better than other temporal link prediction algorithms. We also find the item rankings generated by our approach match very well with that existed in the real world.}, 
keywords={graph theory;information retrieval;social networking (online);DBLP author-conference dataset;Delicious dataset;Flickr dataset;PageRank;STEP;TPG;bipartite network;link prediction;online social network;projected item graph;score for temporal prediction;scoring function;temporal bipartite projection;temporal projection graph;user-item network;Computational complexity;Computational modeling;Educational institutions;History;Social network services;Training;PageRank;bipartite network;bipartite network projection;link prediction}, 
doi={10.1109/BigData.2014.7004444}, 
month={Oct},}
@INPROCEEDINGS{6065415, 
author={R. Jain and D. Doermann}, 
booktitle={2011 International Conference on Document Analysis and Recognition}, 
title={Offline Writer Identification Using K-Adjacent Segments}, 
year={2011}, 
pages={769-773}, 
abstract={This paper presents a method for performing offline writer identification by using K-adjacent segment (KAS) features in a bag-of-features framework to model a user's handwriting. This approach achieves a top 1 recognition rate of 93% on the benchmark IAM English handwriting dataset, which outperforms current state of the art features. Results further demonstrate that identification performance improves as the number of training samples increase, and additionally, that the performance of the KAS features extend to Arabic handwriting found in the MADCAT dataset.}, 
keywords={document image processing;handwritten character recognition;natural language processing;Arabic handwriting;IAM English handwriting dataset;K-adjacent segment;KAS features;MADCAT dataset;bag-of-features framework;offline writer identification;user handwriting;Accuracy;Feature extraction;Hidden Markov models;Image segmentation;Testing;Training;Vectors;Codebook;Document Forensics;Handwriting;K-Adjacent Segments;Local Features;Writer Identification}, 
doi={10.1109/ICDAR.2011.159}, 
ISSN={1520-5363}, 
month={Sept},}
@INPROCEEDINGS{7866117, 
author={Y. Yang and F. Wang and F. Jiang and S. Jin and J. Xu}, 
booktitle={2016 IEEE First International Conference on Data Science in Cyberspace (DSC)}, 
title={A Topic Model for Hierarchical Documents}, 
year={2016}, 
pages={118-126}, 
abstract={Uncovering the topics over short text corpus has become increasingly important with the bursty development of online communications. However, conventional topic mining methods may fail due to the lack of contexts in each of the short text. Fortunately, a large proportion of online short texts often co-occur with lengthy texts, such as reviews with product descriptions and comments with news articles. These two kinds of texts are hierarchically organized and the hidden topical relationships between them can be utilized to enhance topic learning for both sides. Therefore, in this paper, we propose a topic model for (h)ierarchical (d)ocuments, referred as hdLDA, to capture the hierarchical structure of these texts. Specifically, in hdLDA each short text has a probability distribution over two topics, one from a set of topics underlying lengthy texts and the other one from a topic set formed only by short texts. Through this assumption, the topics of short texts and lengthy documents in hdLDA are learned in a mutually reinforced way. Extensive experiments on a dataset of news articles and user comments demonstrate that our approach discovers more prominent and comprehensive topics for both short texts and lengthy documents, compared with baseline and state-of-art methods.}, 
keywords={probability;text analysis;hdLDA;hidden topical relationships;hierarchical documents;online short text corpus;probability distribution;topic model;Analytical models;Blogs;Context;Context modeling;Data models;Probability distribution;Semantics;Hierarchical Documents;Short Texts;Topic Model}, 
doi={10.1109/DSC.2016.97}, 
month={June},}
@INPROCEEDINGS{7396788, 
author={Z. Meng and F. Gandon and C. F. Zucker}, 
booktitle={2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
title={Simplified Detection and Labeling of Overlapping Communities of Interest in Question-and-Answer Sites}, 
year={2015}, 
volume={1}, 
pages={107-114}, 
abstract={In many social networks, people interact based on their interests. Community detection algorithms are then useful to reveal the sub-structures of a network and in particular interest groups. Identifying these users' communities and the interests that bind them can help us assist their life-cycle. Certain kinds of online communities such as question-and-answer (Q&A) sites or forums, have no explicit social network structure. Therefore, many traditional community detection techniques do not apply directly. In this paper, we propose TTD (Topic Trees Distributions) an efficient approach for extracting topic from Q&A sites in order to detect communities of interest. Then we compare three detection methods we applied on a dataset extracted from the popular Q&A site StackOverflow. Our method based on topic modeling and user membership assignment is shown to be much simpler and faster while preserving the quality of the detection.}, 
keywords={question answering (information retrieval);social networking (online);Q and A sites;StackOverflow;TTD;online communities;overlapping community of interest detection;overlapping community of interest labeling;question-and-answer sites;social network structure;topic extraction;topic modeling;topic trees distributions;user membership assignment;Cascading style sheets;Clustering algorithms;Computational modeling;Detection algorithms;HTML;Layout;Social network services}, 
doi={10.1109/WI-IAT.2015.184}, 
month={Dec},}
@INPROCEEDINGS{6425648, 
author={N. Dokoohaki and M. Matskin}, 
booktitle={2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining}, 
title={Mining Divergent Opinion Trust Networks through Latent Dirichlet Allocation}, 
year={2012}, 
pages={879-886}, 
abstract={While the focus of trust research has been mainly on defining and modeling various notions of social trust, less attention has been given to modeling opinion trust. When speaking of social trust mainly homophily (similarity) has been the most successful metric for learning trustworthy links, specially in social web applications such as collaborative filtering recommendation systems. While pure homophily such as Pearson coefficient correlation and its variations, have been favorable to finding taste distances between individuals based on their rated items, they are not necessarily useful in finding opinion distances between individuals discussing a trending topic, e.g. Arab spring. At the same time text mining techniques, such as vector-based techniques, are not capable of capturing important factors such as saliency or polarity which are possible with topical models for detecting, analyzing and suggesting aspects of people mentioning those tags or topics. Thus, in this paper we are proposing to model opinion distances using probabilistic information divergence as a metric for measuring the distances between people's opinion contributing to a discussion in a social network. To acquire feature sets from topics discussed in a discussion we use a very successful topic modeling technique, namely Latent Dirichlet Allocation (LDA). We use the distributions resulting to model topics for generating social networks of group and individual users. Using a Twitter dataset we show that learned graphs exhibit properties of real-world like networks.}, 
keywords={collaborative filtering;data mining;distance measurement;probability;recommender systems;social networking (online);trusted computing;Pearson coefficient correlation;Twitter;collaborative filtering recommendation system;distance measurement;divergent opinion trust network mining;latent Dirichlet allocation;probabilistic information divergence;social Web application;social network;social trust;topic modeling technique;trustworthy link;Analytical models;Biological system modeling;Computational modeling;Measurement;Probabilistic logic;Twitter;LDA;opinion mining;topic models;trust network;twitter}, 
doi={10.1109/ASONAM.2012.158}, 
month={Aug},}
@INPROCEEDINGS{6626049, 
author={J. Zheng and Z. Zhang and B. Ciepłuch and A. C. Winstanley and P. Mooney and R. Jacob}, 
booktitle={2013 21st International Conference on Geoinformatics}, 
title={A PostGIS-based pedestrian way finding module using OpenStreetMap data}, 
year={2013}, 
pages={1-5}, 
abstract={Open source GIS (OSG) is a fast developing field. When OSG is combined with Web2.0 and Service Orientated Architectures (SOA) technologies and more applications of Public Participation GIS, it has many advantages over commercial GIS software. Despite this, OSG still needs more improvement in terms of stability and functional integrity. In order to build more robust, more practical, and more functional LBS applications, this research investigates pedestrian-orientated wayfinding, with special requirements as its study topic. We describe some Web 2.0 routing APIs which can be easily used to provide general shortest path planning. However, these APIs cannot provide guidance services for specific user groups with special requirements, such as tourists in small towns. We take Maynooth as case-study. Maynooth is the only University town in Ireland with a population of approximately 20,000. This research uses OpenStreetMap (OSM) as spatial data source. OSM contains very spatially rich dataset. It is stored and managed in PostGIS/PostgreSQL. Through previous work on LBS applications using the CloudMade Routing API and OSM data, we present a Java-based wayfinding module implementing a restricted area version of Dijkstra algorithm. A set of native PostGIS spatial functions are used to improve performance of the routing algorithm. Results from our wayfinding algorithm are presented and compared with those obtained by using the CloudMade Routing API. Our results are promising and show that this special version of Dijkstra algorithm can take advantage of the spatial data stored in OSM. This work provides a base to build more effective pedestrian wayfinding algorithms which can be implemented in open source software and open APIs. This approach provides a feasible and economical LBS solution for small towns, villages and tourism regions outside larger cities.}, 
keywords={Internet;Java;application program interfaces;geographic information systems;path planning;pedestrians;public domain software;service-oriented architecture;visual databases;Dijkstra algorithm;Ireland;Java-based wayfinding module;LBS applications;Maynooth;OSG;OSM data;OpenStreetMap data;PostGIS spatial functions;PostGIS-based pedestrian wayfinding module;PostGIS/PostgreSQL;SOA;University town;Web 2.0;functional integrity;open API;open source GIS;open source software;performance improvement;routing algorithm;service orientated architectures;shortest path planning;spatial data source;stability;Buildings;Geometry;Navigation;Path planning;Roads;Routing;Software;CloudMade;OpenStreetMap(OSM);Pedestrian;PostGIS;Wayfinding}, 
doi={10.1109/Geoinformatics.2013.6626049}, 
ISSN={2161-024X}, 
month={June},}
@INPROCEEDINGS{5593340, 
author={H. P. Martínez and K. Hullett and G. N. Yannakakis}, 
booktitle={Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games}, 
title={Extending neuro-evolutionary preference learning through player modeling}, 
year={2010}, 
pages={313-320}, 
abstract={In this paper we propose a methodology for improving the accuracy of models that predict self-reported player pairwise preferences. Our approach extends neuro-evolutionary preference learning by embedding a player modeling module for the prediction of player preferences. Player types are identified using self-organization and feed the preference learner. Our experiments on a dataset derived from a game survey of subjects playing a 3D prey/predator game demonstrate that the player model-driven preference learning approach proposed improves the performance of preference learning significantly and shows promise for the construction of more accurate cognitive and affective models.}, 
keywords={behavioural sciences computing;cognitive systems;computer games;learning (artificial intelligence);neural nets;user interfaces;3D prey-predator game;affective model;cognitive model;neuroevolutionary preference learning;player model-driven preference learning;player modeling;self-organization;self-reported player pairwise preference;Accuracy;Adaptation model;Computational modeling;Feature extraction;Games;Neurons;Predictive models}, 
doi={10.1109/ITW.2010.5593340}, 
ISSN={2325-4270}, 
month={Aug},}
@INPROCEEDINGS{7603395, 
author={T. Yue and A. M. Wei and H. B. Wang and X. D. Deng and S. D. Cheng}, 
booktitle={2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
title={A comprehensive data-driven approach to evaluating quality of experience on large-scale internet video service}, 
year={2016}, 
pages={1479-1486}, 
abstract={Internet video is currently one of the most popular Internet services, accounting for more than 50% of the overall Internet traffic. Various access patterns of users worldwide contribute to increasing complex scenarios of Internet video. Under the circumstances, there emerges a valuable yet challenging topic that industry and academia keep focusing on: how to evaluate quality of experience (QoE) of Internet video in the wild? In this paper, we propose a comprehensive data-driven approach to evaluate QoE based on massive dataset extracted from a large-scale Internet video service provider. With the methods of feature engineering and fuzzy theory, we exploit user's situational features and session's quality features. Then we conceive an appropriate QoE evaluating model called bagging-based Bayesian factorization machine to correlate the aforementioned features with user's QoE. The experimental results demonstrate that our approach is adaptive for QoE evaluation on Internet video both in efficiency and effectiveness. Moreover, our approach achieves higher degree of accuracy compared with baseline methods, including what associated works present.}, 
keywords={Bayes methods;Internet;fuzzy set theory;matrix decomposition;quality of experience;telecommunication services;telecommunication traffic;Internet traffic;QoE;bagging-based Bayesian factorization machine;comprehensive data-driven approach;fuzzy theory;large-scale Internet video service;quality of experience;Bayes methods;Bit rate;Feature extraction;Internet;Quality of service;Streaming media;Bayesian factorization machine;Internet video;data-driven;feature engineering;massive data;quality of experience}, 
doi={10.1109/FSKD.2016.7603395}, 
month={Aug},}
@INPROCEEDINGS{7727511, 
author={S. A. F. Soares and G. S. Neto and M. Roisenberg}, 
booktitle={2016 International Joint Conference on Neural Networks (IJCNN)}, 
title={Improving the Incremental Gaussian Mixture Neural Network model for spatial interpolation and geostatistical simulation}, 
year={2016}, 
pages={2507-2514}, 
abstract={An important feature present in neural network models is their ability to learn from data, even when the user does not have much information about the particular dataset. However, the most popular models do not perform well in spatial interpolation problems due to their difficulty in accurately modeling spatial correlation between samples. On the other hand, one of the most important geostatistical methods for spatial interpolation, Kriging, performs very well but requires some expert knowledge to fit the correlation model (semivariogram). In this work, we adapt the Incremental Gaussian Mixture Network (IGMN) neural network model for spatial interpolation and geostatistical sequential simulation applications. Results show that our approach outperforms Multilayer Perceptron (MLP) and the original IGMN, especially in anisotropic and sparse datasets. Also, we propose an algorithm for Sequential Gaussian Simulation that uses IGMN instead of Kriging and can successfully generate equally probable realizations of the defined grid. To the best of our knowledge, this is the first time a Neural Network model is specialized for spatial interpolation applications and has the ability to perform a geostatistical simulation.}, 
keywords={Gaussian processes;interpolation;mixture models;neural nets;statistical analysis;IGMN neural network model;anisotropic datasets;correlation model;geostatistical methods;geostatistical sequential simulation;incremental Gaussian mixture neural network model;kriging;semivariogram;sequential Gaussian simulation;sparse datasets;spatial correlation;spatial interpolation;Adaptation models;Covariance matrices;Data models;Interpolation;Market research;Mathematical model;Neural networks}, 
doi={10.1109/IJCNN.2016.7727511}, 
month={July},}
@INPROCEEDINGS{6838654, 
author={M. L. Brocardo and I. Traore and I. Woungang}, 
booktitle={2014 IEEE 28th International Conference on Advanced Information Networking and Applications}, 
title={Toward a Framework for Continuous Authentication Using Stylometry}, 
year={2014}, 
pages={106-115}, 
abstract={Continuous Authentication (CA) consists of monitoring and checking repeatedly and unobtrusively user behavior during a computing session in order to discriminate between legitimate and impostor behaviors. Stylometry analysis, which consists of checking whether a target document was written or not by a specific individual, could potentially be used for CA. In this work, we adapt existing stylometric features and develop a new authorship verification model applicable for continuous authentication. We use existing lexical, syntactic, and application specific features, and propose new features based on n-gram analysis. We start initially with a large features set, and identify a reduced number of user-specific features by computing the information gain. In addition, our approach includes a strategy to circumvent issues regarding unbalanced dataset which is an inherent problem in stylometry analysis. We use Support Vector Machine (SVM) for classification. Experimental evaluation based on the Enron email dataset involving 76 authors yields very promising results consisting of an Equal Error Rate (EER) of 12.42% for message blocks of 500 characters.}, 
keywords={authorisation;document handling;support vector machines;CA;EER;Enron email dataset;SVM;authorship verification model;computing session;continuous authentication;equal error rate;impostor behaviors;legitimate behaviors;n-gram analysis;stylometry analysis;support vector machine;target document;user-specific features;Authentication;Electronic mail;Feature extraction;Forensics;Support vector machines;Syntactics;Training;Continuous authentication;authorship verification;biometrics systems;classification;n-gram features;security;short message verification;stylometry;text mining;writeprint}, 
doi={10.1109/AINA.2014.18}, 
ISSN={1550-445X}, 
month={May},}
@ARTICLE{7864468, 
author={L. Micallef and G. Palmas and A. Oulasvirta and T. Weinkauf}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Towards Perceptual Optimization of the Visual Design of Scatterplots}, 
year={2017}, 
volume={23}, 
number={6}, 
pages={1588-1599}, 
abstract={Designing a good scatterplot can be difficult for non-experts in visualization, because they need to decide on many parameters, such as marker size and opacity, aspect ratio, color, and rendering order. This paper contributes to research exploring the use of perceptual models and quality metrics to set such parameters automatically for enhanced visual quality of a scatterplot. A key consideration in this paper is the construction of a cost function to capture several relevant aspects of the human visual system, examining a scatterplot design for some data analysis task. We show how the cost function can be used in an optimizer to search for the optimal visual design for a user's dataset and task objectives (e.g., “reliable linear correlation estimation is more important than class separation”). The approach is extensible to different analysis tasks. To test its performance in a realistic setting, we pre-calibrated it for correlation estimation, class separation, and outlier detection. The optimizer was able to produce designs that achieved a level of speed and success comparable to that of those using human-designed presets (e.g., in R or MATLAB). Case studies demonstrate that the approach can adapt a design to the data, to reveal patterns without user intervention.}, 
keywords={crowdsourcing;data visualisation;Matlab;R;class separation;cost function;data analysis task;human visual system;human-designed presets;linear correlation estimation;optimal visual design;outlier detection;perceptual optimization;quality metrics;task objectives;user dataset;visual quality enhancement;visual scatterplot design;Correlation;Cost function;Data analysis;Data visualization;Measurement;Visualization;Scatterplot;crowdsourcing;optimization;perception}, 
doi={10.1109/TVCG.2017.2674978}, 
ISSN={1077-2626}, 
month={June},}
@INPROCEEDINGS{6248023, 
author={F. X. Yu and R. Ji and M. H. Tsai and G. Ye and S. F. Chang}, 
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Weak attributes for large-scale image retrieval}, 
year={2012}, 
pages={2949-2956}, 
abstract={Attribute-based query offers an intuitive way of image retrieval, in which users can describe the intended search targets with understandable attributes. In this paper, we develop a general and powerful framework to solve this problem by leveraging a large pool of weak attributes comprised of automatic classifier scores or other mid-level representations that can be easily acquired with little or no human labor. We extend the existing retrieval model of modeling dependency within query attributes to modeling dependency of query attributes on a large pool of weak attributes, which is more expressive and scalable. To efficiently learn such a large dependency model without overfitting, we further propose a semi-supervised graphical model to map each multiattribute query to a subset of weak attributes. Through extensive experiments over several attribute benchmarks, we demonstrate consistent and significant performance improvements over the state-of-the-art techniques. In addition, we compile the largest multi-attribute image retrieval dateset to date, including 126 fully labeled query attributes and 6,000 weak attributes of 0.26 million images.}, 
keywords={image classification;image representation;image retrieval;attribute-based querying;automatic classifier score;large-scale image retrieval;mid-level representation;multiattribute image retrieval dataset;multiattribute query;semisupervised graphical model;weak attribute;Equations;Graphical models;Humans;Image retrieval;Mathematical model;Training;Yttrium}, 
doi={10.1109/CVPR.2012.6248023}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{1521500, 
author={M. Xu and L. T. Chia and J. Jin}, 
booktitle={2005 IEEE International Conference on Multimedia and Expo}, 
title={Affective content analysis in comedy and horror videos by audio emotional event detection}, 
year={2005}, 
pages={4 pp.-}, 
abstract={We study the problem of affective content analysis. In this paper, we think of affective contents as those video/audio segments, which may cause an audience's strong reactions or special emotional experiences, such as laughing or fear. Those emotional factors are related to the users' attention, evaluation, and memories of the content. The modeling of affective effects depends on the video genres. In this work, we focus on comedy and horror films to extract the affective content by detecting a set of so-called audio emotional events (AEE) such as laughing, horror sounds, etc. Those AEE can be modeled by various audio processing techniques, and they can directly reflect an audience's emotion. We use the AEE as a clue to locate corresponding video segments. Domain knowledge is more or less employed at this stage. Our experimental dataset consists of 40-minutes comedy video and 40-minutes horror film. An average recall and precision of above 90% is achieved. It is shown that, in addition to rich visual information, an appropriate usage of special audios is an effective way to assist affective content analysis.}, 
keywords={audio signal processing;emotion recognition;image segmentation;video signal processing;AEE;affective content analysis;audio emotional event detection;audio processing technique;comedy video;domain knowledge;horror video;video-audio segmentation;visual information;Design engineering;Event detection;Hidden Markov models;Indexing;Information analysis;Information technology;Motion pictures;Multimedia databases;Optical films;Videos;Affective content;Audio emotional event}, 
doi={10.1109/ICME.2005.1521500}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{7218619, 
author={F. M. F. Wong and Z. Liu and M. Chiang}, 
booktitle={2015 IEEE Conference on Computer Communications (INFOCOM)}, 
title={On the efficiency of social recommender networks}, 
year={2015}, 
pages={2317-2325}, 
abstract={We study a fundamental question that arises in social recommender systems: whether it is possible to simultaneously maximize (a) an individual's benefit from using a social network and (b) the efficiency of the network in disseminating information. To tackle this question, our study consists of three components. First, we introduce a stylized stochastic model for recommendation diffusion. Such a model allows us to highlight the connection between user experience at the individual level, and network efficiency at the macroscopic level. We also propose a set of metrics for quantifying both user experience and network efficiency. Second, based on these metrics, we extensively study the tradeoff between the two factors in a Yelp dataset, concluding that Yelp's social network is surprisingly efficient, though not optimal. Finally, we design a friend recommendation and news feed curation algorithm that can simultaneously address individuals' need to connect to high quality friends, and service providers' need to maximize network efficiency in information propagation.}, 
keywords={recommender systems;social networking (online);Yelp dataset;Yelp social network;friend recommendation;high-quality friends;information dissemination;information propagation;macroscopic level;network efficiency;network efficiency maximization;news feed curation algorithm;recommendation diffusion;service providers;social recommender networks;stylized stochastic model;user experience;Business;Computational modeling;Eigenvalues and eigenfunctions;Measurement;Recommender systems;Social network services;Stochastic processes}, 
doi={10.1109/INFOCOM.2015.7218619}, 
ISSN={0743-166X}, 
month={April},}
@INPROCEEDINGS{1048261, 
author={A. Franco and A. Lumini and D. Maio}, 
booktitle={Object recognition supported by user interaction for service robots}, 
title={Eigenspace merging for model updating}, 
year={2002}, 
volume={2}, 
pages={156-159 vol.2}, 
abstract={The Karhunen-Loeve transform (KLT) is an optimal method for dimensionality reduction, widely applied in image compression, reconstruction and retrieval, pattern recognition and classification. The basic idea consists in evaluating, starting from a set of representative examples, a reduced space, which takes into account the structure of the data distribution as much as possible, and representing each element in such an uncorrelated space. Unfortunately, KLT has the drawback of requiring a periodical recomputation in presence of a dynamic dataset. This work presents a novel efficient approach to merge multiple eigenspaces, which provides an incremental method to compute an eigenspace model by successively adding new sets of elements. Experimental results show that the merged model grants performances as good as a one obtained by a batch procedure.}, 
keywords={Karhunen-Loeve transforms;eigenvalues and eigenfunctions;image retrieval;pattern classification;Karhunen-Loeve transform;approximation errors;dimensionality reduction;dynamic databases;eigenspace merging;image compression;image reconstruction;image retrieval;model updating;pattern classification;two-space merging;Covariance matrix;Eigenvalues and eigenfunctions;Electronic mail;Feature extraction;Image coding;Image retrieval;Indexing;Karhunen-Loeve transforms;Merging;Pattern recognition}, 
doi={10.1109/ICPR.2002.1048261}, 
ISSN={1051-4651}, 
month={},}
@INPROCEEDINGS{7409480, 
author={J. Peng and Y. Zhai and J. Qiu}, 
booktitle={2015 7th International Conference on Modelling, Identification and Control (ICMIC)}, 
title={Learning latent factor from review text and rating for recommendation}, 
year={2015}, 
pages={1-6}, 
abstract={In this paper, we propose a model to recommend related products to users. Our model combines the metrits of latent factor model and probabilistic topic model such as latent Dirichlet allocation(LDA), aiming to learn latent user factors from observed reviews rating and latent items factors from reviews text. It provides an interpretable latent factor for users and items. Experiments on a realworld dataset show that our model outperform state-of-the-art methods on the task of recommender system.}, 
keywords={learning (artificial intelligence);probability;recommender systems;text analysis;LDA;latent Dirichlet allocation;latent factor
@ARTICLE{7903688, 
author={M. Kren and A. Kos and Y. Zhang and A. Kos and U. Sedlar}, 
journal={IEEE Transactions on Industrial Informatics}, 
title={Public Interest Analysis Based on Implicit Feedback of IPTV Users}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Modern information systems make it increasingly easy to gain more insight into the public interest, which is becoming more and more important in diverse public and corporate activities and processes. The disadvantage of existing research that focuses on mining the information from social networks and online communities is that it doesn’t uniformly represent all population groups and that the content can be subjected to self-censoring or curation. In this paper we propose and describe a framework and a method for estimating public interest from the implicit negative feedback collected from the IPTV audience. Our research focuses primarily on the channel change events and their match with the content information obtained from closed captions. The presented framework is based on concept modeling, viewership profiling, and combines the implicit viewer reactions (channel changes) into an interest score. The proposed framework addresses both above mentioned disadvantages or concerns. It is able to cover a much broader population, and it can detect even minor variations in user behavior. We demonstrate our approach on a large pseudonymized real-world IPTV dataset provided by an ISP, and show how the results correlate with different trending topics and with parallel classical long-term population surveys.}, 
keywords={Context;Data mining;IPTV;Informatics;Sociology;Statistics;IPTV;Implicit user feedback;public interest estimation;viewership profiling}, 
doi={10.1109/TII.2017.2695371}, 
ISSN={1551-3203}, 
month={},}
@INPROCEEDINGS{6418834, 
author={Y. Gao and C. Zhang}, 
booktitle={2012 3rd IEEE International Conference on Network Infrastructure and Digital Content}, 
title={A session-oriented retrieval model based on Markov random field}, 
year={2012}, 
pages={641-645}, 
abstract={In this paper, we study how to use the search session information to improve the retrieval accuracy. We propose a session-oriented retrieval model based on Markov random field. This model introduces the correlations between query terms as a retrieval factor into the retrieval process. It also presents a dynamic update algorithm based on the analysis of users' search behavior. Our model implements a complete session-oriented information retrieval framework finally. We use ClueWeb09 category B dataset and TREC 2010 (2011) Session dataset to quantitatively evaluate the model. Experimental results show that our model can improve retrieval performance substantially using the search session information.}, 
keywords={Markov processes;information retrieval;ClueWeb09 category B dataset;Markov random field;TREC 2010;complete session-oriented information retrieval framework;dynamic update algorithm;retrieval accuracy;retrieval performance;retrieval process;search session information;session-oriented retrieval model;Accuracy;Analytical models;Helium;Information retrieval;Joints;Markov random fields;Mathematical model;Implicit feedback;Information retrieval;Markov random field;Search session;Term dependence}, 
doi={10.1109/ICNIDC.2012.6418834}, 
ISSN={2374-0272}, 
month={Sept},}
@INPROCEEDINGS{4215817, 
author={R. Birke and M. Mellia and M. Petracca and D. Rossi}, 
booktitle={IEEE INFOCOM 2007 - 26th IEEE International Conference on Computer Communications}, 
title={Understanding VoIP from Backbone Measurements}, 
year={2007}, 
pages={2027-2035}, 
abstract={VoIP has widely been addressed as the technology that will change the Telecommunication model opening the path for convergence. Still today this revolution is far from being complete, since the majority of telephone calls are originated by circuit-oriented networks. In this paper for the first time to the best of our knowledge, we present a large dataset of measurements collected from the FastWeb backbone, which is one of the first worldwide Telecom operator to offer VoIP and high-speed data access to the end-user. Traffic characterization will focus on several layers, focusing on both end-user and ISP perspective. In particular, we highlight that, among loss, delay and jitter, only the first index may affect the VoIP call quality. Results show that the technology is mature to make the final step, allowing the integration of data and real-time services over the Internet.}, 
keywords={Internet telephony;telecommunication traffic;FastWeb backbone;Internet;VoIP call quality;VoIP traffic characterization;high-speed data access;telecommunication model;Circuits;IP networks;Internet telephony;Jitter;Local area networks;Monitoring;Spine;Telecommunication traffic;Time measurement;Web and internet services}, 
doi={10.1109/INFCOM.2007.235}, 
ISSN={0743-166X}, 
month={May},}
@INPROCEEDINGS{6778302, 
author={A. Zamberletti and I. Gallo and S. Albertini}, 
booktitle={2013 2nd IAPR Asian Conference on Pattern Recognition}, 
title={Robust Angle Invariant 1D Barcode Detection}, 
year={2013}, 
pages={160-164}, 
abstract={Barcode reading mobile applications that identify products from pictures taken using mobile devices are widely used by customers to perform online price comparisons or to access reviews written by others. Most of the currently available barcode reading approaches focus on decoding degraded barcodes and treat the underlying barcode detection task as a side problem that can be addressed using appropriate object detection methods. However, the majority of modern mobile devices do not meet the minimum working requirements of complex general purpose object detection algorithms and most of the efficient specifically designed barcode detection algorithms require user interaction to work properly. In this paper, we present a novel method for barcode detection in camera captured images based on a supervised machine learning algorithm that identifies one-dimensional barcodes in the two-dimensional Hough Transform space. Our model is angle invariant, requires no user interaction and can be executed on a modern mobile device. It achieves excellent results for two standard one-dimensional barcode datasets: WWU Muenster Barcode Database and ArTe-Lab 1D Medium Barcode Dataset. Moreover, we prove that it is possible to enhance the overall performance of a state-of-the-art barcode reading algorithm by combining it with our detection method.}, 
keywords={Hough transforms;bar codes;cameras;image coding;learning (artificial intelligence);mobile computing;object detection;1D barcode identification;ArTe-Lab 1D medium barcode dataset;WWU Muenster barcode database;barcode detection task;barcode reading algorithm;barcode reading approaches;barcode reading mobile applications;camera captured images;degraded barcode decoding;mobile devices;object detection algorithms;product identification;robust angle invariant 1D barcode detection;supervised machine learning algorithm;two-dimensional Hough transform space;Accuracy;Cameras;Databases;Decoding;Image edge detection;Mobile handsets;Transforms;1D Barcode;Hough Transform;Object Detection}, 
doi={10.1109/ACPR.2013.17}, 
ISSN={0730-6512}, 
month={Nov},}
@INPROCEEDINGS{7838123, 
author={M. Limmer and H. P. A. Lensch}, 
booktitle={2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
title={Infrared Colorization Using Deep Convolutional Neural Networks}, 
year={2016}, 
pages={61-68}, 
abstract={This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which ensures a perfect pixel to pixel registration.}, 
keywords={CCD image sensors;feedforward neural nets;image colour analysis;image registration;image resolution;infrared imaging;roads;traffic engineering computing;NIR pixels;RGB color spectrum;RGB pixels;advanced driver assistance systems;deep multiscale convolutional neural networks;infrared colorization;multi CCD NIR camera;multi CCD RGB camera;near-infrared images;pixel-to-pixel registration;real-world dataset;road scene images;Cameras;Convolution;Feature extraction;Gray-scale;Image color analysis;Neural networks;Roads;Bilateral Filter;CNN;Colorization;DNN;Multi-scale;NIR;Near Infrared;Neural Networks}, 
doi={10.1109/ICMLA.2016.0019}, 
month={Dec},}
@INPROCEEDINGS{7264381, 
author={B. H. Huang and B. R. Dai}, 
booktitle={2015 16th IEEE International Conference on Mobile Data Management}, 
title={A Weighted Distance Similarity Model to Improve the Accuracy of Collaborative Recommender System}, 
year={2015}, 
volume={2}, 
pages={104-109}, 
abstract={Collaborative filtering is one of the most widely used methods to provide product recommendation in online stores. The key component of the method is to find similar users or items by using user-item matrix so that products can be recommended based on the similarities. However, traditional collaborative filtering approaches compute the similarity between a target user and the other user without considering a target item. More specifically, they give an equal weight to each of the items which are rated by both users. However, we think that the similarity between the target item and each of the co-rated items is a very important factor when we calculate the similarity between two users. Therefore, in this paper we propose a new similarity function that takes similarities between a target item and each of the co-rated items and the proportion of common ratings into account. Experimental results from Movie Lens dataset show that the method improves accuracy of recommender system significantly.}, 
keywords={collaborative filtering;matrix algebra;recommender systems;Movie Lens dataset;co rated items;collaborative filtering;collaborative recommender system accuracy improvement;common rating proportion;online stores;product recommendation;similarity function;target item;user-item matrix;weighted distance similarity model;Accuracy;Collaboration;Computational modeling;Predictive models;Recommender systems;Social network services;Collaborative filtering;Recommendation system;Similarity measure}, 
doi={10.1109/MDM.2015.43}, 
ISSN={1551-6245}, 
month={June},}
@INPROCEEDINGS{6765503, 
author={Z. Ma and X. Shu and G. Yan}, 
booktitle={2013 International Joint Conference on Awareness Science and Technology Ubi-Media Computing (iCAST 2013 UMEDIA 2013)}, 
title={An improved algorithm on micro-blog community detection}, 
year={2013}, 
pages={563-568}, 
abstract={Micro-blogging is becoming increasingly prevalent in global world, which not only subverts the traditional means of communication, but also changes the entire media environment. Discovery of micro-blog community is of great value. However, the classical community discovery algorithms are generally based on links or interests only to recognize the traditional single community and limited to detect micro-blog communities effectively. Sometimes interaction among users is characterized by user's social information, but it's difficult to obtain in micro-blog. This work introduces the social network model based on the new social-networking characteristic called "following" which is employed in micro-blog. Based on the label propagation algorithm, we adopt the users' relationship, which was defined as the interaction from topic and hyperlink relations, and propose a micro-blog label propagation algorithm to detect communities. The experiment results on a real-world micro-blog dataset illustrate the reasonable and effective of our method. Experiment results over a real-world micro-blog data set illustrate the effectiveness and efficiency provided by our approach.}, 
keywords={social networking (online);classical community discovery algorithms;hyperlink relations;label propagation algorithm;micro-blog community detection;micro-blog label propagation algorithm;social network model;topic relations;Algorithm design and analysis;Blogs;Clustering algorithms;Communities;Equations;Mathematical model;Social network services;community discovery;link analysis;micro blog;topic model}, 
doi={10.1109/ICAwST.2013.6765503}, 
month={Nov},}
@INPROCEEDINGS{7817120, 
author={N. Li and L. J. Latecki}, 
booktitle={2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)}, 
title={Enhanced Affinity Inference Based Recommender Systems}, 
year={2016}, 
pages={597-601}, 
abstract={In this paper, we focus on improving the prediction accuracy and scalability of affinity inference based recommender systems, which predict unknown ratings based on the inferred affinities between abstract objects of users and item-rating pairs. Instead of treating each item evenly when inferring affinities, we propose to take into account the variance in ratings of each item. We also propose to reduce the number of item-rating pairs defined for each item, which can significantly reduce the computing time of affinity inference. Experimental results on the standard MovieLens dataset demonstrate the improvements.}, 
keywords={inference mechanisms;recommender systems;MovieLens dataset;enhanced affinity inference;prediction accuracy;recommender systems;Atmospheric modeling;Clustering algorithms;Collaboration;Motion pictures;Prediction algorithms;Recommender systems;Transforms}, 
doi={10.1109/WI.2016.0103}, 
month={Oct},}
@INPROCEEDINGS{6977054, 
author={S. Negi and R. Balasubramanyan and S. Chaudhury}, 
booktitle={2014 22nd International Conference on Pattern Recognition}, 
title={Discovering User-Communities and Associated Topics from YouTube}, 
year={2014}, 
pages={1958-1963}, 
abstract={Most of the popular multimedia sharing web-sites such as YouTube, Flickr etc not only allow users to author and upload content but also facilitate "social" networking amongst users. These social interactions can be in the form of - user-to-user interactions i.e. adding existing users to friend or contact list or user-to-content interactions : commenting on a video or picture, marking a picture/video as "favorite", subscribing to a user created "channel" etc. Analyzing these social interactions jointly with the content metadata (such as the description of the video, keywords associated with the image/video etc) can reveal interesting insights about user activity on these social media platforms. In this paper, we propose an unsupervised method that jointly models "social" interaction and content metadata in YouTube to discover user-communities and the nature of topics beings discussed in these communities. We report the effectiveness of the proposed method on real-world dataset.}, 
keywords={social networking (online);YouTube;content metadata;multimedia sharing Websites;social interactions;social media platforms;user-to-content interactions;user-to-user interactions;Blogs;Communities;Electronic mail;Maintenance engineering;Multimedia communication;YouTube}, 
doi={10.1109/ICPR.2014.342}, 
ISSN={1051-4651}, 
month={Aug},}
@INPROCEEDINGS{4594723, 
author={Zhang Yun and Feng Boqin}, 
booktitle={2008 8th IEEE International Conference on Computer and Information Technology}, 
title={Tag-based user modeling using formal concept analysis}, 
year={2008}, 
pages={485-490}, 
abstract={Social tagging systems have been largely adopted by users as useful and powerful tools to organize, browse and share interested resources on the web. All the tags used and resources collected by a user constitute the userpsilas personal tag space, which contains valuable information that can be used for building and enhancing the user model. In this paper, we propose an approach to mine user profile from onepsilas personal tag space. The proposed approach is based on the theory of concept lattices, which provides a powerful, well-founded, and computationally-tractable framework to model onepsilas personal tag space in which tags and collocated resources are represented and to compute such a transformation. We define several properties for each concept generated to evaluate the degree of such concept to depict the userpsilas interest, and then a novel algorithm is introduced to construct a hierarchical user profile from the built lattice. Experiments are carried out on the dataset collected from delicious and a detailed real-life case study is presented. Results show that the hierarchical user profile can describe the target userpsilas interests well enough, as well as organizing and browsing the tags and collocated resources effectively. Several research issues and technical details are discussed to further improve tag-based user profile for personalized services in the end.}, 
keywords={Internet;social aspects of automation;user modelling;World Wide Web;computationally-tractable framework;concept lattices;formal concept analysis;hierarchical user profile;personal tag space;social tagging systems;tag-based user modeling;Books;Lattices;Music;Pediatrics;Security;Software;Tagging}, 
doi={10.1109/CIT.2008.4594723}, 
month={July},}
@INPROCEEDINGS{6424968, 
author={A. N. Khan and A. Muhammad and A. M. M. Enriquez}, 
booktitle={2012 IEEE Fifth International Conference on Utility and Cloud Computing}, 
title={Mining for Norms in Clouds: Complying to Ethical Communication through Cloud Text Data Mining}, 
year={2012}, 
pages={327-332}, 
abstract={As the world is realizing the power and efficiency of cloud computing, enhanced security and intelligence is needed in communication to filter out unethical data violating norms in clouds. No filtering categorization has been currently proposed. Numerous lists of banned, unethical and objectionable words have been developed with limited user satisfaction. Lists are usually manually generated, with some programmable extensibility for online forums and public newsgroups. We define a tool and methodology to categorize the censor data. We statistically grow words in the categorized data and tag the hidden neutral words with meaning in context. Using Computational Linguistics tools and modifying them to suit our means, we analyze sample text from gigabytes of email newsgroup dataset over Cloud Servers. A sample result dataset of the most frequently used words breaking the norms in recent cloud communication is presented in the results in broad categories. The categories separate cloud-server data found in newsgroups related to internet crimes, fraud, theft, anti-state elements, and other material of legal importance. Thus this study demonstrates a tag cloud of most frequent critical words in communications from legal and ethical point-of-view in the current scenario of cloud databases.}, 
keywords={cloud computing;computational linguistics;data mining;ethical aspects;fraud;information filtering;information resources;law;security of data;text analysis;Internet crime;antistate element;banned words;censor data;cloud communication;cloud computing;cloud database;cloud server;cloud text data mining;cloud-server data;computational linguistics tools;critical words;data categorization;email newsgroup dataset;ethical communication;ethical point-of-view;filtering categorization;fraud;hidden neutral words;intelligence;legal importance;legal point-of-view;norm mining;objectionable words;online forum;public newsgroup;security;text analysis;theft;unethical data violating norms;unethical words;user satisfaction;Cloud computing;Data mining;Law;Security;Servers;Tag clouds;Censorship;Cloud Servers;Ethical Norms;Hidden Markov Model;Security;Tag Cloud;Text Data Mining}, 
doi={10.1109/UCC.2012.59}, 
month={Nov},}
@INPROCEEDINGS{7339002, 
author={L. Van Thinh}, 
booktitle={2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
title={Predicting the quality of web services based on two layer model}, 
year={2015}, 
pages={37-43}, 
abstract={Recently, Web service has become an important issue in the research community. Especially, predicting the Quality of Service (QoS) for users has been a hot topic which needs researching and applicating. In the other hand, with the rapid growth of the number of service providers and users, it results a large number of datasets. It significantly effects on the QoS as management and supervision to describe the functional and non-functional characteristics of Web service. In this context, predicting QoS on big data dataset is an urgent issue that needs to be solved. In this paper, we present a new model to handle this issue based on a Restricted Boltzmann Machines (RBM), it is called Two Layer Model (TLM). We have used this model to deal with the big data datasets and the model used in efficient learning and inference procedures to predict the missing QoS value of web service. Our experiments have been performed based on two data sets in the WS-DREAM dataset and the experimental results have proved that the proposed model was effective.}, 
keywords={Big Data;Boltzmann machines;Web services;quality of service;Big data;QoS value;RBM;TLM;WS-DREAM dataset;Web service;quality of service;restricted Boltzmann machine;two layer model;Big data;Computational modeling;Data models;Mathematical model;Predictive models;Quality of service;Web services;Restricted Boltzmann Machines;Two Layers Model;Web service;predicting the QoS values}, 
doi={10.1109/ICSESS.2015.7339002}, 
ISSN={2327-0586}, 
month={Sept},}
@INPROCEEDINGS{7438668, 
author={P. McAllister and H. Zheng and R. Bond and A. Moorhead}, 
booktitle={2015 IEEE International Conference on Engineering, Technology and Innovation/ International Technology Management Conference (ICE/ITMC)}, 
title={Semi-automated system for predicting calories in photographs of meals}, 
year={2015}, 
pages={1-6}, 
abstract={Obesity is increasing globally. Obesity brings with it many chronic conditions. There has been increasing research in the use of ICT interventions to combat obesity using food logging and image calorie analysis. These interventions allow users to document their calorie intake to help promote healthy living. However using food logs may lead to inaccurate readings as the user may incorrectly calculate portion size when recording nutritional information. This paper discusses the use of image nutritional analysis techniques to ascertain a more accurate calorie reading from photographs of food items. The methods employed involve determining a ground truth data set by correlating weight of a food item with its area in cm2. This dataset could then be plotted on a regression model and used to determine calorie content of future portions. The proposed system uses a semi-automated approach to allow users to manually draw around the food portion using a polygonal tool. Results show that the application achieved a reasonable accuracy in predicting the calorie content of food item portions with a 11.82% percentage error.}, 
keywords={diseases;food products;image colour analysis;image segmentation;medical computing;regression analysis;ICT;calorie intake;calorie prediction;calorie reading;chronic conditions;food item photographs;food item weight correlation;food logging;ground truth data set;image calorie analysis;image nutritional analysis techniques;meal photograph;nutritional information recording;obesity;polygonal tool;portion size;regression model;semiautomated system;Image color analysis;Image segmentation;Indexes;Mathematical model;Obesity;Packaging;Pediatrics;calorie;image;management;obesity;regression;segmentation;semi-automated}, 
doi={10.1109/ICE.2015.7438668}, 
month={June},}
@INPROCEEDINGS{7867649, 
author={A. K. Alhassan and A. A. Alfaki}, 
booktitle={2017 International Conference on Communication, Control, Computing and Electronics Engineering (ICCCCEE)}, 
title={Color and texture fusion-based method for content-based Image Retrieval}, 
year={2017}, 
pages={1-6}, 
abstract={Content-based image retrieval (CBIR) is a technique uses visual contents such as color, texture and shape to search images from large scale image databases according to users' interest. In a CBIR, visual image content is represented in form of image features, which are extracted automatically and there is no manual intervention, thus eliminating the dependency on humans in the feature extraction stage. Recent studies in CBIR get the similarity results and retrieve images based on one type of feature which are color, texture or shape. In this study authors proposed a fusion based retrieval model for merging results taken from color and texture image features based different fusion methods. After implementing our proposed retrieval model on Wang image dataset which widely used in CBIR, the results show that CombMEAN fusion approach has the best and high precision value and outperformed both individual color and texture retrieval model in both top10 and top20 retrieved images.}, 
keywords={content-based retrieval;feature extraction;image colour analysis;image fusion;image representation;image retrieval;image texture;CBIR;CombMEAN fusion;Wang image dataset;color fusion;content-based image retrieval;fusion methods;image feature extraction;image search;texture fusion-based method;texture retrieval;visual image content representation;Feature extraction;Image color analysis;Image retrieval;Shape;Visualization;CBIR;Color moment;Gabor function;Texture}, 
doi={10.1109/ICCCCEE.2017.7867649}, 
month={Jan},}
@INPROCEEDINGS{5072506, 
author={H. Verkasalo}, 
booktitle={2009 Fourth International Conference on Internet and Web Applications and Services}, 
title={Open Mobile Platforms: Modeling the Long-Tail of Application Usage}, 
year={2009}, 
pages={112-118}, 
abstract={In management research, the long tail phenomenon is typically linked to the long-tail of product demand distribution, particularly under electronic distribution, storing and consumption of content. This paper discusses the role of open mobile software platforms in creating the market for niche mobile applications. In particular, the transformation of the mobile industry from mobile communications towards computer and Internet industries is discussed, with a particular focus on the open software platforms of mobile phones that facilitate innovation. This study makes a hypothesis that open software platforms are boosting the use of niche applications. The paper consequently collects empirical data on Smartphone usage over three consequent years in Finland. The dataset of 1 145 smartphone users is analyzed in studying whether the long-tail phenomenon is evident in the demand for mobile applications. The analysis of usage-level data reveals that the application demand is more heterogenic in the newest panel study than in the earlier studies. In other words, though the top 5% of applications typically represent more than 90% of total application usage, the bottom 80% of applications (the long-tail part) already represent 2.10% of total observed application usage in 2007, whereas this tail is only 1.39% in 2006 and 0.89% in 2005. In the newest dataset of 2007, 6.9% of top applications represent 93.1% of total smartphone usage. The analysis reveals a U-relationship between the number of users and mean usage frequency of applications. This means that many of the niche applications are being used actively by those who installed them, suggesting that the value of add-on applications is high.}, 
keywords={Internet;mobile computing;Internet industries;Smartphone usage;electronic distribution;open mobile platforms;open software platforms;product demand distribution;usage-level data analysis;Application software;Communication industry;Computer industry;Consumer electronics;Content management;Internet;Mobile communication;Mobile computing;Mobile handsets;Probability distribution;add-on applications;long-tail;mobile applications;mobile software platforms}, 
doi={10.1109/ICIW.2009.24}, 
month={May},}
@INPROCEEDINGS{6967131, 
author={J. Zhao and J. Ma}, 
booktitle={Proceedings of 2013 3rd International Conference on Computer Science and Network Technology}, 
title={An improved slope one algorithm based on tag frequency}, 
year={2013}, 
pages={369-372}, 
abstract={Technology of Collaborative filtering algorithm recommends items to the target user with information of other users who have similar tastes with the target user. The basic Slope One collaborative filtering algorithm simply uses the linear regression model to predict the ratings of items. Based on the Slope One scheme, an improved algorithm considering the frequency of items' tags information is proposed in this paper. Firstly, the tag frequency of rated items is chosen to represent the vector of the target user's preference. After that, the similarity between rated items and unrated one is calculated and according to it, the unrated items' rating can be predicted. Experiments on MovieLens dataset show that the proposed approach gives better prediction accuracy.}, 
keywords={collaborative filtering;recommender systems;MovieLens dataset;item rating prediction;linear regression model;slope one collaborative filtering algorithm;tag frequency;user preference;Algorithm design and analysis;Collaboration;Filtering;Filtering algorithms;Motion pictures;Prediction algorithms;Vectors;Collaborative Filtering;Recommend System;Slope One;Tag Frequency}, 
doi={10.1109/ICCSNT.2013.6967131}, 
month={Oct},}
@INPROCEEDINGS{7777911, 
author={U. Mahbub and R. Chellappa}, 
booktitle={2016 IEEE 7th Annual Ubiquitous Computing, Electronics Mobile Communication Conference (UEMCON)}, 
title={PATH: Person authentication using trace histories}, 
year={2016}, 
pages={1-8}, 
abstract={In this paper, a solution to the problem of Active Authentication using trace histories is addressed. Specifically, the task is to perform user verification on mobile devices using historical location traces of the user as a function of time. Considering the movement of a human as a Markovian motion, a modified Hidden Markov Model (HMM)-based solution is proposed. The proposed method, namely the Marginally Smoothed HMM (MSHMM), utilizes the marginal probabilities of location and timing information of the observations to smooth-out the emission probabilities while training. Hence, it can efficiently handle unforeseen observations during the test phase. The verification performance of this method is compared to a sequence matching (SM) method, a Markov Chain-based method (MC) and an HMM with basic Laplace Smoothing (HMM-lap). Experimental results using the location information of the UMD Active Authentication Dataset-02 (UMDAA02) and the GeoLife dataset are presented. The proposed MSHMM method outperforms the compared methods in terms of equal error rate (EER). Additionally, the effects of different parameters on the proposed method are discussed.}, 
keywords={authorisation;geographic information systems;hidden Markov models;mobile computing;pattern matching;probability;smoothing methods;EER;GeoLife dataset;HMM-lap;Laplace smoothing;MSHMM;Markov Chain-based method;Markovian motion;PATH;SM method;UMD Active Authentication Dataset-02;UMDAA02;active authentication problem;emission probabilities;equal error rate;marginally smoothed HMM;mobile devices;modified HMM-based solution;modified hidden Markov model-based solution;person authentication;sequence matching method;trace histories;user verification;Authentication;Data mining;Global Positioning System;Hidden Markov models;History;Smart phones;Trajectory;Active authentication;geo-location-based verification;hidden Markov models}, 
doi={10.1109/UEMCON.2016.7777911}, 
month={Oct},}
@INPROCEEDINGS{7830213, 
author={P. Akulwar and S. Pardeshi}, 
booktitle={2016 International Conference on Inventive Computation Technologies (ICICT)}, 
title={Bayesian Probabilistic Matrix Factorization #x2014; A dive towards recommendation}, 
year={2016}, 
volume={3}, 
pages={1-5}, 
abstract={Matrix factorization is a well known technique which discovers latent features among users and items. This method brings the advantage of reducing data sparcity and cold start problem. The different Matrix factorization methods such as SVD, PMF, NMF etc. exists. But all these suffer from certain drawback especially when dataset is extremely sparse. The Bayesian Probabilistic Matrix Factorization (BPMF) method proves to be more efficient and provides prediction that leads to better accuracy. The significance of BPMF is to avoid parameter tuning and provides predictive distribution. To enhance user satisfaction and loyalty particularly when the huge volume of data is available, there is need of recommender system. Hence, the idea of BPMF is extended towards recommendation where top N queries are recommended to users using BPMF method liaison with Cholesky decomposition, Gibbs sampling technique, K nearest neighbor method. The experimental work describes that the BPMF method when used in query recommendation provides better results.}, 
keywords={Bayes methods;collaborative filtering;matrix decomposition;query processing;recommender systems;sampling methods;BPMF;Bayesian probabilistic matrix factorization;Cholesky decomposition;Gibbs sampling technique;K nearest neighbor method;collaborative filtering;query recommendation;recommender system;Bayes methods;Collaboration;Computational modeling;Matrix decomposition;Prediction algorithms;Probabilistic logic;Symmetric matrices;Bayesian Probabilistic Matrix Factorization;Cholesky decomposition;Gibbs sampling technique;K nearest neighbor;Matrix factorization}, 
doi={10.1109/INVENTIVE.2016.7830213}, 
month={Aug},}
@ARTICLE{5290694, 
author={P. Bak and F. Mansmann and H. Janetzko and D. Keim}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Spatiotemporal Analysis of Sensor Logs using Growth Ring Maps}, 
year={2009}, 
volume={15}, 
number={6}, 
pages={913-920}, 
abstract={Spatiotemporal analysis of sensor logs is a challenging research field due to three facts: a) traditional two-dimensional maps do not support multiple events to occur at the same spatial location, b) three-dimensional solutions introduce ambiguity and are hard to navigate, and c) map distortions to solve the overlap problem are unfamiliar to most users. This paper introduces a novel approach to represent spatial data changing over time by plotting a number of non-overlapping pixels, close to the sensor positions in a map. Thereby, we encode the amount of time that a subject spent at a particular sensor to the number of plotted pixels. Color is used in a twofold manner; while distinct colors distinguish between sensor nodes in different regions, the colors' intensity is used as an indicator to the temporal property of the subjects' activity. The resulting visualization technique, called growth ring maps, enables users to find similarities and extract patterns of interest in spatiotemporal data by using humans' perceptual abilities. We demonstrate the newly introduced technique on a dataset that shows the behavior of healthy and Alzheimer transgenic, male and female mice. We motivate the new technique by showing that the temporal analysis based on hierarchical clustering and the spatial analysis based on transition matrices only reveal limited results. Results and findings are cross-validated using multidimensional scaling. While the focus of this paper is to apply our visualization for monitoring animal behavior, the technique is also applicable for analyzing data, such as packet tracing, geographic monitoring of sales development, or mobile phone capacity planning.}, 
keywords={biology computing;biosensors;colour graphics;data loggers;data visualisation;Alzheimer transgenic mice;colors intensity;distinct colors;geographic monitoring;growth ring maps;hierarchical clustering;map distortions;mobile phone capacity planning;multidimensional scaling;nonoverlapping pixels;packet tracing;sensor logs spatiotemporal analysis;spatial data;visualization technique;Animal behavior;Data analysis;Data mining;Data visualization;Humans;Mice;Monitoring;Multidimensional systems;Navigation;Spatiotemporal phenomena;animal behavior;dense pixel displays;spatiotemporal visualization;visual analytics;Alzheimer Disease;Animals;Animals, Genetically Modified;Behavior, Animal;Cluster Analysis;Computational Biology;Computer Graphics;Disease Models, Animal;Female;Male;Mice;Spatial Behavior;Time Factors}, 
doi={10.1109/TVCG.2009.182}, 
ISSN={1077-2626}, 
month={Nov},}
@INPROCEEDINGS{7033510, 
author={M. Taherpour and H. Shaken and M. Mali}, 
booktitle={2014 International Congress on Technology, Communication and Knowledge (ICTCK)}, 
title={Improvement of recommender systems using confidence-aware trust}, 
year={2014}, 
pages={1-7}, 
abstract={Collaborative Filtering (CF) is one of the most successful recommendation techniques. Regardless of its success, it still suffers from some weaknesses such as data sparsity and user cold-start problems, resulting in poor recommendation accuracy and reduced coverage. Trust-based recommendation methods incorporate the additional information from the user's social trust network into collaborative filtering and can better solve such problems. However in these methods the level of confidence in direct and indirect trust estimations is under question. In this paper, an innovative Confidence-Aware Trust (CAT)-based recommendation approach is proposed within the CF framework An evaluation is performed on the Movie Lens dataset. Experimental results indicate that the CAT approach outperforms existing recommendation algorithms in terms of recommendation accuracy and coverage.}, 
keywords={collaborative filtering;recommender systems;trusted computing;CAT-based recommendation approach;CF framework;MovieLens dataset;collaborative filtering;confidence-aware trust;recommendation accuracy;recommendation coverage;recommender systems improvement;Accuracy;Educational institutions;Equations;Mathematical model;Measurement;Recommender systems;Cold-start;Collaborative filtering;Confidence;Data sparsity;Recommender systems;Trust}, 
doi={10.1109/ICTCK.2014.7033510}, 
month={Nov},}
@INPROCEEDINGS{7037250, 
author={C. Jiang and Y. Chen and K. J. R. Liu}, 
booktitle={2014 IEEE Global Communications Conference}, 
title={Evolutionary social information diffusion analysis}, 
year={2014}, 
pages={2911-2916}, 
abstract={Nowadays, social networks are extremely large-scale with tremendous information flows, where understanding how the information diffuse over social networks becomes an important research issue. Most of the existing works on information diffusion analysis are based on either network structure modeling or empirical approach with dataset mining. However, the information diffusion is also heavily influenced by network users' decisions, actions and their socio-economic connections, which is generally ignored by existing works. In this paper, we propose an evolutionary game theoretic framework to model the dynamic information diffusion process in social networks. To verify our theoretical analysis, we conduct experiments by using Facebook network and real-world information spreading dataset of Memetracker. Experiment results show that the proposed game theoretic framework is effective and practical in modeling the social network users' information forwarding behaviors.}, 
keywords={game theory;graph theory;social networking (online);Facebook network;Memetracker;dynamic information diffusion process model;evolutionary game theoretic framework;evolutionary social information diffusion analysis;network user actions;network user decisions;real-world information spreading dataset;social network user information forwarding behaviors;socio-economic connections;Biological system modeling;Diffusion processes;Evolution (biology);Facebook;Games;Tin}, 
doi={10.1109/GLOCOM.2014.7037250}, 
ISSN={1930-529X}, 
month={Dec},}
@INPROCEEDINGS{5385109, 
author={X. Hongxia and L. Weifeng}, 
booktitle={2009 International Forum on Computer Science-Technology and Applications}, 
title={Distributed Database Searching System Based on Alchemi}, 
year={2009}, 
volume={1}, 
pages={160-163}, 
abstract={With the information expanding and network popularization, the way that accessing single database can not satisfy the demands for users. The requirement to access distributed database is increased. This paper firstly implemented distributed database query with the grid computing architecture Alchemi which is based on .NET. Then, a distributed database searching system model was constructed by grid computing technology, while the architecture and the operation process of the model were described in detail. Finally, the model was achieved with the platform of Microsoft .NET. According to the analyses of the configuration information and experimental results, it is proved that, compared with the current distributed database searching system, the system that developed by Alchemi is not only easy to configure, but also able to assure the efficiency and accuracy of the query.}, 
keywords={distributed databases;grid computing;query processing;Alchemi;Microsoft .NET;configuration information;distributed database access;distributed database query;distributed database searching system;grid computing architecture;Application software;Computer architecture;Distributed computing;Distributed databases;Grid computing;Resource management;Security;Spatial databases;Web services;Yarn;.NET Remoting;Alchemi;dataset;distributed database;grid computing;grid thread}, 
doi={10.1109/IFCSTA.2009.46}, 
month={Dec},}
@INPROCEEDINGS{6921947, 
author={R. Ramya and M. Parvathy and K. Sundarakantham and S. Mercy Shalinie}, 
booktitle={2013 Fifth International Conference on Advanced Computing (ICoAC)}, 
title={Trust dependence network for recommender systems}, 
year={2013}, 
pages={179-184}, 
abstract={The recent invasion of social networks makes it inevitable to construct a trust network. Recommender systems at present uses advanced parallelism in web development. But attaining trustworthiness in such a system has been a challenging task in research work for several years. To overcome the issues in the existing work such as sparsity, scalability and to eliminate distrust users in the network, the proposed framework focused on some influential metric features such as similarity and centrality measures. Finally an optimized, trustworthy network is generated which is experimentally verified with the dataset from an online sharing community network Epinion.com.}, 
keywords={recommender systems;social networking (online);trusted computing;Epinion.com;Web development;centrality measure;metric features;online sharing community network;parallelism;recommender systems;similarity measure;social networks;trust dependence network;Communities;Computational modeling;Educational institutions;Prediction algorithms;Sea measurements;Social network services;Betweenness Centrality;Distrust;Social networks;Trust networks}, 
doi={10.1109/ICoAC.2013.6921947}, 
ISSN={2377-6927}, 
month={Dec},}
@INPROCEEDINGS{6092674, 
author={F. Zhang}, 
booktitle={2011 Fifth International Conference on Management of e-Commerce and e-Government}, 
@INPROCEEDINGS{7783920, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={Towards Systematic Analysis and Summary of DUV-Based Dataset Usage Information}, 
year={2016}, 
pages={169-172}, 
abstract={The forthcoming Dataset Usage Vocabulary (DUV) standard under development by the W3C Data on the Web Best Practices Working Group can be used to describe and share dataset usage information on the Web, thus facilitating better communication between data publishers and consumers. Despite the significant progress in the standardization, there is a lack of systematic research on approaches and tools for analyzing and summarizing DUV-based dataset usage information to promote the re-use of data. This paper therefore proposes a Framework for Analysis and Summary of DUV-based Dataset Usage Information (FASDUI). FASDUI can provide users (data publishers and consumers) with a friendly user interface designed according to DUV's Citation Model, Usage Model, and Feedback Model, and employ automatic algorithms to systematically analyze dataset usage data and then present the summarized usage information in the user interface. Our prototype implementation of FASDUI and preliminary experimental results show that FASDUI is feasible and implementable.}, 
keywords={Datasets Usage Vocabulary (DUV);dataset usage information;information summary;systematic analysis;user interface design}, 
doi={10.1109/WISA.2016.13}, 
month={Sept},}
@INPROCEEDINGS{7498974, 
author={B. M. Fazenda and P. Kendrick and T. J. Cox and F. Li and I. Jackson}, 
booktitle={2016 Eighth International Conference on Quality of Multimedia Experience (QoMEX)}, 
title={Perception and automated assessment of audio quality in user generated content: An improved model}, 
year={2016}, 
pages={1-6}, 
abstract={Technology to record sound, available in personal devices such as smartphones or video recording devices, is now ubiquitous. However, the production quality of the sound on this user-generated content is often very poor: distorted, noisy, with garbled speech or indistinct music. Our interest lies in the causes of the poor recording, especially what happens between the sound source and the electronic signal emerging from the microphone, and finding an automated method to warn the user of such problems. Typical problems, such as distortion, wind noise, microphone handling noise and frequency response, were tested. A perceptual model has been developed from subjective tests on the perceived quality of such errors and data measured from a training dataset composed of various audio files. It is shown that perceived quality is associated with distortion and frequency response, with wind and handling noise being just slightly less important. In addition, the contextual content of the audio sample was found to modulate perceived quality at similar levels to degradations such as wind and rendering those introduced by handling noise negligible.}, 
keywords={audio coding;audio recording;audio files;audio quality;audio sample;automated assessment;perceived quality;perceptual model;training dataset;user generated content;Acoustic distortion;Algorithm design and analysis;Data models;Degradation;Frequency measurement;User-generated content;audio quality;audio quality of experience;distortion;handling noise;perception;wind noise}, 
doi={10.1109/QoMEX.2016.7498974}, 
month={June},}
@INPROCEEDINGS{7780701, 
author={V. Ramanathan and J. Huang and S. Abu-El-Haija and A. Gorban and K. Murphy and L. Fei-Fei}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Detecting Events and Key Actors in Multi-person Videos}, 
year={2016}, 
pages={3043-3053}, 
abstract={Multi-person event recognition is a challenging task, often with many people active in the scene but only a small subset contributing to an actual event. In this paper, we propose a model which learns to detect events in such videos while automatically "attending" to the people responsible for the event. Our model does not use explicit annotations regarding who or where those people are during training and testing. In particular, we track people in videos and use a recurrent neural network (RNN) to represent the track features. We learn time-varying attention weights to combine these features at each time-instant. The attended features are then processed using another RNN for event detection/ classification. Since most video datasets with multiple people are restricted to a small number of videos, we also collected a new basketball dataset comprising 257 basketball games with 14K event annotations corresponding to 11 event classes. Our model outperforms state-of-the-art methods for both event classification and detection on this new dataset. Additionally, we show that the attention mechanism is able to consistently localize the relevant players.}, 
keywords={feature extraction;image classification;image representation;object detection;object tracking;recurrent neural nets;video signal processing;RNN;event classification;event detection;multiperson event recognition;multiperson videos;recurrent neural network;time-varying attention weights;track features representation;video datasets;video tracking;Analytical models;Context;Detectors;Games;Recurrent neural networks;Training;Videos}, 
doi={10.1109/CVPR.2016.332}, 
month={June},}
@INPROCEEDINGS{7823198, 
author={S. Easwaramoorthy and S. Thamburasa and K. Aravind and S. B. Bhushan and H. Rajadurai}, 
booktitle={2016 International Conference on Inventive Computation Technologies (ICICT)}, 
title={Heterogeneous classifier model for E-mail spam classification using FSO feature selection method}, 
year={2016}, 
volume={1}, 
pages={1-6}, 
abstract={In this Computer world, E-mail is one of the popular modes of communication due to its easy accessibility and low cost. Due to the advantages of time, speed and cost effectiveness, a lot of people use it for commercial advertisement purposes resulting in unnecessary e-mails at user inboxes called spam. Spam is the unnecessary and unwanted commercial e-mail. It is also known as junk e-mail. It is sending unnecessary e-mail message with profit-making data to in discriminated group of recipients. It is waste of storage space, time, and network bandwidth. E-mail classifier classifies the group of mails into ham and spam based on its data content. E-mail classifications system, which clean the spam e-mails from inbox, move it to the spam folder. The proposed e-mail classification system includes two stages, such as training stage and testing stage. Initial stage, input e-mail message is sent to the feature selection module to pick the suitable feature for spam classification. In this paper, firefly and GSO algorithm is efficiently combined to pick the appropriate features from the big dimensional area using correlation. Once the finest feature space is determined through FSO algorithm, the E-mail classification is accomplished using weighted based majority voting system. The classifiers applied for classifying e-mails are naive bayes algorithm, neural networks and decision tree. The UCI spambase dataset is utilized for e-mail spam classification. The research result validation of the proposed technique is made through evaluation metrics such as, precision, recall and accuracy.}, 
keywords={Bayes methods;advertising;decision trees;neural nets;pattern classification;profitability;unsolicited e-mail;FSO feature selection method;GSO algorithm;UCI spambase dataset;big dimensional area;commercial advertisement purposes;commercial e-mail;decision tree;e-mail spam classification;evaluation metrics;heterogeneous classifier model;junk e-mail;naive Bayes algorithm;network bandwidth;neural networks;profit-making data;storage space;testing stage;training stage;Algorithm design and analysis;Classification algorithms;Decision trees;Feature extraction;Neural networks;Unsolicited electronic mail;Decision Tree;E-mail;Firefly;Group Search Optimiser;Naive Bayes;Neural networks;Spam;Spambase classification;Spambase dataset;classification}, 
doi={10.1109/INVENTIVE.2016.7823198}, 
month={Aug},}
@ARTICLE{6327294, 
author={A. Endert and P. Fiaux and C. North}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering}, 
year={2012}, 
volume={18}, 
number={12}, 
pages={2879-2888}, 
abstract={Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users' analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user's reasoning and intuition.}, 
keywords={data visualisation;inference mechanisms;ForceSPIRE;analytical reasoning;clustering model;cognitively demanding task;expressive interactions;human intuition;keyword weighting;mathematical models;model steering;parametric modifications;semantic interaction;sensemaking;spatially clustering data;visual analytic prototype;visual analytic tools;visualization;Analytical models;Mathematical model;Semantics;User interfaces;Visual analytics;User Interaction;analytic reasoning;sensemaking;visual analytics;visualization}, 
doi={10.1109/TVCG.2012.260}, 
ISSN={1077-2626}, 
month={Dec},}
@INPROCEEDINGS{7509835, 
author={A. Li and Q. Lv and Y. Qiao and J. Yang}, 
booktitle={2016 IEEE International Conference on Big Data Analysis (ICBDA)}, 
title={Improving mobility prediction performance with state based prediction method when the user departs from routine}, 
year={2016}, 
pages={1-7}, 
abstract={With the rapid increase of the mobile users, mobility prediction has attracted more and more attention. For the moment, a lot of location prediction methods have shown that humans are highly predictable in their movements. However, the method of predicting locations when the user's behavior unexpectedly transits from routine to irregular pattern is undiscovered. In this paper, we propose a practical model based on State Based Prediction (SBP) method to predict the place to be visited when the user departs from the routine. First, we predict the routine level of user locations to discover the irregular places. Second, we use Markov method to predict future locations. Then, when user departs from the routine, we use SBP method to conduct prediction, meanwhile the prediction results of the other places remain consistent with the output of the Markov predictor. Experiments with the real mobile dataset show that our prediction model outperforms other basic mobility predictors, for example, the accuracy of our method can reach more than 83%, which is higher than the accuracy of 60% achieved by Lempel-Ziv (LZ) predictor.}, 
keywords={Markov processes;mobile computing;Lempel-Ziv predictor;Markov method;location prediction;mobility prediction performance;state based prediction;Entropy;Markov processes;Mobile communication;Mobile computing;Predictive models;Systems architecture;Trajectory;State Based Prediction (SBP);departures from routine;mobility prediction}, 
doi={10.1109/ICBDA.2016.7509835}, 
month={March},}
@INPROCEEDINGS{6137420, 
author={P. Cremonesi and A. Tripodi and R. Turrin}, 
booktitle={2011 IEEE 11th International Conference on Data Mining Workshops}, 
title={Cross-Domain Recommender Systems}, 
year={2011}, 
pages={496-503}, 
abstract={Most recommender systems work on single domains, i.e., they recommend items related to the same domain where users have expressed ratings. However, the integration of different domains into one recommender system could allow users to span over different types of items. For instance, users that have watched live TV programs could like to be recommended with on-demand movies, music, mobile applications, friends to connect to, etc. This paper focuses on cross-domain collaborative recommender systems, whose aim is to suggest items related to multiple domains. We first formalize the cross-domain problem in order to provide a common framework for the classification and the evaluation of state-of-the-art algorithms. We later define a new class of cross-domain algorithms based on neighborhood collaborative filtering, either item-based or user-based. The main idea is to first model the classical similarity relationships (e.g., Pearson, cosine) as a direct graph and to later explore all possible paths connecting users or items in order to find new, cross-domain, relationships. The algorithms have been tested on three cross-domain scenarios artificially reproduced by partitioning the Netflix dataset.}, 
keywords={graph theory;groupware;information filtering;recommender systems;Netflix dataset;cross domain collaborative recommender system;direct graph;live TV programs;mobile applications;music;neighborhood collaborative filtering;on-demand movies;Algorithm design and analysis;Artificial neural networks;Collaboration;Correlation;Motion pictures;Prediction algorithms;Recommender systems;cross-domain;neighborhood-based;recommender systems;transitive closure}, 
doi={10.1109/ICDMW.2011.57}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{7448725, 
author={J. Liu and Y. Wang and H. Tao}, 
booktitle={2015 IEEE/CIC International Conference on Communications in China (ICCC)}, 
title={An improved matrix factorization model under multidimensional context situation}, 
year={2015}, 
pages={1-6}, 
abstract={Traditional recommender systems have won great success on electronic commerce. However, when the user-item rating record matrix is sparse, traditional recommendation algorithms perform poor, which is known as the cold-start problem. Recently, more and more contextual features have been proven to be valuable information for improving the accuracy of recommendation, and newly formed context-aware recommendation systems (CARS) provide a way to solve the cold-start problem by using some certain features such as users location, mood and social relationship. In order to handle multidimensional context, this paper first extracts relevant contextual information by calculating the information entropy, then divides the contextual information into three categories - user context, item context and interaction context. Finally, we extend the matrix factorization (MF) model to integrate the context information. Experimental results on LDOS-CoMoDa dataset have shown that our approach provides improvement in terms of recommendation accuracy.}, 
keywords={entropy;information retrieval;matrix decomposition;recommender systems;CARS;LDOS-CoMoDa dataset;cold-start problem;context-aware recommendation systems;contextual features;contextual information extraction;information entropy;interaction context;item context;matrix factorization model;multidimensional context situation;recommendation accuracy;user context;user-item rating record matrix;Automobiles;Context;Context modeling;Mood;Motion pictures;Prediction algorithms;Sparse matrices}, 
doi={10.1109/ICCChina.2015.7448725}, 
month={Nov},}
@INPROCEEDINGS{6258147, 
author={F. Wang and H. Wang and K. Xu}, 
booktitle={2012 32nd International Conference on Distributed Computing Systems Workshops}, 
title={Diffusive Logistic Model Towards Predicting Information Diffusion in Online Social Networks}, 
year={2012}, 
pages={133-139}, 
abstract={Online social networks have recently become an effective and innovative channel for spreading information and influence among hundreds of millions of end users. Most of prior work either carried out empirical studies or focus on the information diffusion modeling in temporal dimension, little attempt has been given on understanding information diffusion over both temporal and spatial dimensions. In this paper, we propose a Partial Differential Equation (PDE), specifically, a Diffusive Logistic (DL) equation to model the temporal and spatial characteristics of information diffusion. We present the temporal and spatial patterns in a real dataset collected from a social news aggregation site, Digg, and validate the proposed DL equation in terms of predicting the information diffusion process. Our experiment results show that the DL model is able to characterize and predict the process of information propagation in online social networks. For example, for the most popular news with 24,099 votes in Digg, the average prediction accuracy of DL model over all distances during the first 6 hours is 92.08%. To the best of our knowledge, this paper is the first attempt to use PDE-based model to study the information diffusion process in both temporal and spatial dimensions in online social networks.}, 
keywords={information dissemination;partial differential equations;social networking (online);spatiotemporal phenomena;DL equation;Digg;PDE;diffusive logistic model;information diffusion prediction;information propagation;online social network;partial differential equation;social news aggregation site;spatial dimension;temporal dimension;Accuracy;Diffusion processes;Equations;Logistics;Mathematical model;Predictive models;Social network services}, 
doi={10.1109/ICDCSW.2012.16}, 
ISSN={1545-0678}, 
month={June},}
@INPROCEEDINGS{7514608, 
author={S. Yadav and S. Subramanian}, 
booktitle={2016 International Conference on Computational Techniques in Information and Communication Technologies (ICCTICT)}, 
title={Detection of Application Layer DDoS attack by feature learning using Stacked AutoEncoder}, 
year={2016}, 
pages={361-366}, 
abstract={An Application Layer Distributed Denial of Service Attack (DDoS) is one of the biggest concerns for web security. Many detection methods are designed to mitigate DDoS attack based on IP and TCP layer instead of the Application layer. These methods are not suitable for detection of Application layer DDoS attack since most of the IP and TCP layer DDoS attacks are based on request flooding attack. But Application layer DDoS attacks consist of request flooding, session flooding, and asymmetric attack. The solutions available to detect Application layer DDoS attack, detect only limited number of Application layer DDoS attacks. The solutions that detect all types of Application layer DDoS attacks have huge algorithm complexity. One of the major challenges in the detection of an Application layer DDoS attack is the non-availability of features to detect such attacks. Hence it is difficult to model normal user behavior from attack behavior. In this paper, Deep learning architecture is introduced to learn deep features of Application layer DDoS attack. Deep learning architecture consist of very deep neural network, typically more than three layers. In the proposed work the concept of AutoEncoder is applied, which is one of the deep learning based models that learns deep useful features in the Application layer DDoS attack dataset. The Stacked AutoEncoder deep learning architecture, is aimed to receive high level features. The performance of the proposed method was evaluated in terms of the metrics such as false positive rate and detection rate. Comparison of the proposed method with the existing methods reveals that the proposed method performs better than the existing methods.}, 
keywords={computer network security;learning (artificial intelligence);neural nets;IP layer;Stacked AutoEncoder;TCP layer;Web security;application layer DDoS attack detection;application layer distributed denial of service attack;deep learning architecture;feature learning;very deep neural network;Computer crime;Feature extraction;Floods;IP networks;Machine learning;Pattern recognition;Web servers;Application Layer DDoS Attack;AutoEncoder;DDoS;Deep learning;Feature learning;Stacked AutoEncoder}, 
doi={10.1109/ICCTICT.2016.7514608}, 
month={March},}
@INPROCEEDINGS{7377394, 
author={M. a. Lebre and F. L. Mouel and E. Menard}, 
booktitle={2015 14th International Conference on ITS Telecommunications (ITST)}, 
title={On the importance of real data for microscopic urban vehicular mobility trace}, 
year={2015}, 
pages={22-26}, 
abstract={Vehicular networks reflect user mobility behavior and present complex microscopic and macroscopic mobility patterns. Microscopic mobility is often simplified in macroscopic systems and we argue that its impact is too largely neglected. Notwithstanding improvements in realistically modeling and predicting mobility, few vehicular traces - especially complex microscopic ones - are available to validate such models. In this paper, we present a realistic synthetic dataset of vehicular mobility over two daily traffic peaks in a small area: the Europarc roundabout in the town of Creteil, France. We outline how the description and comprehensive representation of local mobility at an intersection, such as the roundabout chosen here, is important for any interpretation made of it.}, 
keywords={mobile radio;traffic engineering computing;Europarc roundabout;France;microscopic mobility pattern;microscopic urban vehicular mobility trace;user mobility behavior;vehicular mobility synthetic dataset;vehicular networks;Cities and towns;Data models;Microscopy;Roads;Telecommunications;Topology;Vehicles;microscopic vehicular mobility;real data;roundabout;simulation;synthetic trace;traffic flows;traffic lights}, 
doi={10.1109/ITST.2015.7377394}, 
month={Dec},}
@INPROCEEDINGS{6545966, 
author={M. Song and M. C. Kim}, 
booktitle={2013 International Conference on Social Intelligence and Technology}, 
title={RT^2M: Real-Time Twitter Trend Mining System}, 
year={2013}, 
pages={64-71}, 
abstract={The advent of social media is changing the existing information behavior by letting users access to real-time online information channels without the constraints of time and space. It also generates a huge amount of data worth discovering novel knowledge. Social media, therefore, has created an enormous challenge for scientists trying to keep pace with developments in their field. Most of the previous studies have adopted broad-brush approaches which tend to result in providing limited analysis. To handle these problems properly, we introduce our real-time Twitter trend mining system, RT2M, which operates in real-time to process big stream datasets available on Twitter. The system offers the functions of term co-occurrence retrieval, visualization of Twitter users by query, similarity calculation between two users, Topic Modeling to keep track of changes of topical trend, and analysis on mention-based user networks. We also demonstrate an empirical study on 2012 Korean presidential election. The case study reveals Twitter could be a useful source to detect and predict the advent and changes of social issues, and analysis of mention-based user networks could show different aspects of user behaviors.}, 
keywords={data mining;data visualisation;information retrieval;politics;social networking (online);Korean presidential election;RT2M;Twitter user visualization;big stream dataset processing;cooccurrence retrieval;information behavior;mention-based user network;novel knowledge discovery;real-time Twitter trend mining system;real-time online information channel;similarity calculation;social issues;social media;topic modeling;topical trend change tracking;user behavior;user query;Communities;Market research;Media;Moon;Nominations and elections;Real-time systems;Twitter;Korean presidential election;network analysis;real-time Twitter trend miming system;social media mining;topic modeling}, 
doi={10.1109/SOCIETY.2013.19}, 
month={May},}
@INPROCEEDINGS{6382847, 
author={R. Pan and G. Xu and P. Dolog and Y. Zong}, 
booktitle={2012 Second International Conference on Cloud and Green Computing}, 
title={Group Division for Recommendation in Tag-Based Systems}, 
year={2012}, 
pages={399-404}, 
abstract={The common usage of tags in these systems is to add the tagging attribute as an additional feature to re-model users or resources over the tag vector space, and in turn, making tag-based recommendation or personalized recommendation. With the help of tagging data, user annotation preference and document topical tendency are substantially coded into the profiles of users or documents. However, obtaining the proper relationship among user, resource and tag is still a challenge in social annotation based recommendation researches. In this paper, we utilize the relationship from between tags and resources and between tags and users to extract group information. With the help of such relationship, we can obtain the Topic-Groups based on the bipartite relationship between tags and resources, and Interest-Groups based on the bipartite relationship between tags and users. The preliminary experiments have been conducted on Movie Lens dataset to compare our proposed approach with the traditional collaborative filtering recommendation approach approach in terms of precision, and the result demonstrates that our approach could considerably improve the performance of recommendations.}, 
keywords={collaborative filtering;recommender systems;user interfaces;Movie Lens dataset;collaborative filtering recommendation approach;document topical tendency;group division;group information extraction;interest-groups;personalized recommendation system;recommendation performance;social annotation based recommendation research;tag-based recommendation system;topic-groups;user annotation preference;Clustering algorithms;Collaboration;Partitioning algorithms;Recommender systems;Semantics;Tagging;Vectors;Interest-Groups;Recommender System;Social Tagging;Topic-Groups}, 
doi={10.1109/CGC.2012.124}, 
month={Nov},}
@ARTICLE{6583164, 
author={J. Liu and Y. Yang and Z. Huang and Y. Yang and H. T. Shen}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={On the Influence Propagation of Web Videos}, 
year={2014}, 
volume={26}, 
number={8}, 
pages={1961-1973}, 
abstract={We propose a novel approach to analyze how a popular video is propagated in the cyberspace, to identify if it originated from a certain sharing-site, and to identify how it reached the current popularity in its propagation. In addition, we also estimate their influences across different websites outside the major hosting website. Web video is gaining significance due to its rich and eye-ball grabbing content. This phenomenon is evidently amplified and accelerated by the advance of Web 2.0. When a video receives some degree of popularity, it tends to appear on various websites including not only video-sharing websites but also news websites, social networks or even Wikipedia. Numerous video-sharing websites have hosted videos that reached a phenomenal level of visibility and popularity in the entire cyberspace. As a result, it is becoming more difficult to determine how the propagation took place - was the video a piece of original work that was intentionally uploaded to its major hosting site by the authors, or did the video originate from some small site then reached the sharing site after already getting a good level of popularity, or did it originate from other places in the cyberspace but the sharing site made it popular. Existing study regarding this flow of influence is lacking. Literature that discuss the problem of estimating a video's influence in the whole cyberspace also remains rare. In this article we introduce a novel framework to identify the propagation of popular videos from its major hosting site's perspective, and to estimate its influence. We define a Unified Virtual Community Space (UVCS) to model the propagation and influence of a video, and devise a novel learning method called Noise-reductive Local-and-Global Learning (NLGL) to effectively estimate a video's origin and influence. Without losing generality, we conduct experiments on annotated dataset collected from a major video sharing site to evaluate the effectiveness of the framework. - urrounding the collected videos and their ranks, some interesting discussions regarding the propagation and influence of videos as well as user behavior are also presented.}, 
keywords={Internet;information retrieval;learning (artificial intelligence);social aspects of automation;NLGL method;UVCS;Web 2.0;Web site;Web videos;Wikipedia;cyberspace;influence propagation;news Web sites;noise-reductive local-and-global learning method;popularity degree;social networks;unified virtual community space;user behavior;video analysis;video-sharing Web sites;Communities;Cyberspace;Encyclopedias;Internet;Social network services;Videos;Clustering;Data mining;Modeling structured;Video influence estimation;and association rules;classification;textual and multimedia data;unified virtual community space;video origin estimation}, 
doi={10.1109/TKDE.2013.142}, 
ISSN={1041-4347}, 
month={Aug},}
@INPROCEEDINGS{7026298, 
author={G. Kotsev and L. T. Nguyen and M. Zeng and J. Zhang}, 
booktitle={6th International Conference on Mobile Computing, Applications and Services}, 
title={User exercise pattern prediction through mobile sensing}, 
year={2014}, 
pages={182-188}, 
abstract={Even though the health benefits of regular exercising are well known, an average person has difficulty maintaining physical activity on a regular basis. One of the main reasons for this is lack of motivation. With their increasing ubiquity, wireless devices and smartphones and their sensing capabilities now can be involved in solving this issue. Many mobile applications have been developed with which people are able to keep track of their exercises, become more aware of their physical condition, and be more motivated. The collected data is also a good source for researchers in understanding the exercise patterns and the main factors influencing people to exercise. Understanding those factors will allow better applications to be built, which helps motivate people. In this work, we quantitatively analyze a dataset collected from over 10,000 users. To better understand the user exercise patterns, we identify a set of factors influencing their exercise patterns. Based on these insights, we develop a prediction model to predict users' future exercise activities.}, 
keywords={bioinformatics;biological techniques;biomechanics;body sensor networks;health care;smart phones;exercise activities;exercise pattern prediction;health benefits;mobile applications;mobile sensing;physical activity;physical condition;regular exercising;sensing capabilities;smartphones;wireless devices;Cities and towns;Mobile communication;Predictive models;Sensors;Time series analysis;Wireless communication;Wireless sensor networks}, 
doi={10.4108/icst.mobicase.2014.257797}, 
month={Nov},}
@INPROCEEDINGS{7017058, 
author={J. Niu and S. Huang and M. Stojmenovic}, 
booktitle={2014 IEEE 33rd International Performance Computing and Communications Conference (IPCCC)}, 
title={Patterns and modeling of group growth in online social networks}, 
year={2014}, 
pages={1-8}, 
abstract={We investigate the group growth in online social networks, by analyzing six different user groups (two million users in total) in Douban Network. The size and longevity of posts in the Douban dataset demonstrate a power-law distribution with exponential cutoff and heavy tail, respectively. The frequency of user interactions follows a two-stage power-law distribution, which can distinguish different types of users. The growth of the number of users and the number of posts/replies generated by the users in a given and same time period, in each group, follow an exponential pattern at the initial stage and oscillate dramatically during the rest of the processes. The number of posts/replies has a power-law relation with the number of active users within a period of time. We propose an empirical growth model, Twisted Growth (TG), to portray the relation between the number of users and the amount of the contents they generated. The model derives equations based on the historical data for deciding coefficients, and the assumtion that the contents in one group will attract new users to join, which will lead to growth of users. Further, the newcomers together with original users will create new contents. We validate our TG model through theoretical analysis and simulations over real datasets.}, 
keywords={social networking (online);Douban network;TG;empirical growth model;group growth;historical data;online social networks;twisted growth;two-stage power-law distribution;user interactions;Analytical models;Communities;Data models;Evolution (biology);Mathematical model;Social network services;Solid modeling;generative models;group growth;online social networks}, 
doi={10.1109/PCCC.2014.7017058}, 
ISSN={1097-2641}, 
month={Dec},}
@INPROCEEDINGS{7371773, 
author={C. Pham}, 
booktitle={2015 Seventh International Conference on Knowledge and Systems Engineering (KSE)}, 
title={MobiRAR: Real-Time Human Activity Recognition Using Mobile Devices}, 
year={2015}, 
pages={144-149}, 
abstract={In this paper we present MobiRAR, a real-time human activity recognition system using mobile devices. The system utilizes the acceleration sensing data from the accelerometer commonly instrumented in mobile devices such as smart phones or smart watches. Our activity recognition method comprises of four steps: data processing, segmentation, feature extraction, and classification. Particularly, the set of features extracted from acceleration sensing data is invariant to device rotations. The proposed method is rigorously evaluated through a dataset consisting of 10 everyday activities including unknown activities collected from 17 users. The results demonstrate that the activities can be distinguished with the overall accuracies of more than 93% precision and recall for individual evaluation and over 80% precision and recall for subject independent evaluation. These results are really promising for practical applications acquiring the recognition of human activities using mobile devices such as energy expenditure estimation and human behavior monitoring.}, 
keywords={accelerometers;estimation theory;feature extraction;gesture recognition;image classification;image segmentation;smart phones;MobiRAR;acceleration sensing data;accelerometer;data processing;energy expenditure estimation;feature extraction;human behavior monitoring;image classification;image segmentation;mobile device;real-time human activity recognition system;smart phone;smart watch;Acceleration;Accelerometers;Feature extraction;Hidden Markov models;Mobile handsets;Real-time systems;Sensors;accelerometer;activity recognition;feature extraction;mobile devices;sensing}, 
doi={10.1109/KSE.2015.43}, 
month={Oct},}
@ARTICLE{7829329, 
author={B. Böse and B. Avasarala and S. Tirthapura and Y. Y. Chung and D. Steiner}, 
journal={IEEE Systems Journal}, 
title={Detecting Insider Threats Using RADISH: A System for Real-Time Anomaly Detection in Heterogeneous Data Streams}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-12}, 
abstract={We present a scalable system for high-throughput real-time analysis of heterogeneous data streams. Our architecture enables incremental development of models for predictive analytics and anomaly detection as data arrives into the system. In contrast with batch data-processing systems, such as Hadoop, that can have high latency, our architecture allows for ingest and analysis of data on the fly, thereby detecting and responding to anomalous behavior in near real time. This timeliness is important for applications such as insider threat, financial fraud, and network intrusions. We demonstrate an application of this system to the problem of detecting insider threats, namely, the misuse of an organization’s resources by users of the system and present results of our experiments on a publicly available insider threat dataset.}, 
keywords={Computational modeling;Computer architecture;Computers;Electronic mail;Monitoring;Real-time systems;Security;Anomaly detection;insider threat;real-time analytics;streaming analytics}, 
doi={10.1109/JSYST.2016.2558507}, 
ISSN={1932-8184}, 
month={},}
@INPROCEEDINGS{5332802, 
author={F. Faradji and R. K. Ward and G. E. Birch}, 
booktitle={2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society}, 
title={A self-paced BCI using stationary wavelet packets}, 
year={2009}, 
pages={962-965}, 
abstract={The stationary wavelet packet analysis is exploited for the first time in the design of a self-paced BCI based on mental tasks. The BCI system is custom designed to achieve a zero false positive rate, as false activations highly restricts the applications of BCIs in real life. The EEG signals of four subjects performing five different mental tasks are used as the dataset. The stationary wavelet packets decompose the signal into eight components. The features used are the autoregressive coefficients obtained by applying autoregressive modeling on the resultant wavelet components. Classification is a two-stage process. The first stage is based on quadratic discriminant analysis which is extremely fast. The second stage is a simple majority voting classifier. During model selection, which is performed via 5-folded cross-validation, the combination of decomposed components and the autoregressive model order that yield the best performance are selected. Results show enhancements in the overall performance for three subjects comparing to our previously designed BCI.}, 
keywords={autoregressive processes;brain-computer interfaces;deconvolution;electroencephalography;pattern classification;statistical analysis;wavelet transforms;EEG signals;autoregressive coefficient;brain-computer interface;quadratic discriminant analysis;self paced BCI design;signal classification;signal decomposition;stationary wavelet packet analysis;Algorithms;Brain;Cognition;Electrocardiography;Evoked Potentials;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Signal Processing, Computer-Assisted;User-Computer Interface}, 
doi={10.1109/IEMBS.2009.5332802}, 
ISSN={1094-687X}, 
month={Sept},}
@INPROCEEDINGS{7745473, 
author={J. Bai and L. Li and D. Zeng and J. Lin}, 
booktitle={2016 IEEE Conference on Intelligence and Security Informatics (ISI)}, 
title={Social role clustering with topic model}, 
year={2016}, 
pages={211-213}, 
abstract={In this paper, we propose a new role analyzing paradigm for social networks enlightened by topic modeling, which can be adopted as a primitive building block in various security related tasks, such as hidden community finding, important person recognizing and so on. We first present the social network under analyzing as a heterogeneous network constructed by both the users and the subjects discussed among them. We then view this network in a Bag-of-Users schema, which mimics its classical Bag-of-Words counterpart. In this schema, the subjects discussed are treated as “documents” while the users are treated as “words” which construct the “documents”. Based on this novel presentation, we finally apply topic modeling technology to perform the social role clustering. Experiments on a practical security-related social network dataset prove the effectiveness of our approach.}, 
keywords={document handling;pattern clustering;security of data;social networking (online);Bag-of-Users schema;heterogeneous network;security-related social network dataset;social role clustering;topic modeling;Analytical models;Computational modeling;Data models;Finance;Heterogeneous networks;Security;Social network services;hidden community;network structure mining;social network;social role;topic model}, 
doi={10.1109/ISI.2016.7745473}, 
month={Sept},}
@INPROCEEDINGS{5705777, 
author={M. K. K. Devi and R. T. Samy and S. V. Kumar and P. Venkatesh}, 
booktitle={2010 IEEE International Conference on Computational Intelligence and Computing Research}, 
title={Probabilistic neural network approach to alleviate sparsity and cold start problems in collaborative recommender systems}, 
year={2010}, 
pages={1-4}, 
abstract={Collaborative Recommender system helps the online users to identify the right product during electronic purchasing. The collaborative recommender system identifies the similar users based on the purchasing or rating behavior to the active user and then recommends the product based on the similar users. Collaborative recommender system is widely used in majority of the existing online recommender system such as orkut, google, amazon, walmart etc. Besides it popularity, is suffers due to sparsity, cold start and scalability recommender system. Extensive research is going on to overcome these problems. In this paper, Probabilistic neural network (PNN) is used to calculate the trust between users based on rating matrix. Using the calculated trust, sparse rating matrix is smoothened, by predicting the rating values of the nonrated items in the rating matrix. Using this smoothened rating matrix, the trust is calculated for online active users. The calculated trust is used to recommend product. Experiments are conducted using dataset such as movielens. Based on the performance metrics, it is proved that the proposed method performs better than the benchmark and some existing systems.}, 
keywords={Internet;consumer behaviour;groupware;neural nets;probability;purchasing;recommender systems;security of data;sparse matrices;cold start;collaborative recommender system;electronic purchasing;nonrated items;online active users;online recommender system;probabilistic neural network;rating behavior;scalability recommender system;smoothened rating matrix;sparse rating matrix;Collaboration;Computational modeling;Mathematical model;Probabilistic logic;Recommender systems;Sparse matrices;Collaborative Recommender System;Probabilistic Neural Networks (PNN);cold start item (new item) problem;cold start user (new user) problem;sparsity problem}, 
doi={10.1109/ICCIC.2010.5705777}, 
month={Dec},}
@INPROCEEDINGS{5152775, 
author={Y. Wei and E. Brunskill and T. Kollar and N. Roy}, 
booktitle={2009 IEEE International Conference on Robotics and Automation}, 
title={Where to go: Interpreting natural directions using global inference}, 
year={2009}, 
pages={3761-3767}, 
abstract={An important component of human-robot interaction is that people need to be able to instruct robots to move to other locations using naturally given directions. When giving directions, people often make mistakes such as labelling errors (e.g., left vs. right) and errors of omission (skipping important decision points in a sequence). Furthermore, people often use multiple levels of granularity in specifying directions, referring to locations using single object landmarks, multiple landmarks in a given location, or identifying large regions as a single location. The challenge is to identify the correct path to a destination from a sequence of noisy, possibly erroneous directions. In our work we cast this problem as probabilistic inference: given a set of directions, an agent should automatically find the path with the geometry and physical appearance to maximize the likelihood of those directions. We use a specific variant of a Markov Random Field (MRF) to represent our model, and gather multi-granularity representation information using existing large tagged datasets. On a dataset of route directions collected in a large third floor university building, we found that our algorithm correctly inferred the true final destination in 47 out of the 55 cases successfully followed by humans volunteers. These results suggest that our algorithm is performing well relative to human users. In the future this work will be included in a broader system for autonomously constructing environmental representations that support natural human-robot interaction for direction giving.}, 
keywords={Markov processes;human-robot interaction;inference mechanisms;Markov random field;broader system;global inference;human-robot interaction;labelling errors;multigranularity representation information;probabilistic inference;Educational institutions;Floors;Geometry;Human robot interaction;Inference algorithms;Labeling;Markov random fields;Robotics and automation;Robustness;Simultaneous localization and mapping}, 
doi={10.1109/ROBOT.2009.5152775}, 
ISSN={1050-4729}, 
month={May},}
@INPROCEEDINGS{7926181, 
author={J. Craley and T. S. Murray and D. R. Mendat and A. G. Andreou}, 
booktitle={2017 51st Annual Conference on Information Sciences and Systems (CISS)}, 
title={Action recognition using micro-Doppler signatures and a recurrent neural network}, 
year={2017}, 
pages={1-5}, 
abstract={This paper explores the long short-term memory (LSTM) recurrent neural network for human action recognition from micro-Doppler signatures. The recurrent neural network model is evaluated using the Johns Hopkins MultiModal Action (JHUMMA) dataset. In testing we use only the active acoustic micro-Doppler signatures. We compare classification performed using hidden Markov model (HMM) systems trained on both micro-Doppler sensor and Kinect data with LSTM classification trained only on the micro-Doppler signatures. For HMM systems we evaluate the performance of product of expert based systems and systems trained on concatenated sensor data. By testing with leave one user out (LOUO) cross-validation we verify the ability of these systems to generalize to new users. We find that LSTM systems trained only on micro-Doppler signatures outperform the other models evaluated.}, 
keywords={Doppler measurement;acoustic signal processing;hidden Markov models;pose estimation;recurrent neural nets;HMM systems;JHUMMA dataset;Johns Hopkins multimodal action dataset;Kinect data;LOUO cross-validation;LSTM classification;LSTM recurrent neural network;acoustic microDoppler signatures;action recognition;expert based systems;hidden Markov model systems;leave one user out cross-validation;long short-term memory recurrent neural network;microDoppler sensor;microDoppler signatures;Hidden Markov models;Legged locomotion;Recurrent neural networks;Spectrogram;Testing;Training;Ultrasonic imaging}, 
doi={10.1109/CISS.2017.7926181}, 
month={March},}
@INPROCEEDINGS{6020048, 
author={J. Wang and J. Yin and Y. Liu and C. Huang}, 
booktitle={2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)}, 
title={Trust-based Collaborative Filtering}, 
year={2011}, 
volume={4}, 
pages={2650-2654}, 
abstract={Collaborative Filtering is one of the most successful techniques of Recommender Systems. Despite its success, similarity-based Collaborative Filtering methods suffer from inherent weakness: users tend to rate few items. As a result, the similarity is not easily computed. This paper aims to solve the above problem by introducing the trust metric into Collaborative Filtering. We develop a novel computation model of trust by incorporating the tastes of users. Then we propagate trust throughout the trust relationship network, and more potential neighbors can be found. At last, we make recommendations based on trust-based Collaborative Filtering. Experimental results on a real extremely sparse dataset have shown best performance of our method in terms of MAE and Coverage when compared with similarity-based Collaborative Filtering methods.}, 
keywords={groupware;information filtering;recommender systems;MAE;recommender system;trust based collaborative filtering;trust computation model;trust metric;trust relationship network;Collaboration;Educational institutions;Measurement;Motion pictures;Recommender systems;Web sites;Collaborative Filtering;Recommender Systems;Tastes;Trust}, 
doi={10.1109/FSKD.2011.6020048}, 
month={July},}
@ARTICLE{6786410, 
author={X. Han and W. Wei and C. Miao and J. P. Mei and H. Song}, 
journal={IEEE Computational Intelligence Magazine}, 
title={Context-Aware Personal Information Retrieval From Multiple Social Networks}, 
year={2014}, 
volume={9}, 
number={2}, 
pages={18-28}, 
abstract={People use a variety of social networking services to collect and organize web information for future reuse. When such contents are actually needed as reference to reply a post in an online conversation, however, the user may not be able to retrieve them with proper cues or may even forget their existence at all. In this paper, we study this problem in the online conversation context and investigate how to automatically retrieve the most context-relevant previously-seen web information without user intervention. We propose a Context-aware Personal Information Retrieval (CPIR) algorithm, which considers both the participatory and implicit-topical properties of the context to improve the retrieval performance. Since both the context and the user's web information are usually short and ambiguous, the participatory context is utilized to formulate and expand the query. Moreover, the implicit-topical context is exploited to implicitly determine the importance of each web information of the targeting user in the given context. The experimental results using real-world dataset demonstrate that CPIR can achieve significant improvements over several baselines.}, 
keywords={Internet;query processing;social networking (online);ubiquitous computing;CPIR algorithm;context-aware personal information retrieval algorithm;implicit-topical property;multiple social network services;online conversation context;participatory context;user Web information;Communities;Computational intelligence;Content management;Context awareness;Context modeling;Information retrieval;Social network services;Web sites}, 
doi={10.1109/MCI.2014.2307222}, 
ISSN={1556-603X}, 
month={May},}
@ARTICLE{7506112, 
author={X. Li and L. Li and F. Flohr and J. Wang and H. Xiong and M. Bernhard and S. Pan and D. M. Gavrila and K. Li}, 
journal={IEEE Transactions on Intelligent Transportation Systems}, 
title={A Unified Framework for Concurrent Pedestrian and Cyclist Detection}, 
year={2017}, 
volume={18}, 
number={2}, 
pages={269-281}, 
abstract={Extensive research interest has been focused on protecting vulnerable road users in recent years, particularly pedestrians and cyclists, due to their attributes of vulnerability. However, comparatively little effort has been spent on detecting pedestrian and cyclist together, particularly when it concerns quantitative performance analysis on large datasets. In this paper, we present a unified framework for concurrent pedestrian and cyclist detection, which includes a novel detection proposal method (termed UB-MPR) to output a set of object candidates, a discriminative deep model based on Fast R-CNN for classification and localization, and a specific postprocessing step to further improve detection performance. Experiments are performed on a new pedestrian and cyclist dataset containing 30 490 annotated pedestrian and 26 771 cyclist instances in over 50 000 images, recorded from a moving vehicle in the urban traffic of Beijing. Experimental results indicate that the proposed method outperforms other state-of-the-art methods significantly.}, 
keywords={driver information systems;least squares approximations;object detection;pedestrians;road safety;Beijing;UB-MPR;concurrent pedestrian detection;cyclist detection;fast R-CNN;quantitative performance analysis;urban traffic;vulnerable road user protection;Benchmark testing;Detectors;Feature extraction;Proposals;Roads;Vehicles;Multiple potential regions;R-CNN;pedestrian and cyclist detection;upper body detection}, 
doi={10.1109/TITS.2016.2567418}, 
ISSN={1524-9050}, 
month={Feb},}
@INPROCEEDINGS{7403715, 
author={A. Popiel and P. Kazienko and T. Kajdanowicz}, 
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
title={MuNeG #x2014; The framework for multilayer network generator}, 
year={2015}, 
pages={1316-1323}, 
abstract={It is a common problem that cost of extracting data for network analysis could be very high. Also sometimes in the Internet is it hard to find graph with desired features such as node degree or clustering level. Because of that graph generators can than be very helpful. In the past bunch of models of such generators was developed: random graphs, small worlds and scale free networks. All of these generators were developed to quickly and efficiently create networks with desired parameters. However all of this models produce single layer graphs. Domain of multiplexes or multilayer graphs has not already been so deeply analysed, also because it is hard to collect multilayer data among real datasets or there is hard to define what kind of information layers exactly should represent. Proposed MuNeG - Multilayer Network Generator can produce, based on set of input parameters, multiplex networks - networks where each node has its counterpart in each layer. The carried out experiments proved that MuNeG graphs have different network and social parameters depends on input values. This feature gives user a very handful tool to generate multiplex networks on purpose of social network or complex network analysis. Generator features, input parameters and their influence on so called graph theory measures such as: node degree, average shortest path, diameter or clustering are described in the following article.}, 
keywords={data analysis;network theory (graphs);pattern clustering;MuNeG graphs;clustering;multilayer network generator;multiplex networks;Data mining;Feature extraction;Generators;Labeling;Multiplexing;Nonhomogeneous media;Social network services;Collective Classification;Complex Networks;Network Generation;Synthethic Dataset}, 
doi={10.1145/2808797.2808902}, 
month={Aug},}
@INPROCEEDINGS{7751621, 
author={C. Zhao and J. Shi and T. Jiang and J. Zhao and J. Chen}, 
booktitle={2016 16th International Symposium on Communications and Information Technologies (ISCIT)}, 
title={Application of deep belief nets for collaborative filtering}, 
year={2016}, 
pages={201-205}, 
abstract={Currently, collaborative filtering (CF) is one of the most popular and successful recommendation technologies. However, the algorithm is easily affected by data sparsity, leading to poor recommendation accuracy. Recently, Deep Belief Nets (DBNs) have been successfully applied in many research areas including image classification and phone recognition. In this paper, to solve the data sparsity problem in CF, we propose a hybrid recommendation model based on DBNs and K-nearest neighbor (KNN) algorithm in which the user-based KNN algorithm makes predictions using the user features extracted by the DBNs. We also present efficient learning and inference methods for this hybrid model and demonstrate that DBNs can be successfully applied to CF. Finally, we carried out several experiments on MovieLens dataset which demonstrate that our hybrid model can achieve better recommendation results than some other CF methods which are widely used.}, 
keywords={belief networks;collaborative filtering;feature extraction;recommender systems;CF methods;DBN;K-nearest neighbor algorithm;KNN algorithm;MovieLens dataset;collaborative filtering;data sparsity problem;deep belief nets;feature extraction;hybrid recommendation model;image classification;inference methods;learning;phone recognition;recommendation technologies;Algorithm design and analysis;Classification algorithms;Collaboration;Decision support systems;Filtering;Inference algorithms;Prediction algorithms;Deep Belief Nets;K-nearest neighbor;Restricted Boltzmann Machines;collaborative filtering;sparsity}, 
doi={10.1109/ISCIT.2016.7751621}, 
month={Sept},}
@INPROCEEDINGS{4494325, 
author={J. Engler}, 
booktitle={IEEE SoutheastCon 2008}, 
title={Mining periodic patterns in manufacturing test data}, 
year={2008}, 
pages={389-395}, 
abstract={Mining of periodic patterns in time series databases is an important data mining problem with many applications. Previous articles have considered the mining of periodic patterns in datasets that range from standard market basket datasets to datasets containing information about the movement activities of cellular phone users. Each of these studies offer solutions to the given domain but lack the ability to address the domain of manufacturing test data. This paper proposes a general model for discovery of periodic patterns within datasets related to the manufacturing of electronic goods. Three general phases are considered. The discretization of the original dataset is first to be discussed, followed by the clustering of the dataset into state related clusters and finally the discovery of periodic patterns in the state transitions of the tests.}, 
keywords={consumer electronics;data mining;mobile handsets;production engineering computing;time series;cellular phone users;data mining problem;electronic goods manufacturing;manufacturing test data;mining periodic patterns in manufacturing test data;time series databases;Cellular phones;Context modeling;Data mining;Databases;Electronic equipment manufacture;Electronic equipment testing;Manufacturing;Phase detection;Software testing;Spatiotemporal phenomena}, 
doi={10.1109/SECON.2008.4494325}, 
ISSN={1091-0050}, 
month={April},}
@INPROCEEDINGS{7152615, 
author={V. Fernández and V. Méndez and T. F. Pena}, 
booktitle={2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing}, 
title={BigDataDIRAC: Deploying Distributed Big Data Applications}, 
year={2015}, 
pages={1177-1180}, 
abstract={The Distributed Infrastructure with Remote Agent Control (DIRAC) software framework allows a user community to manage computing activities in different grid and cloud environments. Many communities from several fields (LHCb, Belle II, Creatis, DIRAC4EGI multiple community portal, etc.) use DIRAC to run jobs in distributed environments. Google created the MapReduce programming model offering an efficient way of performing distributed computation over large data sets. Several enterprises are providing Hadoop cloud based resources to their users, and are trying to simplify the usage of Hadoop in the cloud. Based in these two robust technologies, we have created BigDataDIRAC, a solution which gives users the opportunity to access multiple Big Data resources scattered in different geographical areas, such as access to grid resources. This approach opens the possibility of offering not only grid and cloud to the users, but also Big Data resources from the same DIRAC environment. Proof of concept is shown using three computing centers in two countries, and with four Hadoop clusters. Our results demonstrate the ability of BigDataDIRAC to manage jobs driven by dataset location stored in the Hadoop File System (HDFS) of the Hadoop distributed clusters. DIRAC is used to monitor the execution, collect the necessary statistical data, and upload the results from the remote HDFS to the SandBox Storage machine. The tests produced the equivalent of 5 days continuous processing.}, 
keywords={Big Data;cloud computing;grid computing;statistical analysis;BigDataDIRAC;Google;HDFS;Hadoop cloud based resources;Hadoop distributed clusters;Hadoop file system;MapReduce programming model;SandBox Storage machine;cloud environments;computing activities;distributed big data applications;distributed infrastructure with remote agent control software framework;grid environments;statistical data;user community;Big data;Catalogs;Computer architecture;Monitoring;Physics;Portals;Software;Big Data;Cloud Computing;DIRAC;Hadoop;Hive;MapReduce;Multi-cloud environment}, 
doi={10.1109/CCGrid.2015.109}, 
month={May},}
@INPROCEEDINGS{6890290, 
author={S. Y. Wang and J. C. Wang and Y. H. Yang and H. M. Wang}, 
booktitle={2014 IEEE International Conference on Multimedia and Expo (ICME)}, 
title={Towards time-varying music auto-tagging based on CAL500 expansion}, 
year={2014}, 
pages={1-6}, 
abstract={Music auto-tagging refers to automatically assigning semantic labels (tags) such as genre, mood and instrument to music so as to facilitate text-based music retrieval. Although significant progress has been made in recent years, relatively little research has focused on semantic labels that are time-varying within a track. Existing approaches and datasets usually assume that different fragments of a track share the same tag labels, disregarding the tags that are time-varying (e.g., mood) or local in time (e.g., instrument solo). In this paper, we present a new dataset dedicated to time-varying music auto-tagging. The dataset, called CAL500exp, is an enriched version of the well-known CAL500 dataset used for conventional track-level tagging. Given the tag set of CAL500, eleven subjects with strong music background were recruited to annotate the time-varying tag labels. A new user interface for annotation is developed to reduce the subject's annotation effort yet increase the quality of labels. Moreover, we present an empirical evaluation that demonstrates the performance improvement CAL500exp brings about for time-varying music auto-tagging. By providing more accurate and consistent descriptions of music content in a finer granularity, CAL500exp may open new opportunities to understand and to model the temporal context of musical semantics.}, 
keywords={information retrieval;music;musical instruments;semantic Web;text analysis;user interfaces;CAL500 dataset;CAL500exp dataset;musical semantics;semantic labels;subject annotation effort;text-based music retrieval;time-varying music auto-tagging;time-varying tag labels;track-level tagging;user interface;Instruments;Multiple signal classification;Semantics;Tagging;Training;User interfaces;Vectors;Music auto-tagging;annotation interface;dataset construction;temporal context;time-varying}, 
doi={10.1109/ICME.2014.6890290}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{7860216, 
author={M. A. Habib and M. A. Rakib and M. A. Hasan}, 
booktitle={2016 19th International Conference on Computer and Information Technology (ICCIT)}, 
title={Location, time, and preference aware restaurant recommendation method}, 
year={2016}, 
pages={315-320}, 
abstract={The location-based social networks introduce a platform to understand the users' preferences. In general, the users of the social networks promote the exchange of data within their own networks. Such data are being used in the literature for a wide variety of location-aware recommendation systems. We investigate the role of amalgamation of real-time GPS logs, and historical check-in data in developing a restaurant recommendation system. In this paper, we propose a novel location, time and preference aware restaurant recommendation method using the users' current geospatial location, historical check-in data of the users, and the time of the recommendation request. In the proposed method, users' check-in histories are analyzed individually to discover users' visiting trends, food preference trends, and overall popularity of the restaurants. At the same time, each restaurant's operation time and the distance are modeled separately to compute recommendation scores of the restaurants. The recommendation scores are computed by considering four key factors, namely, i) user's preference scores, ii) the distance of the restaurants, iii) the time of a day, and iv) the popularity scores of the restaurants. Each of these key factors is modeled carefully to estimate realistic recommendation scores for the restaurants in a given geospatial range. We tested our proposed method using an available online dataset. The experimental results confirm the effectiveness of the proposed method.}, 
keywords={Global Positioning System;catering industry;mobile computing;real-time systems;recommender systems;social networking (online);geospatial location;historical check-in data;location aware restaurant recommendation;location-based social networks;preference aware restaurant recommendation;real-time GPS logs;time aware restaurant recommendation;Computational modeling;Data models;Global Positioning System;History;Market research;Probabilistic logic;Social network services;Restaurant recommendations;check-in logs;mutual reinforcement learning;social networks}, 
doi={10.1109/ICCITECHN.2016.7860216}, 
month={Dec},}
@INPROCEEDINGS{7389674, 
author={C. H. Hsieh and W. C. Chao and P. W. Liu and C. W. Li}, 
booktitle={2015 International Carnahan Conference on Security Technology (ICCST)}, 
title={Cyber security risk assessment using an interpretable evolutionary fuzzy scoring system}, 
year={2015}, 
pages={153-158}, 
abstract={An efficient and effective security risk assessment benefits a lot on realizing the potential threats changing, uncovering emergency when maintaining cyber security, and maximize utilization of available resource. However, traditional cyber security risk assessments are usually based on knowledge-driven approach which is suffered from demanding lots of proper domain knowledge and time-consuming human interaction to generate assessment model. In this research, aiming to alleviate the efforts taken by domain experts, we propose a novel interpretable evolutionary fuzzy scoring system, which is innovated in data-driven way, for cyber security risk assessing. The design process of the proposed method is elaborately optimized according to three objectives: accurate, compact, and most important, interpretable. Performance of proposed method is evaluated by both well-known machine learning benchmarks and real cyber security risk assessment dataset. Experimental results deliver insights as followings: 1) The delivered real-valued scoring can successfully quantify the degree of cyber security risk, just like the conventional knowledge-driven methods do. 2) The proposed scoring system can be further modified as a wrapper method to making alert, when given system-suggested or human-specified value as cyber risk alert threshold in advance. 3) The derived scoring system with a compact fuzzy rule base can generate interpretable result that depicts clear data distribution to users.}, 
keywords={evolutionary computation;expert systems;fuzzy set theory;learning (artificial intelligence);risk management;security of data;assessment model;cyber security risk assessing;cyber security risk assessment dataset;data distribution;domain expert;domain knowledge;fuzzy rule base;interpretable evolutionary fuzzy scoring system;knowledge-driven approach;knowledge-driven method;machine learning benchmark;security risk assessment benefit;time-consuming human interaction;wrapper method;Computer security;Linear programming;Optimization;Organizations;Pragmatics;Risk management;Training}, 
doi={10.1109/CCST.2015.7389674}, 
month={Sept},}
@INPROCEEDINGS{6228114, 
author={J. Yao and B. Cui and Z. Xue and Q. Liu}, 
booktitle={2012 IEEE 28th International Conference on Data Engineering}, 
title={Provenance-based Indexing Support in Micro-blog Platforms}, 
year={2012}, 
pages={558-569}, 
abstract={Recently, lots of micro-blog message sharing applications have emerged on the web. Users can publish short messages freely and get notified by the subscriptions instantly. Prominent examples include Twitter, Facebook's statuses, and Sina Weibo in China. The Micro-blog platform becomes a useful service for real time information creation and propagation. However, these messages' short length and dynamic characters have posed great challenges for effective content understanding. Additionally, the noise and fragments make it difficult to discover the temporal propagation trail to explore development of micro-blog messages. In this paper, we propose a provenance model to capture connections between micro-blog messages. Provenance refers to data origin identification and transformation logging, demonstrating of great value in recent database and workflow systems. To cope with the real time micro-message deluge, we utilize a novel message grouping approach to encode and maintain the provenance information. Furthermore, we adopt a summary index and several adaptive pruning strategies to implement efficient provenance updating. Based on the index, our provenance solution can support rich query retrieval and intuitive message tracking for effective message organization. Experiments conducted on a real dataset verify the effectiveness and efficiency of our approach. Provenance refers to data origin identification and transformation monitoring, which has been demonstrated of great value in database and workflow systems. In this paper, we propose a provenance model in micro-blog platforms, and design an indexing scheme to support provenance-based message discovery and maintenance, which can capture the interactions of messages for effective message organization. To cope with the real time micro-message tornadoes, we introduce a novel virtual annotation grouping approach to encode and maintain the provenance information. Furthermore, we design a summary index and adaptive prun- ng strategies to facilitate efficient message update. Based on this provenance index, our approach can support query and message tracking in micro-blog systems. Experiments conducted on real datasets verify the effectiveness and efficiency of our approach.}, 
keywords={Internet;electronic messaging;indexing;query processing;social networking (online);China;Facebook statuses;Sina Weibo;Twitter;World Wide Web;adaptive pruning strategies;data origin identification;database;message grouping approach;message organization;micro-blog message sharing applications;micro-blog messages;micro-blog platforms;provenance-based indexing support;real time information creation;real time information propagation;real time micro-message deluge;rich query retrieval;short messages;support provenance-based message discovery;temporal propagation trail;transformation logging;transformation monitoring;workflow systems;Blogs;Context;Indexing;Media;Noise;Twitter}, 
doi={10.1109/ICDE.2012.36}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{6495333, 
author={M. Wang and X. Zuo and Y. Wang and W. Zuo}, 
booktitle={2012 Fourth International Symposium on Information Science and Engineering}, 
title={A Method of Predicting News Update Time Combining Exponential Smoothing and Naive Bayes}, 
year={2012}, 
pages={227-231}, 
abstract={The time of web page update appears to be erratic, how the user can fast access to valuable information has become one of the hot spots. From the view of application, we can use mathematical models to forecast the update time of news reports, although it can not be completely accurate. In this paper, we proposed a combined predict algorithm for news update. Firstly, we applied the Exponential Smoothing method to our dataset. Secondly, we leveraged the Naive Bayes Model for prediction. Finally, we combined two methods for Combination Forecasting. Through the experiments, we show that Combination Forecasting method outperforms other methods while estimating localized rate of updates.}, 
keywords={Bayes methods;Web sites;forecasting theory;Web page update time;combination forecasting;exponential smoothing method;mathematical model;naive Bayes model;news reports;news update time prediction;update time forecasting;Combination Forecasting;Exponential Smoothing Method;Naive Bayes Model;News Update Time}, 
doi={10.1109/ISISE.2012.57}, 
ISSN={2160-1283}, 
month={Dec},}
@ARTICLE{6547775, 
author={J. F. Delgado Saa and M. Çetin}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={Discriminative Methods for Classification of Asynchronous Imaginary Motor Tasks From EEG Data}, 
year={2013}, 
volume={21}, 
number={5}, 
pages={716-724}, 
abstract={In this work, two methods based on statistical models that take into account the temporal changes in the electroencephalographic (EEG) signal are proposed for asynchronous brain-computer interfaces (BCI) based on imaginary motor tasks. Unlike the current approaches to asynchronous BCI systems that make use of windowed versions of the EEG data combined with static classifiers, the methods proposed here are based on discriminative models that allow sequential labeling of data. In particular, the two methods we propose for asynchronous BCI are based on conditional random fields (CRFs) and latent dynamic CRFs (LDCRFs), respectively. We describe how the asynchronous BCI problem can be posed as a classification problem based on CRFs or LDCRFs, by defining appropriate random variables and their relationships. CRF allows modeling the extrinsic dynamics of data, making it possible to model the transitions between classes, which in this context correspond to distinct tasks in an asynchronous BCI system. On the other hand, LDCRF goes beyond this approach by incorporating latent variables that permit modeling the intrinsic structure for each class and at the same time allows modeling extrinsic dynamics. We apply our proposed methods on the publicly available BCI competition III dataset V as well as a data set recorded in our laboratory. Results obtained are compared to the top algorithm in the BCI competition as well as to methods based on hierarchical hidden Markov models (HHMMs), hierarchical hidden CRF (HHCRF), neural networks based on particle swarm optimization (IPSONN) and to a recently proposed approach based on neural networks and fuzzy theory, the S-dFasArt. Our experimental analysis demonstrates the improvements provided by our proposed methods in terms of classification accuracy.}, 
keywords={Markov processes;brain-computer interfaces;electroencephalography;medical signal processing;neural nets;particle swarm optimisation;signal classification;statistical analysis;S-dFasArt;asynchronous brain-computer interface;asynchronous imaginary motor task;conditional random field;discriminative method;electroencephalographic signal classification;fuzzy theory;hierarchical hidden CRF;hierarchical hidden Markov models;imaginary motor tasks;latent dynamic;neural networks;particle swarm optimization;sequential labeling;static classifiers;statistical models;windowed versions;Brain–computer interface (BCI);brain states;conditional random fields (CRFs);discriminative models;imaginary motor tasks;sensorimotor rhythms;sequential labeling;Algorithms;Brain-Computer Interfaces;Computer Graphics;Data Interpretation, Statistical;Electroencephalography;Humans;Imagination;Linear Models;Models, Neurological;Motor Skills;Psychomotor Performance;User-Computer Interface}, 
doi={10.1109/TNSRE.2013.2268194}, 
ISSN={1534-4320}, 
month={Sept},}
@INPROCEEDINGS{7726690, 
author={M. Tavakolan and C. Menon}, 
booktitle={2016 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)}, 
title={Classification of same arm movements using different kernels learning support vector machine}, 
year={2016}, 
pages={1-4}, 
abstract={A robust model is sought for the identification of electroencephalographic (EEG) signals including movements of three distinct parts of the user's arm, namely hand, elbow and shoulder. This study investigates the classification performances of the same upper limb motor movements using various kernel functions of the support vector machine (SVM). Polynomial, linear and radial basis (RBF) functions were utilized on a dataset of seven able-bodied individuals. The results obtained showed that the RBF kernel function provided superior classification performance compared with the other kernel functions for these specific tasks. The presented approach is promising for implementing mental task classification, especially for identification of the same upper limb movements.}, 
keywords={electroencephalography;medical signal processing;polynomials;radial basis function networks;signal classification;support vector machines;RBF kernel function;arm movements;electroencephalographic signal identification;mental task classification;radial basis functions;robust model;support vector machine;upper limb motor movements;Brain modeling;Electroencephalography;Feature extraction;Kernel;Pattern recognition;Rhythm;Support vector machines;SVM;classification;motor movements}, 
doi={10.1109/CCECE.2016.7726690}, 
month={May},}
@INPROCEEDINGS{5700817, 
author={E. Mower and M. J. Matarić and S. Narayanan}, 
booktitle={2010 IEEE Spoken Language Technology Workshop}, 
title={Robust representations for out-of-domain emotions using Emotion Profiles}, 
year={2010}, 
pages={25-30}, 
abstract={The proper representation of emotion is of vital importance for human-machine interaction. A correct understanding of emotion would allow interactive technology to appropriately respond and adapt to users. In human-machine interaction scenarios it is likely that over the course of an interaction, the human interaction partner will express an emotion not seen during the training of the machine's emotion models. It is therefore crucial to prepare for such eventualities by developing robust representations of emotion that can distinctly represent emotions regardless of whether the data were seen during training of the representation. This novel work demonstrates that an Emotion Profile (EP) representation introduced in [1], a representation composed of the confidences of four binary emotion-specific classifiers, can distinctly represent emotions unseen during training. The classification accuracy increases by only 0.35% over the full dataset when the data excluded from the EP training is included. The results demonstrate that EPs are a robust method for emotion representation.}, 
keywords={emotion recognition;human computer interaction;pattern classification;binary emotion specific classifiers;emotion profiles;human machine interaction;out-of-domain emotions representations;Audio-Visual Emotion;Emotion Classification;Emotion Profiles;Emotion Representation}, 
doi={10.1109/SLT.2010.5700817}, 
month={Dec},}
@INPROCEEDINGS{6717319, 
author={T. Liang and L. Ji and L. Chen and J. Wu and Z. Wu}, 
booktitle={2013 IEEE 6th International Conference on Service-Oriented Computing and Applications}, 
title={Collaborative QoS Prediction via Matrix Factorization and Topic Model}, 
year={2013}, 
pages={282-289}, 
abstract={With the explosive growth of Web services on the Internet, Quality-of-Service-based (QoS) service selection is becoming an important issue of service-oriented computing. The QoS values of services to current users are all supposed to be known in the previous works, while lots of them are not known in reality. In order to predict the missing data, many approaches have been employed in recent years. However, those approaches don't carefully consider the online cold-start scenario where many new registered Web services haven't been involved even once. This paper proposes a collaborative QoS prediction framework named CQP integrating matrix factorization with probabilistic topic model. This approach builds an integral latent user and Web service representative space, and can be applied online to predict QoS value and handle the online cold-start problem. To validate our methods, some approaches and our algorithm are conducted on a real-world dataset. The experiment result demonstrates that the proposed approach outperforms the previous works in prediction accuracy.}, 
keywords={Web services;matrix decomposition;probability;quality of service;service-oriented architecture;CQP integrating matrix factorization;Internet;QoS service selection;Web service representative space;Web services;collaborative QoS prediction framework;integral latent user;online cold-start scenario;prediction accuracy;probabilistic topic model;quality-of-service-based service selection;real-world dataset;service-oriented computing;Collaboration;Predictive models;Probabilistic logic;Quality of service;Vectors;Web services;QoS prediction;Web service;collabrative filtering;topic model}, 
doi={10.1109/SOCA.2013.69}, 
ISSN={2163-2871}, 
month={Dec},}
@INPROCEEDINGS{7028915, 
author={S. Arberet and A. Hutter}, 
booktitle={IEEE PES Innovative Smart Grid Technologies, Europe}, 
title={Non-intrusive load curve disaggregation using sparse decomposition with a translation-invariant boxcar dictionary}, 
year={2014}, 
pages={1-6}, 
abstract={We present a non-intrusive load monitoring (NILM) method based on sparse decomposition techniques in order to extract the individual appliance signals from the aggregated load curve of an household. The proposed method is generic and does not need to be adapted for each home. It is based on a translation-invariant boxcar dictionary of atoms, each of them modeling a complete on-off appliance activity event. We evaluated our algorithm on synthetic and real load curve dataset. Experiments showed that we can extract the individual appliance signals with more than 20 dB of signal-to-distortion ratio (SDR), and estimate the energy consumption of the main consumers with a relative error less than 1%. We think that this ability to automatically provide accurate feedback information on the user appliances consumption through a single point of measurement open new perspective in demand-side management.}, 
keywords={demand side management;sparse matrices;NILM method;SDR;aggregated load curve;demand-side management;energy consumption;nonintrusive load curve disaggregation;signal-to-distortion ratio;sparse decomposition;translation-invariant boxcar dictionary;Dictionaries;Energy consumption;Estimation;Heat pumps;Matching pursuit algorithms;Washing machines;demand side management;energy disaggregation;load management;nonintrusive load monitoring;sparse signal approximation}, 
doi={10.1109/ISGTEurope.2014.7028915}, 
ISSN={2165-4816}, 
month={Oct},}
@INPROCEEDINGS{7363860, 
author={E. Diaz-Aviles and F. Pinelli and K. Lynch and Z. Nabi and Y. Gkoufas and E. Bouillet and F. Calabrese and E. Coughlan and P. Holland and J. Salzwedel}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={Towards real-time customer experience prediction for telecommunication operators}, 
year={2015}, 
pages={1063-1072}, 
abstract={Telecommunications operators (telcos) traditional sources of income, voice and SMS, are shrinking due to customers using over-the-top (OTT) applications such as WhatsApp or Viber. In this challenging environment it is critical for telcos to maintain or grow their market share, by providing users with as good an experience as possible on their network. But the task of extracting customer insights from the vast amounts of data collected by telcos is growing in complexity and scale everey day. How can we measure and predict the quality of a user's experience on a telco network in real-time? That is the problem that we address in this paper. We present an approach to capture, in (near) real-time, the mobile customer experience in order to assess which conditions lead the user to place a call to a telco's customer care center. To this end, we follow a supervised learning approach for prediction and train our Restricted Random Forest model using, as a proxy for bad experience, the observed customer transactions in the telco data feed before the user places a call to a customer care center. We evaluate our approach using a rich dataset provided by a major African telecommunication's company and a novel big data architecture for both the training and scoring of predictive models. Our empirical study shows our solution to be effective at predicting user experience by inferring if a customer will place a call based on his current context. These promising results open new possibilities for improved customer service, which will help telcos to reduce churn rates and improve customer experience, both factors that directly impact their revenue growth.}, 
keywords={Big Data;customer satisfaction;learning (artificial intelligence);telecommunication computing;telecommunication network management;Big Data architecture;Viber;WhatsApp;customer care center;customer insights;customer transactions;mobile customer experience;realtime customer experience prediction;restricted random forest model;supervised learning approach;telco network;telecommunication operators;user experience;Big data;Context;Feeds;Mobile communication;Predictive models;Real-time systems;Big Data;Customer Care;Predictive Analytics;Telecom operators}, 
doi={10.1109/BigData.2015.7363860}, 
month={Oct},}
@INPROCEEDINGS{6014339, 
author={A. Omrani and K. Santhisree and Damodaram}, 
booktitle={2011 IEEE 3rd International Conference on Communication Software and Networks}, 
title={Clustering sequential data with OPTICS}, 
year={2011}, 
pages={591-594}, 
abstract={The Web has enormous, various and knowledgeable data for data mining research. The web is a biggest knowledgeable database with various types of data to mine. One of interesting type of data is user behaviour that mine from server log files. Many algorithms are for clustering and then discover the knowledge from database. In this paper we use OPTICS ("Ordering Points To Identify the Clustering Structure") algorithm to find density based clusters on a social music website data (Last.fm website is a free social platform that share listed music with so different music genres). After pre-processing on music dataset and removing unprofitable data from the dataset was ready to clustering. The clusters are generated by OPTICS algorithm and the average of inter cluster and intra cluster are calculated. Then results are visualized and Euclidean distance measure is used to compare results of intra cluster and inter cluster analyses. Finally showed behavior of clusters that made by OPTICS algorithm on a sequential data.}, 
keywords={data mining;database management systems;music;pattern clustering;social networking (online);Euclidean distance measure;OPTICS;OPTICS algorithm;data mining research;database;density based clusters;intercluster analyses;intracluster analyses;music dataset;music genres;ordering points to identify the clustering structure algorithm;sequential data clustering;server log files;social music website data;visualized distance measure;Adaptation models;Biology;Biomedical optical imaging;Optics;Clustering algorithm OPTICS;Sequence mining}, 
doi={10.1109/ICCSN.2011.6014339}, 
month={May},}
@INPROCEEDINGS{7754251, 
author={N. D. Londhe and M. K. Ahirwal and P. Lodha}, 
booktitle={2016 International Conference on Communication and Signal Processing (ICCSP)}, 
title={Machine learning paradigms for speech recognition of an Indian dialect}, 
year={2016}, 
pages={0780-0786}, 
abstract={Present era is full of speech recognition based services and products. The machine learning paradigms is at the centre stage of speech recognition methodology. Automatic speech recognition (ASR) technology has vastly evolved in recent years including emerging applications in mobile computing, natural user interface, and man-machine assistive technology. In this paper, it's the first time we are presenting ASR designs based on two important machine learning paradigms Artificial Neural Network (ANN) and Support Vector Machine (SVM) for an rare and geographically important Indian dialect `Chhattisgarhi'. The conventional feed-forward ANN and SVM have been applied on the dataset of maximum 50 isolated words of 15 speakers. The performance of these machine learning paradigms is compared with state of art Hidden Markov Model (HMM). The tendency of ASR to be speaker dependent and independent has been extensively investigated with speaker variation experiments. Furthermore the reliability and stability of ASR has been confirmed with numerical validation. The exhaustive review of ASR techniques from the literature along with the ASR systems designed on Indian languages is presented.}, 
keywords={feedforward neural nets;hidden Markov models;learning (artificial intelligence);natural language processing;speech recognition;support vector machines;ASR reliability;ASR stability;Chhattisgarhi;HMM;Indian dialect;Indian languages;SVM;artificial neural network;automatic speech recognition technology;feedforward ANN;hidden Markov model;machine learning;man-machine assistive technology;mobile computing;natural user interface;speaker dependent ASR;speaker independent ASR;speech recognition based products;speech recognition based services;support vector machine;Data mining;Feature extraction;Hidden Markov models;Mel frequency cepstral coefficient;Speech;Speech recognition;Support vector machines;ANN;HMM;MFCC;Machine learning paradigm;SVM}, 
doi={10.1109/ICCSP.2016.7754251}, 
month={April},}
@INPROCEEDINGS{7503714, 
author={R. Minelli and A. Mocci and R. Robbes and M. Lanza}, 
booktitle={2016 IEEE 24th International Conference on Program Comprehension (ICPC)}, 
title={Taming the IDE with fine-grained interaction data}, 
year={2016}, 
pages={1-10}, 
abstract={Integrated Development Environments (IDEs) lack effective support to browse complex relationships between source code elements. As a result, developers are often forced to exploit multiple user interface components at the same time, bringing the IDE into a complex, “chaotic” state. Keeping track of these relationships demands increased source code navigation and cognitive load, leading to productivity deficits documented in observational studies. Beyond small-scale studies, the amount and nature of the chaos experienced by developers in the wild is unclear, and more importantly it is unclear how to tame it. Based on a dataset of fine-grained interaction data, we propose several metrics to characterize and quantify the “level of chaos” of an IDE. Our results suggest that developers spend, on average, more than 30% of their time in a chaotic environment, and that this may affect their productivity. To support developers, we devise and evaluate simple strategies that automatically alter the UI of the IDE. We find that even simple strategies may considerably reduce the level of chaos both in terms of effective space occupancy and time spent in a chaotic environment.}, 
keywords={chaos;programming environments;source code (software);user interfaces;IDE;UI;chaotic environment;cognitive load;fine-grained interaction data;integrated development environment;source code navigation;user interface;Chaos;Context;Data models;Java;Navigation;Productivity;Software}, 
doi={10.1109/ICPC.2016.7503714}, 
month={May},}
@ARTICLE{7188529, 
author={L. Zhang and L. Guo and L. Xu}, 
journal={China Communications}, 
title={Research on e-mail communication network evolution model based on user information propagation}, 
year={2015}, 
volume={12}, 
number={7}, 
pages={108-118}, 
abstract={E-mail communication network evolution model based on user information propagation is studied. First, mathematical representation of weighted e-mail communication network is proposed, and network center parameters of Enron dataset and the distribution of node degree and strength are analyzed. Then, some rules of e-mail communication network evolution are found. Second, the model of e-mail information propagation is described, and e-mail communication network evolution model based on user information propagation is proposed. Lastly, the simulation proves the correctness of the distribution characteristic of degree and strength of the model proposed and then verifies that the model proposed is closer to the real situation of e-mail communication network through parameter comparison. This research provides the basis for other researches on social network evolution and data communication.}, 
keywords={electronic mail;social networking (online);Enron dataset;data communication;distribution characteristic;e-mail communication network evolution model;e-mail information propagation;mathematical representation;network center parameter;node degree;node strength;parameter comparison;social network evolution;user information propagation;weighted e-mail communication network;Communication networks;Electronic mail;Joining processes;Mathematical model;Postal services;Social network services;Vehicle dynamics;e-mail;information propagation;network evolution;social network analysis}, 
doi={10.1109/CC.2015.7188529}, 
ISSN={1673-5447}, 
month={July},}
@INPROCEEDINGS{7837948, 
author={Q. Liu and S. Wu and D. Wang and Z. Li and L. Wang}, 
booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
title={Context-Aware Sequential Recommendation}, 
year={2016}, 
pages={1053-1058}, 
abstract={Since sequential information plays an important role in modeling user behaviors, various sequential recommendation methods have been proposed. Methods based on Markov assumption are widely-used, but independently combine several most recent components. Recently, Recurrent Neural Networks (RNN) based methods have been successfully applied in several sequential modeling tasks. However, for real-world applications, these methods have difficulty in modeling the contextual information, which has been proved to be very important for behavior modeling. In this paper, we propose a novel model, named Context-Aware Recurrent Neural Networks (CA-RNN). Instead of using the constant input matrix and transition matrix in conventional RNN models, CA-RNN employs adaptive context-specific input matrices and adaptive context-specific transition matrices. The adaptive context-specific input matrices capture external situations where user behaviors happen, such as time, location, weather and so on. And the adaptive context-specific transition matrices capture how lengths of time intervals between adjacent behaviors in historical sequences affect the transition of global sequential features. Experimental results show that the proposed CA-RNN model yields significant improvements over state-of-the-art sequential recommendation methods and context-aware recommendation methods on two public datasets, i.e., the Taobao dataset and the Movielens-1M dataset.}, 
keywords={matrix algebra;recommender systems;recurrent neural nets;ubiquitous computing;CA-RNN;adaptive context-specific input matrices;adaptive context-specific transition matrices;constant input matrix;context-aware recurrent neural networks;context-aware sequential recommendation;global sequential features;Adaptation models;Business process re-engineering;Context;Context modeling;History;Mathematical model;Recurrent neural networks}, 
doi={10.1109/ICDM.2016.0135}, 
month={Dec},}
@INPROCEEDINGS{5356540, 
author={F. A. Alsulaiman and N. Sakr and J. J. Valdé and A. E. Saddik and N. D. Georganas}, 
booktitle={2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications}, 
title={Feature selection and classification in genetic programming: Application to haptic-based biometric data}, 
year={2009}, 
pages={1-7}, 
abstract={In this paper, a study is conducted in order to explore the use of genetic programming, in particular gene expression programming (GEP), in finding analytic functions that can behave as classifiers in high-dimensional haptic feature spaces. More importantly, the determined explicit functions are used in discovering minimal knowledge-preserving subsets of features from very high dimensional haptic datasets, thus acting as general dimensionality reducers. This approach is applied to the haptic-based biometrics problem; namely, in user identity verification. GEP models are initially generated using the original haptic biometric datatset, which is imbalanced in terms of the number of representative instances of each class. This procedure was repeated while considering an under-sampled (balanced) version of the datasets. The results demonstrated that for all datasets, whether imbalanced or under-sampled, a certain number (on average) of perfect classification models were determined. In addition, using GEP, great feature reduction was achieved as the generated analytic functions (classifiers) exploited only a small fraction of the available features.}, 
keywords={feature extraction;genetic algorithms;haptic interfaces;pattern classification;analytic function;dimensionality reducers;feature selection;gene expression programming;genetic programming;haptic dataset;haptic-based biometric data;haptic-based biometrics problem;high-dimensional haptic feature space;perfect classification model;Bioinformatics;Biometrics;Computational intelligence;Functional programming;Gene expression;Genetic programming;Haptic interfaces;Information technology;Torque;Virtual environment}, 
doi={10.1109/CISDA.2009.5356540}, 
ISSN={2329-6267}, 
month={July},}
@ARTICLE{7346460, 
author={X. Cai and W. Zhou and L. Wu and J. Luo and H. Li}, 
journal={IEEE Transactions on Multimedia}, 
title={Effective Active Skeleton Representation for Low Latency Human Action Recognition}, 
year={2016}, 
volume={18}, 
number={2}, 
pages={141-154}, 
abstract={With the development of depth sensors, low latency 3D human action recognition has become increasingly important in various interaction systems, where response with minimal latency is a critical process. High latency not only significantly degrades the interaction experience of users, but also makes certain interaction systems, e.g., gesture control or electronic gaming, unattractive. In this paper, we propose a novel active skeleton representation towards low latency human action recognition . First, we encode each limb of the human skeleton into a state through a Markov random field. The active skeleton is then represented by aggregating the encoded features of individual limbs. Finally, we propose a multi-channel multiple instance learning with maximum-pattern-margin to further boost the performance of the existing model. Our method is robust in calculating features related to joint positions, and effective in handling the unsegmented sequences. Experiments on the MSR Action3D, the MSR DailyActivity3D, and the Huawei/3DLife-2013 dataset demonstrate the effectiveness of the model with the proposed novel representation, and its superiority over the state-of-the-art low latency recognition approaches.}, 
keywords={Markov processes;bone;image coding;image recognition;image representation;image sensors;learning (artificial intelligence);random processes;Huawei/3DLife-2013 dataset;MSR Action3D;MSR DailyActivity3D;Markov random field;active skeleton representation;depth sensors;encoded features;interaction systems;low latency 3D human action recognition;maximum-pattern-margin;multichannel multiple instance learning;unsegmented sequence handling;Acceleration;Feature extraction;Markov random fields;Noise measurement;Skeleton;Three-dimensional displays;Videos;Action recognition;Markov random field;depth camera;low latency;multiple instance learning}, 
doi={10.1109/TMM.2015.2505089}, 
ISSN={1520-9210}, 
month={Feb},}
@INPROCEEDINGS{6776194, 
author={D. Agarwal and N. Soni and A. M. Namboodiri}, 
booktitle={2013 Fourth National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG)}, 
title={Salient object detection in SfM point cloud}, 
year={2013}, 
pages={1-4}, 
abstract={In this paper we present a max-flow min-cut based salient object detection in 3D point cloud that results from Structure from Motion (SfM) pipeline. The SfM pipeline generates noisy point cloud due to the unwanted scenes captured along with the object in the image dataset of SfM. The background points being sparse and not meaningful, it becomes necessary to remove them. Hence, any further processes (like surface reconstruction) utilizing the cleaned up model will have no hinderance from the noise removed. We present a novel approach where the camera centers are used to segment out the salient object. The algorithm is completely autonomous and does not need any user input. We test our proposed method on Indian historical models reconstructed through SfM. We evaluate the results in terms of selectivity and specificity.}, 
keywords={image motion analysis;image segmentation;object detection;3D point cloud;Indian historical models;SfM pipeline;SfM point cloud;background points;camera centers;image dataset;max-flow min-cut based salient object detection;salient object segmentation;selectivity;specificity;structure from motion;surface reconstruction;Approximation algorithms;Cameras;Image reconstruction;Image segmentation;Pipelines;Surface morphology;Three-dimensional displays}, 
doi={10.1109/NCVPRIPG.2013.6776194}, 
month={Dec},}
@ARTICLE{7056448, 
author={E. C. Kara and M. Bergés and G. Hug}, 
journal={IEEE Transactions on Smart Grid}, 
title={Impact of Disturbances on Modeling of Thermostatically Controlled Loads for Demand Response}, 
year={2015}, 
volume={6}, 
number={5}, 
pages={2560-2568}, 
abstract={Aggregations of thermostatically controlled loads (TCLs) have been shown to hold promise as demand response resources. However, the evaluation of these promises has relied on simulations of individual TCLs that make important assumptions about the thermal dynamics and properties of the loads, the end-user's interactions with individual TCLs and the disturbances to their operation. In this paper, we first propose a data-driven modeling strategy to simulate individual TCLs-specifically, household refrigeration units (HRUs)-that allows us to relax some of these assumptions and evaluate the validity of the approaches proposed to date. Specifically, we fit probability distributions to a year-long dataset of power measurements for HRUs and use these models to create more realistic simulations. We then derive the aggregate system equations using a bottom-up approach that results in a more flexible [linear time invariant (LTI)] system. Finally, we quantify the plant-model mismatch and evaluate the proposed strategy with the more realistic simulation. Our results show that the effects of invalid assumptions about the disturbances and time-invariant properties of individual HRUs may be mitigated by a faster sampling of the state variables and that, when this is not possible, the proposed LTI system reduces the plant-model mismatch.}, 
keywords={demand side management;refrigerators;statistical distributions;HRU;LTI system;TCL;bottom-up approach;data-driven modeling strategy;demand response;household refrigeration units;linear time invariant system;power measurements;probability distributions;thermostatically controlled loads modeling;Load modeling;Mathematical model;Sociology;Statistics;Switches;Temperature distribution;Thermal resistance;Energy management;energy storage;load modeling}, 
doi={10.1109/TSG.2015.2406316}, 
ISSN={1949-3053}, 
month={Sept},}
@INPROCEEDINGS{6890193, 
author={J. Mo and Y. Feng and A. Jia and S. Huang and Y. Qin and D. Zhao}, 
booktitle={2014 IEEE International Conference on Multimedia and Expo (ICME)}, 
title={Community-based matrix factorization for scalable music recommendation on smartphones}, 
year={2014}, 
pages={1-6}, 
abstract={Mobile karaoke has attracted more attention as a popular mobile entertainment and social network platform, where music recommendations are highly desired to improve its user experiences. Traditional music recommendation methods suffer from the data sparsity issue and usually ignore the social interactions among users. In this paper, we propose a novel parallel community-based matrix factorization method which exploits implicit user behavior data to model user preferences from both social level, via community detection, and individual level. Both offline evaluation on a real dataset from Changba and online traffic investigations show the effectiveness of our method.}, 
keywords={collaborative filtering;entertainment;matrix decomposition;music;recommender systems;smart phones;social networking (online);telecommunication traffic;Changba;community detection;community-based matrix factorization;data sparsity;mobile entertainment;mobile karaoke;online traffic investigation;scalable music recommendation method;smartphone;social network;Communities;Data models;Mobile communication;Predictive models;Recommender systems;Social network services;Training;Collaborative Filtering;MapReduce;Matrix Factorization;Music Recommendation;Recommender Systems;Social Recommendation}, 
doi={10.1109/ICME.2014.6890193}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{4457251, 
author={P. Owotoki and F. Mayer-Lindenberg}, 
booktitle={Sixth International Conference on Machine Learning and Applications (ICMLA 2007)}, 
title={LEONARDO - The computational intelligence (CI) model selection wizard}, 
year={2007}, 
pages={322-329}, 
abstract={The need for tools to aid the selection of the CI models that lie at the heart of many AI systems has never been greater, due to the mainstreaming of data mining and other AI applications. LEONARDO -our contribution to this process- is a recommender system that selects and ranks applicable CI models for a given problem based on the peculiarities of the domain as determined by the user's preferences and dataset characteristics. Leonardo's recommendations are based on two knowledge bases. One contains the description of 65 CI models and provides the Meta knowledge for pruning the space of all CI models to only those applicable to the current task. The second KB contains the performance results of over 200 datasets on the applicable CI models. LEONARDO's ranking is achieved by using the performance information of the k entries, from this KB, nearest in similarity to the new domain dataset.}, 
keywords={data mining;learning (artificial intelligence);AI systems;computational intelligence model;data mining;knowledge based system;recommender system;Application software;Artificial intelligence;Computational intelligence;Computational modeling;Data mining;Heart;Impedance matching;Machine learning;Recommender systems;User interfaces}, 
doi={10.1109/ICMLA.2007.57}, 
month={Dec},}
@INPROCEEDINGS{6128501, 
author={X. Si and A. Yin and X. Huang and X. Yuan and X. Liu and G. Wang}, 
booktitle={2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming}, 
title={Parallel Optimization of Queries in XML Dataset Using GPU}, 
year={2011}, 
pages={190-194}, 
abstract={As XML is playing a crucial role in web services, databases, and document processing, efficient processing of XML queries has become an important issue. On the other hand, due to the increasing number of users, high throughput of XML queries is also required to execute tens of thousands of queries in a short time. Given the great success of GPGPU (General-Purpose computations on the Graphics Processors), we propose a parallel XML query model based on GPU, which mainly consists of two efficient task distribution strategies, to improve the efficiency and throughput of XML queries. We have developed a parallel simplified XPath language using Compute Unified Device Architecture (CUDA) on GPU, and evaluate our model on a recent NVIDIA GPU in comparison with its counterpart on eight-core CPU. The experiment results show that our model achieves both higher throughput and efficiency than CPU-based XML query.}, 
keywords={Web services;XML;graphics processing units;parallel architectures;query processing;CUDA;GPGPU;GPU;NVIDIA GPU;Web services;XML dataset;XML query efficiency improvement;XML query processing;XML query throughput improvement;compute unified device architecture;databases;document processing;general-purpose computations-on-the-graphics processors;parallel XML query model;parallel optimization;parallel simplified XPath language;task distribution strategies;Context;Databases;Graphics processing unit;Load modeling;Throughput;XML;GPU;XML query;parallel optimization}, 
doi={10.1109/PAAP.2011.30}, 
ISSN={2168-3034}, 
month={Dec},}
@ARTICLE{7885122, 
author={B. Cao and X. Liu and M. M. Rahman and B. Li and J. Liu and M. Tang}, 
journal={IEEE Transactions on Services Computing}, 
title={Integrated Content and Network-Based Service Clustering and Web APIs Recommendation for Mashup Development}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={The rapid growth in the number and diversity of Web APIs, coupled with the myriad of functionally similar Web APIs, makes it difficult to find most suitable Web APIs for users to accelerate and accomplish Mashup development. Even if the existing methods show improvements in Web APIs recommendation, it is still challenging to recommend Web APIs with high accuracy and good diversity. In this paper, we propose an integrated content and network-based service clustering and Web APIs recommendation method for Mashup development. This method, first develop a two-level topic model by using the relationship among Mashup services to mine the latent useful and novel topics for better service clustering accuracy. Moreover, based on the clustering results of Mashups, it designs a collaborative filtering (CF) based Web APIs recommendation algorithm. This algorithm, exploits the implicit co-invocation relationship between Web APIs inferred from the historical invocation history between Mashups clusters and the corresponding Web APIs, to recommend diverse Web APIs for each Mashups clusters. The method is expected to not only find much better matched Mashups with high accuracy, but also diversify the recommendation result of Web APIs with full coverage. Finally, based on a real-world dataset from ProgrammableWeb, we conduct a comprehensive evaluation to measure the performance of our method. Compared with existing methods, experimental results show that our method significantly improves the accuracy and diversity of recommendation results in terms of precision, recall, purity, entropy, DCG and HMD.}, 
keywords={Algorithm design and analysis;Clustering algorithms;Electronic mail;History;Mashups;Search engines;Random Walk;Service Content;Service Network;Two-Level Topic Model;Web APIs Recommendation}, 
doi={10.1109/TSC.2017.2686390}, 
ISSN={1939-1374}, 
month={},}
@ARTICLE{4015474, 
author={D. Acevedo and D. Laidlaw}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Subjective Quantification of Perceptual Interactions among some 2D Scientific Visualization Methods}, 
year={2006}, 
volume={12}, 
number={5}, 
pages={1133-1140}, 
abstract={We present an evaluation of a parameterized set of 2D icon-based visualization methods where we quantified how perceptual interactions among visual elements affect effective data exploration. During the experiment, subjects quantified three different design factors for each method: the spatial resolution it could represent, the number of data values it could display at each point, and the degree to which it is visually linear. The class of visualization methods includes Poisson-disk distributed icons where icon size, icon spacing, and icon brightness can be set to a constant or coupled to data values from a 2D scalar field. By only coupling one of those visual components to data, we measured filtering interference for all three design factors. Filtering interference characterizes how different levels of the constant visual elements affect the evaluation of the data-coupled element. Our novel experimental methodology allowed us to generalize this perceptual information, gathered using ad-hoc artificial datasets, onto quantitative rules for visualizing real scientific datasets. This work also provides a framework for evaluating visualizations of multi-valued data that incorporate additional visual cues, such as icon orientation or color}, 
keywords={Poisson distribution;data visualisation;graphical user interfaces;visual perception;2D icon-based visualization methods;2D scientific visualization methods;Poisson-disk distributed icons;ad-hoc artificial datasets;filtering interference;icon brightness;icon size;icon spacing;multivalued data visualization;perceptual interactions;scientific dataset visualization;spatial resolution;visual elements;Brightness;Data visualization;Design optimization;Displays;Filtering;Graphics;Information analysis;Interference;Spatial resolution;Stress;2D visualization methods;Perception models;perceptual interactions;visual design.;visualization evaluation}, 
doi={10.1109/TVCG.2006.180}, 
ISSN={1077-2626}, 
month={Sept},}
@INPROCEEDINGS{6622247, 
author={B. MacKellar and C. Schweikert and S. A. Chun}, 
booktitle={2013 IEEE 12th International Conference on Cognitive Informatics and Cognitive Computing}, 
title={Patient-oriented clinical trials search through semantic integration of Linked Open Data}, 
year={2013}, 
pages={218-225}, 
abstract={Patients facing a serious disease often want to be able to search for relevant clinical trials for new or more effective alternative treatments. The NIH makes all of its trials available on a website, in fact, for this purpose. Its search facility, however, is difficult to use and requires the patient to sift through lengthy text descriptions for relevant information. Our overall aim is to build a system that allows for a more patient-focused clinical trial search facility. In this paper, we present a semantic integration approach using RDF triples to develop an integrated clinical trial knowledge representation, by linking different Linked Open Data such as clinical trials provided by NIH as well as the drug side effects dataset SIDER. The integration model uses UMLS to link concepts from different sources with consistent semantics and ontological knowledge. Patient-oriented functions that our prototype system provides include semantic search and query with reasoning ability, and semantic-link browsing where an exploration of one concept leads to other concepts easily via links which can provide visual search for the end users.}, 
keywords={data integration;inference mechanisms;medical administrative data processing;ontologies (artificial intelligence);query processing;RDF triples;SIDER drug side effects dataset;UMLS;clinical trial knowledge representation;linked open data;ontological knowledge;patient-oriented clinical trial search facility;patient-oriented functions;query-with-reasoning ability;semantic data integration approach;semantic search ability;semantic-link browsing;text descriptions;visual search;Clinical trials;Data models;Drugs;Knowledge representation;Resource description framework;Semantics;Unified modeling language;Linked Open Data;clinical trials;knowledge representation;semantic web}, 
doi={10.1109/ICCI-CC.2013.6622247}, 
month={July},}
@INPROCEEDINGS{7266681, 
author={B. H. Soleimani and E. N. De Souza and C. Hilliard and S. Matwin}, 
booktitle={2015 18th International Conference on Information Fusion (Fusion)}, 
title={Anomaly detection in maritime data based on geometrical analysis of trajectories}, 
year={2015}, 
pages={1100-1105}, 
abstract={Anomaly detection is an important use of the Automatic Identification Systems (AIS), because it offers support to users to evaluate if a vessel is in trouble or causing trouble. For instance, it can be used to detect if a ship is doing something that may cause an accident or if it has changed its route to avoid bad weather condition. In this work, a new method for finding anomalies in the ships' movements is proposed. The method analyzes the trajectory of ships from a geometrical perspective. The trajectory of the ship is compared with a near-optimal path that is generated by a graph search algorithm. The proposed method extracts some scale-invariant features from the real trajectory and also from the optimal movement pattern, and it compares the two sets of features to generate an abnormality score. The method is unsupervised and it does not require training. Instead of labeling the trajectories as normal/abnormal it calculates a score value that denotes the extent of abnormality. The scoring scheme provides a ranking system in which the user can sort the trajectories based on their abnormality score. This is useful when dealing with large number of trajectories and the user wants to picks the most abnormal cases. For the evaluation, the method was run on three months data of North Pacific Ocean and score values were generated. Among the entire dataset, 100 randomly chosen trajectories were labeled by an expert. After applying a threshold on the score value, the proposed method had 94% accuracy.}, 
keywords={graph theory;marine engineering;search problems;ships;AIS;North Pacific Ocean;abnormality score;anomaly detection;automatic identification systems;bad weather condition;geometrical trajectories analysis;graph search algorithm;maritime data;near-optimal path;optimal movement pattern;randomly chosen trajectories;scale-invariant features;score values;ship movements;ship trajectory;Clustering algorithms;Feature extraction;Joining processes;Marine vehicles;Mathematical model;Oceans;Trajectory}, 
month={July},}
@ARTICLE{6684155, 
author={F. Hao and G. Min and M. Lin and C. Luo and L. T. Yang}, 
journal={IEEE Transactions on Parallel and Distributed Systems}, 
title={MobiFuzzyTrust: An Efficient Fuzzy Trust Inference Mechanism in Mobile Social Networks}, 
year={2014}, 
volume={25}, 
number={11}, 
pages={2944-2955}, 
abstract={Mobile social networks (MSNs) facilitate connections between mobile users and allow them to find other potential users who have similar interests through mobile devices, communicate with them, and benefit from their information. As MSNs are distributed public virtual social spaces, the available information may not be trustworthy to all. Therefore, mobile users are often at risk since they may not have any prior knowledge about others who are socially connected. To address this problem, trust inference plays a critical role for establishing social links between mobile users in MSNs. Taking into account the nonsemantical representation of trust between users of the existing trust models in social networks, this paper proposes a new fuzzy inference mechanism, namely MobiFuzzyTrust, for inferring trust semantically from one mobile user to another that may not be directly connected in the trust graph of MSNs. First, a mobile context including an intersection of prestige of users, location, time, and social context is constructed. Second, a mobile context aware trust model is devised to evaluate the trust value between two mobile users efficiently. Finally, the fuzzy linguistic technique is used to express the trust between two mobile users and enhance the human's understanding of trust. Real-world mobile dataset is adopted to evaluate the performance of the MobiFuzzyTrust inference mechanism. The experimental results demonstrate that MobiFuzzyTrust can efficiently infer trust with a high precision.}, 
keywords={fuzzy reasoning;fuzzy set theory;graph theory;mobile computing;security of data;social networking (online);trusted computing;MSN;MobiFuzzyTrust inference mechanism;distributed public virtual social spaces;fuzzy linguistic technique;fuzzy trust inference mechanism;mobile context aware trust model;mobile devices;mobile social networks;mobile users;nonsemantical trust representation;real-world mobile dataset;social links;trust graph;trust models;trust value evaluation;Computational modeling;Context;Context modeling;Mobile communication;Mobile handsets;Pragmatics;Social network services;Mobile social networks;fuzzy inference;linguistic terms;mobile context;trust}, 
doi={10.1109/TPDS.2013.309}, 
ISSN={1045-9219}, 
month={Nov},}
@ARTICLE{7173048, 
author={W. Min and B. K. Bao and C. Xu and M. S. Hossain}, 
journal={IEEE Transactions on Multimedia}, 
title={Cross-Platform Multi-Modal Topic Modeling for Personalized Inter-Platform Recommendation}, 
year={2015}, 
volume={17}, 
number={10}, 
pages={1787-1801}, 
abstract={In this paper, we investigate a novel cross- platform multimedia problem: given two platforms, Flickr and Foursquare, we conduct the recommendation between these two platforms, namely the photo recommendation from Flickr to Foursquare users and the venue recommendation from Foursquare to Flickr users. Such inter-platform recommendations enable users from one single platform to enjoy different recommendation services effectively . To solve the problem, we propose a cross- platform multi-modal topic model ( CM3TM), which is capable of: 1) differentiating between two kinds of topics, i.e., platform- specific topics only relevant to a certain platform and shared topics characterizing the knowledge shared by different platforms and 2) aligning multiple modalities from different platforms. Specifically, CM3TM can not only split the topic space into the shared topic space and platform-specific topic space and learn them simultaneously, but also enable the alignment among different modalities through the learned topic space. Given the location information, we applied the proposed CM3TM into two inter-platform recommendation applications: 1) personalized venue recommendation from Foursquare to Flickr users and 2) personalized image recommendation from Flickr to Foursquare users. We have conducted experiments on the collected large-scale real-world dataset from Flickr and Foursquare. Qualitative and quantitative evaluation results validate the effectiveness of our method and demonstrate the advantage of connecting different platforms with different modalities for the inter-platform recommendation.}, 
keywords={multimedia computing;recommender systems;CM3TM;Flickr;Foursquare;cross-platform multimedia problem;cross-platform multimodal topic modeling;interplatform recommendation applications;interplatform recommendations;personalized image recommendation;personalized interplatform recommendation;personalized venue recommendation;photo recommendation;platform-specific topic space;platform-specific topics;qualitative evaluation;quantitative evaluation;recommendation services;shared topic space;Animals;Bridges;Correlation;Data models;Multimedia communication;Probabilistic logic;Twitter;Cross-platform;recommendation;topic model}, 
doi={10.1109/TMM.2015.2463226}, 
ISSN={1520-9210}, 
month={Oct},}
@INPROCEEDINGS{7799648, 
author={W. Xia and Z. Xu and J. Wei and H. Tian}, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={DQFIRD: Towards Data Quality-Based Filtering and Ranking of Datasets for Data Portals}, 
year={2016}, 
pages={18-23}, 
abstract={The Data on the Web Best Practices Working Group, as part of W3C Data Activity, is standardizing the Data Quality Vocabulary (DQV) for expressing data quality of datasets published on the Web. By exploiting such DQV-based quality metadata associated to the datasets in a data portal, data consumers can achieve data quality-based filtering and ranking of datasets on the portal's conventional search results to obtain desired datasets with high data-quality. Despite the significant progress in standardization, there is a lack of systematic research on approaches and tools for data quality-based filtering and ranking of Web published datasets. This paper therefore proposes a generic software framework for Data Quality-based Filtering and Ranking of Datasets (DQFIRD) in data portals. DQFIRD adopts faceted search (or faceted exploration) techniques to filter the search results of a data portal based on quality metadata about the resulting datasets, and then ranks the filtered datasets according to numeric values of quality measurements in the metadata. We designed the main algorithms of DQFIRD and implemented a prototype of DQFIRD using Java and Jena API. Furthermore, we used the prototype to conduct case study experiments and time efficiency test on the Faceted Taxonomy Materialization (FTM) algorithm, the most time-consuming online operation algorithm in DQFIRD. The results indicate that the proposed DQFIRD approach is implementable and effective, and it has low time complexity because the run-time of the FTM algorithm exhibits approximately a linear growth rate as the size of the relevant dataset quality metadata increases.}, 
keywords={Java;application program interfaces;data handling;information filtering;meta data;portals;vocabulary;DQFIRD;DQV-based quality metadata;FTM algorithm;Java;Jena API;W3C Data Activity;Web Best Practices Working Group;data portals;data quality vocabulary;data quality-based filtering;data quality-based ranking;faceted exploration techniques;faceted search techniques;faceted taxonomy materialization;online operation algorithm;Context;Data models;Prototypes;Resource description framework;User interfaces;Visualization;W3C;Data Quality Vocabulary (DQV);data portal;data quality-based filtering and ranking;datasets;faceted search;quality metadata}, 
doi={10.1109/WISA.2016.14}, 
month={Sept},}
@INPROCEEDINGS{4270326, 
author={P. Srinivasan and J. Shi}, 
booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Bottom-up Recognition and Parsing of the Human Body}, 
year={2007}, 
pages={1-8}, 
abstract={Recognizing humans, estimating their pose and segmenting their body parts are key to high-level image understanding. Because humans are highly articulated, the range of deformations they undergo makes this task extremely challenging. Previous methods have focused largely on heuristics or pairwise part models in approaching this problem. We propose a bottom-up parsing of increasingly more complete partial body masks guided by a parse tree. At each level of the parsing process, we evaluate the partial body masks directly via shape matching with exemplars, without regard to how the parses are formed. The body is evaluated as a whole, not the sum of its constituent parses, unlike previous approaches. Multiple image segmentations are included at each of the levels of the parsing, to augment existing parses or to introduce ones. Our method yields both a pose estimate as well as a segmentation of the human. We demonstrate competitive results on this challenging task with relatively few training examples on a dataset of baseball players with wide pose variation. Our method is comparatively simple and could be easily extended to other objects.}, 
keywords={gesture recognition;image matching;image segmentation;body parts segmentation;bottom-up recognition;human body parsing;multiple image segmentations;parsing process;Belief propagation;Biological system modeling;Humans;Image edge detection;Image recognition;Image segmentation;Laboratories;Shape;Skin;Tree graphs}, 
doi={10.1109/CVPR.2007.383301}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7840960, 
author={G. Xu and J. Qi and D. Huang and M. Daneshmand}, 
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
title={Detecting spammers on social networks based on a hybrid model}, 
year={2016}, 
pages={3062-3068}, 
abstract={The prosperity of social networks provides users with convenient communication but also attracts a large number of spammers. To solve this problem, this paper combines supervised learning and unsupervised learning algorithms, and proposes a novel hybrid model based on OPTICS and SVM. First, we collected a dataset from Sina Weibo including 10,000 users and 134,188 messages; then extracted the content based features and user behavior based features from the dataset; afterwards, we applied the features into the hybrid model to establish the classification model. The experiment shows that the proposed approach is capable of detecting spammers effectively with 87.6% spammers and 94.7% legitimate users correctly classified.}, 
keywords={pattern classification;pattern clustering;social networking (online);support vector machines;unsolicited e-mail;unsupervised learning;OPTICS;SVM;Sina Weibo;classification model;content based feature extraction;hybrid model;legitimate users;social networks;spammer detection;supervised learning;unsupervised learning algorithm;user behavior based feature extraction;Data mining;Feature extraction;Optics;Social network services;Supervised learning;Support vector machines;Unsupervised learning;hybrid model;social network;spammer}, 
doi={10.1109/BigData.2016.7840960}, 
month={Dec},}
@INPROCEEDINGS{6150003, 
author={O. H. Embarak and D. W. Corne}, 
booktitle={2011 Developments in E-systems Engineering}, 
title={Detecting Vicious Users in Recommendation Systems}, 
year={2011}, 
pages={339-344}, 
abstract={Spam and noisy ratings affect the performance of recommendation systems which can lead to incorrect estimations and predictions. The challenge is to discover noisy ratings early in order to isolate its impact. In this paper we suggest an analysis using positive feedback which considers the user's level of confidence, and grades the user from completely honest to complete dishonest. The calculated user's level of confidence is computed based upon the detected level of honesty and affect his ratings. Each domain of ontologies has a calculated region of rejection and non-rejection using each user confidence level, placing his ratings in one region or another and thereby affecting his level of confidence. We used a Movie Lens of 1M ratings dataset to perform the required training. Suggested method has distinguished perfectly between Normal, Excess, Inferiority, and completely dishonest.}, 
keywords={computer crime;ontologies (artificial intelligence);recommender systems;unsolicited e-mail;movie lens;noisy ratings;ontologies;positive feedback;recommendation system;spam;user confidence level;vicious user detection;Equations;Lenses;Mathematical model;Motion pictures;Noise measurement;Robustness;Training;Robustness recommendation systems;personal recommendation;robustness problem;web personalization}, 
doi={10.1109/DeSE.2011.49}, 
month={Dec},}
@INPROCEEDINGS{7344639, 
author={D. A. Salter and A. Tamrakar and B. Siddiquie and M. R. Amer and A. Divakaran and B. Lande and D. Mehri}, 
booktitle={2015 International Conference on Affective Computing and Intelligent Interaction (ACII)}, 
title={The Tower Game Dataset: A multimodal dataset for analyzing social interaction predicates}, 
year={2015}, 
pages={656-662}, 
abstract={We introduce the Tower Game Dataset for computational modeling of social interaction predicates. Existing research in affective computing has focused primarily on recognizing the emotional and mental state of a human based on external behaviors. Recent research in the social science community argues that engaged and sustained social interactions require the participants to jointly coordinate their verbal and non-verbal behaviors. With this as our guiding principle, we collected the Tower Game Dataset consisting of multimodal recordings of two players participating in a tower building game, in the process communicating and collaborating with each other. The format of the game was specifically chosen as it elicits spontaneous communication from the participants through social interaction predicates such as joint attention and entrainment. The dataset will be made public and we believe that it will foster new research in the area of computational social interaction modeling.}, 
keywords={data analysis;social sciences computing;affective computing;computational modeling;computational social interaction modeling;entrainment;joint attention;multimodal dataset;social interaction predicates analysis;tower building game;tower game dataset;Affective computing;Buildings;Computational modeling;Games;Kinematics;Poles and towers;Psychology;Computational Social Psychology;Social Interaction}, 
doi={10.1109/ACII.2015.7344639}, 
month={Sept},}
@INPROCEEDINGS{6020930, 
author={M. Naqvi and A. U. Mailk and S. Razzaq and M. T. Afzal}, 
booktitle={International Conference on Computer Networks and Information Technology}, 
title={Extraction and visualization of citation network}, 
year={2011}, 
pages={205-209}, 
abstract={The digital information is expanding with exponential curve. It becomes difficult to find the relevant information. For example, in scientific domain, to identify the most relevant paper for the focused paper, one needs to explore the focused paper's citations using some citation index such as Google Scholar etc. However, the focused paper may receive 100's and sometimes 1000's of citations. With this huge number of citations, it becomes difficult for users to find the highly relevant papers with respect to the focused paper. While exploring the relevant papers from citations of the focused paper, the user may end up with finding only few. This is because of the fact that authors often cite a paper for providing a background study of the domain and hence cited and cited-by paper have no strong relationship. Currently users need to skim all citations to filter such papers. There is a dire need to develop a system that could automate the process of finding the most relevant papers from the citations. In the current work, we have proposed such a system which ranks the most relevant cited papers for a cited-by paper. The system conceptualizes citations as a graph model. Furthermore, the system employs number of weights to find the most relevant papers. Such weights include topic weight, keyword weight, and author weight. The edge between two nodes in the graph gets a maximum Weight if both are the most relevant papers. Subsequently, a visualization technique has been designed to visualize the graph. We have used the dataset of Journal of Universal Computer Science having 1400 papers and 6090 citations. The manual inspection has revealed that the top ranked papers were actually the most relevant papers.}, 
keywords={citation analysis;data visualisation;graph theory;Google Scholar;Journal of Universal Computer Science dataset;author weight;citation index;citation network extraction;citation network visualization;cited-by paper;digital information;graph model;keyword weight;topic weight;Context;Protocols;Visualization;Citation Graphs;Citation Weights;Information Discovery Citations;Visualization}, 
doi={10.1109/ICCNIT.2011.6020930}, 
ISSN={2223-6317}, 
month={July},}
@INPROCEEDINGS{6567043, 
author={J. Cao and H. Gao and L. E. Li and B. Friedman}, 
booktitle={2013 Proceedings IEEE INFOCOM}, 
title={Enterprise social network analysis and modeling: A tale of two graphs}, 
year={2013}, 
pages={2382-2390}, 
abstract={Like their public counterpart such as Facebook and Twitter, enterprise social networks are poised to revolutionize how people interact in the workplace. There is a pressing need to understand how people are using these social networks. Unlike the public social networks like Facebook or Twitter which are normally characterized using the social graph or the interaction graph, enterprise social networks are also governed by an organization graph. Based on a six month dataset collected from May through October 2011 of a large enterprise social network, we study the characteristics of activities of its enterprise social network. We observe that the user attributes in the organization graph such as geographic location (eg. country) and his/her rank in the company hierarchy have a significant impact on how the user uses the social network and how user interacts with each other. We then build formal statistical models of user interaction graphs in enterprise social network and quantify effects of user attributes from organization graphs on these interactions. Furthermore, as the enterprise social network medium bring users from diverse locations and social status forming ad-hoc communities, our statistical model can be further enhanced by including these ad-hoc communities.}, 
keywords={graph theory;social networking (online);statistical analysis;Facebook;Twitter;enterprise social network analysis;formal statistical model;geographic location;organization graph;public social network;social graph;user interaction graph;Blogs;Communities;Companies;Logistics;Twitter}, 
doi={10.1109/INFCOM.2013.6567043}, 
ISSN={0743-166X}, 
month={April},}
@INPROCEEDINGS{6642447, 
author={R. Wald and T. M. Khoshgoftaar and A. Napolitano and C. Sumner}, 
booktitle={2013 IEEE 14th International Conference on Information Reuse Integration (IRI)}, 
title={Predicting susceptibility to social bots on Twitter}, 
year={2013}, 
pages={6-13}, 
abstract={The popularity of the Twitter social networking site has made it a target for social bots, which use increasingly-complex algorithms to engage users and pretend to be humans. While much research has studied how to identify such bots in the process of spam detection, little research has looked at the other side of the question - detecting users likely to be fooled by bots. In this paper, we examine a dataset consisting of 610 users who were messaged by Twitter bots, and determine which features describing these users were most helpful in predicting whether or not they would interact with the bots (through replies or following the bot). We then use six classifiers to build models for predicting whether a given user will interact with the bot, both using the selected features and using all features. We find that a users' Klout score, friends count, and followers count are most predictive of whether a user will interact with a bot, and that the Random Forest algorithm produces the best classifier, when used in conjunction with one of the better feature ranking algorithms (although poor feature ranking can actually make performance worse than no feature ranking). Overall, these results show promise for helping understand which users are most vulnerable to social bots.}, 
keywords={social networking (online);software agents;unsolicited e-mail;Klout score;Twitter bots;Twitter social networking site;feature ranking algorithms;increasingly-complex algorithms;random forest algorithm;social bots;spam detection;susceptibility prediction;Feature extraction;Measurement;Pragmatics;Predictive models;Support vector machines;Twitter;Twitter;feature selection;social bots}, 
doi={10.1109/IRI.2013.6642447}, 
month={Aug},}
@INPROCEEDINGS{5664676, 
author={W. Chen and S. Fong}, 
booktitle={2010 Fifth International Conference on Digital Information Management (ICDIM)}, 
title={Social network collaborative filtering framework and online trust factors: A case study on Facebook}, 
year={2010}, 
pages={266-273}, 
abstract={Recommender systems have been proposed to exploit the potential of social network by filtering the information and offer recommendations to a user that he is predicted to like. Collaborative Filtering (CF) is believed to be a suitable underlying technique for recommender systems on social network, because CF gathers tastes of similar users; and social network provides such a collaborative social environment. One inherent challenge however for running CF on social network is quantitative estimation of trust between friends. Although many researchers investigated trust metrics and models in social network, none so far has efficiently integrated them in a CF algorithm. The contribution of this paper is a framework of collaborative filtering on social network, and a novel approach in measuring trust factors by data-mining over a survey dataset provided by The Facebook Project. The quantitatively estimated trust factors can be used as input parameters in the CF algorithm. Facebook is taken as a case study here to illustrate the concepts.}, 
keywords={belief networks;information filtering;recommender systems;social networking (online);Facebook;information filtering;online trust factors;recommender systems;social network collaborative filtering;trust metrics;Algorithm design and analysis;Collaboration;Facebook;Measurement;Recommender systems}, 
doi={10.1109/ICDIM.2010.5664676}, 
month={July},}
@INPROCEEDINGS{7469175, 
author={L. Zhao and B. Xiao}, 
booktitle={2015 8th International Symposium on Computational Intelligence and Design (ISCID)}, 
title={Matrix Factorization Based Models Considering Item Categories and User Neighbors}, 
year={2015}, 
volume={2}, 
pages={470-473}, 
abstract={Matrix factorization is a popular collaborative filtering method for recommendation techniques with predictive accuracy and good scalability. In this paper, we propose two models on the basis of basic matrix factorization, namely CW-MF, NICW-MF. CW-MF considers user's preference on item categories and NICW-MF takes into account the impact of user's neighbors to minimize the preference between user and his neighbors. We conduct empirical experiments on MovieLens dataset, and results show that our two models perform well.}, 
keywords={collaborative filtering;matrix decomposition;recommender systems;CW-MF;MovieLens dataset;NICW-MF;collaborative filtering method;item categories;matrix factorization based models;preference minimization;recommendation techniques;user neighbors;Collaboration;Computational modeling;Data models;Filtering;Motion pictures;Predictive models;Presses;collaborative filtering;item categories;matrix factorization;recommendation;user neighbors}, 
doi={10.1109/ISCID.2015.154}, 
month={Dec},}
@INPROCEEDINGS{6729581, 
author={N. Barbieri and F. Bonchi and G. Manco}, 
booktitle={2013 IEEE 13th International Conference on Data Mining}, 
title={Influence-Based Network-Oblivious Community Detection}, 
year={2013}, 
pages={955-960}, 
abstract={How can we detect communities when the social graphs is not available? We tackle this problem by modeling social contagion from a log of user activity, that is a dataset of tuples (u, i, t) recording the fact that user u "adopted" item i at time t. This is the only input to our problem. We propose a stochastic framework which assumes that item adoptions are governed by un underlying diffusion process over the unobserved social network, and that such diffusion model is based on community-level influence. By fitting the model parameters to the user activity log, we learn the community membership and the level of influence of each user in each community. This allows to identify for each community the "key" users, i.e., the leaders which are most likely to influence the rest of the community to adopt a certain item. The general framework can be instantiated with different diffusion models. In this paper we define two models: the extension to the community level of the classic (discrete time) Independent Cascade model, and a model that focuses on the time delay between adoptions. To the best of our knowledge, this is the first work studying community detection without the network.}, 
keywords={learning (artificial intelligence);social networking (online);stochastic processes;classic independent cascade model;community membership learning;community-level influence;diffusion model;diffusion process;influence-based network-oblivious community detection;item adoptions;social contagion modelling;social network;stochastic framework;user activity log;Adaptation models;Communities;Delays;Peer-to-peer computing;Social network services;Standards;Stochastic processes;community detection;social influence}, 
doi={10.1109/ICDM.2013.164}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{7313239, 
author={N. Bicocchi and M. Mamei and A. Sassi and F. Zambonelli}, 
booktitle={2015 IEEE 18th International Conference on Intelligent Transportation Systems}, 
title={Opportunistic Ride Sharing via Whereabouts Analysis}, 
year={2015}, 
pages={875-881}, 
abstract={Smart phones and social networking tools allow to collect large-scale data about mobility habits of people. These data can support advanced forms of sharing, coordination and cooperation possibly able to reduce the overall demand for mobility. We present a methodology, based on the extraction of suitable information from mobility traces, to identify rides along the same trajectories that are amenable for ride sharing. Results on a real dataset show that, assuming users are willing to share rides and tolerate 1Km detours, about 60% of trips could be saved.}, 
keywords={intelligent transportation systems;smart phones;social networking (online);opportunistic ride sharing;smart phones;social networking;whereabouts analysis;Cities and towns;Clustering algorithms;Computational modeling;Correlation;Data mining;Poles and towers;Vehicles}, 
doi={10.1109/ITSC.2015.147}, 
ISSN={2153-0009}, 
month={Sept},}
@INPROCEEDINGS{6123322, 
author={A. E. Abduraman and S. A. Berrani and B. Merialdo}, 
booktitle={2011 IEEE International Symposium on Multimedia}, 
title={Audio Recurrence Contribution to a Video-based TV Program Structuring Approach}, 
year={2011}, 
pages={35-40}, 
abstract={This paper addresses the problem of unsupervised TV programs structuring. Program structuring allows direct and non linear access to the desired parts of a program. Our work addresses the structuring of recurrent TV programs like news, entertainment programs, TV shows, TV magazines. In a previous work we proposed a program structuring method based on the detection of video recurrences. In this paper we extend our study to audio recurrences and verify their influence on the final structuring. We evaluate the structuring results on both approaches (audio and video) separately and jointly. We use for evaluation a 62 hours dataset corresponding to 97 episodes of TV programs.}, 
keywords={audio signal processing;image segmentation;indexing;television applications;video signal processing;TV magazines;TV shows;audio recurrence contribution;automatic video indexing;automatic video segmentation;entertainment programs;news;unsupervised TV programs structuring;video recurrence detection;video-based TV program structuring approach;Detection algorithms;Feature extraction;Games;Hidden Markov models;Particle separators;TV;Visualization;Audio and Video Recurrence Detection;Non-linear Browsing;TV Content Indexing;TV Program Structuring}, 
doi={10.1109/ISM.2011.15}, 
month={Dec},}
@INPROCEEDINGS{6832202, 
author={H. Luo and B. Guo and Zhiwenyu and Z. Wang and Y. Feng}, 
booktitle={2013 IEEE 10th International Conference on High Performance Computing and Communications 2013 IEEE International Conference on Embedded and Ubiquitous Computing}, 
title={Friendship Prediction Based on the Fusion of Topology and Geographical Features in LBSN}, 
year={2013}, 
pages={2224-2230}, 
abstract={Friendship prediction in social networks is useful for various applications, such as friend/place recommendation and privacy management. In this paper, we propose a friendship prediction approach by fusing the topology and geographical features in location based social networks (LBSNs). We investigate the features of users' relationship both online and offline and quantify the contributions of selected features through information gain metric. Three key features are selected, namely user social topology, location category, and check-in location. Friendship is predicted based on the fusion of the selected online/offline features. Three inference models are selected to infer the friendship, including Random Forests, Support Vector Machine (SVM), and Naive Bayes. The proposed approach is validated by intensive empirical evaluations using the collected Foursquare and Jiepang datasets.}, 
keywords={Bayes methods;feature selection;inference mechanisms;learning (artificial intelligence);sensor fusion;social networking (online);support vector machines;topology;Foursquare dataset;Jiepang dataset;LBSN;SVM model;check-in location;empirical evaluations;feature quantification;feature selection;friend recommendation;friendship prediction approach;geographical feature fusion;inference models;information gain metric;location based social networks;location category;naive Bayes model;offline user relationship features;online user relationship features;place recommendation;privacy management;random forest model;support vector machine model;topology feature fusion;user social topology;Analytical models;Data mining;Network topology;Predictive models;Social network services;Support vector machines;Topology;Friendship Prediction;Geographical Features;LBSNs;Online/Offline Interaction;Social Topology}, 
doi={10.1109/HPCC.and.EUC.2013.319}, 
month={Nov},}
@INPROCEEDINGS{6544841, 
author={B. Kanagal and A. Ahmed and S. Pandey and V. Josifovski and L. Garcia-Pueyo and J. Yuan}, 
booktitle={2013 IEEE 29th International Conference on Data Engineering (ICDE)}, 
title={Focused matrix factorization for audience selection in display advertising}, 
year={2013}, 
pages={386-397}, 
abstract={Audience selection is a key problem in display advertising systems in which we need to select a list of users who are interested (i.e., most likely to buy) in an advertising campaign. The users' past feedback on this campaign can be leveraged to construct such a list using collaborative filtering techniques such as matrix factorization. However, the user-campaign interaction is typically extremely sparse, hence the conventional matrix factorization does not perform well. Moreover, simply combining the users feedback from all campaigns does not address this since it dilutes the focus on target campaign in consideration. To resolve these issues, we propose a novel focused matrix factorization model (FMF) which learns users' preferences towards the specific campaign products, while also exploiting the information about related products. We exploit the product taxonomy to discover related campaigns, and design models to discriminate between the users' interest towards campaign products and non-campaign products. We develop a parallel multi-core implementation of the FMF model and evaluate its performance over a real-world advertising dataset spanning more than a million products. Our experiments demonstrate the benefits of using our models over existing approaches.}, 
keywords={advertising;collaborative filtering;matrix decomposition;FMF;advertising campaign;audience selection;campaign products;collaborative filtering techniques;display advertising systems;focused matrix factorization model;parallel multicore implementation;product taxonomy;real-world advertising dataset;user-campaign interaction;users feedback;Advertising;Collaboration;Computational modeling;Mathematical model;Sparse matrices;Taxonomy;Vectors}, 
doi={10.1109/ICDE.2013.6544841}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{6861048, 
author={M. Diaby and E. Viennet}, 
booktitle={2014 IEEE Eighth International Conference on Research Challenges in Information Science (RCIS)}, 
title={Taxonomy-based job recommender systems on Facebook and LinkedIn profiles}, 
year={2014}, 
pages={1-6}, 
abstract={This paper presents taxonomy-based recommender systems that propose relevant jobs to Facebook and LinkedIn users; they are being developed by Work4, a San Francisco-based software company and the Global Leader in Social and Mobile Recruiting that offers Facebook recruitment solutions; to use its applications, Facebook or LinkedIn users explicitly grant access to some parts of their data, and they are presented with the jobs whose descriptions are matching their profiles the most. In this paper, we use the O*NET-SOC taxonomy, a taxonomy that defines the set of occupations across the world of work, to develop a new taxonomy-based vector model for social network users and job descriptions suited to the task of job recommendation; we propose two similarity functions based on the AND and OR fuzzy logic's operators, suited to the proposed vector model. We compare the performance of our proposed vector model to the TF-IDF model using our proposed similarity functions and the classic heuristic measures; the results show that the taxonomy-based vector model outperforms the TF-IDF model. We then use SVMs (Support Vector Machines) with a mechanism to handle unbalanced datasets, to learn similarity functions from our data; the learnt models yield better results than heuristic similarity measures. The comparison of our methods to two methods of the literature (a matrix factorization method and the Collaborative Topic Regression) shows that our best method yields better results than those two methods in terms of AUC. The proposed taxonomy-based vector model leads to an efficient dimensionality reduction method in the task of job recommendation.}, 
keywords={data reduction;fuzzy logic;recommender systems;recruitment;social networking (online);support vector machines;AND fuzzy logic operators;Facebook profiles;LinkedIn profiles;O*NET-SOC taxonomy;OR fuzzy logic operators;SVM;classic heuristic measures;dimensionality reduction method;job descriptions;job recommendation;similarity functions;social network users;support vector machines;taxonomy-based job recommender systems;taxonomy-based vector model;unbalanced dataset handling;Data models;Facebook;LinkedIn;Ontologies;Recommender systems;Vectors;Dimensionality reduction;Facebook;Job recommendation;LinkedIn;O*NET;SVM;Taxonomy}, 
doi={10.1109/RCIS.2014.6861048}, 
ISSN={2151-1349}, 
month={May},}
@INPROCEEDINGS{7829914, 
author={P. Moradi and F. Rezaimehr and S. Ahmadian and M. Jalili}, 
booktitle={2016 Sixteenth International Conference on Advances in ICT for Emerging Regions (ICTer)}, 
title={A trust-aware recommender algorithm based on users overlapping community structure}, 
year={2016}, 
pages={162-167}, 
abstract={Retrieving items in online e-commerce systems with abundance of products is time consuming for users. To deal with this issue, recommender systems (RS) aims to help users by suggesting their interested items in the presence of thousands of products. Generally, RS algorithms are constructed based on similarity between users and/or items (e.g., a user is likely to purchase the same items as his/her most similar users). In this paper, we introduce a novel time-aware recommendation algorithm that is based on overlapping community structure between users. Users' interests might change over time, and thus accurate modelling of dynamic users' tastes is a challenging issue in designing efficient recommendation systems. The users-items interaction network is often highly sparse in real systems, for which many recommenders fail to provide accurate predictions. We apply the proposed algorithm on a benchmark dataset. Our proposed recommendation algorithm overcomes these challenges and show better precision as compared to the state-of-the-art recommenders.}, 
keywords={electronic commerce;information retrieval;purchasing;recommender systems;social networking (online);trusted computing;dynamic user taste modelling;item retrieval;online e-commerce systems;time-aware recommendation;trust-aware recommender algorithm;user interests;user overlapping community structure;user-item interaction network;Algorithm design and analysis;Clustering algorithms;Collaboration;Heuristic algorithms;Prediction algorithms;Recommender systems;Reliability;Recommender algorithms;network science;overlapping community structure;reliability;social networks;trust}, 
doi={10.1109/ICTER.2016.7829914}, 
month={Sept},}
@ARTICLE{7914643, 
author={X. Qian and D. Lu and Y. Wang and L. Zhu and Y. Y. Tang and M. Wang}, 
journal={IEEE Transactions on Image Processing}, 
title={Image Re-ranking based on Topic Diversity}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Social media sharing websites allow users to annotate images with free tags, which significantly contribute to the development of the web image retrieval. Tag-based image search is an important method to find images shared by users in social networks. However, how to make the top ranked result relevant and with diversity is challenging. In this paper, we propose a topic diverse ranking approach for tag-based image retrieval with the consideration of promoting the topic coverage performance. First, we construct a tag graph based on the similarity between each tag. Then community detection method is conducted to mine the topic community of each tag. After that, inter-community and intra-community ranking are introduced to obtain the final retrieved results. In the inter-community ranking process, an adaptive random walk model is employed to rank the community based on the multi-information of each topic community. Besides, we build an inverted index structure for images to accelerate the searching process. Experimental results on Flickr dataset and NUS-Wide datasets show the effectiveness of the proposed approach.}, 
keywords={Adaptation models;Cultural differences;Image retrieval;Semantics;Social network services;Visualization;Image search;Re-ranking;Social Media;Tag-based Image Retrieval;Topic Community}, 
doi={10.1109/TIP.2017.2699623}, 
ISSN={1057-7149}, 
month={},}
@ARTICLE{7782753, 
author={C. M. Salgado and J. L. Viegas and C. S. Azevedo and M. C. Ferreira and S. M. Vieira and J. M. d. C. Sousa}, 
journal={IEEE Transactions on Fuzzy Systems}, 
title={Takagi-Sugeno fuzzy modeling using mixed fuzzy clustering}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={This paper proposes the use of mixed fuzzy clustering (MFC) algorithm to derive Takagi-Sugeno (TS) fuzzy models. Mixed fuzzy clustering handles both time invariant and multivariate time variant features, allowing the user to control the weight of each component in the clustering process. Two model designs based on MFC are investigated. In the first, the antecedent fuzzy sets of the TS model are obtained from the clusters obtained by the MFC algorithm. In the second, fuzzy models based on fuzzy c-means (FCM) are constructed over the input space of the partition matrix generated by MFC. The proposed fuzzy modeling approaches are used in health care classification problems, where time series of unequal lengths are very common. MFC-based TS fuzzy models outperform FCM-based TS fuzzy models in 4 out of 5 datasets and k- Nearest Neighbors classifiers in 5 out of 5 datasets. Dynamic time warping performs better than the Euclidean distance in 1 dataset and similarly in the remaining. Given the different nature of time variant and invariant data, the choice of a clustering algorithm that treats data differently should be considered for model construction.}, 
keywords={Clustering algorithms;Data models;Fuzzy sets;Partitioning algorithms;Prototypes;Takagi-Sugeno model;Time series analysis;Mixed fuzzy clustering;Takagi- Sugeno;dynamic time warping;feature transformation;fuzzy modeling;intensive care units}, 
doi={10.1109/TFUZZ.2016.2639565}, 
ISSN={1063-6706}, 
month={},}
@INPROCEEDINGS{7545993, 
author={G. V. Pedrosa and A. J. M. Traina}, 
booktitle={2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)}, 
title={Encoding Visual Attention Features for Effective Biomedical Images Retrieval}, 
year={2016}, 
pages={235-240}, 
abstract={This paper proposes a novel model, called Similarity Based on Visual Attention Features (SimVisual), to enhance the similarity analysis between images by considering features extracted from salient regions mapped by visual attention models. Visual attention models have demonstrated to be very useful for encoding perceptual semantic information of the image content. Thus, aggregating saliency features into the final image representation is a powerful asset to enhance the similarity analysis between images, while increasing the accuracy in retrieval tasks. The goal of SimVisual is to combine different saliency models with traditional image descriptors, aimed at increasing the descriptive power of these descriptors without modifying the original algorithms. We performed some experiments using a large dataset composed of 32 different biomedical images categories, and the results show that SimVisual boosts the retrieval accuracy up to 13% considering simple image descriptors, such as Color Histograms. The experiments on SimVisual shows that it is a valuable approach to increase the efficacy of content-based image retrieval systems, without user interactions.}, 
keywords={content-based retrieval;feature extraction;image colour analysis;image representation;image retrieval;medical image processing;SimVisual;biomedical image categories;biomedical image retrieval;color histograms;content-based image retrieval systems;feature extraction;image content;image descriptors;image representation;perceptual semantic information encoding;retrieval accuracy;salient regions;similarity analysis enhancement;similarity based on visual attention features;visual attention models;Biological system modeling;Biomedical imaging;Computational modeling;Feature extraction;Image color analysis;Image retrieval;Visualization;image processing;image retrieval;medical image;visual attention features}, 
doi={10.1109/CBMS.2016.22}, 
month={June},}
@ARTICLE{7070713, 
author={Z. Li and Q. Wu and K. Salamatian and G. Xie}, 
journal={IEEE Transactions on Multimedia}, 
title={Video Delivery Performance of a Large-Scale VoD System and the Implications on Content Delivery}, 
year={2015}, 
volume={17}, 
number={6}, 
pages={880-892}, 
abstract={Video delivery performance is the main factor that affects Internet video quality. Characterizing the video delivery performance, especially the delivery throughput, can help content providers as well as Internet service providers (ISPs) in system optimization and network planning. Based on a unique dataset consisting of 20 million video download speed measurements , this paper comprehensively studies the video delivery throughput of a large-scale commercial video-on- demand (VoD) system. We observe that user speed exhibits a large variation over time of day as well as across provincial locations. In particular, the worst performance of day is 30% lower than the peak performance . The analysis also reveals that video download speed has a notable impact on Internet video quality, which in turn influences user engagement . The impact, however, becomes limited when the speed increases beyond a certain threshold, which is mostly dependent on the video encoded bitrates. We further examine the interaction between Internet infrastructure and video delivery throughput using the linear regression model and find that crossing the ISP or regional network border yields 15-20% speed loss. Based on these observations , we finally evaluate the potential of edge caching and hybrid CDN-P2P in the improvement of video download performance and video quality.}, 
keywords={Internet;regression analysis;telecommunication network planning;video on demand;ISP;Internet infrastructure;Internet service providers;Internet video quality;VoD system;content delivery;hybrid CDN-P2P;linear regression model;system optimization;video delivery performance;video download performance;video download speed measurements;video quality;video-on-demand system;Bit rate;Internet;Quality assessment;Servers;Streaming media;Throughput;Video recording;Content delivery;Internet video;delivery performance;measurement}, 
doi={10.1109/TMM.2015.2417771}, 
ISSN={1520-9210}, 
month={June},}
@INPROCEEDINGS{6061134, 
author={J. Chou and K. Wu and Prabhat}, 
booktitle={2011 IEEE International Conference on Cluster Computing}, 
title={FastQuery: A Parallel Indexing System for Scientific Data}, 
year={2011}, 
pages={455-464}, 
abstract={Modern scientific datasets present numerous data management and analysis challenges. State-of-the-art index and query technologies such as FastBit can significantly improve accesses to these datasets by augmenting the user data with indexes and other secondary information. However, a challenge is that the indexes assume the relational data model but the scientific data generally follows the array data model. To match the two data models, we design a generic mapping mechanism and implement an efficient input and output interface for reading and writing the data and their corresponding indexes. To take advantage of the emerging many-core architectures, we also develop a parallel strategy for indexing using threading technology. This approach complements our on-going MPI-based parallelization efforts. We demonstrate the flexibility of our software by applying it to two of the most commonly used scientific data formats, HDF5 and NetCDF. We present two case studies using data from a particle accelerator model and a global climate model. We also conducted a detailed performance study using these scientific datasets. The results show that FastQuery speeds up the query time by a factor of 2.5x to 50x, and it reduces the indexing time by a factor of 16 on 24 cores.}, 
keywords={data analysis;data models;database indexing;message passing;natural sciences computing;parallel processing;query processing;relational databases;FastBit;FastQuery;HDF5;MPI-based parallelization;NetCDF;array data model;data access;data analysis;data management;data reading;data writing;generic mapping mechanism;global climate model;input-output interface;many-core architecture;parallel indexing system;particle accelerator model;query technology;relational data model;scientific data format;scientific dataset;threading technology;user data;Arrays;Data models;Indexing;Layout;Libraries}, 
doi={10.1109/CLUSTER.2011.86}, 
ISSN={1552-5244}, 
month={Sept},}
@INPROCEEDINGS{7387970, 
author={A. Pentel}, 
booktitle={2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA)}, 
title={Employing think-aloud protocol to connect user emotions and mouse movements}, 
year={2015}, 
pages={1-5}, 
abstract={This paper describes unobtrusive method for user confusion detection by monitoring mouse movements. A special computer game was designed to collect mouse logs. Think-aloud protocol was used in order to identify the states of confusion. Features extracted from mouse movement's logs were used in training dataset. Support Vector Machines, Logistic Regression, C4.5 and Random Forest were used to build classification models. Model generated by Random Forest yield to best classification results with f-score 0.938.}, 
keywords={behavioural sciences computing;computer games;mouse controllers (computers);pattern classification;regression analysis;support vector machines;C4.5;computer game;f-score;logistic regression;mouse logs;mouse movements;random forest;support vector machines;think-aloud protocol;unobtrusive method;user confusion detection;user emotions;Brain modeling;Context;Data collection;Games;Mice;Protocols;Standards;affective computing;behavioral biometrics;confusion detection;mouse dynamics;thinking aloud protocol}, 
doi={10.1109/IISA.2015.7387970}, 
month={July},}
@INPROCEEDINGS{7562073, 
author={M. Morsey and A. Willner and R. Loughnane and M. Giatili and C. Papagianni and I. Baldin and P. Grosso and Y. Al-Hazmi}, 
booktitle={2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)}, 
title={DBcloud: Semantic Dataset for the cloud}, 
year={2016}, 
pages={207-212}, 
abstract={In cloud environments, the process of matching requests from users with the available computing resources is a challenging task. This is even more complex in federated environments, where multiple providers cooperate to offer enhanced services, suitable for distributed applications. In order to resolve these issues, a powerful modeling methodology can be adopted to facilitate expressing both the request and the available computing resources. This, in turn, leads to an effective matching between the request and the provisioned resources. For this purpose, the Open-Multinet ontologies were developed, which leverage the expressive power of Semantic Web technologies to describe infrastructure components and services. These ontologies have been adopted in a number of federated testbeds. In this article, DBcloud is presented, a system that provides access to Open-Multinet open data via endpoints. DBcloud can be used to simplify the process of discovery and provisioning of cloud resources and services.}, 
keywords={cloud computing;ontologies (artificial intelligence);resource allocation;semantic Web;DBcloud;Open-Multinet ontologies;Open-Multinet open data;cloud environments;cloud resource discovery;cloud resource provisioning;distributed applications;federated environments;federated testbeds;request matching;semantic Web technologies;semantic dataset;Cloud computing;Data mining;Monitoring;Ontologies;Resource description framework;XML;Interclouds;Linked Open Data;OWL;RDF;infrastructure federation;knowledge extraction;testbeds}, 
doi={10.1109/INFCOMW.2016.7562073}, 
month={April},}
@INPROCEEDINGS{7097999, 
author={Z. Zhang and H. Anada and J. Kawamoto and K. Sakurai}, 
booktitle={2015 IEEE 29th International Conference on Advanced Information Networking and Applications}, 
title={Detection of Illegal Players in Massively Multiplayer Online Role Playing Game by Classification Algorithms}, 
year={2015}, 
pages={406-413}, 
abstract={Online games have become one of the most popular games in recent years. However, fraud such as real money trading and the use of game bot, has also increased accordingly. In order to maintain a balance in the virtual world, the operators of online games have taken a stern response to the players who conduct fraud. In this study, we have sorted out players' behaviors based on players' game playing time in order to support and find potentially illegal players in the MMORPG. In this paper, we added a topic model to the experiment and used k-means as a major tool to classify the players in the World of War craft Avatar History Dataset and find potentially illegal players.}, 
keywords={behavioural sciences;computer games;fraud;pattern classification;MMORPG;World of Warcraft Avatar History Dataset;classification algorithms;fraud;illegal player detection;k-means;massively multiplayer online role playing game;online games;player behaviors;player game playing time;virtual world;Algorithm design and analysis;Avatars;Classification algorithms;Clustering algorithms;Euclidean distance;Games;Resource management;Online game;classify;fraud;k-means;topic model}, 
doi={10.1109/AINA.2015.214}, 
ISSN={1550-445X}, 
month={March},}
@INPROCEEDINGS{7558032, 
author={T. Liang and L. Chen and J. Wu and A. Bouguettaya}, 
booktitle={2016 IEEE International Conference on Web Services (ICWS)}, 
title={Exploiting Heterogeneous Information for Tag Recommendation in API Management}, 
year={2016}, 
pages={436-443}, 
abstract={As web-enabled software becomes the standard for business processes, the ways organizations, partners and customers interface with it have become a critical differentiator in the market place, i.e., API Economy. With the rapid proliferation of APIs, it is increasingly important for users to effectively manage objective APIs in kinds of API markets, e.g., ProgramableWeb (PW), Mashape, etc. In this paper, to facilitate the process of API management, we propose a graphbased recommendation approach called ATRec to automatically assign tags to unlabeled APIs by exploiting both graph structure information and semantic similarity. Specifically, ATRec first leverages the multi-type relations (i.e., among APIs, mashups, and mashup assigned tags) to construct a heterogeneous network, in which a Random Walk with Restart (RWR) model is applied to alleviate the total cold start problem where no API has ever been tagged. Furthermore, we apply the recommended API tags in two API management scenarios (API search, API recommendation). Comprehensive experiments based on a real dataset crawled from PW demonstrate the effectiveness of the proposed approach.}, 
keywords={Internet;application program interfaces;graph theory;random processes;recommender systems;API management;API markets;API recommendation;API search;ATRec;Web-enabled software;business processes;graph structure information;graph-based recommendation;heterogeneous information;heterogeneous network;multitype relations;random walk with restart model;semantic similarity;tag recommendation;unlabeled API;Business;Collaboration;Heterogeneous networks;Mashups;Semantics;Tagging;Tensile stress}, 
doi={10.1109/ICWS.2016.63}, 
month={June},}
@ARTICLE{7805258, 
author={N. Ahmidi and L. Tao and S. Sefati and Y. Gao and C. Lea and B. Bejar and L. Zappella and S. Khudanpur and R. Vidal and G. D. Hager}, 
journal={IEEE Transactions on Biomedical Engineering}, 
title={A Dataset and Benchmarks for Segmentation and Recognition of Gestures in Robotic Surgery}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Objective: State-of-the-art techniques for surgical data analysis report promising results for automated skill assessment and action recognition. The contributions of many of these techniques, however, are limited to study-specific data and validation metrics, making assessment of progress across the field extremely challenging. Methods: In this paper, we address two major problems for surgical data analysis: (1) lack of uniform shared datasets and benchmarks and (2) lack of consistent validation processes. We address the former by presenting the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a public dataset we have created to support comparative research benchmarking. JIGSAWS contains synchronized video and kinematic data from multiple performances of robotic surgical tasks by operators of varying skill. We address the latter by presenting a welldocumented evaluation methodology and reporting results for six techniques for automated segmentation and classification of time-series data on JIGSAWS. These techniques comprise four temporal approaches for joint segmentation and classification: Hidden Markov Model, Sparse HMM, Markov semi-Markov Conditional Random Field, and Skip-Chain CRF; and two feature-based ones that aim to classify fixed segments: Bag of spatiotemporal Features and Linear Dynamical Systems. Results: Most methods recognize gesture activities with approximately 80% overall accuracy under both leave-one-supertrial- out and leave-one-user-out cross-validation settings. Conclusion: Current methods show promising results on this shared dataset, but room for significant progress remains, particularly for consistent prediction of gesture activities across different surgeons. Significance: The results reported in this paper provide the first systematic and uniform evaluation of surgical activity recognition techniques on the benchmark database.}, 
keywords={Benchmark testing;Hidden Markov models;Kinematics;Measurement;Needles;Robots;Surgery;Benchmark robotic dataset;BoF;CRF;HMM;LDS;MsM;activity recognition;kinematics and video;surgical motion}, 
doi={10.1109/TBME.2016.2647680}, 
ISSN={0018-9294}, 
month={},}
@INPROCEEDINGS{5684656, 
author={R. Islam and Y. Xiang}, 
booktitle={2010 5th International ICST Conference on Communications and Networking in China}, 
title={Email classification using data reduction method}, 
year={2010}, 
pages={1-5}, 
abstract={Classifying user emails correctly from penetration of spam is an important research issue for anti-spam researchers. This paper has presented an effective and efficient email classification technique based on data filtering method. In our testing we have introduced an innovative filtering technique using instance selection method (ISM) to reduce the pointless data instances from training model and then classify the test data. The objective of ISM is to identify which instances (examples, patterns) in email corpora should be selected as representatives of the entire dataset, without significant loss of information. We have used WEKA interface in our integrated classification model and tested diverse classification algorithms. Our empirical studies show significant performance in terms of classification accuracy with reduction of false positive instances.}, 
keywords={data reduction;information filtering;pattern classification;unsolicited e-mail;WEKA interface;data filtering method;data reduction method;instance selection method;spam;user emails classification;Accuracy;Classification algorithms;Electronic mail;Feature extraction;Filtering;Support vector machines;Training}, 
doi={10.4108/chinacom.2010.59}, 
month={Aug},}
@ARTICLE{7458855, 
author={G. Zhao and X. Qian and C. Kang}, 
journal={IEEE Transactions on Big Data}, 
title={Service Rating Prediction by Exploring Social Mobile Users #x2019; Geographical Locations}, 
year={2017}, 
volume={3}, 
number={1}, 
pages={67-78}, 
abstract={Recently, advances in intelligent mobile device and positioning techniques have fundamentally enhanced social networks, which allows users to share their experiences, reviews, ratings, photos, check-ins, etc. The geographical information located by smart phone bridges the gap between physical and digital worlds. Location data functions as the connection between user's physical behaviors and virtual social networks structured by the smart phone or web services. We refer to these social networks involving geographical information as location-based social networks (LBSNs). Such information brings opportunities and challenges for recommender systems to solve the cold start, sparsity problem of datasets and rating prediction. In this paper, we make full use of the mobile users’ location sensitive characteristics to carry out rating prediction. We mine: 1) the relevance between user's ratings and user-item geographical location distances, called as user-item geographical connection, 2) the relevance between users’ rating differences and user-user geographical location distances, called as user-user geographical connection. It is discovered that humans’ rating behaviors are affected by geographical location significantly. Moreover, three factors: user-item geographical connection, user-user geographical connection, and interpersonal interest similarity, are fused into a unified rating prediction model. We conduct a series of experiments on a real social rating network dataset Yelp. Experimental results demonstrate that the proposed approach outperforms existing models.}, 
keywords={Big data;Mobile communication;Predictive models;Recommender systems;Smart phones;Social network services;Geographical location;location-based social networks;rating prediction;recommender system}, 
doi={10.1109/TBDATA.2016.2552541}, 
month={March},}
@INPROCEEDINGS{7724494, 
author={D. Phukan and A. K. Singha}, 
booktitle={2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom)}, 
title={Feasibility analysis for popularity prediction of stack exchange posts based on its initial content}, 
year={2016}, 
pages={1397-1402}, 
abstract={In technology related question and answer websites, people working on corresponding technologies post questions about the problems that they encounter and seek answers to them. The popular posts having greater views that stay active for longer periods of time are the ones that occur commonly for many users. The objective of this paper is to predict such posts at its arrival time. Using the prediction results, the problems that will gain popularity over time can be addressed before they become very common. The posts in question and answer websites have a longer lifespan than other online content such as news articles which are short lived and have a high user activity at the beginning that decrease with time. This indicates that the user activity in question and answer websites is not an accurate measure of popularity in an early stage. In this paper, we have used the initial content in a post as the predictors of popularity rather than using conventional methods such as shares, comments etc. We used three models to predict the popularity using the dataset of android.stackexchange.com, a popular question and answer website for android developers.}, 
keywords={Android (operating system);Web sites;question answering (information retrieval);android developers;android.stackexchange.com;feasibility analysis;initial content;online content;popularity prediction;question and answer Web sites;stack exchange posts;Electronic mail;Niobium;Smoothing methods;Social network services;Software engineering;Support vector machines;Training;Classification;Linear Models;Machine learning;Popularity;Prediction;Predictive models;Question and Answer websites}, 
month={March},}
@INPROCEEDINGS{6225617, 
author={M. Novák and M. Biňas and F. Jakab}, 
booktitle={2012 ELEKTRO}, 
title={Unobtrusive anomaly detection in presence of elderly in a smart-home environment}, 
year={2012}, 
pages={341-344}, 
abstract={In the paper we present a method for anomaly detection in user's activities utilizing data from unobtrusive sensors. A service for a smart-home environment using this method adapts to behaviour of a user and may provide alarms to a carer or other responsible person if unusual activity is detected. As an unusual activity we consider: long periods of inactivity, lacking activity, unusual presence and changes in daily activity patterns. Anomaly detection is based on a composition of unsupervised classification technique Self Organizing Maps and next activity prediction employing Markov model. Finally we present a short experimental study realized on a dataset provided by MavHome project.}, 
keywords={Markov processes;geriatrics;handicapped aids;home automation;home computing;pattern classification;self-organising feature maps;sensors;unsupervised learning;Markov model;MavHome project;elderly;next activity prediction;self organizing maps;smart home environment;unobtrusive anomaly detection;unobtrusive sensor;unsupervised classification technique;unusual activity;user activity;Markov processes;Monitoring;Predictive models;Self organizing feature maps;Senior citizens;Sensors;Vectors;Markov model;Self Organizing Maps;anomaly detection;behavioural patterns;classification;smart-home}, 
doi={10.1109/ELEKTRO.2012.6225617}, 
month={May},}
@INPROCEEDINGS{6511671, 
author={X. Li and T. Murata}, 
booktitle={2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology}, 
title={Using Multidimensional Clustering Based Collaborative Filtering Approach Improving Recommendation Diversity}, 
year={2012}, 
volume={3}, 
pages={169-174}, 
abstract={In this paper, we present a hybrid recommendation approach for discovering potential preferences of individual users. The proposed approach provides a flexible solution that incorporates multidimensional clustering into a collaborative filtering recommendation model to provide a quality recommendation. This facilitates to obtain user clusters which have diverse preference from multi-view for improving effectiveness and diversity of recommendation. The presented algorithm works in three phases: data preprocessing and multidimensional clustering, choosing the appropriate clusters and recommending for the target user. The performance of proposed approach is evaluated using a public movie dataset and compared with two representative recommendation algorithms. The empirical results demonstrate that our proposed approach is likely to trade-off on increasing the diversity of recommendations while maintaining the accuracy of recommendations.}, 
keywords={information filtering;recommender systems;collaborative filtering recommendation model;data preprocessing;diverse preference;hybrid recommendation;multidimensional clustering based collaborative filtering;public movie dataset;quality recommendation;recommendation diversity;representative recommendation algorithm;collaborative filtering;multidimensional clustering;recommendation diversity;recommender systems}, 
doi={10.1109/WI-IAT.2012.229}, 
month={Dec},}
@INPROCEEDINGS{7449648, 
author={P. K. Chavda and J. S. Dhobi}, 
booktitle={2015 5th Nirma University International Conference on Engineering (NUiCONE)}, 
title={Web users browsing behavior prediction by implementing support vector machines in MapReduce using cloud based Hadoop}, 
year={2015}, 
pages={1-6}, 
abstract={The motivation behind the work is that the prediction of web user's browsing behavior while serving the Internet, reduces the user's browsing access time and avoids the visit of unnecessary pages to ease network traffic. This research work introduces parallel Support Vector Machines for web page prediction. The web contains an enormous amount of data and web data increases exponentially, but the training time for Support vector machine is very large. That is, SVM's suffer from a widely recognized scalability problems in both memory requirements and computation time when the input dataset is too large. To address this, we aimed at training the Support vector machine model in MapReduce programming model of Hadoop framework, since the MapReduce programming model has the ability to rapidly process a large amount of data in parallel. MapReduce works in tandem with Hadoop Distributed File System (HDFS). The so proposed approach will solve the scalability problem of present SVM algorithm. The performance of the proposed approach is evaluated in Amazon cloud EC2 using cloud-based Hadoop. Our experiments show the effectiveness in term of training time and also improve the preprocessing time. We find in our research study that a number of nodes increased the training time of proposed algorithm is decreased. We checked that parallelization of SMO has no more negative effect on the accuracy level, as compared to the standard approach.}, 
keywords={Web sites;behavioural sciences computing;cloud computing;data handling;network operating systems;parallel processing;support vector machines;Amazon cloud EC2;HDFS;Hadoop distributed file system;MapReduce;MapReduce programming model;SMO parallelization;SVM algorithm;Web page prediction;Web users browsing behavior prediction;cloud based Hadoop;memory requirements;parallel data processing;parallel support vector machines;preprocessing time;scalability problems;training time;Classification algorithms;Markov processes;Prediction algorithms;Predictive models;Support vector machines;Training;Web pages;Amazon EC2;HDFS;Hadoop;MapReduce;Support Vector Machines;Web Page Prediction}, 
doi={10.1109/NUICONE.2015.7449648}, 
month={Nov},}
@INPROCEEDINGS{7866132, 
author={Y. Zuo and K. Zhang}, 
booktitle={2016 IEEE First International Conference on Data Science in Cyberspace (DSC)}, 
title={Using Structural Features to Characterize Social Ties}, 
year={2016}, 
pages={235-242}, 
abstract={Social ties are crucial to understand group behaviors in social networks. Since users rarely label their friends explicitly in social networks, characterizing social ties and understanding their strength of these ties is a critical problem. In this paper, we apply logistic regression model to infer the strength of social ties based on the structural features of social networks, and we produce two new features: cliqueness and linkness. In addition, we use the Strong Triadic Closure principle as a global constraint. We test our model on four different datasets: Facebook, American College Football, Les Miserables and Zachary's Karate Club. Exper-imental results demonstrate that the performance of our model with structural features is satisfactory.}, 
keywords={behavioural sciences;regression analysis;social networking (online);American College Football dataset;Facebook dataset;Les Miserables dataset;Zachary Karate Club dataset;cliqueness;global constraint;group behaviors;linkness;logistic regression model;social networks;social ties characterization;social ties strength;strong triadic closure principle;structural features;Adaptation models;Dispersion;Facebook;Labeling;Logistics;Psychology;logistic regression model;social networks;social ties;structural features}, 
doi={10.1109/DSC.2016.63}, 
month={June},}
@INPROCEEDINGS{7324374, 
author={Y. Huang and P. Greve}, 
booktitle={2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)}, 
title={Large scale graph mining for web reputation inference}, 
year={2015}, 
pages={1-6}, 
abstract={The explosion of the number of devices and users on the Internet results in massive amounts of data and information. This poses the most complex challenges in security we have ever faced. The detection of malicious domains and Internet protocol (IP) addresses has been a hot topic in cyber security. We present a scalable and effective graph inference system for detecting malicious domains and IP addresses. The goal is to protect Internet users from network threats. Based on the loopy belief propagation algorithm, the system infers every domain or IP reputation, flagging it with high reputation as malicious one. We have evaluated the system with 75 million-node graph constructed from the huge dataset (500 gigabytes). The system attains performance with 86% and 87% area under receiver operating curves for inferring domain and IP reputations respectively. We demonstrate that the graphical solution provides rapid assessment of safe or risky sites on McAfee's data. It provides an automatic tool for web reputation inference in the field and serves as an assisting tool for “first pass” classification and triaging.}, 
keywords={IP networks;Internet;computer network security;data mining;graph theory;IP addresses;IP reputations;Internet protocol addresses;Web reputation inference;cyber security;first pass classification;graph inference system;graph mining;loopy belief propagation algorithm;malicious domain detection;network threats;Bipartite graph;Data mining;IP networks;Internet;Testing;Training;Uniform resource locators;Big Data analysis;Graphical modeling;threat detection;web reputation}, 
doi={10.1109/MLSP.2015.7324374}, 
ISSN={1551-2541}, 
month={Sept},}
@INPROCEEDINGS{7736905, 
author={N. Gonzalez and E. P. Calot and J. S. Ierache}, 
booktitle={2016 International Conference of the Biometrics Special Interest Group (BIOSIG)}, 
title={A Replication of Two Free Text Keystroke Dynamics Experiments under Harsher Conditions}, 
year={2016}, 
pages={1-6}, 
abstract={Replication of experiments lies at the very core of the scientific process. In spite of this, the relevance and relative count of replication studies has fallen sharply in the recent past in several disciplines and a similar trend is apparent in computing science and software engineering. Keystroke dynamics studies have not been exempted from this rule, where replication studies are still no more than a handful and have often lead to astonishingly varying error rates in comparison with the original study, both for static passwords and free texts. The effect of the typing environment and the user emotional state is obviously a concern for free text analysis, as much as his physiological states like stress and tiredness. Thus, the real world performance of a certain method in a specific setting could vary drastically compared with the laboratory setting even though the latter might not be consciously biased. In this paper we examine and compare the performance of two techniques for keystroke dynamics analysis in a free text dataset under evaluation conditions which are harsher than the originally used: the free text extension to the R and A distances method of Bergadano, Gunetti and Picardi and the authors' method based on finite context modeling. Some of their key properties proved to be extrapolatable outside the realm of ideal conditions; the impostor pass rate of A and R distances showed little change, combined distances proved consistently better than pure ones and their best combination remained the same. Others did not, like the relative efficacy of n-graphs, the performance of A distances and -expectedly- the false alarm and correct classification rate for both methods.}, 
keywords={biometrics (access control);graph theory;text analysis;finite context modeling;free text analysis;free text keystroke dynamics experiment replication;harsher conditions;n-graph efficacy;physiological states;static passwords;typing environment;user emotional state;Authentication;Biometrics (access control);Error analysis;Keyboards;Length measurement;Measurement uncertainty;Terminology}, 
doi={10.1109/BIOSIG.2016.7736905}, 
month={Sept},}
@INPROCEEDINGS{7934910, 
author={M. Iyengar and A. Sarkar and S. Singh}, 
booktitle={2017 7th International Conference on Modeling, Simulation, and Applied Optimization (ICMSAO)}, 
title={A Collaborative Filtering based model for recommending graduate schools}, 
year={2017}, 
pages={1-5}, 
abstract={Recently, with the surge of students pursuing graduate studies after completing their bachelors, there is a lack of open source resources which could point out universities and programs, based on an individual's profile. In this paper, we present our novel approach of predicting universities for graduate studies based on one's whole profile. A model is built which is able to predict the list of top-‘n’ universities based on the user profile. In our implementation, a user profile comprises one's undergraduate grades, graduate examination scores of GRE or GMAT, other exams like TOEFL, research experience and publications, work experience, and number of relevant projects. Data is collected from a variety of sources comprising peoples' profiles, who got through to graduate programs and is fed into the model, in order to serve as benchmark for an incoming query. For the model to predict the list of universities best suiting the user, Collaborative Filtering is used in order to compare the user's profile to the existing dataset. The output is a list of universities, to which an individual could apply to, based on the profile.}, 
keywords={Collaboration;Computer science;Correlation;Filtering;Filtering algorithms;Measurement;Predictive models;Collaborative Filtering;Cosine Similarity;Euclidean Distance;Pearson Coefficient;ratings matrix;recommendation engine}, 
doi={10.1109/ICMSAO.2017.7934910}, 
month={April},}
@INPROCEEDINGS{7477732, 
author={N. Lee and K. M. Kitani}, 
booktitle={2016 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
title={Predicting wide receiver trajectories in American football}, 
year={2016}, 
pages={1-9}, 
abstract={Predicting the trajectory of a wide receiver in the game of American football requires prior knowledge about the game (e.g., route trees, defensive formations) and an accurate model of how the environment will change over time (e.g., opponent reaction strategies, motion attributes of players). Our aim is to build a computational model of the wide receiver, which takes into account prior knowledge about the game and short-term predictive models of how the environment will change over time. While prior knowledge of the game is readily accessible, it is quite challenging to build predictive models of how the environment will change over time. We propose several models for predicting short-term motions of opponent players to generate dynamic input features for our wide receiver forecasting model. In particular, we model the wide receiver with a Markov Decision Process (MDP), where the reward function is a linear combination of static features (prior knowledge about the game) and dynamic features (short-term prediction of opponent players). Since the dynamic features change over time, we make recursive calls to an inference procedure over the MDP while updating the dynamic features. We validate our technique on a video dataset of American football plays. Our results show that more informed models that accurately predict the motions of the defensive players are better at forecasting wide receiver plays.}, 
keywords={Markov processes;forecasting theory;image motion analysis;sport;American football;Markov decision process;defensive players motion prediction;dynamic features;opponent players short-term motion prediction;reward function;short-term predictive models;static features;wide receiver forecasting model;wide receiver trajectory prediction;Dynamics;Forecasting;Games;Predictive models;Receivers;Training;Trajectory}, 
doi={10.1109/WACV.2016.7477732}, 
month={March},}
@INPROCEEDINGS{6378972, 
author={A. Atanasov and M. Srinivasan and T. Weinzierl}, 
booktitle={IEEE Symposium on Large Data Analysis and Visualization (LDAV)}, 
title={Query-driven parallel exploration of large datasets}, 
year={2012}, 
pages={23-30}, 
abstract={Recent advances in supercomputing capabilities pose a multi-faceted data retrieval challenge to the exploration and visualisation of the obtained results: the bandwidth between visualisation devices and the high-performance computing (HPC) clusters neither scales with the simulation data nor with the compute power, the total memory footprint of the data on the supercomputer often exceeds the aggregate memory on the visualisation, and the data has to be distributed among several visualisation nodes working in parallel to render a visual. In the present paper, we introduce an on-demand data exploration paradigm that leverages HPC capabilities and distributed visualisation without requiring a large memory footprint on the visualisation cluster. Regions of interest within the data are specified by the user in the form of queries. These queries, augmented by node identifiers on the visualisation cluster, are automatically distributed among multiple compute nodes of the HPC cluster. The compute nodes work in parallel to assemble and merge data in response to the user query until the data distribution matches the visualisation cluster's topology. Query results are then simultaneously streamed to the right visualisation nodes. Our approach allows for interactive exploration of data residing on HPC resources, irrespective of memory footprint. The streaming of data to the visualisation nodes scales with the bandwidth of the interconnecting network and the HPC cluster's domain decomposition, while the latter is hidden from the visualisation and can change dynamically. We demonstrate the capability of our query-driven approach with a turbulent mixing dataset, and show that it supports interactive data exploration on HPC systems.}, 
keywords={data visualisation;interactive systems;parallel processing;query processing;rendering (computer graphics);HPC clusters;data distribution;data visualisation;distributed visualisation;domain decomposition;high-performance computing;interactive data exploration;interconnecting network bandwidth;large datasets;multifaceted data retrieval;on-demand data exploration paradigm;parallel visualisation nodes;query-driven approach;query-driven parallel exploration;rendering;simulation data;supercomputer;turbulent mixing dataset;user query response;visualisation devices;Computational modeling;Data models;Data visualization;Distributed databases;Load modeling;Supercomputers;Topology;On-demand data exploration;computational steering;distributed visualisation;large-scale data}, 
doi={10.1109/LDAV.2012.6378972}, 
month={Oct},}
@INPROCEEDINGS{5283845, 
author={A. Budura and D. Bourges-Waldegg and J. Riordan}, 
booktitle={2009 International Conference on Computational Science and Engineering}, 
title={Deriving Expertise Profiles from Tags}, 
year={2009}, 
volume={4}, 
pages={34-41}, 
abstract={We propose a novel approach to the problem of expertise mining in an enterprise, taking advantage of online social applications deployed within the enterprise. Based on the assumption that the userspsila interactions with such social software reflect to some extent their expertise, we devise a probabilistic method for identifying the main areas of expertise of users based solely on their set of tags extracted from a social bookmarking system. We base our approach on statistical language models, which we adapt to fit our unique setting. We train and validate our model on a real world dataset extracted from two IBM-internal applications. Our results show that our approach provides a viable alternative to other methods that rely on documents extracted from the enterprise corpora.}, 
keywords={business data processing;data mining;document handling;social networking (online);static induction transistors;statistical analysis;IBM-internal applications;document extraction;enterprise corpora;expertise mining;expertise profiles;online social applications;probabilistic method;real world dataset;social bookmarking system;social software;statistical language models;tags;user interactions;Application software;Collaborative software;Data mining;Information services;Internet;Java;Laboratories;Tagging;Videos;Web sites}, 
doi={10.1109/CSE.2009.404}, 
month={Aug},}
@INPROCEEDINGS{5362260, 
author={A. Li and L. Song}, 
booktitle={2009 Second International Symposium on Knowledge Acquisition and Modeling}, 
title={Estimating Gas Concentration of Coal Mines Based on ISGNN}, 
year={2009}, 
volume={1}, 
pages={138-141}, 
abstract={Online detecting failure of gas sensors in mine wells is an important problem. A key step for solution of the problem is estimating sample values of detected gas sensor, according to sample values of other gas sensors. We propose a scheme based on ISGNN (iteration self-generating neural networks) to estimate gas concentration of coal mines in this paper. First of all, sensors whose correlation information entropy between these sensors and detected gas sensor is the smallest are selected as input features of the classifier. Then ISGNN was employed as a classifier, and estimated sample values of detected gas sensor. Iteration learning self-generating neural network (ISGNN) is an improved self-generating neural network (SGNN). It has inherited the advantages of SGNN, for example, users do not need to set network structures and parameters and it has better precision. Real world gas monitoring dataset was used for experiment. The experimental results show that proposed method is efficient.}, 
keywords={coal;entropy;gas sensors;mining;neural nets;coal mines;correlation information entropy;failure online detection;gas concentration estimation;gas sensors;iteration self-generating neural networks;network structures;real world gas monitoring dataset;Computer science;Educational institutions;Gas detectors;Information entropy;Knowledge acquisition;Knowledge engineering;Monitoring;Multisensor systems;Neural networks;Sensor phenomena and characterization;Estimating gas concentration;Gas concentration modeling;Generating Neural Networks;ISGNN}, 
doi={10.1109/KAM.2009.132}, 
month={Nov},}
@INPROCEEDINGS{6126975, 
author={X. Dang and W. Wang and K. Wang and M. Dong and L. Yin}, 
booktitle={2011 IEEE SENSORS Proceedings}, 
title={A user-independent sensor gesture interface for embedded device}, 
year={2011}, 
pages={1465-1468}, 
abstract={Sensor based gesture interaction gains increasing attention in ubiquitous computing; however most algorithms suffer from large database for training and user dependency limitation. This paper summarize the gesture generating model, and propose a new physical analysis based gesture recognition method which can fulfill user-independent no-training gesture recognition. It utilizes principle component analysis for energy distribution analysis to bypass the gratitude influence, and extracts the physical characteristics of the gesture trajectory for classification. In our real-time gesture recognition system, first the acceleration data of a gesture is collected through Bluetooth interface between Wii-Remote and PC; then the energy distribution shape is calculated, then several physical trajectory characteristic is organized as motion feature cascade classification. The proposed method is compared with the published method under the Nokia dataset, whose recognition accuracy could achieve 97.35% for user-independent situation.}, 
keywords={Bluetooth;computerised instrumentation;embedded systems;gesture recognition;image sensors;principal component analysis;ubiquitous computing;Bluetooth interface;Nokia dataset;PC;Wii-remote;embedded device;energy distribution analysis;gesture generating model;gesture trajectory;motion feature cascade classification;physical analysis;principle component analysis;real-time gesture recognition system;sensor based gesture interaction;ubiquitous computing;user-independent no-training gesture recognition;user-independent sensor gesture interface;Acceleration;Accelerometers;Decision trees;Feature extraction;Gesture recognition;Hidden Markov models;Testing}, 
doi={10.1109/ICSENS.2011.6126975}, 
ISSN={1930-0395}, 
month={Oct},}
@INPROCEEDINGS{6311035, 
author={M. Daltayanni and C. Wang and R. Akella}, 
booktitle={2012 Annual SRII Global Conference}, 
title={A Fast Interactive Search System for Healthcare Services}, 
year={2012}, 
pages={525-534}, 
abstract={In this paper we describe the design, development, and evaluation of a general human-machine interaction search system, and its potential and use in the context of a collaboration project with SAP and Saffron. The objective of a specialized version of the system is to provide medical and healthcare information services to users via interactive search for personalized patient needs. Patients usually have questions regarding healthcare, including those which concern illness symptoms, duration and types of treatment, possible drug effects, and more. Authorized personnel would often be ideal in responding to such needs; however they could potentially be very expensive, and not easy to support and maintain. If patients could have access to information at their home, by means of i-phone or online access, this could save time, doctor office visit expenses, as well as valuable and restricted medical time. What is more, information concerning other anonymized and similar patient cases provides knowledge and perspective on a wide range of patient issues. From the doctors' perspective, they typically need to spend time on differential analysis about new patient cases: study symptoms, research possible causes, rank results by emergency priority and treat them accordingly. A search system that would direct a doctor (or patient/user) to similar patient cases would save significant amount of manual search time. The powerful new feature of this system is the storage and mining of past patient cases knowledge, to create metadata to be used in the subsequent retrieval of relevant documents. Finally, the interactive search system would speed up identification of rare cases; for instance, symptoms that do not appear commonly in past cases may require special treatment or expert referral. We build a model which dynamically learns medical needs of interacting MDs and patients. The model works on free or unstructured text, allowing disambiguation of vague words and flexibility in descri- ing medical needs. In addition, both experts with an advanced knowledge of medical terminology, and beginning users using basic medical terms, can achieve high search relevance. Furthermore, our approach obviates the need for the assignment of tags or labels, such as treatment, symptoms, causes, to documents, to respond effectively to user queries. In particular, we build a temporal difference algorithm to predict user's information needs by incorporating both current and predicted knowledge into learning the user profile. Our source of information about the user consists of submitted queries and feedback on the returned results. We tested our system on publicly available medical data (OhsuMed TREC dataset 2002) and we achieved a significant improvement in retrieval accuracy, compared to the literature. We provide quantitative results as well as demonstration screenshots which illustrate a) the value of interaction (user time spent with system versus results accuracy), b) the value of using medical terminology understanding, when compared with simple general words, and c) the value of allowing the maximum number of feedback submissions to vary.}, 
keywords={data mining;health care;human computer interaction;information needs;information retrieval;interactive systems;medical information systems;text analysis;SAP;Saffron;basic medical terms;collaboration project;data mining;data storage;differential analysis;document retrieval;fast interactive search system;free or unstructured text;healthcare information services;human-machine interaction search system;information needs;medical information services;medical terminology;metadata;patient cases knowledge;patient issues;personalized patient needs;publicly available medical data;rare cases;submitted queries;temporal difference algorithm;unstructured text;Diseases;Information services;Medical diagnostic imaging;Senior citizens;Terminology;Unified modeling language;healthcare information services;interactive retrieval;medical data retrieval;reinforcement learning;temporal difference}, 
doi={10.1109/SRII.2012.65}, 
ISSN={2166-0778}, 
month={July},}
@INPROCEEDINGS{7509796, 
author={P. Sun and L. Xu and H. Fan}, 
booktitle={2016 IEEE International Conference on Big Data Analysis (ICBDA)}, 
title={RHadoop-based fuzzy data mining: Architecture, design and system implementation}, 
year={2016}, 
pages={1-5}, 
abstract={Data mining is a challenge for end-users, which requires knowledge and skills on business domains, data mining algorithms and software development. In response to the challenge, we have proposed, designed and implemented a novel data mining system named RFDM (RHadoop-based Fuzzy Data Mining), which supports fuzzy data mining process and experience with user convenience and reduced cost. The system is capable of supporting fully-automated data mining life-cycle activities, with limited user interactions in dataset uploading and data mining configuration. In addition, a RHadoop-based framework has been integrated, which meets the requirements of large-scale datasets in data mining. Experiments have indicated that the RFDM system achieves enhanced performance while supporting fuzzy data mining.}, 
keywords={business data processing;data handling;data mining;fuzzy set theory;parallel processing;RFDM;RHadoop-based fuzzy data mining;business domains;data mining algorithms;fully-automated data mining life-cycle activities;fuzzy data mining process;software development;user convenience;Algorithm design and analysis;Classification algorithms;Clustering algorithms;Data mining;Data models;Machine learning algorithms;Training;MapReduce;RHadoop;data mining;fuzzy;large-scale}, 
doi={10.1109/ICBDA.2016.7509796}, 
month={March},}
@INPROCEEDINGS{6682864, 
author={E. Khadangi and A. Bagheri}, 
booktitle={ICCKE 2013}, 
title={Comparing MLP, SVM and KNN for predicting trust between users in Facebook}, 
year={2013}, 
pages={466-470}, 
abstract={Trust is one of the most important elements in social relationships. This importance becomes more conspicuous when the person's interactions take place in a dynamic environment with high uncertainty. Online social networks are websites where information propagates quickly and once the user has entered, they face a huge amount of information all of which they cannot assess. Therefore, it should be possible to rank the presented information based on issues such as trust. Trust prediction in social networks which do not support implicit rating mechanisms is a challenging problem. It is shown in this paper how it is possible to measure the trust between users in Facebook based on the information of their interactions and profile. For this, a dataset comprising interactions, profile information, and trust amount between users was collected. Then after the pre-process of this data set, three methods MLP1, KNN2 and SVM3 were used for trust prediction. Using 10-fold cross validation, we observed that MLP can measure trust with 83% accuracy. The accuracy of the best KNN model, with k=12, and SVM were 73% and 71% respectively. Recall, precision and area under the ROC curve of SVM and KNN were also significantly lower than MLP. According to these results, MLP can measure the trust between users with high accuracy based on the information of their interactions and profile.}, 
keywords={multilayer perceptrons;security of data;social networking (online);support vector machines;10-fold cross validation;Facebook;KNN;MLP;SVM;dataset;dynamic environment;k-nearest neighbor;multilayer perceptron;online social networks;profile information;support vector machine;trust prediction;Accuracy;Classification algorithms;Correlation;Facebook;Security;Support vector machines;Facebook;Multilayer Perceptron;Online Social Network;Support Vector Machine;Trust Prediction;k-nearest neighbor}, 
doi={10.1109/ICCKE.2013.6682864}, 
month={Oct},}
@INPROCEEDINGS{6708095, 
author={W. Bellante and R. Vilardi and D. Rossi}, 
booktitle={2013 IEEE 18th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)}, 
title={On Netflix catalog dynamics and caching performance}, 
year={2013}, 
pages={89-93}, 
abstract={Multimedia streaming applications have substantially changed the market policy of an increasing number of content providers that offer streaming services to the users. The need for effective video content delivery re-fueled interest for caching: since the Web-like workload of the 90s are not longer fit to describe the new Web of videos, in this work we investigate the suitability of the publicly available Netflix dataset for caching studies. Our analysis shows that, as the dataset continuously evolves (i) a steady state description is not statistically meaningful and (ii) despite the cache hit ratio decreases due to the growth of active movies in the catalog, simple caching replacement approaches are close to the optimum given the growing skew in the popularity distribution over the time. Additionally, we point out that, since the dataset reports logs of movie ratings, anomalies arise when ratings are considered to be movie views. At the same time, we show anomalies yield conservative caching results, that reinforces the soundness of our study.}, 
keywords={cache storage;media streaming;multimedia systems;video on demand;Netflix catalog dynamics;Web-like workload;cache hit ratio;caching replacement approach;market policy;movie ratings;multimedia streaming;popularity distribution;video content delivery;Catalogs;Computational modeling;Computers;Motion pictures;Prefetching;Streaming media;YouTube}, 
doi={10.1109/CAMAD.2013.6708095}, 
ISSN={2378-4865}, 
month={Sept},}
@INPROCEEDINGS{7858564, 
author={T. Wang and R. Huang and X. Wei and F. Zhou}, 
booktitle={2016 International Computer Symposium (ICS)}, 
title={Improving User's Quality of Experience in Imbalanced Dataset}, 
year={2016}, 
pages={690-695}, 
abstract={Currently, traditional algorithm performs not well in terms of predicting the user's complaint in imbalanced IPTV dataset. To solve this problem, we combine status data from the set-top box with data of user's complaints and select the appropriate model to predict user's quality of experience (QoE). Concretely, we firstly perform data cleaning and select suitable attributes from the original dataset. Then, we apply random under-sampling and synthetic over-sampling to the preprocessed dataset. In order to get better performance, we improves the Synthetic Minority Over-sampling Technique (SMOTE) algorithm and combine it with K-means algorithm to generate a new dataset. After these procedures, we use the Naïve Bayes (NB) model in user's complaint dataset. Through the rigorous modeling and prediction, extensive experimental results show that this integrated algorithm performs better than the Borderline-SMOTE algorithm in predicting user's complaints.}, 
keywords={Bayes methods;IPTV;data handling;quality of experience;IPTV dataset;K-means algorithm;NB model;QoE;SMOTE algorithm;data cleaning;naive Bayes model;quality of experience;random under-sampling;synthetic minority over-sampling technique;Classification algorithms;Cleaning;Clustering algorithms;IPTV;Media;Prediction algorithms;Quality of service;K-means;Naïve Bayes model;QoE;SMOTE algorithm},
doi={10.1109/ICS.2016.0142}, 
month={Dec},}
@INPROCEEDINGS{5616275, 
author={A. Daud and J. Li and L. Zhou and L. Zhang and Y. Ding and F. Muhammad}, 
booktitle={2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology}, 
title={Modeling Ontology of Folksonomy with Latent Semantics of Tags}, 
year={2010}, 
volume={1}, 
pages={516-523}, 
abstract={Modeling ontology of folksonomy provides a way of learning light weight ontology's which is a hot topic investigated recently. Previous approaches for modeling ontology of folksonomy either ignores semantics (synonymy, hyponymy or polysemy) or do not simultaneously consider relationships between actors (users), concepts (tags) and instances(resources) or are based on the idea that title words are responsible for generating tags for resources. Latent semantics and user-tag dependencies instead of user-word dependencies however are extremely important. In this paper we address these problems by introducing a latent topic layer into the traditional tripartite Actor-Concept-Instance graph. We thus propose an Actor-Concept-Instance-Topic (ACIT) approach to model ontology from folksonomy in a unified way by directly using tags and users of resources. We illustrate on Bibsonomy dataset that our proposed approach ACIT outperforms title words based approaches Tag-Topic (TT) and (User-Word-Topic) UWT for modeling the ontology of folksonomy.}, 
keywords={graph theory;ontologies (artificial intelligence);social networking (online);Bibsonomy dataset;actor-concept-instance graph;actor-concept-instance-topic approach;folksonomy;ontology modelling;tag latent semantics;user-tag dependencies;user-word dependencies;Folksonomy;Latent Semantics;Light Weight Ontology's;Unsupervised Learning;User-tag Dependencies}, 
doi={10.1109/WI-IAT.2010.10}, 
month={Aug},}
@INPROCEEDINGS{6664352, 
author={K. Wu and J. Chen and W. A. Pruett and R. L. Hester}, 
booktitle={2013 IEEE Symposium on Biological Data Visualization (BioVis)}, 
title={Hummod browser: An exploratory visualization tool for the analysis of whole-body physiology simulation data}, 
year={2013}, 
pages={97-104}, 
abstract={We present HumMod Browser, a multi-scale exploratory visualization tool that allows physiologists to explore human physiology simulation data with more than 6000 attributes. We first present a tag cloud technique to reveal the significance of time-varying attributes and then study how a chain of tag clouds can form an exploratory visuailzation that assist multiple dataset comparison and query. One purpose is to reduce the high cognitive workload of understanding complex interactions within the large attribute space. The HumMod Browser produced can give physiologists flexible control over the visualization displayed for quick understanding of complicated simulation results. The visualization is constructed through the metaphorical bubble interface to allow dynamic view controls and the data relationships and context informaiton unfold as physiologists querying groups of connected bubbles within the hierarchical or causal relationships. HumMod Browser contributions to the interaction design and provides multi-scale coordinated interactive exploration for a new type of physiological modeling data. Two case studies have been reported with real datasets containing more than 6000 physiology attributes, which provide supportive evidence on the usefulness of HumMod Browser in supporting effective large-attribute-space exploration.}, 
keywords={data analysis;data visualisation;interactive systems;medical information systems;physiological models;query processing;user interfaces;HumMod Browser;causal relationships;cognitive workload;context informaiton;dynamic view control;hierarchical relationship;interaction design;large-attribute-space exploration;metaphorical bubble interface;multiple dataset comparison;multiple dataset query;multiscale coordinated interactive exploration;multiscale exploratory visualization tool;physiological modeling data;physiologist querying groups;tag cloud technique;whole-body physiology simulation data analysis;Browsers;Data models;Data visualization;Lungs;Physiology;Tag clouds;Visualization;Information visualization;design study;interaction;physiology}, 
doi={10.1109/BioVis.2013.6664352}, 
month={Oct},}
@INPROCEEDINGS{7164907, 
author={Z. He and Z. Cai and X. Wang}, 
booktitle={2015 IEEE 35th International Conference on Distributed Computing Systems}, 
title={Modeling Propagation Dynamics and Developing Optimized Countermeasures for Rumor Spreading in Online Social Networks}, 
year={2015}, 
pages={205-214}, 
abstract={The spread of rumors in Online Social Networks (OSNs) poses great challenges to the social peace and public order. It is imperative to model propagation dynamics of rumors and develop corresponding countermeasures. Most of the existing works either overlook the heterogeneity of social networks or do not consider the cost of countermeasures. Motivated by these issues, this paper proposes a heterogeneous network based epidemic model that incorporates both the network heterogeneity and various countermeasures. Through analyzing the existence and stability of equilibrium solutions of the proposed ODE (Ordinary Differential Equation) system, the critical conditions that determine whether a rumor continuously propagates or becomes extinct are derived. Moreover, we concern about the cost of the main two types of countermeasures, i.e., Blocking rumors at influential users and spreading truth to clarify rumors. Employing the Pontryagin's maximum principle, we obtain the optimized countermeasures that ensures a rumor can become extinct at the end of an expected time period with lowest cost. Both the critical conditions and the optimized countermeasures provide a real-time decision reference to restrain the rumor spreading. Experiments based on Digg2009 dataset are conducted to evaluate the effectiveness of the proposed dynamic model and the efficiency of the optimized countermeasures.}, 
keywords={differential equations;maximum principle;real-time systems;social networking (online);Digg2009 dataset;ODE system;OSN;Pontryagin maximum principle;blocking rumors;epidemic model;heterogeneous network;modeling propagation dynamics;network heterogeneity;online social networks;ordinary differential equation system;public order;real-time decision reference;rumor spreading;social peace;spreading truth;Asymptotic stability;Differential equations;Mathematical model;Social network services;Stability analysis;Transforms}, 
doi={10.1109/ICDCS.2015.29}, 
ISSN={1063-6927}, 
month={June},}
@INPROCEEDINGS{5286060, 
author={D. Magatti and F. Stella and M. Faini}, 
booktitle={2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology}, 
title={A Software System for Topic Extraction and Document Classification}, 
year={2009}, 
volume={1}, 
pages={283-286}, 
abstract={A software system for topic extraction and automatic document classification is presented. Given a set of documents, the system automatically extracts the mentioned topics and assists the user to select their optimal number. The user-validated topics are exploited to build a model for multi-label document classification. While topic extraction is performed by using an optimized implementation of the Latent Dirichlet Allocation model, multi-label document classification is performed by using a specialized version of the Multi-Net Naive Bayes model. The performance of the system is investigated by using 10,056 documents retrieved from the WEB through a set of queries formed by exploiting the Italian Google Directory. This dataset is used for topic extraction while an independent dataset, consisting of 1,012 elements labeled by humans, is used to evaluate the performance of the Multi-Net Naive Bayes model. The results are satisfactory, with precision being consistently better than recall for the labels associated with the four most frequent topics.}, 
keywords={Conferences;Data mining;Informatics;Intelligent agent;Linear discriminant analysis;Probability distribution;Software prototyping;Software systems;Text categorization;Text mining;classification;text mining;topic extraction}, 
doi={10.1109/WI-IAT.2009.49}, 
month={Sept},}
@INPROCEEDINGS{7544900, 
author={H. Bansal and M. Misra}, 
booktitle={2016 IEEE 6th International Conference on Advanced Computing (IACC)}, 
title={Sybil Detection in Online Social Networks (OSNs)}, 
year={2016}, 
pages={569-576}, 
abstract={Peer to peer and distributed systems are generally susceptible to sybil attacks. Online social networks (OSNs), due to their fat user base and open access nature are also prone to such attacks. Current state-of-art algorithms for sybil attack detection make use of the inherent social graph created among registered users of OSN service. They rely on the inherent trust relationship among these users. No effort is made to combine other characteristic behavior of sybil users with properties of social graph of OSNs to improve detection accuracy of sybil attacks. Sybil identities are also used as gateways for spreading spam content in OSNs. The proposed approach exploits this behavior of sybil users to improve detection accuracy of existing sybil detection algorithms. In the proposed approach, content generated/published by each user is used along with the topological properties of the social graph of registered users. A machine learning model is used for assigning a fractional value called "trust value" which denotes the amount of legitimate content generated by the user. A modification to sybil detection algorithm is proposed which makes use of the trust value of each user to improve the accuracy of detecting a sybil identity. Real dataset from Facebook is crawled and used for analysis and experiments. Analytical results show the superiority of proposed solution. Results are compared with SybilGuard and SybilShield which shows ~14% decrease in false positive rates with very minimal effect on acceptance rate or false negative rate of the sybil detection algorithms. Also, the proposed modification does not affect the performance of existing sybil detection algorithms and can be implemented in a distributed manner.}, 
keywords={data analysis;graph theory;learning (artificial intelligence);security of data;social networking (online);trusted computing;Facebook;OSN;data analysis;machine learning model;online social network;social graph;sybil attack detection;topological property;trust value;Conferences;Detection algorithms;Facebook;Feature extraction;Image edge detection;Open Access;Sybil attack;social network analysis;social spam detection}, 
doi={10.1109/IACC.2016.111}, 
month={Feb},}
@INPROCEEDINGS{6208185, 
author={M. Gajdoš and M. Mikl and R. Mareček}, 
booktitle={2012 19th International Conference on Systems, Signals and Image Processing (IWSSIP)}, 
title={Dataset exploration tool for fMRI group analysis}, 
year={2012}, 
pages={492-495}, 
abstract={In this paper, the software tool designed for group fMRI analysis - a mask_explorer - is introduced. The purpose of the mask_explorer is to enable user friendly exploration of fMRI dataset and to prevent unwanted data loss. This data loss could be caused by automatic discarding of voxels from analysis, which are not containing the information from all subjects included into analysis. At the beginning, some steps of fMRI data preprocessing and single subject statistics are described. The statistical parametric mapping method, as implemented in SPM8 MATLAB toolbox, is used to data processing. The tool mask_explorer runs in the MATLAB environment and is compatible with SPM8 toolbox. Subsequently, the user interface and application of the mask_explorer are shown and described.}, 
keywords={biomedical MRI;human computer interaction;mathematics computing;medical image processing;software tools;statistical analysis;SPM8 MATLAB toolbox;data loss;dataset exploration tool;fMRI data preprocessing;fMRI group analysis;mask_explorer;single subject statistics;software tool;statistical parametric mapping method;user friendly exploration;voxels;Application software;Brain modeling;Educational institutions;Equations;Magnetic resonance imaging;Mathematical model;Vectors;fMRI dataset explorer;functional magnetic resonance imaging;general linear model;group fMRI analysis}, 
ISSN={2157-8672}, 
month={April},}
@INPROCEEDINGS{7497222, 
author={H. Hang and A. Bashir and M. Faloutsos and C. Faloutsos and T. Dumitras}, 
booktitle={2016 IFIP Networking Conference (IFIP Networking) and Workshops}, 
title={ #x201C;Infect-me-not #x201D;: A user-centric and site-centric study of web-based malware}, 
year={2016}, 
pages={234-242}, 
abstract={Malware authors have been using websites to distribute their products as a way to evade spam filters and classic anti-virus engines. Yet there has been relatively little work in modeling the behaviors and temporal properties of websites, as most research focuses on detecting whether a website distributes malware. In this paper we ask: How does web-based malware spread? We conduct an extensive study and follow a website-centric and user-centric point of view. We collect data from four online databases, including Symantec's WINE Project, for a total of more than 600K malicious URLs and over 500K users. First, we find that legitimate but compromised websites constitute 33.1% of the malicious websites in our dataset. In order to conduct this study, we develop a classifier to distinguish between compromised vs. malicious websites with an accuracy of 95.3%, which could be of interest to studies on website profiling. Second, we find that malicious URLs can be surprisingly long-lived, with 10% of malicious sites staying active for three months or more. Third, we observe that a significant number of URLs exhibit the same temporal pattern that suggests a flush-crowd behavior, inflicting most of their damage during the first few days of appearance. Finally, the distribution of the visits to malicious sites per user is skewed, with 1.4% of users visiting more than 10 malicious sites in 8 months. Our study is a first step towards modeling web-based malware propagation as a network-wide phenomenon and enabling researchers to develop realistic assumptions and models.}, 
keywords={Internet;Web sites;invasive software;unsolicited e-mail;Symantec WINE project;Web site profiling;Web-based malware authors;Web-based malware propagation;antivirus engines;flush-crowd behavior;infect-me-not;malicious URL;malicious Web sites;malicious sites;online databases;site-centric study;spam filters;temporal pattern;user-centric study;Computer hacking;Computer science;Databases;Engines;Malware;Telemetry;Uniform resource locators}, 
doi={10.1109/IFIPNetworking.2016.7497222}, 
month={May},}
@INPROCEEDINGS{6775399, 
author={W. Insuwan and U. Suksawatchon and J. Suksawatchon}, 
booktitle={2014 6th International Conference on Knowledge and Smart Technology (KST)}, 
title={Improving missing values imputation in collaborative filtering with user-preference genre and singular value decomposition}, 
year={2014}, 
pages={87-92}, 
abstract={One of the major concerns in collaborative filtering is sensitive to data sparsity. The other word, missing values are occurred when the customers rate to a few products or services, which bring about to less accuracy of the recommendation. Although the centroid of cluster and SVD are able to solve Sparsity problem, their drawbacks are 1) imputed mean is not derived from user preference and 2) imputed mean does not reflect to the real distribution since imputed mean comes from the average. Therefore, we propose “SVDUPMedianCF” in order to solve the defect of the traditional approach which is an imputation missing value by filling the missing values for each customer with the cluster centroid, obtained from K-means algorithm, of such customer along with singular value decomposition (SVD) in collaborative filtering. According to the experimental evaluation based on MovieLens dataset by using 5-fold cross validation, it has found that imputing missing values with the proposed model presents the lowest mean absolute error when comparison with traditional approach significantly. From the experimental result, the proposed model can improve the quality of recommendation results with significant difference (p<;0.05).}, 
keywords={collaborative filtering;data handling;pattern clustering;recommender systems;singular value decomposition;5-fold cross validation;MovieLens dataset;SVD;SVD centroid;SVDUPMedianCF;cluster centroid;collaborative filtering;data sparsity;k-mean clustering;lowest mean absolute error;missing value imputation improvement;recommender system;singular value decomposition;user-preference genre;SVD;collaborative filtering;k-mean clustering;singular value decomposition;sparsity problem;user preference}, 
doi={10.1109/KST.2014.6775399}, 
month={Jan},}
@INPROCEEDINGS{6939549, 
author={S. Maslennikov and E. Litvinov and M. Vaiman and M. Vaiman}, 
booktitle={2014 IEEE PES General Meeting | Conference Exposition}, 
title={Implementation of ROSE for on-line voltage stability analysis at ISO New England}, 
year={2014}, 
pages={1-5}, 
abstract={This paper describes the implementation of Region Of Stability Existence (ROSE) software at ISO-NE for on-line estimation of the power system transfer capability based on voltage and thermal limitations and for security monitoring. The software utilizes State Estimator (SE) and synchrophasor PMU data sets for determining the power system operational margin under normal and contingency conditions. ROSE implementation is based on “hybrid” approach where SE solution (model) is used to compute voltage stability limits, and PMU data (measurements) is used to determine the position of the current operating point. This software allows improving security of transmission system by continuously monitoring operational margin expressed in MW flow or in bus voltage angles, and alarming the operator if the margin violates a pre-defined threshold.}, 
keywords={busbars;graphical user interfaces;phasor measurement;power engineering computing;power system security;power system stability;power system state estimation;power transmission protection;ISO New England;ISO-NE;ROSE software;Region Of Stability Existence software;SE;bus voltage angle;current operating point position determination;graphical user interface;on-line power system transfer capability estimation;online voltage stability analysis;phasor measurement unit;power system operational margin determination;security monitoring;state estimator;synchrophasor PMU dataset;transmission system security;Monitoring;Phasor measurement units;Power system stability;Real-time systems;Security;Stability analysis;Thermal stability;interface limit;on-line voltage stability analysis;operational margin;optimal corrective actions;region of stability existence;synchrophasor measurements}, 
doi={10.1109/PESGM.2014.6939549}, 
ISSN={1932-5517}, 
month={July},}
@INPROCEEDINGS{6511281, 
author={M. Kumar and M. Hanumanthappa and T. V. S. Kumar}, 
booktitle={2012 IEEE 14th International Conference on Communication Technology}, 
title={Intrusion Detection System using decision tree algorithm}, 
year={2012}, 
pages={629-634}, 
abstract={Intrusion Detection System (IDS) is the most powerful system that can handle the intrusions of the computer environments by triggering alerts to make the analysts take actions to stop this intrusion. IDS's are based on the belief that an intruder's behavior will be noticeably different from that of a legitimate user. A variety of intrusion detection systems (IDS) have been employed for protecting computers and networks from malicious attacks by using traditional statistical methods to new data mining approaches in last decades. However, today's commercially available intrusion detection systems are signature-based that are not capable of detecting unknown attacks. In this paper we analyze a classification model for misuse and anomaly attack detection using decision tree algorithm.}, 
keywords={computer network security;data mining;decision trees;digital signatures;IDS;anomaly attack detection;classification model;computer environments;computer protection;data mining approaches;decision tree algorithm;intruder behavior;legitimate user;malicious attacks;misuse attack;signature-based intrusion detection systems;statistical methods;unknown attacks;Decision Tree Algorithm;Intrusion Detection System;KDD Dataset;Network Security}, 
doi={10.1109/ICCT.2012.6511281}, 
month={Nov},}
@INPROCEEDINGS{5196102, 
author={C. Stickel and K. Maier and M. Ebner and A. Holzinger}, 
booktitle={Proceedings of the ITI 2009 31st International Conference on Information Technology Interfaces}, 
title={The modeling of harmonious color combinations for improved usability and UX}, 
year={2009}, 
pages={323-328}, 
abstract={This study compares three different models for the calculation and prediction of harmonious color combinations. Therefore a dataset of user rated color combinations was taken from a large online database. The user rating was compared to the outcome of the three models on this dataset in order to test the performance of the models. The first model based on the idea that color combinations are more pleasing the greater their difference in brightness. The second model is a slightly modified version of Ou & Lou (2006) using chromatic difference, lightness sum, lightness difference and hue effect. The last model was invented by us and is based on an experiment of Polzella & Montgomery (1993). From the outcome of their experiment we generated a lookup table for single color rating. This rating is then used in a formula, which is able to evaluate the color harmony for color combinations up to five colors. This model also performed best in the overall comparison between the three color harmony models.}, 
keywords={brightness;colour;colour vision;brightness;chromatic difference;harmonious color combinations;hue effect;lightness difference;lightness sum;lookup table;usability;user experience;Biomedical informatics;Brightness;Databases;Documentation;Lenses;Predictive models;Retina;Statistics;Testing;Usability;Color Combination;Harmony;Model;Pleasantness;Usability;User Experience}, 
doi={10.1109/ITI.2009.5196102}, 
ISSN={1330-1012}, 
month={June},}
@ARTICLE{1501926, 
author={S. Szpala and M. Wierzbicki and G. Guiraudon and T. M. Peters}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Real-time fusion of endoscopic views with dynamic 3-D cardiac images: a phantom study}, 
year={2005}, 
volume={24}, 
number={9}, 
pages={1207-1215}, 
abstract={Minimally invasive robotically assisted cardiac surgical systems currently do not routinely employ 3-D image guidance. However, preoperative magnetic resonance and computed tomography (CT) images have the potential to be used in this role, if appropriately registered with the patient anatomy and animated synchronously with the motion of the actual heart. This paper discusses the fusion of optical images of a beating heart phantom obtained from an optically tracked endoscope, with volumetric images of the phantom created from a dynamic CT dataset. High quality preoperative dynamic CT images are created by first extracting the motion parameters of the heart from the series of temporal frames, and then applying this information to animate a high-quality heart image acquired at end systole. Temporal synchronization of the endoscopic and CT model is achieved by selecting the appropriate CT image from the dynamic set, based on an electrocardiographic trigger signal. The spatial error between the optical and virtual images is 1.4±1.1 mm, while the time discrepancy is typically 50-100 ms.}, 
keywords={biomedical optical imaging;computerised tomography;electrocardiography;endoscopes;image motion analysis;medical image processing;medical robotics;phantoms;surgery;virtual reality;3D cardiac images;50 to 100 ms;CT model;beating heart phantom;cardiac surgical systems;computed tomography images;dynamic cardiac images;electrocardiographic trigger signal;endoscopic model;endoscopic views;heart motion parameters;magnetic resonance images;minimally invasive surgical systems;optical images;optically tracked endoscope;phantom study;preoperative CT images;real-time fusion;robotically assisted surgical systems;spatial error;temporal synchronization;virtual endoscopy;virtual images;volumetric images;Anatomy;Animation;Computed tomography;Data mining;Endoscopes;Heart;Imaging phantoms;Magnetic resonance;Minimally invasive surgery;Robots;Image guidance;image warping;minimally invasive cardiac surgery;virtual endoscopy;virtual reality;Algorithms;Artificial Intelligence;Cardiovascular Surgical Procedures;Computer Systems;Endoscopy;Heart;Humans;Imaging, Three-Dimensional;Myocardium;Phantoms, Imaging;Radiographic Image Enhancement;Radiographic Image Interpretation, Computer-Assisted;Signal Processing, Computer-Assisted;Subtraction Technique;Surgery, Computer-Assisted;Surgical Procedures, Minimally Invasive;Tomography, X-Ray Computed;User-Computer Interface}, 
doi={10.1109/TMI.2005.853639}, 
ISSN={0278-0062}, 
month={Sept},}
@INPROCEEDINGS{6821053, 
author={W. Jingjing and T. Changhong and L. Xiangwen and C. Guolong}, 
booktitle={2013 International Conference on Cloud Computing and Big Data}, 
title={Mining Social Influence in Microblogging via Tensor Factorization Approach}, 
year={2013}, 
pages={583-591}, 
abstract={Microblogging has become an important social media for creating, sharing, or exchanging information and ideas. Social influence analysis in Microblogging is often exploited for different tasks such as information retrieval, recommendations, businesses intelligence. Most existing methods mostly rely on social links between users, failing to take advantage of characteristics of Microblogging. Furthermore, the size of Microblogging's user (i.e. Microblogger) is very large, which makes computing resource for social influence mining approach can't be satisfied by single computer. In this paper, a tensor factorization framework based on cloud computing platform is proposed for mining social influence in Microblogging. The framework has three components: a feature extraction component, a tensor factorization component and a user influence ranking component. In feature extraction component, features are extracted for capturing user social influence quantitatively through statistical analysis on the Microbloggers' relations. In tensor factorization component, tensor factorization based MapReduce model is presented to infer user's implicit user's relations. Finally, a user influence ranking function is constructed for computing user social influence in user influence ranking component. Experiments on Sina weibo dataset (Chinese Microblogging platform) show that our proposal significantly not only improves the prediction accuracy compared with two baseline methods, but also has competitive advantage for processing massive data from Microblogging.}, 
keywords={cloud computing;data mining;feature extraction;matrix decomposition;statistical analysis;tensors;Chinese microblogging platform;Sina Weibo dataset;businesses intelligence;cloud computing platform;feature extraction component;information retrieval;recommendations;social influence analysis;social influence mining approach;social links;social media;statistical analysis;tensor factorization based MapReduce model;user implicit user relations;user influence ranking component;user influence ranking function;Blogs;Educational institutions;Fans;Feature extraction;Media;Tensile stress;Testing;Cloud Computing;Social Influence;Social Media;Tensor Factorization}, 
doi={10.1109/CLOUDCOM-ASIA.2013.73}, 
month={Dec},}
@ARTICLE{4374086, 
author={B. H. Cho and H. Yu and J. Lee and Y. J. Chee and I. Y. Kim and S. I. Kim}, 
journal={IEEE Transactions on Information Technology in Biomedicine}, 
title={Nonlinear Support Vector Machine Visualization for Risk Factor Analysis Using Nomograms and Localized Radial Basis Function Kernels}, 
year={2008}, 
volume={12}, 
number={2}, 
pages={247-256}, 
abstract={Nonlinear classifiers, e.g., support vector machines (SVMs) with radial basis function (RBF) kernels, have been used widely for automatic diagnosis of diseases because of their high accuracies. However, it is difficult to visualize the classifiers, and thus difficult to provide intuitive interpretation of results to physicians. We developed a new nonlinear kernel, the localized radial basis function (LRBF) kernel, and new visualization system visualization for risk factor analysis (VRIFA) that applies a nomogram and LRBF kernel to visualize the results of nonlinear SVMs and improve the interpretability of results while maintaining high prediction accuracy. Three representative medical datasets from the University of California, Irvine repository and Statlog dataset-breast cancer, diabetes, and heart disease datasets-were used to evaluate the system. The results showed that the classification performance of the LRBF is comparable with that of the RBF, and the LRBF is easy to visualize via a nomogram. Our study also showed that the LRBF kernel is less sensitive to noise features than the RBF kernel, whereas the LRBF kernel degrades the prediction accuracy more when important features are eliminated. We demonstrated the VRIFA system, which visualizes the results of linear and nonlinear SVMs with LRBF kernels, on the three datasets.}, 
keywords={cardiology;data visualisation;diseases;mammography;medical diagnostic computing;nomograms;pattern classification;radial basis function networks;support vector machines;Irvine repository and Statlog dataset;RBF kernels;SVM;University of California;automatic disease diagnosis;breast cancer;classification performance;diabetes;feature selection methods;heart disease datasets;localized radial basis function kernels;nomograms;nonlinear support vector machine visualization system;risk factor analysis;Decision support systems;Feature selection;Localized Radial Basis Function kernel;Nomograms;Support vector machines;Visualization;feature selection;localized radial basis function (LRBF) kernel;nomograms;support vector machines (SVMs);visualization;Artificial Intelligence;Computer Graphics;Diagnosis, Computer-Assisted;Nonlinear Dynamics;Pattern Recognition, Automated;Prognosis;Proportional Hazards Models;Risk Assessment;Risk Factors;User-Computer Interface}, 
doi={10.1109/TITB.2007.902300}, 
ISSN={1089-7771}, 
month={March},}
@ARTICLE{4298149, 
author={V. Grau and H. Becher and J. A. Noble}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Registration of Multiview Real-Time 3-D Echocardiographic Sequences}, 
year={2007}, 
volume={26}, 
number={9}, 
pages={1154-1165}, 
abstract={Real-time 3-D echocardiography opens up the possibility of interactive, fast 3-D analysis of cardiac anatomy and function. However, at the present time its quantitative power cannot be fully exploited due to the limited quality of the images. In this paper, we present an algorithm to register apical and parasternal echocardiographic datasets that uses a new similarity measure, based on local orientation and phase differences. By using phase and orientation to guide registration, the effect of artifacts intrinsic to ultrasound images is minimized. The presented method is fully automatic except for initialization. The accuracy of the method was validated qualitatively, resulting in 85% of the cardiac segments estimated having a registration error smaller than 2 mm, and no segments with an error larger than 5 mm. Robustness with respect to landmark initialization was validated quantitatively, with average errors smaller than 0.2 mm and 0.5o for initialization landmarks rotations of up to 15o and translations of up to 10 mm.}, 
keywords={echocardiography;image registration;medical image processing;apical echocardiographic dataset;automatic image registration;cardiac anatomy analysis;cardiac function analysis;landmark initialization;multiview echocardiographic sequences;parasternal echocardiographic dataset;real time 3D echocardiography;similarity measure;ultrasound images;Anatomy;Biomedical imaging;Biomedical transducers;Echocardiography;Image segmentation;Image sequence analysis;Phase measurement;Probes;Ultrasonic imaging;Ultrasonic variables measurement;Echocardiography;image orientation;image phase;image registration;image sequence analysis;multiscale;Algorithms;Artificial Intelligence;Computer Simulation;Computer Systems;Echocardiography, Three-Dimensional;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Models, Cardiovascular;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique;User-Computer Interface;Ventricular Dysfunction, Left}, 
doi={10.1109/TMI.2007.903568}, 
ISSN={0278-0062}, 
month={Sept},}
@INPROCEEDINGS{6331976, 
author={H. Xiong and D. Zhang and D. Zhang and V. Gauthier}, 
booktitle={2012 9th International Conference on Ubiquitous Intelligence and Computing and 9th International Conference on Autonomic and Trusted Computing}, 
title={Predicting Mobile Phone User Locations by Exploiting Collective Behavioral Patterns}, 
year={2012}, 
pages={164-171}, 
abstract={Location prediction based on cellular network traces has recently spurred lots of interest. However, predicting one's location remains a very challenging task due to the randomness of the human mobility patterns. Our preliminary study included in this paper shows that there is a strong correlation and association among the certain group of users' locations. Through association pattern mining on Reality Mining dataset which involves 32,579 cell tower locations and 350,000 hours of continuous activity information, we observe the highly confident association rules exist among the locations of users, and then we further verify that the associations are indeed caused by the collective behaviors of the mobile phone users. Based on this finding we introduce the collective behavioral patterns (CBP), and then propose CBP-based predictor- a novel prediction schema that aims to forecasting one's locations in next 6 hours based on the locations of other users. Furthermore, we integrate the state-of-the-art i.e., Markov-based predictor with our CBP-based schema to build a hybrid predictor. We evaluate the CBP-based schema and compare the hybrid predictor with the Markov-based predictor through intensive experiments. Experimental results show that CBP-based predictor achieves good precision and the hybrid predictor produces higher prediction accuracy than the state-of-the-art scheme at cell tower level in the forthcoming one to six hours. Finally it is verified that collective behavioral patterns can be used to predict user locations as well as to improve the performance of existing predictors.}, 
keywords={Markov processes;cellular radio;data mining;mobile computing;mobile handsets;CBP-based predictor;Markov-based predictor;association pattern mining;association rules;cell tower locations;cellular network traces;collective behavioral patterns;continuous activity information;human mobility patterns;mobile phone user locations prediction;reality mining dataset;Association rules;Mobile handsets;Noise;Poles and towers;Predictive models;Trajectory}, 
doi={10.1109/UIC-ATC.2012.28}, 
month={Sept},}
@INPROCEEDINGS{7515955, 
author={J. Gong and L. Cai and Y. Shen}, 
booktitle={2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)}, 
title={Evaluating accuracy and performance of GPU-accelerated random walk computation on heterogeneous networks}, 
year={2016}, 
pages={541-545}, 
abstract={Random walk is an effective network-based method for information mining. However, the computational complexity of random walk limits its application on large scale datasets. This study evaluating accuracy and performance of graphics processing units (GPUs) in accelerating random walk computation on heterogeneous networks. We first introduce a general heterogeneous network to describe user-item relevance model. We apply GPU-accelerated random walk method to this network and investigate the performance of the method. We demonstrate our method via large-scale experiments across MovieLens datasets and obtain a more than 12× speedup. The results shows the effectiveness and promise of the approach.}, 
keywords={data mining;graphics processing units;GPU-accelerated random walk computation;MovieLens dataset;graphics processing units;heterogeneous networks;information mining;random walk method;user-item relevance model;Acceleration;Graphics processing units;Heterogeneous networks;Matrix converters;Motion pictures;Sparse matrices;CUDA;heterogeneous network;parallel computing;random walk;software evaluation}, 
doi={10.1109/SNPD.2016.7515955}, 
month={May},}
@INPROCEEDINGS{7298667, 
author={Junho Yim and Heechul Jung and ByungIn Yoo and Changkyu Choi and Dusik Park and Junmo Kim}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Rotating your face using multi-task deep neural network}, 
year={2015}, 
pages={676-684}, 
abstract={Face recognition under viewpoint and illumination changes is a difficult problem, so many researchers have tried to solve this problem by producing the pose- and illumination- invariant feature. Zhu et al. [26] changed all arbitrary pose and illumination images to the frontal view image to use for the invariant feature. In this scheme, preserving identity while rotating pose image is a crucial issue. This paper proposes a new deep architecture based on a novel type of multitask learning, which can achieve superior performance in rotating to a target-pose face image from an arbitrary pose and illumination image while preserving identity. The target pose can be controlled by the user's intention. This novel type of multi-task model significantly improves identity preservation over the single task model. By using all the synthesized controlled pose images, called Controlled Pose Image (CPI), for the pose-illumination-invariant feature and voting among the multiple face recognition results, we clearly outperform the state-of-the-art algorithms by more than 4~6% on the MultiPIE dataset.}, 
keywords={face recognition;learning (artificial intelligence);lighting;neural nets;pose estimation;CPI;controlled pose image;deep architecture;face recognition;identity preservation;illumination image;multitask deep neural network;multitask learning;pose-illumination-invariant feature;target-pose face image rotation;Face;Face recognition;Feature extraction;Image reconstruction;Lighting;Three-dimensional displays;Training}, 
doi={10.1109/CVPR.2015.7298667}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{942110, 
author={C. Niles and N. Jeremijenko}, 
booktitle={Proceedings Fifth International Conference on Information Visualisation}, 
title={Collated Path: a one-dimensional interface element to promote user orientation and sense-making activities in the Semantic Web}, 
year={2001}, 
pages={555-562}, 
abstract={This paper details the design of a one-dimensional interface element, the “Collated Path” an effort to create a very, light-weight information manipulation tool that could be used within a user's own dataset (e.g. their history list), which could then be used in combination with other searches. It is optimized to present views of the Semantic Web useful for everyday applications. Emphasis was placed on creating a tool with Simple elements that could be combined to create sophisticated views. Whereas existing Web information tools such as browser history and search tools like Sherlock present a limited search/retrieve view, the Collated Path creates a view that encourages the interaction and tangibility of such data enabling effective integration into the workspace. The model uses the metaphor of “pages”, representing Web resources, which are “collated” in two variables according to some retrieved or generated values, such as time accessed, popularity, or relevance to search terms. In addition, the design is general enough to accept values from an RDF model, the W3C's metadata standard for the Web. The conceptual framework is not restricted to Web pages, but the “page” metaphor connects the concept with previous research that have promoted document piles, and other visualization methods, while giving it a more grounded basis for discussion}, 
keywords={Internet;data visualisation;information resources;meta data;user interfaces;Collated Path;Internet;RDF model;Semantic Web;Sherlock;data visualization;dataset;information manipulation tool;metadata standard;one-dimensional interface;search terms;search tools;user orientation;Displays;History;Humans;Libraries;Network address translation;Resource description framework;Search engines;Semantic Web;Web pages;Web search}, 
doi={10.1109/IV.2001.942110}, 
month={},}
@ARTICLE{7192715, 
author={M. Sun and P. Mi and C. North and N. Ramakrishnan}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={BiSet: Semantic Edge Bundling with Biclusters for Sensemaking}, 
year={2016}, 
volume={22}, 
number={1}, 
pages={310-319}, 
abstract={Identifying coordinated relationships is an important task in data analytics. For example, an intelligence analyst might want to discover three suspicious people who all visited the same four cities. Existing techniques that display individual relationships, such as between lists of entities, require repetitious manual selection and significant mental aggregation in cluttered visualizations to find coordinated relationships. In this paper, we present BiSet, a visual analytics technique to support interactive exploration of coordinated relationships. In BiSet, we model coordinated relationships as biclusters and algorithmically mine them from a dataset. Then, we visualize the biclusters in context as bundled edges between sets of related entities. Thus, bundles enable analysts to infer task-oriented semantic insights about potentially coordinated activities. We make bundles as first class objects and add a new layer, “in-between”, to contain these bundle objects. Based on this, bundles serve to organize entities represented in lists and visually reveal their membership. Users can interact with edge bundles to organize related entities, and vice versa, for sensemaking purposes. With a usage scenario, we demonstrate how BiSet supports the exploration of coordinated relationships in text analytics.}, 
keywords={data visualisation;pattern classification;text analysis;BiSet visual analytics technique;biclusters visualization;cluttered visualizations;data analytics;semantic edge bundling;sensemaking purpose;task-oriented semantic insights;text analytics;Data analytics;Encoding;Image edge detection;Semantics;Visualization;Bicluster;coordinated relationship;semantic edge bundling;Cluster Analysis;Computer Graphics;Humans;Semantics;Software;User-Computer Interface}, 
doi={10.1109/TVCG.2015.2467813}, 
ISSN={1077-2626}, 
month={Jan},}
@INPROCEEDINGS{4740835, 
author={G. Xu and Y. Zhang and X. Yi}, 
booktitle={2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology}, 
title={Modelling User Behaviour for Web Recommendation Using LDA Model}, 
year={2008}, 
volume={3}, 
pages={529-532}, 
abstract={Web users exhibit a variety of navigational interests through clicking a sequence of Web pages. Analysis of Web usage data will lead to discover Web user access pattern and facilitate users locate more preferable Web pages via collaborative recommending technique. Meanwhile, latent semantic analysis techniques provide a powerful means to capture user access pattern and associated task space. In this paper, we propose a collaborative Web recommendation framework, which employs Latent Dirichlet Allocation (LDA) to model underlying topic-simplex space and discover the associations between user sessions and multiple topics via probability inference. Experiments conducted on real Website usage dataset show that this approach can achieve better recommendation accuracy in comparison to existing techniques. The discovered topic-simplex expression can also provide a better interpretation of user navigational preference.}, 
keywords={Internet;Web sites;behavioural sciences computing;boundary-value problems;groupware;information filters;probability;user modelling;LDA model;Web pages;Web recommendation;Web usage data;Web user access pattern;collaborative recommending technique;latent Dirichlet allocation;latent semantic analysis techniques;probability inference;real Website usage dataset;user behaviour modelling;Collaboration;Data models;Filtering;Inference algorithms;Intelligent agent;Linear discriminant analysis;Mathematical model;Navigation;Pattern analysis;Web pages;Latent Dirichlet Allocation;Web Recommendation;Web Usage Mining}, 
doi={10.1109/WIIAT.2008.313}, 
month={Dec},}
@INPROCEEDINGS{7880240, 
author={G. Ansari and T. Ahmad and M. N. Doja}, 
booktitle={2016 Ninth International Conference on Contemporary Computing (IC3)}, 
title={Review ranking method for spam recognition}, 
year={2016}, 
pages={1-5}, 
abstract={E-commerce websites are becoming popular among customers who are buying products online. Online reviews play a major role in selling of online products. Reviews give the customer a complete overview of the product thus making it popular or unpopular among buyers thus increasing its sales. In order to increase sales of a product, reviewers are writing fake reviews. In this paper, a review ranking method is proposed. This method assigns a score to each review based on different parameters. The reviews having high score are considered to be more helpful or genuine and thus are ranked higher than the reviews having lower score. The lower ranked reviews are fake reviews and thus they are non-useful to the users. The proposed approach is an effective approach which avoids heavy computation of learning. Evaluation on real-life flipkart review dataset shows a precision of 83.3% thus showing the effectiveness of proposed model.}, 
keywords={customer satisfaction;data mining;electronic commerce;learning (artificial intelligence);e-commerce Web sites;fake reviews;genuine reviews;helpful reviews;online product buying;online product selling;online reviews;real-life flipkart review dataset;review ranking method;spam recognition;Computational modeling;Data models;Itemsets;Measurement;Radiation detectors;Statistical analysis;Writing;ranking;rating deviation;spam detection}, 
doi={10.1109/IC3.2016.7880240}, 
month={Aug},}
@INPROCEEDINGS{6240373, 
author={E. Hoque and J. Stankovic}, 
booktitle={2012 6th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth) and Workshops}, 
title={AALO: Activity recognition in smart homes using Active Learning in the presence of Overlapped activities}, 
year={2012}, 
pages={139-146}, 
abstract={We present AALO: a novel Activity recognition system for single person smart homes using Active Learning in the presence of Overlapped activities. AALO applies data mining techniques to cluster in-home sensor firings so that each cluster represents instances of the same activity. Users only need to label each cluster as an activity as opposed to labeling all instances of all activities. Once the clusters are associated to their corresponding activities, our system can recognize future activities. To improve the activity recognition accuracy, our system preprocesses raw sensor data by identifying overlapping activities. The evaluation of activity recognition performance on a 26-day dataset shows that compared to Naive Bayesian (NB), Hidden Markov Model (HMM), and Hidden Semi Markov Model (HSMM) based activity recognition systems, our average time slice error (24.15%) is much lower than NB (53.04%), and similar to HMM (29.97%) and HSMM (26.29%). Thus, our active learning based approach performs as good as the state of the art supervised techniques (HMM and HSMM).}, 
keywords={data mining;hidden Markov models;home computing;learning (artificial intelligence);AALO;HMM;Naive Bayesian;active learning;data mining techniques;hidden Markov model;hidden semi Markov model based activity recognition systems;in-home sensor firings;novel activity recognition system;overlapped activities;raw sensor data;smart homes;supervised techniques;Clustering algorithms;Microwave filters}, 
doi={10.4108/icst.pervasivehealth.2012.248600}, 
ISSN={2153-1633}, 
month={May},}
@INPROCEEDINGS{1526251, 
author={Hao Wu and Liguang Ma and Xianghong Hua and Xinzhou Wang}, 
booktitle={Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.}, 
title={GIS-based digital mining management information system: a case in Laozhaiwan gold mine}, 
year={2005}, 
volume={1}, 
pages={3 pp.-}, 
abstract={With the development of digital information technology in mining industry, the concept of DM (digital mining) and MGIS (mining geographical information system) are becoming the research focus but not perfect. How to effectively manage the dataset of geological, surveying and mineral products grade is the key point that concerned the sustainable development and standardized management in mining industry. Based on the existing combined GIS and remote sensing technology, we propose a model named DMMIS (digital mining management information system), which is composed of the database layer, the ActiveX layer and the user interface layer. The system is used in Laozhaiwan gold mine, Yunnan Province of China, which is shown to demonstrate the feasibility of the research and development achievement stated in this paper. Finally, some conclusions and constructive advices for future research work are given.}, 
keywords={database management systems;geographic information systems;geophysical techniques;management information systems;mining;remote sensing;surveying;ActiveX layer;China;GIS-based digital mining management information system;Laozhaiwan gold mine;Yunnan Province;database layer;digital information technology;mining geographical information system;mining industry;remote sensing technology;standardized management;surveying;sustainable development;user interface layer;Delta modulation;Geographic Information Systems;Geology;Gold;Information technology;Management information systems;Minerals;Mining industry;Remote sensing;Sustainable development}, 
doi={10.1109/IGARSS.2005.1526251}, 
ISSN={2153-6996}, 
month={July},}
@INPROCEEDINGS{6909746, 
author={L. Fiaschi and F. Diego and K. Gregor and M. Schiegg and U. Koethe and M. Zlatic and F. A. Hamprecht}, 
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Tracking Indistinguishable Translucent Objects over Time Using Weakly Supervised Structured Learning}, 
year={2014}, 
pages={2736-2743}, 
abstract={We use weakly supervised structured learning to track and disambiguate the identity of multiple indistinguishable, translucent and deformable objects that can overlap for many frames. For this challenging problem, we propose a novel model which handles occlusions, complex motions and non-rigid deformations by jointly optimizing the flows of multiple latent intensities across frames. These flows are latent variables for which the user cannot directly provide labels. Instead, we leverage a structured learning formulation that uses weak user annotations to find the best hyperparameters of this model. The approach is evaluated on a challenging dataset for the tracking of multiple Drosophila larvae which we make publicly available. Our method tracks multiple larvae in spite of their poor distinguishability and minimizes the number of identity switches during prolonged mutual occlusion.}, 
keywords={biology computing;image motion analysis;learning (artificial intelligence);microorganisms;object tracking;complex motions;deformable objects;hyperparameters;latent variables;multiple Drosophila larvae tracking;multiple indistinguishable translucent object tracking;multiple latent intensities;nonrigid deformations;occlusions;weak user annotations;weakly supervised structured learning;Biology;Boundary conditions;Image color analysis;Optimization;Spatiotemporal phenomena;Tracking;Training;latent variables;multicommodity flow;multiple objects tracking;optimization;structured learning}, 
doi={10.1109/CVPR.2014.356}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{5421406, 
author={Z. Liang and C. He and Z. Xin}, 
booktitle={2010 Second International Conference on Computer Modeling and Simulation}, 
title={Feature Based Visualization Algorithm for Large-Scale Flow Data}, 
year={2010}, 
volume={1}, 
pages={194-197}, 
abstract={To analyze large amounts of numerical data, one of the most useful approaches is to use scientific visualization to transform them into graphical images. Flow visualization as one of the challenging topics has played important roles in oceanic data analysis. There are many techniques have been presented in the past decade, but most of them can't get high performance to visualize large-scale flow data in real time. To deduce the computational complexity brought by large flow dataset, feature-based expression will be a helpful way. However, how to get the result images quickly without costing much time for feature extraction and analysis is a very important problem to deal. Based on the common characteristic of flow and the unchangeable scale feather of spiral line, we present a new distributing strategy which needn't locate feature points very accurately and didn't rely on the type of feature fields. The visualization procedure not only can straight forward automatically but also can be changed with user's interactive command. The flow data obtained from the South Sea of China was verified and simulated. The result shows that this method using spiral strategy not templates to setting the seeds to emphasize the interesting fields is much faster and flexible, especially in large-scale flow data visualization.}, 
keywords={computational fluid dynamics;data visualisation;feature extraction;flow simulation;flow visualisation;computational complexity;feature analysis;feature based visualization algorithm;feature extraction;flow visualization;graphical images;large-scale flow data;oceanic data analysis;scientific visualization;Analytical models;Computational efficiency;Computational modeling;Data flow computing;Data visualization;Feature extraction;Image analysis;Large-scale systems;Oceans;Spirals;appending grid controller;feature field;seeds distribution;spiral line;streamline}, 
doi={10.1109/ICCMS.2010.215}, 
month={Jan},}
@ARTICLE{7429793, 
author={C. Xiong and D. M. Johnson and J. J. Corso}, 
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
title={Active Clustering with Model-Based Uncertainty Reduction}, 
year={2017}, 
volume={39}, 
number={1}, 
pages={5-17}, 
abstract={Semi-supervised clustering seeks to augment traditional clustering methods by incorporating side information provided via human expertise in order to increase the semantic meaningfulness of the resulting clusters. However, most current methods are passive in the sense that the side information is provided beforehand and selected randomly. This may require a large number of constraints, some of which could be redundant, unnecessary, or even detrimental to the clustering results. Thus in order to scale such semi-supervised algorithms to larger problems it is desirable to pursue an active clustering method—i.e., an algorithm that maximizes the effectiveness of the available human labor by only requesting human input where it will have the greatest impact. Here, we propose a novel online framework for active semi-supervised spectral clustering that selects pairwise constraints as clustering proceeds, based on the principle of uncertainty reduction. Using a first-order Taylor expansion, we decompose the expected uncertainty reduction problem into a gradient and a step-scale, computed via an application of matrix perturbation theory and cluster-assignment entropy, respectively. The resulting model is used to estimate the uncertainty reduction potential of each sample in the dataset. We then present the human user with pairwise queries with respect to only the best candidate sample. We evaluate our method using three different image datasets (faces, leaves and dogs), a set of common UCI machine learning datasets and a gene dataset. The results validate our decomposition formulation and show that our method is consistently superior to existing state-of-the-art techniques, as well as being robust to noise and to unknown numbers of clusters.}, 
keywords={Clustering algorithms;Clustering methods;Computer science;Electronic mail;Reliability;Semantics;Uncertainty;Active clustering;image clustering;semi-supervised clustering;uncertainty reduction}, 
doi={10.1109/TPAMI.2016.2539965}, 
ISSN={0162-8828}, 
month={Jan},}
@ARTICLE{4376179, 
author={T. Schultz and H. Theisel and H. P. Seidel}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Topological Visualization of Brain Diffusion MRI Data}, 
year={2007}, 
volume={13}, 
number={6}, 
pages={1496-1503}, 
abstract={Topological methods give concise and expressive visual representations of flow fields. The present work suggests a comparable method for the visualization of human brain diffusion MRI data. We explore existing techniques for the topological analysis of generic tensor fields, but find them inappropriate for diffusion MRI data. Thus, we propose a novel approach that considers the asymptotic behavior of a probabilistic fiber tracking method and define analogs of the basic concepts of flow topology, like critical points, basins, and faces, with interpretations in terms of brain anatomy. The resulting features are fuzzy, reflecting the uncertainty inherent in any connectivity estimate from diffusion imaging. We describe an algorithm to extract the new type of features, demonstrate its robustness under noise, and present results for two regions in a diffusion MRI dataset to illustrate that the method allows a meaningful visual analysis of probabilistic fiber tracking results.}, 
keywords={biodiffusion;biomedical MRI;brain;data visualisation;feature extraction;medical image processing;probability;tracking;brain diffusion MRI data visualization;feature extraction;generic tensor fields;probabilistic fiber tracking method;topological visualization;Algorithm design and analysis;Anatomy;Brain;Data visualization;Humans;Magnetic resonance imaging;Noise robustness;Tensile stress;Topology;Uncertainty;Diffusion tensor;probabilistic fiber tracking;tensor topology;uncertainty visualization.;Algorithms;Brain;Computer Graphics;Computer Simulation;Diffusion Magnetic Resonance Imaging;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Models, Biological;Models, Statistical;Nerve Fibers, Myelinated;Neural Pathways;Numerical Analysis, Computer-Assisted;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface}, 
doi={10.1109/TVCG.2007.70602}, 
ISSN={1077-2626}, 
month={Nov},}
@INPROCEEDINGS{5563737, 
author={D. Anand and K. K. Bharadwaj}, 
booktitle={2010 3rd International Conference on Computer Science and Information Technology}, 
title={Adaptive user similarity measures for recommender systems: A genetic programming approach}, 
year={2010}, 
volume={8}, 
pages={121-125}, 
abstract={Recommender systems signify the shift from the paradigm of “searching for items” to “discovering items” and have been employed by an increasing number of e-commerce sites for matching users to their preferences. Collaborative Filtering is a popular recommendation technique which exploits the past user-item interactions to determine user similarity. The preferences of such similar users are leveraged to offer suggestions to the active user. Even though several techniques for similarity assessment have been suggested in literature, no technique has been proven to be optimal under all contexts/data conditions. Hence, we propose a two-stage process to assess user similarity, the first is to learn the optimal transformation function to convert the raw ratings data to preference data by employing genetic programming, and the second is to utilize the preference values, so derived, to compute user similarity. The application of such learnt user bias gives rise to adaptive similarity measures, i.e. similarity estimates that are dataset dependent and hence expected to work best under any data environment. We demonstrate the superiority of our proposed technique by contrasting it to traditional similarity estimation techniques on four different datasets representing varied data environments.}, 
keywords={genetic algorithms;groupware;information filtering;recommender systems;adaptive user similarity measure;collaborative filtering;data environment;genetic programming;item discovery;item searching;optimal transformation function;preference value;raw ratings data;recommender system;similarity assessment;similarity estimation;user-item interaction;Collaboration;Computational modeling;Educational institutions;Genetics;IP networks;Collaborative Filtering;Genetic Programming;Recommender Systems;Similarity Measures}, 
doi={10.1109/ICCSIT.2010.5563737}, 
month={July},}
@INPROCEEDINGS{5985705, 
author={L. Cao and D. Das and Y. Kobayashi and Y. Kuno}, 
booktitle={2011 IEEE International Conference on Mechatronics and Automation}, 
title={Object spatial recognition for service robots: Where is the front?}, 
year={2011}, 
pages={875-880}, 
abstract={Exactly same objects produce dramatically different images depending on their poses to the camera and result in great ambiguity for spatial recognition. Different poses of same objects also lead to different orientations in the use of intrinsic system. Our current study is focusing on this issue and can be divided into three phases. First, we propose an object pose-estimation model which is capable of recognizing unseen views. We achieve this goal by building a discrete key-pose structure parameterized by an azimuth and using PHOG [20] descriptor to measure the shape correspondence between two images. A large number of instances are learned at the training stage through semi-supervised. Then, we show experimental results on our own dataset. Second, according to the analyzed criteria in the use of intrinsic system, we recognize the frontal orientation of an intrinsic geometry object combining with pose-estimation results (e.g., a LCD screen). Finally, we summarize our integrated model which is able to classify object category, estimate object pose, distinguish intrinsic spatial relations between reference and target objects and locate the target under users' instructions.}, 
keywords={intelligent robots;learning (artificial intelligence);object recognition;pose estimation;robot vision;service robots;shape measurement;PHOG descriptor;discrete key-pose structure;frontal orientation recognition;intrinsic geometry object;intrinsic system;object pose-estimation model;object spatial recognition;semisupervised learning;service robot;shape correspondence measurement;training stage;Azimuth;Computational modeling;Estimation;Histograms;Robots;Shape;Training}, 
doi={10.1109/ICMA.2011.5985705}, 
ISSN={2152-7431}, 
month={Aug},}
@INPROCEEDINGS{6846859, 
author={S. Bu and X. Hong and Z. Peng and Q. Li}, 
booktitle={Proceedings of the 2014 IEEE 18th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
title={Integrating meta-path selection with user-preference for top-k relevant search in heterogeneous information networks}, 
year={2014}, 
pages={301-306}, 
abstract={Relevance search in heterogeneous information networks is a basic and crucial operation which is usually used in recommendation, clustering and anomaly detection. Nowadays most existing relevance search methods focus on objects in homogeneous information networks. In this paper, we propose a method to find the top-k most relevant objects to a specific one in heterogeneous networks. It is a two phase process that we get the initial relevance score based on the method of pair wise random walk along given meta-paths, which is a meta-level description of the path instances in heterogeneous information networks, and then take user preference into consideration to calculate the weights combination of meta-paths and model the problem into a multi-objective linear planning problem which can be solved with the method of generic algorithm. Besides, to ensure the efficiency, we use matrix computation and selective materialization to avoid the recursive computation of pair wise random walk. What's more, we propose an effective pruning method to skip unnecessary objects computations. The experiments on IMDB and DBLP dataset show that the method can gain a better accuracy and efficiency.}, 
keywords={information networks;information retrieval;matrix algebra;pattern clustering;recommender systems;DBLP dataset;IMDB dataset;anomaly detection;clustering;generic algorithm;heterogeneous information network;matrix computation;meta-level description;meta-path selection integration;multiobjective linear planning problem;pair wise random walk recursive computation;path instances;pruning method;recommendation;relevance score;selective materialization;top-k most relevant objects;top-k relevant search;two phase process;user-preference;Accuracy;Computational modeling;Computer science;Educational institutions;Motion pictures;Planning;Search problems;Heterogeneous information networks;graph partitioning;relevance search;user-preference search}, 
doi={10.1109/CSCWD.2014.6846859}, 
month={May},}
@ARTICLE{6151971, 
author={M. Ruffini and D. Mehta and B. O’Sullivan and L. Quesada and L. Doyle and D. Payne}, 
journal={IEEE/OSA Journal of Optical Communications and Networking}, 
title={Deployment Strategies for Protected Long-Reach PON}, 
year={2012}, 
volume={4}, 
number={2}, 
pages={118-129}, 
abstract={The mass deployment of fiber access networks is probably the most important network upgrade strategy for operators over the coming decade. Next-generation networks, and in particular the long-reach passive optical network (LR-PON) solution, aim to increase the long-term economic viability and sustainability of fiber-to-the-premises (FTTP) deployment. The LR-PON solution achieves this by minimizing the number of nodes and the amount of electronic equipment required within the network. Since an LR-PON replaces the metro backhaul network, which is usually a protected part of the network, protecting the long-reach part of an LR-PON against failure becomes a critical issue that needs to be taken into account. In this paper, we introduce a novel protection mechanism which, by spreading the load generated by a node failure over the network, can significantly reduce the overall protection capacity required. We then present a practical FTTP deployment scenario based on our protected LR-PON architecture for a European country. The problem is modeled using integer linear programming, and the optimization results, obtained using a real dataset provided by a national operator, show that a small number of metro/core nodes can provide protected connection to FTTP users. By applying a detailed cost model to the outcome of the optimization, we are able to show that our LR-PON deployment strategy, which minimizes the overall protection capacity, rather than just minimizing fiber distances in the LR-PON, can significantly reduce costs.}, 
keywords={integer programming;linear programming;passive optical networks;telecommunication network management;telecommunication network reliability;European country;deployment strategies;electronic equipment;fiber access networks;fiber-to-the-premise deployment;integer linear programming;long reach passive optical network;node failure;overall protection capacity;protected long reach PON;protection mechanism;IP networks;Optical network units;Passive optical networks;Power cables;Routing;SONET;Cost analysis;Long-reach passive optical networks;Network optimization;Network protection and resiliency}, 
doi={10.1364/JOCN.4.000118}, 
ISSN={1943-0620}, 
month={February},}
@INPROCEEDINGS{6999481, 
author={A. K. Ziemann and D. W. Messinger and P. S. Wenger}, 
booktitle={2014 IEEE Western New York Image and Signal Processing Workshop (WNYISPW)}, 
title={An adaptive k-nearest neighbor graph building technique with applications to hyperspectral imagery}, 
year={2014}, 
pages={32-36}, 
abstract={The analysis of remotely sensed spectral imagery has a variety of applications in both the public and private sectors, including tracking urban development, monitoring the spread of diseased crops, and mapping environmental disasters. The high spatial and spectral resolutions in hyperspectral imagery (HSI) make it particularly desirable for these types of analyses, as HSI sensors capture “color” information beyond what the human eye can see; this allows for greater differentiation between materials. However, those same properties can make HSI more difficult to analyze: traditional statistical or linear data models are not always able to well-model the high-dimensional HSI data for materially cluttered scenes. In recent years, the literature has shown an increase in the use of graph theory-based models for HSI analysis. These models are often used as the foundation for data transformations and manifold learning algorithms including Locally Linear Embedding, Commute Time Distance, and ISOMAP. A challenge associated with the graph building techniques used in these transformations is that they are typically k-nearest neighbor (kNN) graphs, which requires the user to designate a universal k value for the dataset. There is a need for an adaptive approach to building a kNN graph for HSI analysis so as to handle the particular characteristics of hyperspectral data in the spectral space, such as the sparse regions of data due to anomalies or rare targets in the scene, and the dense regions of data due to background clusters. Here, we present adaptive nearest neighbors, or ANN, which identifies a different k value for each pixel, so that pixels in denser regions have a higher k value and pixels in sparser regions have a lower k value. The resulting ANN graphs will be compared against kNN, and will be shown for synthetic data as well as hyperspectral data. While the focus here is on HSI, the ANN technique is applicable to any type of data analysis using a grap- -based model.}, 
keywords={geophysical image processing;graph theory;hyperspectral imaging;image classification;image colour analysis;image resolution;image sensors;learning (artificial intelligence);pattern clustering;remote sensing;ANN graphs;HSI analysis;HSI sensors;adaptive approach;adaptive k-nearest neighbor graph building technique;background clusters;color information;denser regions;graph-based model;high spatial resolutions;high spectral resolutions;hyperspectral data;hyperspectral imagery;k-nearest neighbor graphs;kNN graphs;remotely sensed spectral imagery;sparse regions;spectral space;Artificial neural networks;Buildings;Computational modeling;Data models;Hyperspectral imaging;Manifolds;Graph theory;adaptive nearest neighbors;hyperspectral;k nearest neighbors;remote sensing}, 
doi={10.1109/WNYIPW.2014.6999481}, 
month={Nov},}
@INPROCEEDINGS{7809378, 
author={F. Mercaldo and V. Nardone and A. Santone and C. A. Visaggio}, 
booktitle={2016 IEEE/ACM 4th FME Workshop on Formal Methods in Software Engineering (FormaliSE)}, 
title={Download Malware? No, Thanks. How Formal Methods Can Block Update Attacks}, 
year={2016}, 
pages={22-28}, 
abstract={In mobile malware landscape there are many techniques to inject malicious payload in a trusted application: one of the most common is represented by the so-called update attack. After an apparently innocuous application is installed on the victim's device, the user is asked to update the application, and a malicious behavior is added to the application. In this paper we propose a static method based on model checking able to identify this kind of attack. In addiction, our method is able to localize the malicious payload at methodlevel. We obtain an accuracy very close to 1 in identifying families implementing update attack using a real Android dataset composed by 2,581 samples.}, 
keywords={formal verification;invasive software;mobile computing;Android dataset;malicious payload;mobile malware landscape;model checking;static method;Androids;Java;Loading;Malware;Model checking;Payloads;Smart phones;Malware; Android; Security; Model Checking; Temporal logic}, 
doi={10.1109/FormaliSE.2016.012}, 
month={May},}
@INPROCEEDINGS{5445661, 
author={C. Y. Ting and K. C. Khor and S. Phon-Amnuaisuk}, 
booktitle={2010 Second International Conference on Computer Engineering and Applications}, 
title={Features and Bayesian Network Model of Conceptual Change for INQPRO}, 
year={2010}, 
volume={2}, 
pages={305-309}, 
abstract={Predicting conceptual change in scientific inquiry learning environment is not trivial due to the challenges that stemmed when eliciting a student's implicit properties. The challenges could be more complicated when such learning environment employs exploratory learning approach. One plausible approach to tackle the challenges is by employing data mining approach. In this study, 129 interaction logs were firstly preprocessed and subsequently transformed into structured dataset fits for mining purpose. Feature selection algorithms were performed considering that fact the dataset consists of large number of attributes. The dimension of feature set was reduced via two feature selection algorithms and elicitation of domain expert, resulting in FORA, FRFE, and FDOM, respectively. The feature sets were compared using Naive Bayesian Networks (MNB_DOM, MNB_RFE, MNB_ORA). The second phase of empirical study aimed to investigated the optimal BN model for capturing knowledge about conceptual change. To do that, a machine-learned Bayesian Network (MLBN) was constructed and its performance was compared to MNB_DOM. Findings from empirical studies suggested that (i) classifiers constructed using FDOM outperformed FORA and FRFE and (ii) the classifier MNB_DOM outperformed .Mnb dom in predicting conceptual change, suggesting that MLBN is a better classifier than MNB_DOM in capturing knowledge about conceptual change in INQPRO, a scientific inquiry learning environment developed in this research work.}, 
keywords={belief networks;data mining;feature extraction;learning (artificial intelligence);user modelling;FDOM;FORA;FRFE;INQPRO;MNB_DOM;MNB_ORA;MNB_RFE;conceptual change prediction;data mining;feature selection algorithm;naive Bayesian network;scientific inquiry learning environment;Application software;Bayesian methods;Cognition;Cognitive science;Computer applications;Computer networks;Data mining;Information technology;Problem-solving;Writing;Bayesian Networks;Conceptual Change;Feature Selection;Student Modeling}, 
doi={10.1109/ICCEA.2010.211}, 
month={March},}
@INPROCEEDINGS{7423066, 
author={Q. Hu and G. Wang and P. S. Yu}, 
booktitle={2015 IEEE Conference on Collaboration and Internet Computing (CIC)}, 
title={Public Information Sharing Behaviors Analysis over Different Social Media}, 
year={2015}, 
pages={62-69}, 
abstract={What types of information are popularly shared on social media sites such as Facebook, Twitter, Linked In and Google+? Does each of the social network cares about different topics? Can we uncover the different patterns behind the sharing behavior of social network users? This paper addresses the above questions by analyzing public information spreading data on different online social networks, identifying the spreading characteristics, and modeling the spreading patterns. In order to conduct statistical studies and build models to achieve our goals, we first extract data from the "share" buttons in news articles published by mainstream news websites. Such buttons are important to initiate the propagation of the news in social media. Through statistical findings, we demonstrate that both the share counts and topics of news vary a lot in different social networks. Additionally, based on the time series data displaying how news articles accumulate their share counts, we propose a K-medoids based clustering scheme, Clustering based on Similar Volume and Shape (CSVS), on a newly designed Scaling Aware Shifting Invariant (SASI) distance measure to uncover the different types of sharing patterns of news articles in social media. Through experiments on the collected dataset, we demonstrate that the proposed CSVS is able to cluster news with similar sharing patterns into the same cluster, by taking into consideration both of their share counts and shapes.}, 
keywords={electronic publishing;pattern clustering;public information systems;social networking (online);statistical analysis;time series;CSVS;Facebook;Google+;K-medoids based clustering scheme;LinkedIn;SASI distance measure;Twitter;Websites;clustering based on similar volume and shape;news articles;online social networks;public information sharing behavior analysis;scaling aware shifting invariant distance measure;social media sites;social network users;spreading pattern modeling;statistical findings;time series data;Facebook;Google;LinkedIn;Media;Shape;Twitter;Information Network;Information Sharing Behavior;Social Network;Time Series Clustering;Time Series Data}, 
doi={10.1109/CIC.2015.13}, 
month={Oct},}
@INPROCEEDINGS{7086173, 
author={V. Kaganov and A. Korolyov and M. Krylov and I. Mashechkin and M. Petrovskiy}, 
booktitle={2014 14th International Conference on Hybrid Intelligent Systems}, 
title={Hybrid method for active authentication using keystroke dynamics}, 
year={2014}, 
pages={61-66}, 
abstract={Nowadays there is a growing interest to active authentication methods in security society. These methods are used for user identity validation with behavioral biometrics such as keystroke or mouse moving dynamics. In this article a new hybrid method for active authentication using keystroke dynamics is presented. This method is a combination of a new keystroke data representation model based on potential functions and machine learning algorithms based on decision trees. The proposed method is tested in static and dynamic authentication scenarios on the benchmark Si6 dataset and datasets collected by the authors. The experimental results confirm that the proposed hybrid method is applicable to reallife authentication systems.}, 
keywords={authorisation;biometrics (access control);decision trees;keyboards;learning (artificial intelligence);active authentication method;authentication system;behavioral biometrics;benchmark Si6 dataset;decision trees;dynamic authentication;hybrid method;keystroke data representation model;keystroke dynamics;machine learning algorithm;potential functions;security;static authentication;user identity validation;Authentication;Data models;Decision trees;Keyboards;Pressing;Standards;Support vector machine classification;active authentication;decision trees;keystroke dynamics;machine learning;potential functions}, 
doi={10.1109/HIS.2014.7086173}, 
month={Dec},}
@INPROCEEDINGS{6957238, 
author={Y. Wang and J. Luo and A. Song and F. Dong}, 
booktitle={2014 43rd International Conference on Parallel Processing}, 
title={A Sampling-Based Hybrid Approximate Query Processing System in the Cloud}, 
year={2014}, 
pages={291-300}, 
abstract={Sampling-based approximate query processing method provides the way, in which the users can save their time and resources for 'Big Data' analytical applications, if the estimated results can satisfy the accuracy expectation earlier before a long wait for the final accurate results. Online aggregation (OLA) is such an attractive technology to respond aggregation queries by calculating approximate results with the confidence interval getting tighter over time. It has been built into the MapReuduce-based cloud system for big data analytics, which allows users to monitor the query progress and save money by killing the computation earlier once sufficient accuracy has been obtained. Unfortunately, there exists a major obstacle that is the estimation failure of OLA affects the OLA performance, which is resulted from the biased sample set that violates the unbiased assumption of OLA sampling. To handle this problem, we first propose a hybrid approximate query processing model to improve the overall OLA performance, where a dynamic scheme switching mechanism is deliberately designed to switch unpromising OLA queries into the bootstrap scheme for further processing, avoiding the whole dataset scanning resulted from the OLA estimation failure. In addition, we also present a progressive estimation method to reduce the false positive ratio of our dynamic scheme switching mechanism. Moreover, we have implemented our hybrid approximate query processing system in Hadoop, and conducted extensive experiments on the TPC-H benchmark for skewed data distribution. Our results demonstrate that our hybrid system can produce acceptable approximate results within a time period one order of magnitude shorter compared to the original OLA over Hadoop.}, 
keywords={cloud computing;estimation theory;query processing;sampling methods;Hadoop;MapReduce;OLA estimation failure;TPC-H benchmark;big data analytics;bootstrap scheme;cloud system;confidence interval;data distribution;dynamic scheme switching mechanism;hybrid approximate query processing;online aggregation;progressive estimation method;sampling method;Accuracy;Aggregates;Educational institutions;Estimation;Query processing;Silicon;Switches}, 
doi={10.1109/ICPP.2014.38}, 
ISSN={0190-3918}, 
month={Sept},}
@INPROCEEDINGS{4406465, 
author={C. I. Lee and C. J. Tsai and J. H. Wu and W. P. Yang}, 
booktitle={Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)}, 
title={A Decision Tree-Based Approach to Mining the Rules of Concept Drift}, 
year={2007}, 
volume={4}, 
pages={639-643}, 
abstract={In a database, the concept of an example might change along with time, which is known as concept drift. When the concept drift occurs, the classification model built by using old dataset is not suitable for predicting new coming dataset. Although many algorithms had been proposed to solve this problem, they focus only on updating the classification model. However, in a real life users might be very interested in the rules of concept drift. For example, doctors would desire to know the main causes more for disease variation since such rules would enable them to diagnose patients more correctly and quickly. In this paper, we propose a concept drift rule mining tree to accurately discover the rule of concept drift. The main contributions of this paper are: a) we address the problem of mining concept-drifting rule which was ignored in the past; b) our method can accurately mine the rule of concept drift.}, 
keywords={data mining;decision trees;classification model;concept drift;decision tree;rule mining;Association rules;Data mining;Databases;Decision trees;Diseases;Itemsets;Predictive models;Technology management;Training data;Weather forecasting}, 
doi={10.1109/FSKD.2007.16}, 
month={Aug},}
@INPROCEEDINGS{4740463, 
author={Z. Zhuang and C. Brunk and P. Mitra and C. L. Giles}, 
booktitle={2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology}, 
title={Towards Click-Based Models of Geographic Interests in Web Search}, 
year={2008}, 
volume={1}, 
pages={293-299}, 
abstract={With the recent surge in the volume of search queries that explicitly or implicitly express users' geographical interests, to accurately infer users' locality preference becomes an increasingly important yet challenging issue. We study two click-based models of the distribution of such geographical interests by mining the user click stream data in the search engine logs, addressing three important issues in spatial Web search. First, search queries and documents can be classified by the models according to their spatial specificity. Second, the geographic center(s) of interests for queries and documents can be inferred. Finally, the model can be applied to generate relevance features for search ranking. We evaluated our proposals on a large dataset with about 10,000 unique queries sampled from the Yahoo! Search query logs, and about 450 million user clicks on 1.4 million unique Web pages over a six-months period. We report about 90% accuracy and about 3% false positive rate in identifying search queries with or without specific geographical interests, as well as statistically significant improvement in relevance ranking over a strong baseline.}, 
keywords={Internet;data mining;geographic information systems;query formulation;click-based model;geographic interests;search query classification;spatial Web search;user click stream data mining;Bridges;Intelligent agent;Learning systems;Proposals;Search engines;Surges;USA Councils;Visualization;Web pages;Web search;click mining;geographic search;geographical information retrieval;geographical query;query classification;relevance ranking;visualization}, 
doi={10.1109/WIIAT.2008.365}, 
month={Dec},}
@INPROCEEDINGS{6314274, 
author={Y. Carlinet and T. D. Huynh and B. Kauffmann and F. Mathieu and L. Noirie and S. Tixeuil}, 
booktitle={2012 8th International Wireless Communications and Mobile Computing Conference (IWCMC)}, 
title={Four months in daily motion: Dissecting user video requests}, 
year={2012}, 
pages={613-618}, 
abstract={The growth of User-Generated Content (UGC) traffic makes the understanding of its nature a priority for network operators, content providers and equipment suppliers. In this paper, we study a four-month dataset that logs all video requests to DailyMotion made by a fixed subset of users. We were able to infer user sessions from raw data, to propose a Markovian model of these sessions, and to study video popularity and its evolution over time. The presented results are a first step for synthesizing an artificial (but realistic) traffic that could be used in simulations or experimental testbeds.}, 
keywords={Markov processes;telecommunication traffic;video communication;Markovian model;daily motion;equipment suppliers;network operators;user video requests;user-generated content traffic;Analytical models;Correlation;Markov processes;Monitoring;Noise measurement;Probes;Streaming media;Dataset analysis;UGC;modeling}, 
doi={10.1109/IWCMC.2012.6314274}, 
ISSN={2376-6492}, 
month={Aug},}
@INPROCEEDINGS{6636623, 
author={G. Roffo and C. Segalin and A. Vinciarelli and V. Murino and M. Cristani}, 
booktitle={2013 10th IEEE International Conference on Advanced Video and Signal Based Surveillance}, 
title={Reading between the turns: Statistical modeling for identity recognition and verification in chats}, 
year={2013}, 
pages={99-104}, 
abstract={Identity safekeeping has recently become an important problem for the social web: as a case study, we focus here on instant messaging platforms, proposing novel soft-biometric cues for user recognition and verification. Specifically, we design a set of features encoding effectively how a person converses: since chats are crossbreeds of written text and face-to-face verbal communication, the features inherit equally from textual authorship attribution and conversational analysis of speech. Importantly, our cues ignore completely the semantics of the chat, relying solely on non-verbal aspects, taking care of possible privacy and ethical issues. We apply our approach on a novel dataset of 94 different individuals, whose chat conversations have been recorded for an average period of five months; recognition rate, intended as normalized AUC on CMC curve, is 95.73%, while verification rate amounts to 95.66%, as normalized AUC on ROC curve.}, 
keywords={biometrics (access control);data privacy;electronic messaging;ethical aspects;social networking (online);statistical analysis;CMC curve;ROC curve;chat conversations;ethical issues;face-to-face verbal communication;identity recognition;identity safekeeping;identity verification;instant messaging platforms;normalized AUC;privacy issues;recognition rate;social Web;soft-biometric cues;speech conversational analysis;statistical modeling;textual authorship attribution;user recognition;user verification;verification rate;written text;Correlation;Data mining;Educational institutions;Feature extraction;Histograms;Probes;Standards}, 
doi={10.1109/AVSS.2013.6636623}, 
month={Aug},}
@INPROCEEDINGS{5634029, 
author={G. Moultazem and F. Sedes}, 
booktitle={2009 Fifth International Conference on Signal Image Technology and Internet Based Systems}, 
title={Remodelling Geographic Data to Facilitate the Interrogation by Sketch}, 
year={2009}, 
pages={264-270}, 
abstract={The available amount of geographic datasets has considerably grown. This is due to the increasing number of different devices used in collecting such data. Moreover, the amazing progress of the web that allows sharing and accessing to any type of information has further increased this availability. However, query evaluation in spatial data is often expensive, because these data have complex structures and they are available in different formats and scales. Recently, systems have been developed to allow users to sketch what they are looking for instead of translating their requests into verbal query statements. These systems try to close the gap between user and spatial data, so the user can focus on his query rather than spending his time formulating the request. With an intention to make this process easy, the spatial data, in the database, must be reformulated to answer this new type of request. This paper presents a method for remodelling the spatial data in database in order to establish a retrieval information system by sketching.}, 
keywords={Internet;data structures;geographic information systems;peer-to-peer computing;query processing;visual databases;complex data structure;geographic data remodelling;geographic dataset;information access;information retrieval system;information sharing;query evaluation;spatial data;verbal query statement;Context;Data models;Semantics;Shape;Spatial databases;Visualization;query evaluation;remodelling;sketch;spatial data}, 
doi={10.1109/SITIS.2009.51}, 
month={Nov},}
@ARTICLE{7830895, 
author={B. Xia and Y. Fan and C. Wu and B. Bai and J. Zhang}, 
journal={Tsinghua Science and Technology}, 
title={A method for predicting service deprecation in service systems}, 
year={2017}, 
volume={22}, 
number={01}, 
pages={52-61}, 
abstract={An increasing number of web services are being invoked by users to create user applications (e.g., mashups). However, over time, a few good services in service systems have become deprecated, i.e., the service is initially available and is invoked by service users, but later becomes unavailable. Therefore, the prediction of service deprecation has become a key issue in creating reliable long-term user applications. Most existing research has overlooked service deprecation in service systems and has failed to consider long-term service reliability when making service recommendations. In this paper, we propose a method for predicting service deprecation, which comprises two components: Service Comprehensive Feature Modeling (SCFM) for extracting service features relevant to service deprecation and Deprecated Service Prediction (DSP) for building a service deprecation prediction model. Our experimental results on a real-world dataset demonstrate that our method yields an improved Area-Under-the-Curve (AUC) value over existing methods and thus has better accuracy in service deprecation prediction.}, 
keywords={Web services;feature extraction;DSP;SCFM;Web services;improved AUC value;improved area-under-the-curve value;long-term service reliability;long-term user applications;service comprehensive feature modeling;service deprecation prediction model;service feature extraction;service recommendations;service systems;user applications;Digital signal processing;Feature extraction;Frequency modulation;Mashups;Predictive models;Probabilistic logic;Reliability;Latent Dirichlet Allocation (LDA);extreme learning machine;service deprecation predict;web service}, 
doi={10.1109/TST.2017.7830895}, 
month={February},}
@INPROCEEDINGS{7498304, 
author={W. Wang and H. Yin and S. Sadiq and L. Chen and M. Xie and X. Zhou}, 
booktitle={2016 IEEE 32nd International Conference on Data Engineering (ICDE)}, 
title={SPORE: A sequential personalized spatial item recommender system}, 
year={2016}, 
pages={954-965}, 
abstract={With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important way of helping users discover interesting locations to increase their engagement with location-based services. Although human movement exhibits sequential patterns in LBSNs, most current studies on spatial item recommendations do not consider the sequential influence of locations. Leveraging sequential patterns in spatial item recommendation is, however, very challenging, considering 1) users' check-in data in LBSNs has a low sampling rate in both space and time, which renders existing prediction techniques on GPS trajectories ineffective; 2) the prediction space is extremely large, with millions of distinct locations as the next prediction target, which impedes the application of classical Markov chain models; and 3) there is no existing framework that unifies users' personal interests and the sequential influence in a principled manner. In light of the above challenges, we propose a sequential personalized spatial item recommendation framework (SPORE) which introduces a novel latent variable topic-region to model and fuse sequential influence with personal interests in the latent and exponential space. The advantages of modeling the sequential effect at the topic-region level include a significantly reduced prediction space, an effective alleviation of data sparsity and a direct expression of the semantic meaning of users' spatial activities. Furthermore, we design an asymmetric Locality Sensitive Hashing (ALSH) technique to speed up the online top-k recommendation process by extending the traditional LSH. We evaluate the performance of SPORE on two real datasets and one large-scale synthetic dataset. The results demonstrate a significant improvement in SPORE's ability to recommend spatial items, in terms of both effectiveness and efficiency, compared with the state-of-the-art methods.}, 
keywords={recommender systems;ALSH technique;LBSN;SPORE;asymmetric locality sensitive hashing;data sparsity;exponential space;human movement;latent space;latent variable topic-region;location-based services;location-based social networks;online top-k recommendation process;personal interests;prediction space;sequential effect modeling;sequential influence;sequential patterns;sequential personalized spatial item recommender system;spatial item recommendation;users spatial activities;Additives;Global Positioning System;Hidden Markov models;Markov processes;Predictive models;Semantics;Trajectory}, 
doi={10.1109/ICDE.2016.7498304}, 
month={May},}
@INPROCEEDINGS{7049874, 
author={Nguyen Thanh Hai}, 
booktitle={The 2015 IEEE RIVF International Conference on Computing Communication Technologies - Research, Innovation, and Vision for Future (RIVF)}, 
title={A novel approach for location promotion on location-based social networks}, 
year={2015}, 
pages={53-58}, 
abstract={Maximizing the spread of influence was recently studied in several models of social networks. For location-based social networks, it also plays an important role, so a further research about this field is necessary. In this study, based on users' movement histories and their friendships, we first design the Predicting Mobility in the Near Future (PMNF) model to capture human mobility. Human mobility is inferred from the model by taking into account the following three features: (1) the regular movement of users, (2) the movement of friends of users, (3) hot regions, the most attractive places for all users. Second, from the result of predicting movements of users at each location, we determine influence of each user on friends with the condition that friends are predicted to come to the location. Third, the Influence Maximization (IM) algorithms are proposed to find a set of k influential users who can make the maximum influence on their friends according to either the number of influenced users (IM num) or the total of probability of moving the considered location of influenced users (IM score). The model and algorithms are evaluated on three large datasets collected by from 40,000 to over 60,000 users for each dataset over a period of two years in the real world at over 500,000 checked-in points as well as 400,000 to nearly 2,000,000 friendships also considered. The points are clustered into locations by density-based clustering algorithms such as OPTICS and GRID. As a result, our algorithms give an order of magnitude better performance than baseline approaches like choosing influential users based on the number of check-ins of users and selecting influential users by the number of friends of users. From the result of experiments, we are able to apply to some areas like advertisement to get the most efficient with the minimum costs. We show that our framework reliably determines the most influential users with high accuracy.}, 
keywords={mobile computing;optimisation;pattern clustering;social networking (online);GRID;IM algorithms;OPTICS;PMNF model;advertisement;density-based clustering algorithms;friendships;human mobility;influence maximization algorithms;location promotion;location-based social networks;movement histories;point clustering;predicting mobility in the near future model;Clustering algorithms;History;Prediction algorithms;Predictive models;Social network services;Testing;Training;influence spread;influenced users;influential users;location-based social networks;maximum influence;mobility prediction}, 
doi={10.1109/RIVF.2015.7049874}, 
month={Jan},}
@ARTICLE{7283632, 
author={H. Xiong and D. Zhang and G. Chen and L. Wang and V. Gauthier and L. E. Barnes}, 
journal={IEEE Transactions on Mobile Computing}, 
title={iCrowd: Near-Optimal Task Allocation for Piggyback Crowdsensing}, 
year={2016}, 
volume={15}, 
number={8}, 
pages={2010-2022}, 
abstract={This paper first defines a novel spatial-temporal coverage metric, k-depth coverage, for mobile crowdsensing (MCS) problems. This metric considers both the fraction of subareas covered by sensor readings and the number of sensor readings collected in each covered subarea. Then iCrowd, a generic MCS task allocation framework operating with the energy-efficient Piggyback Crowdsensing task model, is proposed to optimize the MCS task allocation with different incentives and k-depth coverage objectives/ constraints. iCrowd first predicts the call and mobility of mobile users based on their historical records, then it selects a set of users in each sensing cycle for sensing task participation, so that the resulting solution achieves two dual optimal MCS data collection goals-i.e., Goal. 1 near-maximal k-depth coverage without exceeding a given incentive budget or Goal. 2 near-minimal incentive payment while meeting a predefined k-depth coverage goal. We evaluated iCrowd extensively using a large-scale real-world dataset for these two data collection goals. The results show that: for Goal.1, iCrowd significantly outperformed three baseline approaches by achieving 3-60 percent higher k-depth coverage; for Goal.2, iCrowd required 10.0-73.5 percent less incentives compared to three baselines under the same k-depth coverage constraint.}, 
keywords={mobile computing;MCS problems;historical records;iCrowd;k-depth coverage;mobile crowdsensing;near-optimal task allocation;novel spatial-temporal coverage metric;optimal MCS data collection;piggyback crowdsensing;sensor readings;Air quality;Data collection;Measurement;Mobile communication;Poles and towers;Resource management;Sensors;MCS task allocation;Mobile crowdsensing (MCS);incentives}, 
doi={10.1109/TMC.2015.2483505}, 
ISSN={1536-1233}, 
month={Aug},}
@ARTICLE{7576670, 
author={X. Cao and Y. Chen and C. Jiang and K. J. Ray Liu}, 
journal={IEEE Transactions on Signal and Information Processing over Networks}, 
title={Evolutionary Information Diffusion Over Heterogeneous Social Networks}, 
year={2016}, 
volume={2}, 
number={4}, 
pages={595-610}, 
abstract={A huge amount of information, created and forwarded by millions of people with various characteristics, is propagating through the online social networks every day. Understanding the mechanisms of the information diffusion over the social networks is critical to various applications including online advertisement and website management. Different from most of the existing works, we investigate the information diffusion from an evolutionary game-theoretic perspective and try to reveal the underlying principles dominating the complex information diffusion process over the heterogeneous social networks. Modeling the interactions among the heterogeneous users as a graphical evolutionary game, we derive the evolutionary dynamics and the evolutionarily stable states (ESSs) of the diffusion. The different payoffs of the heterogeneous users lead to different diffusion dynamics and ESSs among them, in accordance with the heterogeneity observed in real-world datasets. The theoretical results are confirmed by simulations. We also test the theory on Twitter hashtag dataset. We observe that the derived evolutionary dynamics fit the data well and can predict the future diffusion data.}, 
keywords={evolutionary computation;game theory;social networking (online);ESS;Twitter hashtag dataset;Web site management;evolutionarily stable states;evolutionary game-theoretic perspective;evolutionary information diffusion;heterogeneous social networks;online advertisement;online social networks;Biological system modeling;Diffusion processes;Game theory;Games;Twitter;Information diffusion;evolutionary game theory;heterogeneous social networks}, 
doi={10.1109/TSIPN.2016.2613680}, 
ISSN={2373-776X}, 
month={Dec},}
@INPROCEEDINGS{7073369, 
author={D. Li and G. Chen and Y. Li and W. Xu}, 
booktitle={2014 11th International Computer Conference on Wavelet Actiev Media Technology and Information Processing(ICCWAMTIP)}, 
title={Mining consumer's opinion target based on translation model and word representation}, 
year={2014}, 
pages={97-101}, 
abstract={In recent years, the E-commercial plays an important role in people's daily life. When on the Internet, people often buy commodities from Taobao, Tmall and make comments on them, the comments of the goods may have closely connections with commercial value, which often reflect what's the consumers really care when they choose one piece of good among thousands of other similar ones. How to mining these aspects which the consumers really concern is a problem left unsolved. As a potential effective solution to construct structured information for people's preference, Information Extraction(IE) has attracted more and more scholar's attention. A meaningful research area is Opinion Target Extraction(OTE). This paper proposed a system using translation model as well as word representation method to obtain user's interests on dataset in Chinese. To release the word segmentation error, a finely generated system with new Chinese word detection module is proposed. The experiments on two corpus subjected on digital product verify the effective of our method.}, 
keywords={consumer behaviour;data mining;information retrieval;marketing data processing;Chinese word detection module;IE;Internet;OTE;Taobao;Tmall;consumer opinion target mining;e-commercial;electronic commercial;information extraction;opinion target extraction;translation model;user interest;word representation;word segmentation error;Adaptation models;Bipartite graph;Data mining;Feature extraction;Hidden Markov models;Semantics;Syntactics;Chinese new word detection;User interests mining;opinion target extraction;translation model;word representation}, 
doi={10.1109/ICCWAMTIP.2014.7073369}, 
month={Dec},}
@INPROCEEDINGS{7253955, 
author={A. D. Kadam and S. V. Shinde and S. D. Joshi and S. P. Medhane and S. B. Nikam and S. R. Pawar}, 
booktitle={2015 International Conference on Electrical, Electronics, Signals, Communication and Optimization (EESCO)}, 
title={Hybrid intelligent trail to search engine answering machine: Squat appraisal on pedestal technology (hybrid search machine)}, 
year={2015}, 
pages={1-6}, 
abstract={Arched type Swing in loom of information retrieval system is observed with record progression to information fetch, to knowledge data processing, to intelligent information progression. Subsequent processing machines like document retrieval, text summarization, search engines, rule based machines, expert systems have been developed. These machines have dedicated performance with retrieval measure in particular dimension. Machine learning methods have facilitated reasoning machine with ability like humans. Still a corner in research argues highly intelligent time constraint fact seeking real world information processing machine. Hybrid technology is integration of optimized approaches at various levels of information processing. We proposed a hybrid search answer machine with four techniques of optimization “question reformulation” (from user-intent, profile), “search method” (semantic concept, context, machine learning), “answer presentation” (ranking algorithm), “decision support ” (comparative analysis to choose best techniques to retrieve results). Data corpus is heart of any IR system large dataset facilitates good search which argue to distributed data and computing. Intelligence is reformation proceeds that excel our time and dataset. The machine is designed to facilitated updatable training dataset for fact seeking knowledge acquirement “it trains over data”. Muti-agent model distributed search methodology is proposed. In precise Hybrid extraction of “hybrid models” is performed. Semantic context (concept based) user profiled; best machine learning, decision supportive multiagent distributed search system is proposed. This paper gives underlying technologies overview, with examinations of 30 papers is done as with recent r- view of technology advancement. The review outcomes are orderly placed with 3 research query answering. The outputs of query structure a trail to search engine answering machine. We facilitate research done by scholars on technology perspective we integrate them to draw a sketch of hybrid search answering. In domain “a point of reference” concepts of research are studied, with comparative views on advance in IR. We identify the benchmark of research methods blueprint and explore space of research in area of intelligent machine implementation.}, 
doi={10.1109/EESCO.2015.7253955}, 
month={Jan},}
@INPROCEEDINGS{7297659, 
author={X. He and J. Li and D. Aloi}, 
booktitle={2014 International Conference on Connected Vehicles and Expo (ICCVE)}, 
title={WiFi based indoor localization with adaptive motion model using smartphone motion sensors}, 
year={2014}, 
pages={786-791}, 
abstract={We present an adaptive motion model for tracking the movement of smartphone user by using the motion sensors (accelerometer, gyroscope and magnetometer) embedded in the smartphone. A particle filter based estimator is used to seamlessly fuse the adaptive motion model with a WiFi based indoor localization system. The system applies Gaussian process regression to train the collected WiFi received signal strength (RSS) dataset, and particle filter for the estimation of the smartphone user's location and movement. Simulations were conducted in MATLAB to provide more insights of the proposed approach. The experiments carried out with an iOS device in typical library environment illustrate that our system is an accurate, real-time, highly integrated system.}, 
keywords={Gaussian processes;RSSI;image sensors;particle filtering (numerical methods);regression analysis;smart phones;wireless LAN;Gaussian process regression;RSS dataset;WiFi;WiFi received signal strength;accelerometer;adaptive motion model;gyroscope;iOS device;indoor localization system;magnetometer;particle filter;smartphone motion sensors;Adaptation models;Hidden Markov models;IEEE 802.11 Standard;Legged locomotion;Mathematical model;Particle filters;Sensors;Gaussian process regression;WiFi RSS;adaptive motion model;indoor localization;motion sensors;particle filter;smartphone}, 
doi={10.1109/ICCVE.2014.7297659}, 
ISSN={2378-1289}, 
month={Nov},}
@ARTICLE{6156448, 
author={J. Sang and C. Xu and J. Liu}, 
journal={IEEE Transactions on Multimedia}, 
title={User-Aware Image Tag Refinement via Ternary Semantic Analysis}, 
year={2012}, 
volume={14}, 
number={3}, 
pages={883-895}, 
abstract={Large-scale user contributed images with tags are easily available on photo sharing websites. However, the noisy or incomplete correspondence between the images and tags prohibits them from being leveraged for precise image retrieval and effective management. To tackle the problem of tag refinement, we propose a method of Ranking based Multi-correlation Tensor Factorization (RMTF), to jointly model the ternary relations among user, image, and tag, and further to precisely reconstruct the user-aware image-tag associations as a result. Since the user interest or background can be explored to eliminate the ambiguity of image tags, the proposed RMTF is believed to be superior to the traditional solutions, which only focus on the binary image-tag relations. During the model estimation, we employ a ranking based optimization scheme to interpret the tagging data, in which the pair-wise qualitative difference between positive and negative examples is used, instead of the point-wise 0/1 confidence. Specifically, the positive examples are directly decided by the observed user-image-tag interrelations, while the negative ones are collected with respect to the most semantically and contextually irrelevant tags. Extensive experiments on a benchmark Flickr dataset demonstrate the effectiveness of the proposed solution for tag refinement. We also show attractive performances on two potential applications as the by-products of the ternary relation analysis.}, 
keywords={Web sites;data handling;image retrieval;matrix decomposition;optimisation;semantic Web;tensors;Flickr dataset;RMTF;image retrieval;model estimation;photo sharing Web sites;point-wise 0/1 confidence;ranking based multicorrelation tensor factorization;ranking based optimization scheme;tagging data interpretation;ternary relation analysis;ternary semantic analysis;user-aware image tag refinement;user-aware image-tag associations;user-image-tag interrelations;Correlation;Data models;Media;Noise measurement;Semantics;Tagging;Tensile stress;Factor analysis;social media;tag refinement;tensor factorization}, 
doi={10.1109/TMM.2012.2188782}, 
ISSN={1520-9210}, 
month={June},}
@INPROCEEDINGS{6915979, 
author={H. M. Marin-Castro and V. J. Sosa-Sosa and I. Lopez-Arevalo}, 
booktitle={17th International Conference on Information Fusion (FUSION)}, 
title={A tree-based WQI modeling approach for integrating Web databases}, 
year={2014}, 
pages={1-8}, 
abstract={Everyday, more and more specialized databases (car rental, hotels, airfares, etc.) are available on the Web and can be only queried by means of a Web Query Interface (WQI). Since in the Web is increasing the number of domain-specific databases, it is getting very complicated for end users to explore the information stored in them. In this context, research efforts are focused on building a single (unified) specific-domain WQI that allows user to query and integrate information available in different Web databases. The construction of such integrated WQI, for a given domain, involves several complex tasks, specially the extraction, representation, understanding and mapping of semantic content of each individual WQI associated to a web database. Previous approaches have considered hierarchical models to build integrated WQI, preserving the ancestor-descendant relationships in individual WQIs. In this work, we propose a novel tree-based approach for automatic construction of a hierarchical model of visual content of WQIs, representing their components in a clear and concise form. In the proposed approach, the Document Object Model(DOM) tree of each WQI considered in the integration process is processed by a specialized web resource to obtain relevant visual information in the WQI such as fields (UIs), groups of UIs and super-groups as well as their corresponding labels. This process is guided by a set of 8 design heuristic rules for the right identification of labels and components. Experiments to evaluate the proposed strategy were conducted on the ICQ and Tel-8 datasets of UIUC repository. Our results showed that the proposed tree-based approach for representing the visual components in a WQI has more than 94% of accuracy, improving current reported approaches and making easier the integration process of domain-specifi}, 
keywords={database management systems;document handling;query processing;trees (mathematics);user interfaces;Document Object Model tree;ICQ dataset;Tel-8 dataset;UIUC repository;Web database integration;Web query interface;Web resource;ancestor-descendant relationships;domain-specific databases;heuristic rules;hierarchical models;semantic content extraction;semantic content mapping;semantic content representation;semantic content understanding;tree-based WQI modeling approach;Databases;Engines;HTML;Rendering (computer graphics);Semantics;Vectors;Visualization}, 
month={July},}
@ARTICLE{7506251, 
author={K. Hashmi and Z. Malik and A. Erradi and A. Rezgui}, 
journal={IEEE Transactions on Services Computing}, 
title={QoS Dependency Modeling for Composite Systems}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Software as a service is a well accepted software deployment and distribution model that has grown exponentially in the last few years. One of the biggest benefits of SaaS is the automated composition of different services in a composite system. It allows users to automatically find and bind these services (to maximize the productivity), meeting both functional and non-functional requirements. In this paper, we present a framework for modeling the dependency relationships among different Quality of Service parameters of the component services. Our proposed approach considers the different invocation patterns of component services, and presents a service composition framework to model the dependencies and uses the global QoS for service selection. We evaluate the efficiency of our proposed technique on the WSDream-QoS Dataset [1].}, 
keywords={Art;Interconnected systems;Optimization;Probabilistic logic;Quality of service;Service-oriented architecture;Transportation;Dependency Modeling;QoS;Web Service}, 
doi={10.1109/TSC.2016.2589244}, 
ISSN={1939-1374}, 
month={},}
@INPROCEEDINGS{6002716, 
author={Cheng Hu and Gang Chen and Jiang Ye and Feng Xu}, 
booktitle={2011 International Conference on Multimedia Technology}, 
title={ArcGIS-based development of a land subsidence information management system using C/S and B/S mixing mode strategy: A case study in North China plain}, 
year={2011}, 
pages={3451-3455}, 
abstract={Land subsidence has become one of the incontestable environmental disasters in North China Plain, which has a strong negative impact on the development of local community economy. In order to study the current situation and evolving tendency of subsidence, and accordingly to enhance the level of subsidence prevention and control, a custom-built information system was determined to be developed. This paper elaborates the overall process of software development from the system architecture design to functionality implementation. A mixing mode of C/S and B/S was applied to increase the using flexibility of the system. The subsystem based on C/S mode, for specialized users, was run in Intranet with characteristics of high speed, powerful functionality and tight security, and the other subsystem based on B/S was designed for public users with less functionality in Internet and Browse environment. These two seeming independent subsystems connect a same spatial dataset through ArcGIS SDE technology, in which all the spatial data was organized and stored in Geodatabase model. In C/S subsystem, a methodology of table data relating and filtering was used to regionalize the discrete spatio-temporal field monitoring data. Our subsidence information Management system facilitates to share data socially, provide end-users a cost-saving solution to access up-to-date spatial datasets customized for a specific topic to different users with limited or enough GIS knowledge.}, 
keywords={disasters;geographic information systems;geomorphology;ArcGIS SDE technology;ArcGIS-based development;B/S mixing mode strategy;C/S mixing mode strategy;Geodatabase model;North China Plain;discrete spatio-temporal field monitoring data;environmental disasters;intranet;land subsidence information management system;software development;Geographic Information Systems;Geology;Information management;Interpolation;Monitoring;Spatial databases;ArcGIS;Information Management System;Land subsidence;North China Plain Subsidence}, 
doi={10.1109/ICMT.2011.6002716}, 
month={July},}
@INPROCEEDINGS{7832149, 
author={K. Guan and Y. Zhang and P. Song}, 
booktitle={2016 IEEE International Conference on Information and Automation (ICIA)}, 
title={A personalization recommendation method with time characteristics}, 
year={2016}, 
pages={2012-2015}, 
abstract={Personalization recommendation can effectively solve the negative influence of information overload to users in the environment of Big Data. And the existing personalization recommendation model is insufficient in integrating the time characteristics of users' behavior. We build a new extend model of personalization recommendation based on the method of latent factor model. Time characteristics of users' historical behavior are introduced into new model to improve the predictions' accuracy. we implement the new model based on the method of factorization machines, and verify the validity of new model by using the data of movielens dataset. Experimental results demonstrate better performance of new model in improving the predictions' accuracy of users' preferences compared with existing model.}, 
keywords={Big Data;recommender systems;Big Data;factorization machines;latent factor model;movielens dataset;personalization recommendation method;prediction accuracy improvement;time characteristics;user behavior;user preferences;Collaboration;Computational modeling;Data models;Filtering;Matrix decomposition;Predictive models;Sparse matrices;latent factor model;personalized messages;recommendation systems;time characteristics}, 
doi={10.1109/ICInfA.2016.7832149}, 
month={Aug},}
@INPROCEEDINGS{6628516, 
author={I. Duque and K. Dautenhahn and K. L. Koay and l. Willcock and B. Christianson}, 
booktitle={2013 IEEE RO-MAN}, 
title={A different approach of using Personas in human-robot interaction: Integrating Personas as computational models to modify robot companions' behaviour}, 
year={2013}, 
pages={424-429}, 
abstract={The current paper focuses on a novel integration of the Personas technique into HRI studies, and the definition of a Persona-Based Computational Behaviour Model for achieving socially intelligent robot companions in living environments. Our core interest is the creation of companions adapted to users' needs to support their activities of daily living. The aim is to create a mechanism that allows us to develop initial robot behaviour, i.e. behaviour when first encountering the user, which is already adapted to each user without the necessity of collecting in advance a large dataset to train the system. A persona represents the specific needs of many individuals for a particular scenario. This technique helps us develop initial robot behaviour adapted to user needs, and so reduces the amount of trials that participants have to perform during early stages of the system development. The paper describes how this behaviour model has been created and integrated into a functional architecture, and presents the motivation, background and conceptual framework for this new research direction. Future empirical studies will validate this approach and expand the initial definition of our model.}, 
keywords={assisted living;human-robot interaction;intelligent robots;HRI;Persona-based computational behaviour model;background framework;behaviour model;computational models;conceptual framework;daily living activities;functional architecture;human-robot interaction;living environments;robot companion behaviour modification;socially intelligent robot companions;system development;Adaptation models;Computational modeling;Connectors;Human computer interaction;Human-robot interaction;Robot sensing systems}, 
doi={10.1109/ROMAN.2013.6628516}, 
ISSN={1944-9445}, 
month={Aug},}
@INPROCEEDINGS{7723716, 
author={R. Maher and D. Malone}, 
booktitle={2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)}, 
title={Analysing and Predicting the Runtime of Social Graphs}, 
year={2016}, 
pages={370-376}, 
abstract={The explosion of Social Network Analysis (SNA) in many different areas and the growing need for powerful data analysis has emphasized the importance of in-memory big data processing in computer systems. Particularly, large-scale graphs are gaining much more attention due to their wide range of application. This rise, accompanied by a massive number of vertices and edges, led computations to become increasingly expensive and time consuming. That is why there is a move towards distributed systems or Big Data cluster(s) to provide the required computational power and memory to handle such demand of huge graphs. Thus, figuring out whether a new social graph dataset can be processed successfully on a personal machine or there is a need for a distributed system or big-memory machine is still a remaining open question. In this paper, we try to address this question by providing a comparative analysis for the performance of two of the most well known SNA tools for performing commonly used graph algorithms such as counting Triads, calculating Degree Distribution and finding Clusters which can give an indication of the possibility of carrying out the work on a personal machine. Based on these measurements, we train different supervised machine learning models for predicting the execution time of these algorithms. We compare the accuracy of the different machine learning models and provided the details of the most accurate model that can be exploited by end users to better estimate the execution time expected for processing new social graphs on a personal machine.}, 
keywords={Big Data;data analysis;learning (artificial intelligence);social networking (online);SNA;data analysis;graph edges;graph vertices;in-memory big data processing;large-scale graph;social graph dataset;social network analysis;supervised machine learning models;Algorithm design and analysis;Clustering algorithms;Database languages;Facebook;Machine learning algorithms;Prediction algorithms;Graph Algorithms;Graph Analytics;Performance;Predictive Modeling;Social Graphs;Social Network Analysis}, 
doi={10.1109/BDCloud-SocialCom-SustainCom.2016.62}, 
month={Oct},}
@INPROCEEDINGS{5973927, 
author={Jianwei Wu and Shanshan Hao}, 
booktitle={2011 International Conference on Computer Science and Service System (CSSS)}, 
title={An dynamic mixed type collaborative recommendation algorithm base on RSS subscribing}, 
year={2011}, 
pages={2990-2992}, 
abstract={In order to solve collaborative filtering recommendation system recommended decline in the quality for the sparse dataset, an dynamic mixed type collaborative recommendation algorithm base on RSS subscribing is presented, which users' preference items vector based on information classification of their subscribing from RSS Feed is constructed and a users' comprehensive interest model is built according to interest analysis based on users' subscribing behavior, reading behavior including the information of users' reading self-subscribing and recommended subscribing, then the dynamic recommending is done combining content and collaborative filtering . Experiment result shows that the algorithm is better than the traditional collaborative filtering ones in improving the recommendation dependability and accuracy.}, 
keywords={groupware;information filtering;recommender systems;RSS subscribing;content filtering;dynamic mixed type collaborative filtering recommendation algorithm;reading behavior;really simple syndication;user comprehensive interest model;user preference items vector;user reading selfsubscribing;users subscribing behavior;Algorithm design and analysis;Collaboration;Computers;Feeds;Filtering;Heuristic algorithms;XML;personalized;really simple syndication;recommendation;similarity}, 
doi={10.1109/CSSS.2011.5973927}, 
month={June},}
@INPROCEEDINGS{7569216, 
author={P. Lamabam and K. Chakma}, 
booktitle={2016 IEEE International Conference on Engineering and Technology (ICETECH)}, 
title={A language identification system for code-mixed English-Manipuri Social Media text}, 
year={2016}, 
pages={79-83}, 
abstract={In Social Media, code mixing and code switching take place where users often communicate in two or more languages. The state-of-the-art techniques fail when language identification is done on such code mixed informal texts due to the presence of lexical borrowings, creative spellings and phonetic typing. Therefore, automatic language identification for the code mixed social media texts has become a challenging task in the field of Natural Language Processing(NLP). A dataset was selected containing Twitter and Facebook posts that exhibit code-mixing between English and Manipuri. Some word-level language identification experiments are performed using this dataset such as Trigram-based and Conditional Random Field (CRF)-based models. We find that the CRF-based models have given better accuracies in identifying the languages.}, 
keywords={natural language processing;random processes;social networking (online);text analysis;CRF-based models;Facebook posts;NLP;Trigram-based models;Twitter posts;automatic language identification system;code mixed informal texts;code switching;code-mixed English-Manipuri social media text;conditional random field-based models;creative spellings;lexical borrowings;natural language processing;phonetic typing;Dictionaries;Facebook;Support vector machines;Switches;Training;Twitter;CRF;Natural Language Processing;feature;trigrams}, 
doi={10.1109/ICETECH.2016.7569216}, 
month={March},}
@INPROCEEDINGS{7817044, 
author={Y. Xu and D. Zhou and S. Lawless}, 
booktitle={2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)}, 
title={Inferring Your Expertise from Twitter: Integrating Sentiment and Topic Relatedness}, 
year={2016}, 
pages={121-128}, 
abstract={The ability to understand the expertise of users in Social Networking Sites (SNSs) is a key component for delivering effective information services such as talent seeking and user recommendation. However, users are often unwilling to make the effort to explicitly provide this information, so existing methods aimed at user expertise discovery in SNSs primarily rely on implicit inference. This work aims to infer a user's expertise based on their posts on the popular micro-blogging site Twitter. The work proposes a sentiment-weighted and topic relation-regularized learning model to address this problem. It first uses the sentiment intensity of a tweet to evaluate its importance in inferring a user's expertise. The intuition is that if a person can forcefully and subjectively express their opinion on a topic, it is more likely that the person has strong knowledge of that topic. Secondly, the relatedness between expertise topics is exploited to model the inference problem. The experiments reported in this paper were conducted on a large-scale dataset with over 10,000 Twitter users and 149 expertise topics. The results demonstrate the success of our proposed approach in user expertise inference and show that the proposed approach outperforms several alternative methods.}, 
keywords={inference mechanisms;information services;learning (artificial intelligence);sentiment analysis;social networking (online);SNSs;Twitter;expertise inference;information services;micro-blogging site;sentiment intensity;sentiment relatedness;sentiment-weighted learning model;social networking sites;topic relatedness;topic relation-regularized learning model;user expertise;Analytical models;Computer languages;Knowledge discovery;Sentiment analysis;Twitter;inference model;social network;user expertise}, 
doi={10.1109/WI.2016.0027}, 
month={Oct},}
@INPROCEEDINGS{7603361, 
author={P. Guan and Y. Wang}, 
booktitle={2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)}, 
title={Personalized scientific literature recommendation based on user's research interest}, 
year={2016}, 
pages={1273-1277}, 
abstract={As the rapid growth of digital scientific and technical literatures, the scientific researchers need personalized retrieval to satisfy their research interests urgently. Unlike previous methods that just use simple keyword matching, we strengthen the semantic information of scientific literature by merging metadata such as title, keywords, abstract and citation in this dissertation. Then we use vector space model with tf-idf value of each term to model each scientific literature. We structure user's interest model by subject term vector with different weight. Different weight means different concern degree to each subject term by user. To emphasize the quality of recommended literature, we not only calculate the similarity between user's research interest and literatures, but also consider the total times cited of recommended literatures. As experimental dataset, we collect literatures from Web of Science under the topic of “pressure sensor”. The experimental results show that this recommendation method could well indentify user's research interest. So, it can improve efficiency of user literature retrieval and increase the accuracy of the literature recommendation.}, 
keywords={human factors;information retrieval;merging;meta data;recommender systems;scientific information systems;Web of Science;metadata merging;personalized retrieval;personalized scientific literature recommendation;pressure sensor;semantic information;tf-idf value;user research interest;vector space model;Magnetic field measurement;Metadata;Optical variables measurement;Pressure measurement;Pressure sensors;Temperature measurement;Temperature sensors;metadata;personalized recommendation;scientific literature;similarity}, 
doi={10.1109/FSKD.2016.7603361}, 
month={Aug},}
@INPROCEEDINGS{4658259, 
author={D. Willkomm and S. Machiraju and J. Bolot and A. Wolisz}, 
booktitle={2008 3rd IEEE Symposium on New Frontiers in Dynamic Spectrum Access Networks}, 
title={Primary Users in Cellular Networks: A Large-Scale Measurement Study}, 
year={2008}, 
pages={1-11}, 
abstract={Most existing studies of spectrum usage have been performed by actively sensing the energy levels in specific RF bands including cellular bands. In this paper, we provide a unique, complementary analysis of cellular primary usage by analyzing a dataset collected inside a cellular network operator. One of the key aspects of our dataset is its scale - it consists of data collected over three weeks at hundreds of base stations. We dissect this data along different dimensions to characterize and model primary usage as well as understand its temporal and spatial variations. Our analysis reveals several results that are relevant if dynamic spectrum access (DSA) approaches are to be deployed for cellular frequency bands. For instance, we find that call durations show significant deviations from the often- used exponential distribution, which makes call-based modeling more complicated. We also show that a random walk process, which does not use call durations, can often be used for modeling the aggregate cell capacity. Furthermore, we highlight some applications of our results to improve secondary usage of licensed spectrum.}, 
keywords={cellular radio;exponential distribution;radio spectrum management;telecommunication network management;cellular network operator;dynamic spectrum access;exponential distribution;random walk process;Base stations;Cellular networks;Data analysis;Energy measurement;Energy states;Land mobile radio cellular systems;Large-scale systems;Performance evaluation;Radio frequency;TV broadcasting}, 
doi={10.1109/DYSPAN.2008.48}, 
month={Oct},}
@INPROCEEDINGS{6821585, 
author={D. Xu and S. Yang}, 
booktitle={2014 Fourth International Conference on Communication Systems and Network Technologies}, 
title={Location Prediction in Social Media Based on Contents and Graphs}, 
year={2014}, 
pages={1177-1181}, 
abstract={The location information in social media often provides insight on the repercussions of events and is becoming more and more vital to the applications such as localized search, local news recommendation and emergency detection. However, for the security and privacy concerns, most users in social media are unwilling to publish their location, and they may post non-existing places or submit unspecific place names. We focus on analysis post content and social graph to predict users' resident location in city-level level. Based on a fraction of the users with known locations in their profiles, we propose a new filter to identify location sensitive words in posts content. We also expand the social relationships of users and combine these two approaches to propose a mixture probabilistic model to estimate user location. The experimental results on a large scale of dataset crawled from Tencent weibo demonstrate that our approach achieve an accuracy of 60.2% in city level and outperform state-of-the-art approaches.}, 
keywords={data privacy;graph theory;probability;security of data;social networking (online);Tencent Weibo;city-level;location information;location prediction;mixture probabilistic model;post content analysis;privacy concerns;security concerns;social graph;social media;social relationships;user location estimation;Accuracy;Cities and towns;Computational modeling;Media;Predictive models;Probabilistic logic;Visualization;content based;graph based;location prediction;social media;user profiling}, 
doi={10.1109/CSNT.2014.239}, 
month={April},}
@INPROCEEDINGS{7024522, 
author={A. Drosou and P. Moschonas and D. Tzovaras}, 
booktitle={2013 International Conference of the BIOSIG Special Interest Group (BIOSIG)}, 
title={Robust 3D face recognition from low resolution images}, 
year={2013}, 
pages={1-8}, 
abstract={This paper proposes a combined approach for robust face recognition from low resolution images captured by a low-budget commercial depth camera. The low resolution of the facial region of interest is compensated via oversampling techniques and efficient trimming algorithms for the generation of an accurate 3D facial model. Two state of the art algorithms for geometric feature extraction are then utilized, i.e. the estimation of the Directional Indices between all the isogeodasic stripes of the same facial surface via the 3D WeightedWalkthroughs (3DWW) transformation and the estimation of the Spherical Face Representation (SFR). The biometric signature is then enhanced via user-specific cohort biometric templates for each feature, respectively. The experiments have been carried out on the demanding "BIOTAFTOTITA" dataset and the results are very promising even under difficult scenarios (e.g. looking away instances, grimace, etc.). Despite the obvious superiority of the 3DWW transformation over the SFR, it has been noted that the score level fusion of both algorithms improves the authentication performance of the system. On the contrary, only the 3DWW transformation should be preferred in identification scenarios. Indicatively, the experimental validation on the aforementioned dataset containing 54 subjects illustrates significant succeeds an identification performance of ~ 100% in Rank-1 and Equal Error Rate of 0:25% regarding the authentication performance in the neutral face experiment.}, 
keywords={cameras;face recognition;feature extraction;image representation;image resolution;image sampling;3D facial model;3D weighted walkthrough transformation;3DWW transformation;BIOTAFTOTITA dataset;SFR;biometric signature;directional indices estimation;equal error rate;facial region of interest;facial surface;geometric feature extraction;isogeodasic stripes;low resolution images;low-budget commercial depth camera;oversampling techniques;rank-1;robust 3D face recognition;spherical face representation estimation;trimming algorithms;user-specific cohort biometric templates;Face;Face recognition;Feature extraction;Image resolution;Nose;Robustness;Three-dimensional displays}, 
month={Sept},}
@INPROCEEDINGS{7550929, 
author={Z. Li and W. Shang and M. Yan}, 
booktitle={2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)}, 
title={News text classification model based on topic model}, 
year={2016}, 
pages={1-5}, 
abstract={In modern society, some famous news websites such as Sina and Times server to provide information every day for millions of users. But with the continuous development of information technology, the amount of disorder data is increasing. How to organize the text and make automatically text classification has become a challenge. The traditional manual classification of news text not only consumes a lot of human and financial resources, but also hardly achieved classification task quickly. In this paper, the paper mainly makes a research about the news text classification. It proposes a news text classification model based on Latent Dirichlet Allocation (LDA). Due to the dimension of the news texts is too high, this model uses topic model to make text dimension reduced and get features. At the same time, the paper also makes a research on Softmax regression algorithm to solve multi-class of text problems in our life and make it as model's classifier. The paper evaluates proposed model on a real news dataset and the result of the experiment shows the improved model performs relatively well. The model can effectively reduce the features dimension of the news text and get good classification results.}, 
keywords={Web sites;pattern classification;publishing;regression analysis;text analysis;LDA;Softmax regression algorithm;automatical text classification;disorder data;latent dirichlet allocation;news Web sites;news text classification model;news texts dimension reduction;text organization;text problem multiclass solution;topic model;Classification algorithms;Data models;Logistics;Machine learning algorithms;Resource management;Text categorization;Training;softmax regression;text classification;topic model}, 
doi={10.1109/ICIS.2016.7550929}, 
month={June},}
@INPROCEEDINGS{7332587, 
author={Bin Xu and Jin Qi and Kun Wang and Ye Wang and Xiaoxuan Hu and Yanfei Sun}, 
booktitle={2015 11th International Conference on Heterogeneous Networking for Quality, Reliability, Security and Robustness (QSHINE)}, 
title={An improved artificial bee colony algorithm for cloud computing service composition}, 
year={2015}, 
pages={310-317}, 
abstract={The rapid increase of using cloud computing encourages service vendors to supply services with different features and provide them in a service pool. Service composition (SC) problem in cloud computing environment becomes a key issue because of the increase of service quantity and user requirements of the quality of service experience. To satisfy the demands on quality of service experience and realize an efficient algorithm for SC problem, a quality of experience (QoE) evaluation model based on fuzzy analytic hierarchy process (FAHP) for SC problem is put forward first. Then, an improved artificial bee colony (IABC) optimization algorithm for QoE based SC problem is proposed. The algorithm improves the updating mechanism of scout bees by introducing current global optimal solution to accelerate convergence velocity and eventually to improve the solution quality. Finally, the experimental results on QWS dataset show that IABC has a better performance on QoE based SC problem, compared with original ABC, PSO and DE.}, 
keywords={analytic hierarchy process;ant colony optimisation;cloud computing;FAHP;IABC optimization algorithm;QoE evaluation model;cloud computing environment;cloud computing service composition;fuzzy analytic hierarchy process;improved artificial bee colony;improved artificial bee colony algorithm;quality of experience;service pool;service vendors;supply services;Tin;Wireless sensor networks;Artificial Bee Colony;Quality of Experience;Service Composition}, 
month={Aug},}
@INPROCEEDINGS{7403696, 
author={M. G. Ozsoy and F. Polat and R. Alhajj}, 
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
title={Modeling individuals and making recommendations using multiple social networks}, 
year={2015}, 
pages={1184-1191}, 
abstract={Web-based platforms, such as social networks, review web-sites, and e-commerce web-sites, commonly use recommendation systems to serve their users. The common practice is to have each platform captures and maintains data related to its own users. Later the data is analyzed to produce user specific recommendations. We argue that recommendations could be enriched by considering data consolidated from multiple sources instead of limiting the analysis to data captured from a single source. Integrating data from multiple sources is analogous to watching the behavior and preferences of each user on multiple platforms instead of a limited one platform based vision. Motivated by this, we developed a recommendation framework which utilizes user specific data collected from multiple platforms. To the best of our knowledge, this is the first work aiming to make recommendations by consulting multiple social networks to produce a rich modeling of user behavior. For this purpose, we collected and anonymized a specific dataset that contains information from BlogCatalog, Twitter and Flickr web-sites. We implemented several different types of recommendation methodologies to observe their performances while using single versus multiple features from a single source versus multiple sources. The conducted experiments showed that using multiple features from multiple social networks produces a wider perspective of user behavior and preferences leading to improved recommendation outcome.}, 
keywords={recommender systems;social networking (online);BlogCatalog;Flickr;Twitter;Website;multiple social networks;recommendation methodologies;user behavior modeling;Blogs;Cities and towns;Collaboration;Optimization;Twitter;Individual modeling;Multiple data sources;Recommendation systems;Social networks}, 
doi={10.1145/2808797.2808898}, 
month={Aug},}
@INPROCEEDINGS{6364815, 
author={E. Bonetto and M. Mellia and M. Meo}, 
booktitle={2012 IEEE International Conference on Communications (ICC)}, 
title={Energy profiling of ISP points of presence}, 
year={2012}, 
pages={5973-5977}, 
abstract={Points of Presence (PoP), large aggregation nodes of a telecommunication network in which users lines are interconnected to the ISP backbone network, are relevant elements of the ISP network infrastructure. Motivated by the today interest of both ISPs and researchers to more energy efficient Internet, we investigate the power consumption of PoPs of FASTWEB, a national-wide ISP in Italy. Energy profiling spans a period of one year, and includes both ADSL and FTTH access technologies. This extensive and unique dataset allows us to shed light on energy consumption of ISP networks, which we profile against other measurements, such as external temperature and PoP handled traffic. Results show that energy consumption is independent on the traffic, while it is strongly correlated with both daily and annual variability of temperature, due to air conditioning energy cost. Starting from these results, we investigate some possible strategies to reduce ISP electricity bill. We consider the adoption of energy proportional architectures which are currently being investigated by both manufacturers and researchers. Moreover, we evaluate the possible energy savings using real traffic data and we obtain that simple PoPs energy saving models based on two-three energy operating configuration can achieve results comparable to fully energy proportional model.}, 
keywords={Internet;digital subscriber lines;optical fibre subscriber loops;power consumption;telecommunication networks;ADSL access technology;FASTWEB;FTTH access technology;ISP backbone network;ISP electricity bill;ISP network infrastructure;ISP points of presence;Internet;Italy;PoP handled traffic;aggregation nodes;energy consumption;energy profiling;energy proportional architectures;interconnected users lines;national-wide ISP;power consumption;telecommunication network;Air conditioning;Correlation;Energy consumption;Energy measurement;Green products;Temperature measurement}, 
doi={10.1109/ICC.2012.6364815}, 
ISSN={1550-3607}, 
month={June},}
@INPROCEEDINGS{7363812, 
author={W. Xie and F. Zhu and S. Liu and K. Wang}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={Modelling cascades over time in microblogs}, 
year={2015}, 
pages={677-686}, 
abstract={One of the most important features of microblogging services such as Twitter is how easy it is to re-share a piece of information across the network through various user connections, forming what we call a "cascade". Business applications such as viral marketing have driven a tremendous amount of research effort predicting whether a certain cascade will go viral. Yet the rarity of viral cascades in real data poses a challenge to all existing prediction methods. One solution is to simulate cascades that well fit the real viral ones, which requires our ability to tell how a certain cascade grows over time. In this paper, we build a general time-aware cascade model for each particular cascade, in which the chance of one user's re-sharing behaviour over time is modelled as a hazard function of time. Based on two key observations on user retweeting behaviour, we design an appropriate hazard function specifically for Twitter network. We evaluate our model on a large real Twitter dataset with over two million retweeting cascades. Our experiment results show our proposed model outperforms other baseline models in terms of model fitting. Further, we make use of our model to simulate viral cascades, which are otherwise few and far in-between, to alleviate the imbalance issue in cascade data, offering a 20% boost in viral cascade discovery.}, 
keywords={marketing;social networking (online);Twitter network;baseline models;business applications;cascade data;cascades modelling;hazard function;microblogging services;model fitting;resharing behaviour;retweeting cascades;time-aware cascade model;viral cascade discovery;viral cascades;viral marketing;Analytical models;Business;Data models;Hazards;Mathematical model;Twitter}, 
doi={10.1109/BigData.2015.7363812}, 
month={Oct},}
@INPROCEEDINGS{5373704, 
author={R. Cilla and M. A. Patricio and A. Belanga and J. M. Molina}, 
booktitle={2009 2nd International Symposium on Applied Sciences in Biomedical and Communication Technologies}, 
title={Non-supervised discovering of user activities in visual sensor networks for ambient intelligence applications}, 
year={2009}, 
pages={1-6}, 
abstract={Ambient intelligence systems need to know what the users are doing. In this paper, An architecture for human activity recognition using a visual sensor network is proposed. The video sequence perceived by each camera is locally processed to obtain a local activity label. These activity labels are fused by an upper tier to obtain a global activity label. The activities recognized by the system are not specified a priori, they are discovered using automatic model selection techniques. Then, an expert has to label the discovered activities to give them a semantic meaning. Results of the application of the activity discovering procedure to a smart home dataset are shown.}, 
keywords={cameras;image recognition;image sensors;image sequences;ambient intelligence systems;automatic model selection techniques;camera;global activity label;human activity recognition architecture;local activity label;smart home dataset;user activities;video sequence;visual sensor networks;Ambient intelligence;Application software;Cameras;Hidden Markov models;Humans;Intelligent sensors;Labeling;Smart homes;Supervised learning;Wearable sensors}, 
doi={10.1109/ISABEL.2009.5373704}, 
ISSN={2325-5315}, 
month={Nov},}
@INPROCEEDINGS{1436520, 
author={M. La Cascia and L. Valenti and S. Sclaroff}, 
booktitle={IEEE 6th Workshop on Multimedia Signal Processing, 2004.}, 
title={Fully automatic, real-time detection of facial gestures from generic video}, 
year={2004}, 
pages={175-178}, 
abstract={A technique for the detection of facial gestures from low resolution video sequences is presented. The technique builds upon the automatic 3D head tracker formulation of [M. La Cascia et al., 2000]. The tracker is based on the registration of a texture-mapped cylindrical model. Facial gesture analysis is performed in the texture map by assuming that the residual registration error can be modeled as a linear combination of facial motion templates. Two formulations are proposed and tested. In one formulation, the head and facial motion are estimated in a single, combined linear system. In the other formulation, head motion and then facial motion are estimated in a two-step process. The two-step approach significantly yields better accuracy in facial gesture analysis. The system is demonstrated in detecting two types of facial gestures: "mouth opening" and "eyebrows raising." On a dataset with lots of head motion, the two-step algorithm achieved a recognition accuracy of 70% for the "mouth opening" and an accuracy of 66% for "eyebrows raising" gestures. The algorithm can reliably track and classify facial gestures without any user intervention and runs in real-time.}, 
keywords={gesture recognition;image motion analysis;image registration;image resolution;image sequences;image texture;video signal processing;eyebrows raising;facial gesture analysis;facial gesture detection;facial motion template;generic video;mouth opening;real-time detection;texture-mapped cylindrical model;two-step approach;video sequence;Eyebrows;Face detection;Feature extraction;Head;Image sequences;Motion estimation;Mouth;Optical computing;Tracking;Video sequences}, 
doi={10.1109/MMSP.2004.1436520}, 
month={Sept},}
@INPROCEEDINGS{636788, 
author={M. Derthick and S. F. Roth and J. Kolojejchick}, 
booktitle={Information Visualization, 1997. Proceedings., IEEE Symposium on}, 
title={Coordinating declarative queries with a direct manipulation data exploration environment}, 
year={1997}, 
pages={65-72}, 
abstract={Interactive visualization techniques allow data exploration to be a continuous process, rather than a discrete sequence of queries and results as in traditional database systems. However limitations in expressive power of current visualization systems force users to go outside the system and form a new dataset in order to perform certain operations, such as those involving the relationship among multiple objects. Further, there is no support for integrating data from the new dataset into previous visualizations, so users must recreate them. Visage's information centric paradigm provides an architectural hook for linking data across multiple queries, removing this overhead. This paper describes the addition to Visage of a visual query language, called VQE, which allows users to express more complicated queries than in previous interactive visualization systems. Visualizations can be created from queries and vice versa. When either is updated, the other changes to maintain consistency.}, 
keywords={data analysis;data visualisation;database management systems;graphical user interfaces;interactive systems;query languages;query processing;visual languages;VQE;Visage;consistency;data analysis;data exploration environment;database systems;dataset;declarative queries;direct manipulation;graphical user interfaces;information centric paradigm;interactive visualization;multiple objects;visual query language;Data analysis;Data visualization;Database languages;Database systems;Feedback;Joining processes;Object oriented modeling;Robot kinematics;User interfaces;Visual databases}, 
doi={10.1109/INFVIS.1997.636788}, 
month={Oct},}
@INPROCEEDINGS{6839910, 
author={X. Luo and M. Zhou and Y. Xia and Q. Zhu}, 
booktitle={2014 23rd Wireless and Optical Communication Conference (WOCC)}, 
title={Predicting web service QoS via matrix-factorization-based collaborative filtering under non-negativity constraint}, 
year={2014}, 
pages={1-6}, 
abstract={Matrix-factorization based collaborative filtering is an efficient approach to the problem of user-side quality-of-service (QoS) prediction. In this work, we focus on building a matrix-factorization-based collaborative filtering model for QoS prediction under a non-negativity constraint. The motivation is that since QoS data such as response time, cost and throughput, are all positive, a non-negative model can better demonstrate their characteristics. By investigating a non-negative training process relying on each involved feature, we invent a non-negative latent factor model to deal with the sparse QoS matrix subject to the non-negativity constraint. We subsequently introduce Tikhonov regularization into it to obtain the regularized non-negative latent factor model. Their efficiency is proven by the experimental results on a large industrial dataset.}, 
keywords={Web services;collaborative filtering;matrix algebra;QoS data;QoS prediction;Tikhonov regularization;Web service QoS;collaborative filtering;industrial dataset;matrix-factorization;nonnegative latent factor model;nonnegativity constraint;sparse QoS matrix;Accuracy;Collaboration;Filtering;Mathematical model;Quality of service;Training;Web services;Big Data;Collaborative Filtering;Matrix Factorization;Non-negativity;QoS-prediction}, 
doi={10.1109/WOCC.2014.6839910}, 
ISSN={2379-1268}, 
month={May},}
@ARTICLE{6783754, 
author={D. Wu and G. Zhang and J. Lu}, 
journal={IEEE Transactions on Fuzzy Systems}, 
title={A Fuzzy Preference Tree-Based Recommender System for Personalized Business-to-Business E-Services}, 
year={2015}, 
volume={23}, 
number={1}, 
pages={29-43}, 
abstract={The Web creates excellent opportunities for businesses to provide personalized online services to their customers. Recommender systems aim to automatically generate personalized suggestions of products/services to customers (businesses or individuals). Although recommender systems have been well studied, there are still two challenges in the development of a recommender system, particularly in real-world B2B e-services: (1) items or user profiles often present complicated tree structures in business applications, which cannot be handled by normal item similarity measures and (2) online users' preferences are often vague and fuzzy, and cannot be dealt with by existing recommendation methods. To handle both these challenges, this study first proposes a method for modeling fuzzy tree-structured user preferences, in which fuzzy set techniques are used to express user preferences. A recommendation approach to recommending tree-structured items is then developed. The key technique in this study is a comprehensive tree matching method, which can match two tree-structured data and identify their corresponding parts by considering all the information on tree structures, node attributes, and weights. Importantly, the proposed fuzzy preference tree-based recommendation approach is tested and validated using an Australian business dataset and the MovieLens dataset. Experimental results show that the proposed fuzzy tree-structured user preference profile reflects user preferences effectively and the recommendation approach demonstrates excellent performance for tree-structured items, especially in e-business applications. This study also applies the proposed recommendation approach to the development of a Web-based business partner recommender system.}, 
keywords={business data processing;fuzzy set theory;recommender systems;tree data structures;Australian business dataset;MovieLens dataset;Web-based business partner recommender system;automatic personalized product suggestion generation;automatic personalized service suggestion generation;business applications;comprehensive tree matching method;fuzzy preference tree-based recommender system;fuzzy set techniques;fuzzy tree-structured user preference modeling;node attributes;online user preferences;personalized business-to-business e-services;personalized online services;real-world B2B e-services;tree weights;tree-structured data;tree-structured item recommendation;Business;Data models;Ontologies;Recommender systems;Semantics;Vectors;Vegetation;E-business;fuzzy preferences;recommender systems;tree matching;web-based support system}, 
doi={10.1109/TFUZZ.2014.2315655}, 
ISSN={1063-6706}, 
month={Feb},}
@INPROCEEDINGS{6976794, 
author={Y. Kim and F. C. Pereira and F. Zhao and A. Ghorpade and P. C. Zegras and M. Ben-Akiva}, 
booktitle={2014 22nd International Conference on Pattern Recognition}, 
title={Activity Recognition for a Smartphone Based Travel Survey Based on Cross-User History Data}, 
year={2014}, 
pages={432-437}, 
abstract={In transport modeling and prediction, trip purposes play an important role. The most particular case is activity-based modeling, whereby mobility choices (e.g. mode, path, and departure time) are made in order to carry out specific activities. A current challenge, however, lies on getting appropriate data that relates observed trips with their purpose. Recently, a Smartphone-based travel survey (the Future Mobility Survey, FMS) was conducted in Singapore that collected location data from 793 participants. Each FMS user was required to collect data for at least 14 days and validate at least 5 of them. This dataset presents diverse opportunities in terms of developing machine learning models for the future versions of FMS, where the validation process is intelligent and easy to use (e.g. having pre-filled activities associated to the user traces). This paper proposes a learning model that, given a stop location, identifies the most likely activity associated to it. Our data often contains errors or noise due to limited functionality of physical sensors in a dense area, and human mistakes in the validation process. To alleviate this effect, we generate heterogeneous features by different spatial quantization techniques and apply ensemble learning for a good generalization performance.}, 
keywords={data handling;learning (artificial intelligence);mobile computing;mobility management (mobile radio);object recognition;smart phones;traffic engineering computing;travel industry;user interfaces;Singapore;activity-based modeling;cross-user history data;ensemble learning;future mobility survey;heterogeneous feature generation;human activity recognition;location data collection;machine learning models;mobility choices;prefilled activities;smartphone based travel survey;spatial quantization techniques;transport modeling;transport prediction;user traces;validation process;Accuracy;Decision trees;Frequency-domain analysis;Global Positioning System;Quantization (signal);Sensors}, 
doi={10.1109/ICPR.2014.83}, 
ISSN={1051-4651}, 
month={Aug},}
@INPROCEEDINGS{5893824, 
author={O. H. Embarak}, 
booktitle={2011 International Conference on Innovations in Information Technology}, 
title={A method for solving the cold start problem in recommendation systems}, 
year={2011}, 
pages={238-243}, 
abstract={Recommendation systems become essential in web applications that provide mass services, and aim to suggest automatic items (services) of interest to users. The most popular used technique in such systems is the collaborative filtering (CF) technique, which suffer from some problems such as the cold-start problem, the privacy problem, the user identification problem, the scalability problem, etc. In this paper, we address the cold-start problem by giving recommendations to any new users who have no stored preferences, or recommending items that no user of the community has seen yet. While there have been lots of studies to solve the cold start problem, but it solved only item-cold start, or user-cold start, also provided solutions still suffer from the privacy problem. Therefore, we developed a privacy protected model to solve the cold start problem (in both cases user and item cold start). We suggested two types of recommendation (node recommendation and batch recommendation), and we compared the suggested method with three other alternative methods (Triadic Aspect Method, Naïve Filterbots Method, and MediaScout Stereotype Method), and we used dataset collected from online web news to generate recommendations based on our method and based on the other alternative three methods. We calculated level of novelty, coverage, and precision. We found that our method achieved higher level of novelty in the batch recommendation whilst it achieved higher levels of coverage and precision in the node recommendations technique comparing to these three methods.}, 
keywords={data privacy;information filtering;recommender systems;Web applications;batch recommendation;cold start problem;collaborative filtering technique;item cold start;mediascout stereotype method;naive filterbots method;privacy problem;recommendation systems;scalability problem;triadic aspect method;user cold start;user identification problem;Equations;Filtering algorithms;Information filters;Mathematical model;Semantics;Web sites;Adaptive Web Systems;Personalization Systems;Recommendation Systems;The Cold Start Problem}, 
doi={10.1109/INNOVATIONS.2011.5893824}, 
month={April},}
@INPROCEEDINGS{7443993, 
author={I. Mulasastra and A. Taplaksint}, 
booktitle={2015 IEEE International WIE Conference on Electrical and Computer Engineering (WIECON-ECE)}, 
title={Elementization of Thai postal addresses: A hybrid approach}, 
year={2015}, 
pages={561-564}, 
abstract={Postal addresses are common data among various databases. However, address structures may be defined differently, especially in Thailand, where national data standards have not been established yet. Many information systems allow users to enter addresses in free-form text; all elements of each address are stored in a single field. Comparing free text addresses in many algorithms such as de-duplication and house-holding is difficult. Hence, to enhance data sharing and integration among organizations, elementization of postal addresses in a standard format is essential. This study develops an algorithm for automatically elementizing Thai postal addresses by using a rule-based approach and the Hidden Markov Model. We evaluate our system on a real-life dataset and yield an accuracy of 97%.}, 
keywords={hidden Markov models;text analysis;Thai postal addresses;data sharing;deduplication;elementization;free text addresses;hidden Markov model;house-holding;information systems;national data standards;real-life dataset;standard format;Finite element analysis;Hidden Markov models;Ontologies;Roads;Tagging;Training;Urban areas;Address cleansing;Address elementization;Algorithm;Hidden Markov Model (HMM);Rule-based approach}, 
doi={10.1109/WIECON-ECE.2015.7443993}, 
month={Dec},}
@INPROCEEDINGS{4415337, 
author={X. Hu and Y. Yin and B. Zhang}, 
booktitle={2007 International Conference on Computational Intelligence and Security (CIS 2007)}, 
title={Mining Temporal Web Interesting Patterns}, 
year={2007}, 
pages={227-231}, 
abstract={Previous work on mining web associations focus primar- ily on finding frequent access patterns in the data. However, they ignore an important relationship that web frequent ac- cess patterns have the dynamic characteristic of time vary- ing. It is also important that in database, some items which are infrequent in whole dataset but those depend on the present of a mediator itemset may be frequent in a particu- lar time period, which induce some interesting patterns may not be discover. In this study, our focus is to apply a new mining technique called indirect association onto tempo- ral web data and propose the TIFP-mine algorithm based on a new model WM-graph, which are both capable of ex- tracting all temporal indirect frequent patterns and its tem- poral extended patterns. Experimental results confirm that TIFP-mine algorithm is efficient and effective. Our analysis shows very promising results, especially in terms of identi- fying Web users with distinct interests.}, 
keywords={Association rules;Computational intelligence;Data mining;Data security;Databases;Information filtering;Itemsets;Navigation;Web mining;Web server}, 
doi={10.1109/CIS.2007.105}, 
month={Dec},}
@INPROCEEDINGS{1604667, 
author={Un-Hong Wong and Hon-Cheng Wong and Zesheng Tang}, 
booktitle={Ninth International Conference on Computer Aided Design and Computer Graphics (CAD-CG'05)}, 
title={An interactive system for visualizing 3D human organ models}, 
year={2005}, 
pages={6 pp.-}, 
abstract={Demonstrations play a major role in education process. Although many specimens are readily available, particularly in medicine and biology, demonstrations are commonly performed by showing these static specimens to medical or biological students. Interactive demonstrations can significantly impacts learning. An interactive system for visualizing 3D human organ models can fulfil this need. In this paper, we present techniques to realize such a system which can let the user select and view one or several major organ models extracted from segmented visible human dataset interactively through a simple graphical user interface. The stereoscopic views of these organ models are also achieved with this system running on a PC-based stereo-ready system. In our system, the marching cubes algorithm is used but new implementation we proposed to greatly improve both the speed and quality of surface rendering results is performed. This new implementation can generate the marching cubes cases on-the-fly within the surface extraction process of the models by considering the relationship of the vertices, borders, and surfaces of each voxel. We also describe a new method for specifying the normals for the extracted triangles without the need of physical information (such as intensity values stored in a medical computed tomography (CT) dataset) stored in the voxels. These normals will be used for lighting the extracted models later. Furthermore, a memory arrangement scheme is designed to enhance the usability of the system.}, 
keywords={computer aided instruction;data visualisation;graphical user interfaces;interactive systems;medical image processing;3D human organ model visualization;graphical user interface;interactive system;marching cubes algorithm;medical computed tomography;memory arrangement scheme;segmented visible human dataset;stereoscopic views;surface rendering;Biological system modeling;Computed tomography;Data mining;Educational technology;Graphical user interfaces;Humans;Information technology;Interactive systems;Rendering (computer graphics);Visualization}, 
doi={10.1109/CAD-CG.2005.21}, 
month={Dec},}
@ARTICLE{7152900, 
author={H. Huang and J. Tang and L. Liu and J. Luo and X. Fu}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Triadic Closure Pattern Analysis and Prediction in Social Networks}, 
year={2015}, 
volume={27}, 
number={12}, 
pages={3374-3389}, 
abstract={We study the problem of group formation in online social networks. In particular, we focus on one of the most important human groups-the triad-and try to understand how closed triads are formed in dynamic networks, by employing data from a large microblogging network as the basis of our study. We formally define the problem of triadic closure prediction and conduct a systematic investigation. The study reveals how user demographics, network characteristics, and social properties influence the formation of triadic closure. We also present a probabilistic graphical model to predict whether three persons will form a closed triad in a dynamic network. Different kernel functions are incorporated into the proposed graphical model to quantify the similarity between triads. Our experimental results with the large microblogging dataset demonstrate the effectiveness (+10 percent over alternative methods in terms of F1-Score) of the proposed model for the prediction of triadic closure formation.}, 
keywords={probability;social networking (online);dynamic network;kernel functions;microblogging network;online social network;probabilistic graphical model;triadic closure pattern analysis;triadic closure pattern prediction;Graphical models;Predictive models;Probabilistic logic;Social factors;Social network services;Predictive model;Social Network;Social influence;Social network;Triadic closure;predictive model;social influence;triadic closure}, 
doi={10.1109/TKDE.2015.2453956}, 
ISSN={1041-4347}, 
month={Dec},}
@INPROCEEDINGS{7319884, 
author={A. Razi and F. Afghah and V. Varadan}, 
booktitle={2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
title={Identifying gene subnetworks associated with clinical outcome in ovarian cancer using Network Based Coalition Game}, 
year={2015}, 
pages={6509-6513}, 
abstract={The problem of identifying interacting genes that jointly are associated with a phenotype is considered. When the number of features are extremely large compared to the number of samples, there may be several subsets of features that provide acceptable levels of predictability. This is particularly true in cancer genomics, where we are interested in finding functionally related gene sets likely to jointly drive cancer phenotypes. In this paper, a novel game theoretic solution is proposed by modeling genes as players of a Coalition Game. This method discovers and develops informative gene subnetworks by integrating gene expression profiling of cancer tissues with protein-protein interaction (PPI) networks. These subnetworks are gradually developed by selective addition of candidate genes that present maximal Shapely values in coalition with subnetworks of genes. We applied the proposed algorithm to an ovarian cancer dataset (N = 201), in order to identify optimal subnetworks that can predict cancer progression risk in response to platinum-based therapy. We show improved predictive power of the proposed method when compared to state-of-the-art feature selection methods, with the added advantage of identifying potentially functional gene subnetworks that may provide insights into the mechanisms underlying cancer progression.}, 
keywords={biomedical engineering;cancer;game theory;genetics;medical computing;molecular biophysics;proteins;cancer progression risk;cancer tissues;gene expression profiling;gene subnetworks;network based coalition game;ovarian cancer;platinum-based therapy;protein-protein interaction networks;Bioinformatics;Cancer;Games;Gene expression;Medical treatment;Prediction algorithms;Proteins}, 
doi={10.1109/EMBC.2015.7319884}, 
ISSN={1094-687X}, 
month={Aug},}
@INPROCEEDINGS{7801552, 
author={Y. Fang and S. L. Wei and J. G. Sik}, 
booktitle={2016 13th Conference on Computer and Robot Vision (CRV)}, 
title={Multi-player Detection with Articulated Mixtures-of-Parts Representation Constrained by Global Appearance}, 
year={2016}, 
pages={416-423}, 
abstract={We describe a new representation for multiple articulated sport players for court player detection. Instead modeling object into deformable parts templates or mixtures of small parts which just capture local appearance of parts and spatial relations between parts, our proposed model trained with local part information with global constraint by a structured SVM can capture not only such local appearance and spatial relations above, but also the semantic relations between body parts which are the critical factors for precisely detecting objects and pose estimation. Our approach has several novel properties: (1) we adopt typical articulated part-based model with global appearance constraint to control trade-off between recall and precision for detection (2) we incorporate semantic knowledge about various articulation of court players into our mixture-of-parts model, these semantic knowledge are popularly used for pose estimation (3) after the root (global) and part (local) bounding boxes are predicted by our system, we train a linear least-squares regression model to output the final detection results which yields considerable improvements in performance. In our experiments with very challenging APIDIS basketball dataset and standard INRIA person dataset, it indicates that our detection system achieves state-of-art performance compared to previous works.}, 
keywords={image representation;least squares approximations;object detection;pose estimation;regression analysis;sport;support vector machines;APIDIS basketball dataset;articulated mixtures-of-parts representation;articulated part-based model;court player detection;global appearance constraint;linear least-squares regression model;multiplayer detection;multiple articulated sport player representation;object detection;part bounding boxes;pose estimation;root bounding boxes;semantic knowledge;standard INRIA person dataset;structured SVM;Cameras;Deformable models;Feature extraction;Games;Histograms;Semantics;Support vector machines;ADIPIS;deformable parts;multiple player detection;semantic knowledge;structured SVM}, 
doi={10.1109/CRV.2016.57}, 
month={June},}
@INPROCEEDINGS{7298688, 
author={E. Simo-Serra and S. Fidler and F. Moreno-Noguer and R. Urtasun}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Neuroaesthetics in fashion: Modeling the perception of fashionability}, 
year={2015}, 
pages={869-877}, 
abstract={In this paper, we analyze the fashion of clothing of a large social website. Our goal is to learn and predict how fashionable a person looks on a photograph and suggest subtle improvements the user could make to improve her/his appeal. We propose a Conditional Random Field model that jointly reasons about several fashionability factors such as the type of outfit and garments the user is wearing, the type of the user, the photograph's setting (e.g., the scenery behind the user), and the fashionability score. Importantly, our model is able to give rich feedback back to the user, conveying which garments or even scenery she/he should change in order to improve fashionability. We demonstrate that our joint approach significantly outperforms a variety of intelligent baselines. We additionally collected a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information which can be exploited for our task. We also provide a detailed analysis of the data, showing different outfit trends and fashionability scores across the globe and across a span of 6 years.}, 
keywords={clothing;image texture;random processes;social networking (online);clothing;conditional random field model;diverse image;fashionability factor;heterogeneous dataset;intelligent baseline;meta information;neuroaesthetics;photograph;social Web site;textual information;Atmospheric measurements;Clothing;Computers;Encoding;Fans;Image color analysis;Particle measurements}, 
doi={10.1109/CVPR.2015.7298688}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{6751524, 
author={V. Prinet and D. Lischinski and M. Werman}, 
booktitle={2013 IEEE International Conference on Computer Vision}, 
title={Illuminant Chromaticity from Image Sequences}, 
year={2013}, 
pages={3320-3327}, 
abstract={We estimate illuminant chromaticity from temporal sequences, for scenes illuminated by either one or two dominant illuminants. While there are many methods for illuminant estimation from a single image, few works so far have focused on videos, and even fewer on multiple light sources. Our aim is to leverage information provided by the temporal acquisition, where either the objects or the camera or the light source are/is in motion in order to estimate illuminant color without the need for user interaction or using strong assumptions and heuristics. We introduce a simple physically-based formulation based on the assumption that the incident light chromaticity is constant over a short space-time domain. We show that a deterministic approach is not sufficient for accurate and robust estimation: however, a probabilistic formulation makes it possible to implicitly integrate away hidden factors that have been ignored by the physical model. Experimental results are reported on a dataset of natural video sequences and on the Gray Ball benchmark, indicating that we compare favorably with the state-of-the-art.}, 
keywords={image colour analysis;image sequences;lighting;natural scenes;video signal processing;GrayBall benchmark;camera;deterministic approach;illuminant chromaticity estimation;illuminant color estimation;image sequences;incident light chromaticity;light source;natural video sequences;physically-based formulation;probabilistic formulation;robust estimation;space-time domain;temporal acquisition;temporal sequences;Equations;Estimation;Image color analysis;Lighting;Mathematical model;Vectors;Videos;Color constancy;Image processing;Low-level vision;White balance}, 
doi={10.1109/ICCV.2013.412}, 
ISSN={1550-5499}, 
month={Dec},}
@INPROCEEDINGS{7276757, 
author={F. Franca and S. Schulz and P. Bronsert and P. Novais and M. Boeker}, 
booktitle={2015 International Symposium on Innovations in Intelligent SysTems and Applications (INISTA)}, 
title={Feasibility of an ontology driven tumor-node-metastasis classifier application: A study on colorectal cancer}, 
year={2015}, 
pages={1-7}, 
abstract={The objectives of this work are (1) to develop a classifier application for tumor staging based on a formal representation of the Tumor-Node-Metastasis classification system (TNM), and (2) to show the feasibility of this approach on real data. This paper presents a classifier application for colorectal tumors based on the TNM-O ontology. It was developed in the JAVA using the OWL-API. The TNM-O uses the Foundational Model of Anatomy for representing anatomical entities and BioTopLite2 as a domain-top-level ontology. The classifier application processes input data via a user interface or tabular data. The classification starts with the creation of RDF Individuals for each pathological information item formally described in the ontology. These Individuals are then classified by the HermiT Description Logics reasoner by A-Box classification. A dataset with 382 entries was provided by the pathology department of a university hospital. It was automatically classified with regard to metastatic regional lymph nodes. Results or expert classification by pathologists and automatic classification were compared. The automatic process helped to detect and explain inconsistencies between expert and automatic classifications. This work, we demonstrate the use of semantic technologies in a TNM classifier application separating underlying medical knowledge represented in OWL from process logics. The presented prototypical TNM classifier application shows the potential to be integrated in larger software systems.}, 
keywords={Java;application program interfaces;cancer;knowledge representation languages;medical computing;ontologies (artificial intelligence);pattern classification;tumours;A-box classification;BioTopLite2;HermiT description logics reasoner;JAVA;OWL-API;RDF Individuals;TNM classifier application;TNM-O ontology;anatomical entities;automatic classification;colorectal cancer;colorectal tumors;domain-top-level ontology;formal representation;foundational model of anatomy;medical knowledge representation;metastatic regional lymph nodes;ontology driven tumor-node-metastasis classifier application;pathological information item;process logics;semantic technologies;tabular data;tumor staging;university hospital;user interface;Graphical user interfaces;Lymph nodes;Metastasis;OWL;Ontologies;Tumors}, 
doi={10.1109/INISTA.2015.7276757}, 
month={Sept},}
@INPROCEEDINGS{6693447, 
author={J. Surma}, 
booktitle={2013 International Conference on Social Computing}, 
title={The Privacy Problem in Big Bata Applications: An Empirical Study on Facebook}, 
year={2013}, 
pages={955-958}, 
abstract={When using mobile phones, credit cards, electronic mail, browsing social networks etc., contemporary consumers leave behind thousands of digital footprints. Each footprint reflects actual actions that we take in given place and time. The analysis of thousands of such footprints conducted among large groups of people allows us to examine human behaviour on a scale that has never been imagined in scientific studies concerning psychology and sociology. The results of those analyses already have a significant influence on contemporary management, especially when it comes to new business opportunities in companies that employ business models based on the one-to-one relations with their customers. Nevertheless, this outstanding opportunity implies an enormous privacy problem. We will illustrate this issue by an empirical research based on the data gathered from Facebook, where users are using privacy controls that allow displaying their content only to a selected group of people. Users of such controls will likely continue positing more, even as their network grows or becomes sparser. We test these predictions using a dataset from Facebook gathered from a sample of college students and find statistical support for them. Our conclusions are that individuals are relatively prudent and are actually very aware of the social norms.}, 
keywords={data handling;data privacy;psychology;social networking (online);social sciences computing;statistical analysis;Facebook;big bata applications;business opportunities;contemporary management;credit cards;digital footprints;electronic mail;human behaviour;mobile phones;one-to-one customer relations;privacy problem;psychology;social networks;social norms;sociology;statistical support;Companies;Data privacy;Educational institutions;Facebook;Privacy;big data analysis;privacy problem;social netwrok sites}, 
doi={10.1109/SocialCom.2013.150}, 
month={Sept},}
@INPROCEEDINGS{6831808, 
author={N. K. Kalantari and E. Shechtman and S. Darabi and D. B. Goldman and P. Sen}, 
booktitle={2014 IEEE International Conference on Computational Photography (ICCP)}, 
title={Improving patch-based synthesis by learning patch masks}, 
year={2014}, 
pages={1-8}, 
abstract={Patch-based synthesis is a powerful framework for numerous image and video editing applications such as hole-filling, retargeting, and reshuffling. In all these applications, a patch-based objective function is optimized through a patch search-and-vote process. However, existing techniques typically use fixed-size square patches when comparing the distance between two patches in the search process. This presents a fundamental limitation for these methods, since many patches cover multiple regions that can move, occlude, or otherwise behave independently in source and target images. We address this problem by using masks to down-weight some pixels in the patch-comparison operation. The main challenge is to choose the right mask according to the content during the search-and-vote process. We show how simple user assistance can lead to excellent results in challenging hole-filling examples. In addition, we propose a fully automated solution by learning a model to predict an appropriate mask using a set of features extracted around each patch. The model is trained using a manually annotated dataset, augmented with simulated divergence from ground truth. We demonstrate that our proposed method improves over existing approaches for single-and multi-image hole-filling applications.}, 
keywords={feature extraction;image matching;image segmentation;optimisation;rendering (computer graphics);MIHF;SIHF;feature extraction;image editing applications;learning patch mask;manually annotated dataset;multiimage hole filling;patch comparison operation;patch search-and-vote process;patch-based objective function optimisation;patch-based synthesis improvement;simulated divergence;single image hole filling;user assistance;video editing applications;Boolean functions;Data structures;Feature extraction;Image color analysis;Image edge detection;Predictive models;Training}, 
doi={10.1109/ICCPHOT.2014.6831808}, 
month={May},}
@INPROCEEDINGS{6169869, 
author={H. P. Lai and M. Visani and A. Boucher and J. M. Ogier}, 
booktitle={2012 IEEE RIVF International Conference on Computing Communication Technologies, Research, Innovation, and Vision for the Future}, 
title={Unsupervised and Semi-Supervised Clustering for Large Image Database Indexing and Retrieval}, 
year={2012}, 
pages={1-6}, 
abstract={The feature space structuring methods play a very important role in finding information in large image databases. They organize indexed images in order to facilitate, accelerate and improve the results of further retrieval. Clustering, one kind of feature space structuring, may organize the dataset into groups of similar objects without prior knowledge (unsupervised clustering) or with a limited amount of prior knowledge (semi- supervised clustering). In this paper, we present both formal and experimental comparisons of different unsupervised clustering methods for structuring large image databases. We use different image databases of increasing sizes (Wang, PascalVoc2006, Caltech101, Core130k) to study the scalability of the different approaches. Moreover, a summary of semi-supervised clustering methods is presented and an interactive semi-supervised clustering model using the HMRF-kmeans is experimented on the Wang image database in order to analyse the improvement of the clustering results when user feedbacks are provided.}, 
keywords={content-based retrieval;database indexing;feature extraction;image retrieval;pattern clustering;unsupervised learning;visual databases;HMRF-k means;feature space structuring methods;image database indexing;image retrieval;prior knowledge;scalability;semi-supervised clustering;unsupervised clustering;user feedback;Clustering algorithms;Clustering methods;Context;Image databases;Indexes;Vectors}, 
doi={10.1109/rivf.2012.6169869}, 
month={Feb},}
@INPROCEEDINGS{1226177, 
author={M. Mandelzweig and A. B. Demko and B. Dolenko and R. L. Somorjai and N. L. Pizzi}, 
booktitle={CCECE 2003 - Canadian Conference on Electrical and Computer Engineering. Toward a Caring and Humane Technology (Cat. No.03CH37436)}, 
title={A projection method for the visualization of high-dimensional biomedical datasets}, 
year={2003}, 
volume={3}, 
pages={1453-1456 vol.3}, 
abstract={The analysis and interpretation of high-dimensional biomedical datasets for the purposes of confirmatory or exploratory data analysis is a challenging problem. The process raises issues that are not only typically associated with high-dimensional data but also with software implementations of the visualization models. The relative distance plane projection method, which uses a distance-based mapping for visualizing high dimensional patterns and their relative relationships, addresses these confounding factors. This paper describes the algorithm, its implementation in software, and the specialized user interface. Its functionality is demonstrated using a high-dimensional biomedical dataset.}, 
keywords={data analysis;data visualisation;medical computing;object-oriented programming;user interfaces;biomedical dataset analysis;data visualization;dimensional pattern;distance-based mapping;exploratory data analysis;projection method;projection pursuit;relative distance plane projection method;software implementation;user interface;visualization model;Bioinformatics;Computer displays;Councils;Data analysis;Data visualization;Pattern analysis;Signal to noise ratio;Software algorithms;Testing;User interfaces}, 
doi={10.1109/CCECE.2003.1226177}, 
ISSN={0840-7789}, 
month={May},}
@INPROCEEDINGS{7349762, 
author={S. S. Hasan and R. Brummet and O. Chipara and Y. H. Wu and T. Yang}, 
booktitle={2015 International Conference on Healthcare Informatics}, 
title={In-Situ Measurement and Prediction of Hearing Aid Outcomes Using Mobile Phones}, 
year={2015}, 
pages={525-534}, 
abstract={Audiologists have devised a battery of clinical tests to measure auditory abilities. While these tests can help determine the candidacy of patients for amplification intervention, they do not accurately predict the degree to which a patient would benefit from using a hearing aid (i.e., The hearing aid outcome). Measuring hearing aid outcomes in the real-world is challenging as it not only depends on a patient's auditory abilities, but also on auditory contexts that include characteristics of the listening activity, social context, and acoustic environment. This paper explores the problem of creating predictive models for hearing aid outcomes that incorporate information about auditory abilities, hearing-aid features, and auditory contexts. Our models are built on a dataset collected using a mobile phone application that measures auditory contexts and hearing aid outcomes using Ecological Momentary Assessments. The use of a mobile application allowed us to collect fine-grained hearing aid outcome measures in different auditory contexts. The dataset includes 5671 surveys from 34 patients collected over two years. Our analysis focuses on identifying the features necessary for predicting hearing aid outcomes in different clinical scenarios. Most importantly, we show that models that only included measures of auditory ability as features are cannot predict the hearing aid outcome of a patient with accuracy better than chance. Incorporating information about auditory contexts increases the prediction accuracy to 68%. More excitingly, accuracies as high as 90% can be achieved when a small amount of training data is collected from a patient in-situ. These results suggest that audiologists could prescribe a mobile phone application at the time of dispensing the hearing aid in order to accurately predict a patient's likelihood of becoming a successful and satisfied hearing aid user.}, 
keywords={hearing aids;medical computing;mobile computing;mobile handsets;acoustic environment;amplification intervention;auditory abilities;auditory contexts;clinical tests;ecological momentary assessments;features identification;hearing aid outcomes measurement;hearing aid outcomes prediction;hearing-aid features;listening activity;mobile phone application;prediction accuracy;predictive models;social context;Atmospheric measurements;Auditory system;Battery charge measurement;Context;Mobile handsets;Particle measurements;Standards;Smartphone sensing;data analysis;hearing aids;outcome measures}, 
doi={10.1109/ICHI.2015.101}, 
month={Oct},}
@INPROCEEDINGS{5627459, 
author={G. Zheng and N. Gerber and D. Widmer and C. Stieger and M. Caversaccio and L. P. Nolte and S. Weber}, 
booktitle={2010 Annual International Conference of the IEEE Engineering in Medicine and Biology}, 
title={Automated detection of fiducial screws from CT/DVT volume data for image-guided ENT surgery}, 
year={2010}, 
pages={2325-2328}, 
abstract={This paper presents an automated solution for precise detection of fiducial screws from three-dimensional (3D) Computerized Tomography (CT)/Digital Volume Tomography (DVT) data for image-guided ENT surgery. Unlike previously published solutions, we regard the detection of the fiducial screws from the CT/DVT volume data as a pose estimation problem. We thus developed a model-based solution. Starting from a user-supplied initialization, our solution detects the fiducial screws by iteratively matching a computer aided design (CAD) model of the fiducial screw to features extracted from the CT/DVT data. We validated our solution on one conventional CT dataset and on five DVT volume datasets, resulting in a total detection of 24 fiducial screws. Our experimental results indicate that the proposed solution achieves much higher reproducibility and precision than the manual detection. Further comparison shows that the proposed solution produces better results on the DVT dataset than on the conventional CT dataset.}, 
keywords={computerised tomography;medical image processing;surgery;CT dataset;CT-DVT volume data;DVT volume datasets;computer aided design model;digital volume tomography data;fiducial screw automated detection;image-guided ENT surgery;pose estimation problem;three-dimensional computerized tomography;user-supplied initialization;Computed tomography;Estimation;Fasteners;Feature extraction;Head;Solid modeling;Surgery;Algorithms;Automatic Data Processing;Automation;Cadaver;Equipment Design;Head;Humans;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Internal Fixators;Models, Statistical;Surgery, Computer-Assisted;Tomography, X-Ray Computed}, 
doi={10.1109/IEMBS.2010.5627459}, 
ISSN={1094-687X}, 
month={Aug},}
@INPROCEEDINGS{6996160, 
author={M. Parvathy and R. Ramya and K. Sundarakantham and S. Mercy Shalinie}, 
booktitle={2014 International Conference on Recent Trends in Information Technology}, 
title={Recommendation system with collaborative social tagging exploration}, 
year={2014}, 
pages={1-6}, 
abstract={Recommender system plays a significant role in reducing the information overload on the sites where users have searched and contented. Existing approaches which deals with such recommendation system apply collaborative filtering techniques to specify the most alike users whom they hope to make recommendations. Collaborative Filtering will significantly show better improvement with the enclosure of real data extraction from the suitable tagging system. In this paper, data from social tagging systems are extracted for every individual considering the correlations between users, items, and tag information. Tag information from users is the most decisive factor to predict the personalized suggestion for web users. Here, we rank the available content based tag information with the inclusion of temporal decay of users' behavior over time and the centrality of every node in the network. Finally, we use the common preference metric for effective personalization. Results have been experimentally demonstrated with the empirical dataset MovieLens and provided the results as an alternative recommendation method with simplicity and efficiency.}, 
keywords={collaborative filtering;data handling;recommender systems;MovieLens dataset;Web users;collaborative filtering techniques;collaborative social tagging exploration;content based tag information;real data extraction;recommendation system;Accuracy;Collaboration;Computational modeling;Filtering;Measurement;Motion pictures;Tagging;Collaborative Filtering;Recommendation system;Social Tagging;User Behavior}, 
doi={10.1109/ICRTIT.2014.6996160}, 
month={April},}
@INPROCEEDINGS{7548002, 
author={W. Blaszczak-Bak and A. Sobieraj}, 
booktitle={2016 Baltic Geodetic Congress (BGC Geomatics)}, 
title={Application of Regression Line to Obtain Specified Number of Points in Reduced Large Datasets}, 
year={2016}, 
pages={40-44}, 
abstract={Modern measurement techniques like scanning technology or sonar measurements, provide large datasets, which are a reliable source of information about measured object, however such datasets are sometimes difficult to develop. Therefore, the algorithms for reducing the number of such sets are incorporated into their processing. In the reduction algorithms based on the cartographic generalization method, it is required to input some parameters (e.g. tolerance), which are determined by the user. The choice of the values of parameters, and in results the number of points in the reduced set, is one of the key step in the algorithm's efficiency. Thus, it requires from the user to have the knowledge on how the reduction algorithm works, and what is the relationship between the values of these parameters and the final number of points in reduced set. In this article authors used the regression analysis to explore this aspect of processing the large datasets.}, 
keywords={cartography;data analysis;data mining;regression analysis;cartographic generalization method;large dataset processing;reduced large dataset;reduction algorithm;regression analysis;regression line application;scanning technology;sonar measurement;Algorithm design and analysis;Belts;Correlation;Correlation coefficient;Mathematical model;Prediction algorithms;Regression analysis;data analysis;data preprocessing;regression analysis;research and development}, 
doi={10.1109/BGC.Geomatics.2016.16}, 
month={June},}
@INPROCEEDINGS{5693319, 
author={M. Jamali and G. Haffari and M. Ester}, 
booktitle={2010 IEEE International Conference on Data Mining Workshops}, 
title={Modeling the Temporal Dynamics of Social Rating Networks Using Bidirectional Effects of Social Relations and Rating Patterns}, 
year={2010}, 
pages={344-351}, 
abstract={In this paper we first observe and analyze the temporal behavior of users in a social rating network on expressing ratings and creating social relations. Then, we model the temporal dynamics of a SRN based on our observations and using bidirectional effects of ratings and social relationships. While existing models for other types of social networks have captured some of the factors, our model is the first one to represent all four factors, i.e. social relations-on-ratings (social influence), social relations-on-social relations (transitivity), ratings-on-social relations (selection), and ratings-on-ratings (correlational influence). We also model the strength of each effect throughout the evolution of a SRN. Using our model, we develop a generative model for SRNs. Such a model can serve as basis for several purposes, in particular link prediction, rating prediction and prediction of future community structures. Given the sensitive nature of social network data, there are only very few public social rating network datasets. This motivates the development of generative models to create such synthetic datasets for research purposes. Our experimental study on the Epinions dataset demonstrates that the proposed model produces social rating networks that agree with real world data on a comprehensive set of evaluation criteria much better than existing models.}, 
keywords={social networking (online);Epinions dataset;bidirectional effects;community structures prediction;correlational influence;particular link prediction;public social rating network datasets;rating patterns;rating prediction;ratings-on-ratings;ratings-on-social relations;social influence;social relations-on-ratings;social relations-on-social relations;temporal dynamics modeling;Generative Models;Social Rating Networks;User Behavioral Modeling}, 
doi={10.1109/ICDMW.2010.103}, 
ISSN={2375-9232}, 
month={Dec},}
@ARTICLE{6517848, 
author={L. Chen and J. Wu and H. Jian and H. Deng and Z. Wu}, 
journal={IEEE Transactions on Services Computing}, 
title={Instant Recommendation for Web Services Composition}, 
year={2014}, 
volume={7}, 
number={4}, 
pages={586-598}, 
abstract={Web service composition helps users integrate services to create new large-granularity and value-added composite services. Most recent studies have focused on automatic AI-Planning-based static or dynamic composition at functional- or process-level. However in industry, most business applications are still composed manually or semi-automatically with abundant domain expertise. Consequently, to build a good and reliable composite service is really a time-consuming and professional task. Inspired by the Instant Search of Google, we propose an Instant recommendation approach to provide optimal suggestions while a composition process incrementally proceeds. In our model, we fully utilize the execution log of composite services, and intend to identify appropriate services which have been proved to be more reliable and robust, therefore those services have higher probability to fulfill users' demands. To find the top-k possible composite services in real-time, we adopt the A* search algorithm with various pruning heuristics to dynamically expand the search space efficiently. Experiments on a real-world dataset with 15,959 real Web services crawled from the Internet demonstrate the effectiveness and efficiency of the proposed approach.}, 
keywords={Web services;recommender systems;search problems;A* search algorithm;Google Instant Search;Web service composition;business applications;dynamically searched space expansion;execution log;incrementally proceeded composition process;instant recommendation approach;large-granularity value-added composite services;optimal suggestions;pruning heuristics;real Web service crawling;real-world dataset;service integration;top-k possible composite services;user demands;Algorithm design and analysis;Information retrieval;Internet;Quality of service;Time complexity;Web services;${rm A}^{ast}$;Bayes;Composite service;instant recommendation}, 
doi={10.1109/TSC.2013.32}, 
ISSN={1939-1374}, 
month={Oct},}
@INPROCEEDINGS{6492026, 
author={G. Ma and Y. Qu}, 
booktitle={2012 IEEE 11th International Conference on Signal Processing}, 
title={A local LDA based method for Latent Aspect Rating Analysis on reviews}, 
year={2012}, 
volume={3}, 
pages={2240-2245}, 
abstract={The expanding volume of online reviews has made it an important and challenging task to mine detailed information of opinions in those reviews. In many cases, along with the comment, a user also gives an overall rating on the target entity, which in fact could not reflect the detailed opinions on each aspect of the entity. Therefore, Latent Aspect Rating Analysis (LARA) came into being. The goal of LARA is to infer a latent rating and weight for each aspect based on the overall rating and the review content. Although some methods have been applied to solve this problem, they rely too much on the predefinition of aspects with keywords, which needs supervision and may hence introduce some biases. In this paper, we propose a Local LDA based method for LARA, which includes two stages. In the first stage, we employ Local LDA to discover aspects automatically. In the second stage, we use LRR model to infer the latent rating and weight for each of the discovered aspects. The experimental results on the review dataset demonstrate the advantages of the proposed method over the state-of-the-art methods.}, 
keywords={Internet;data mining;information analysis;LARA;detailed information;latent aspect rating analysis;latent rating;local LDA based method;online reviews;overall rating;review dataset;target entity;Opinion mining;latent rating analysis;local LDA;review aspects}, 
doi={10.1109/ICoSP.2012.6492026}, 
ISSN={2164-5221}, 
month={Oct},}
@INPROCEEDINGS{6038090, 
author={M. Mohajireen and C. Ellepola and M. Perera and I. Kahanda and U. Kanewala}, 
booktitle={2011 6th International Conference on Industrial and Information Systems}, 
title={Relational similarity model for suggesting friends in online social networks}, 
year={2011}, 
pages={334-339}, 
abstract={Suggesting friends is a very important aspect in any online social network. In this paper, we present a relational similarity model for suggesting friends in online social networks, which uses relational features as opposed to the non-relational features that are used in current friend suggestion applications. We take a supervised learning approach and build a model that uses information of not only the two central users but also of their current neighborhoods. We use a dataset from Facebook to evaluate the accuracy of our model by comparing the performance of feature sets belonging to relational/non-relational categories and boolean and numerical sub categories. We show experimentally that the relational information improves the accuracy of boolean features but does not affect the performance of numerical features. Moreover, we show that our overall model is highly accurate in recommending people in online social networks.}, 
keywords={Boolean functions;learning (artificial intelligence);recommender systems;social networking (online);Facebook;boolean features;friend suggestion applications;online social networks;people recommendation;relational similarity model;supervised learning approach;Accuracy;Decision trees;Facebook;Motion pictures;Numerical models;Predictive models;Data mining;Predictive models;Recommender systems;Social network services}, 
doi={10.1109/ICIINFS.2011.6038090}, 
ISSN={2164-7011}, 
month={Aug},}
@INPROCEEDINGS{7829644, 
author={E. Ataie and E. Gianniti and D. Ardagna and A. Movaghar}, 
booktitle={2016 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)}, 
title={A Combined Analytical Modeling Machine Learning Approach for Performance Prediction of MapReduce Jobs in Cloud Environment}, 
year={2016}, 
pages={431-439}, 
abstract={Nowadays MapReduce and its open source implementation, Apache Hadoop, are the most widespread solutions for handling massive dataset on clusters of commodity hardware. At the expense of a somewhat reduced performance in comparison to HPC technologies, the MapReduce framework provides fault tolerance and automatic parallelization without any efforts by developers. Since in many cases Hadoop is adopted to support business critical activities, it is often important to predict with fair confidence the execution time of submitted jobs, for instance when SLAs are established with end-users. In this work, we propose and validate a hybrid approach exploiting both queuing networks and support vector regression, in order to achieve a good accuracy without too many costly experiments on a real setup. The experimental results show how the proposed approach attains a 21% improvement in accuracy over applying machine learning techniques without any support from analytical models.}, 
keywords={cloud computing;data handling;fault tolerance;learning (artificial intelligence);parallel processing;queueing theory;regression analysis;support vector machines;Apache Hadoop;MapReduce jobs;analytical modeling machine learning;automatic parallelization;business critical activity support;cloud environment;fault tolerance;massive dataset handling;open source implementation;performance prediction;queuing networks;support vector regression;Analytical models;Cloud computing;Computational modeling;Data models;Noise measurement;Predictive models;Training;Analytical performance modeling;MapReduce;cloud computing;machine learning}, 
doi={10.1109/SYNASC.2016.072}, 
ISSN={2470-881X}, 
month={Sept},}
@INPROCEEDINGS{7428375, 
author={Z. Li and X. Xie and X. Zhou and J. Guo and R. Bie}, 
booktitle={2015 International Conference on Identification, Information, and Knowledge in the Internet of Things (IIKI)}, 
title={A Generic Framework for Human Motion Recognition Based on Smartphones}, 
year={2015}, 
pages={299-302}, 
abstract={In recent years, human motion recognition based on smartphones has gotten increasing attention in many fields such as mobile health, health tracking and pervasive computing. However, motion recognition performance can be easily affected by variation of phone orientations and positions. Different users have influence on recognition accuracy as well. Most of existing work focuses on one or two respects of above problems, or train different models for different phone positions and orientations. In this paper, we propose a generic framework for human motion recognition based on smartphones, which can effectively discriminate six daily motions regardless of device positions and orientations. We select a set of more robust and effective features to solve performance degradation problem caused by different phone positions, phone orientations and users. In experiments, we access our method using the dataset collected by three volunteers on Android smartphones. The experimental results show that the proposed feature extraction algorithm is better than most existing algorithms.}, 
keywords={feature extraction;image recognition;mobile computing;smart phones;Android smart phone;feature extraction algorithm;human motion recognition;Accelerometers;Correlation;Earth;Feature extraction;Magnetic sensors;Smart phones;Feature extraction;Motion recognition;Smartphones}, 
doi={10.1109/IIKI.2015.71}, 
month={Oct},}
@INPROCEEDINGS{6137297, 
author={C. Yuan}, 
booktitle={2011 IEEE 11th International Conference on Data Mining}, 
title={Multi-task Learning for Bayesian Matrix Factorization}, 
year={2011}, 
pages={924-931}, 
abstract={Data sparsity is a big challenge for collaborative filtering. This problem becomes more serious if the dataset is newly created and has even fewer ratings. By sharing knowledge among different datasets, multi-task learning is a promising technique to address this issue. Most prior work methods directly share objects (users or items) across different datasets. However, object identities and correspondences may not be known in many cases. We extend the previous work of Bayesian matrix factorization with Dirichlet process mixture into a multi-task learning approach by sharing latent parameters among different tasks. Our method does not require object identities and thus is more widely applicable. The proposed model is fully non-parametric in that the dimension of latent feature vectors is automatically determined. Inference is performed using the variational Bayesian algorithm, which is much faster than Gibbs sampling used by most other related Bayesian methods.}, 
keywords={belief networks;collaborative filtering;knowledge representation;learning (artificial intelligence);matrix decomposition;sampling methods;Bayesian matrix factorization;Dirichlet process;Gibbs sampling;collaborative filtering;data sparsity;knowledge sharing;latent feature vectors;multitask learning;multitask learning approach;variational Bayesian algorithm;Bayesian methods;Covariance matrix;Matrix decomposition;Motion pictures;Principal component analysis;Training;Vectors;co-clustering;collaborative filtering;matrix factorization}, 
doi={10.1109/ICDM.2011.107}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{7917548, 
author={J. Rafferty and J. Synnott and C. Nugent and G. Morrison and E. Tamburini}, 
booktitle={2017 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)}, 
title={NFC based dataset annotation within a behavioral alerting platform}, 
year={2017}, 
pages={146-151}, 
abstract={Pervasive and ubiquitous computing increasingly relies on data-driven models learnt from large datasets. This learning process requires annotations in conjunction with datasets to prepare training data. Ambient Assistive Living (AAL) is one application of pervasive and ubiquitous computing that focuses on providing support for individuals. A subset of AAL solutions exist which model and recognize activities/behaviors to provide assistive services. This paper introduces an annotation mechanism for an AAL platform that can recognize, and provide alerts for, generic activities/behaviors. Previous annotation approaches have several limitations that make them unsuited for use in this platform. To address these deficiencies, an annotation solution relying on environmental NFC tags and smartphones has been devised. This paper details this annotation mechanism, its incorporation into the AAL platform and presents an evaluation focused on the efficacy of annotations produced. In this evaluation, the annotation mechanism was shown to offer reliable, low effort, secure and accurate annotations that are appropriate for learning user behaviors from datasets produced by this platform. Some weaknesses of this annotation approach were identified with solutions proposed within future work.}, 
keywords={ambient intelligence;assisted living;behavioural sciences computing;near-field communication;pattern classification;smart phones;AAL platform;ambient assistive living;behavioral alerting platform;dataset annotation;environmental NFC tags;near field communication;smartphones;Conferences;Monitoring;Smart phones;Synchronization;Thermal sensors;Ubiquitous computing;NFC;annotation;behavior detection;machine learning;pervasive computing;smart environment;ubiquitous computing}, 
doi={10.1109/PERCOMW.2017.7917548}, 
month={March},}
@INPROCEEDINGS{7532923, 
author={I. Rodomagoulakis and N. Kardaris and V. Pitsikalis and A. Arvanitakis and P. Maragos}, 
booktitle={2016 IEEE International Conference on Image Processing (ICIP)}, 
title={A multimedia gesture dataset for human robot communication: Acquisition, tools and recognition results}, 
year={2016}, 
pages={3066-3070}, 
abstract={Motivated by the recent advances in human-robot interaction we present a new dataset, a suite of tools to handle it and state-of-the-art work on visual gestures and audio commands recognition. The dataset has been collected with an integrated annotation and acquisition web-interface that facilitates on-the-way temporal ground-truths for fast acquisition. The dataset includes gesture instances in which the subjects are not in strict setup positions, and contains multiple scenarios, not restricted to a single static configuration. We accompany it by a valuable suite of tools as the practical interface to acquire audio-visual data in the robotic operating system, a state-of-the-art learning pipeline to train visual gesture and audio command models, and an online gesture recognition system. Finally, we include a rich evaluation of the dataset providing rich and insightfull experimental recognition results.}, 
keywords={Internet;control engineering computing;gesture recognition;human-robot interaction;learning (artificial intelligence);multimedia computing;operating systems (computers);user interfaces;acquisition web-interface;annotation web-interface;audio commands recognition;audio-visual data;human robot communication;human-robot interaction;learning pipeline;multimedia gesture dataset;online gesture recognition system;robotic operating system;visual gestures;Cameras;Feature extraction;Gesture recognition;Training;Videos;Visualization;Vocabulary;audio commands;human-robot communication;multimedia gesture dataset;visual gesture recognition}, 
doi={10.1109/ICIP.2016.7532923}, 
month={Sept},}
@INPROCEEDINGS{5451899, 
author={Zilong Chen and Yang Lu}, 
booktitle={2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE)}, 
title={A SVM based method for active relevance feedback}, 
year={2010}, 
volume={1}, 
pages={508-513}, 
abstract={In vector space models, traditional relevance feedback techniques, which utilize the terms in the relevant documents to enrich the user's initial query, is an effective method to improve retrieval performance. However, in this process, it also brings some non-relevance terms in the relevant documents in the new query. The number of non-relevance terms will increase according to the repeat of feedback process; it will damage the retrieval performance finally. This paper introduces a SVM Based method for relevance feedback. We train a classifier on the feedback documents and classify the rest of the documents. Thus, in the result list, the relevant documents are in front of the non-relevant documents. The new approach avoids modifying the query via text classification algorithm in the relevance feedback process, and it is a new direction for the relevance feedback techniques. Experiments with TREC dataset demonstrate the effectiveness of this method.}, 
keywords={classification;document handling;query processing;relevance feedback;support vector machines;SVM based method;active relevance feedback;document classifier;feedback document;nonrelevance term;relevant document;retrieval performance;support vector machine;user initial query;vector space model;Classification algorithms;Feature extraction;Information retrieval;Microelectronics;Programming;State feedback;Support vector machine classification;Support vector machines;Text categorization;SVM;relevance feedback;text classification;vector space model}, 
doi={10.1109/ICCAE.2010.5451899}, 
month={Feb},}
@ARTICLE{7568994, 
author={X. Yuan and X. Wang and C. Wang and A. C. Squicciarini and K. Ren}, 
journal={IEEE Transactions on Dependable and Secure Computing}, 
title={Towards Privacy-preserving and Practical Image-centric Social Discovery}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Images are becoming one of the key enablers of user connectivity in social media applications. Many of them are directly exploring image content to suggest new friends with similar interests. To handle the explosive volumes of images, one common trend is to leverage the public cloud as their robust service backend. Despite the convenience, exposing content-rich images to the cloud inevitably raises acute privacy concerns. In this paper, we propose a privacy-preserving architecture for image-centric social discovery services, designed to function over encrypted images. We first adopt the effective Bag-of-Words model to extract the “visual content” of users’ images into respective image profile vectors. We then model the core problem as similarity retrieval of encrypted high-dimensional vectors. To achieve scalable services over millions of encrypted images, we design a secure and efficient index structure, which enables practical and accurate social discovery from the cloud, without revealing any image profile or image content. For completeness, we further enrich our service with secure updates, facilitating user’s image update. Our implementation is deployed at an Android phone and Amazon Cloud, and extensive experiments are conducted on a large Flickr image dataset which demonstrates the desired quality of services.}, 
keywords={Cloud computing;Cryptography;Indexes;Privacy;Social network services;Visualization;Image-centric;Privacy-preserving;Secure outsourcing;Social discovery}, 
doi={10.1109/TDSC.2016.2609930}, 
ISSN={1545-5971}, 
month={},}
@INPROCEEDINGS{6578813, 
author={R. L. Mappus and E. Briscoe}, 
booktitle={2013 IEEE International Conference on Intelligence and Security Informatics}, 
title={Layered behavioral trace modeling for threat detection}, 
year={2013}, 
pages={173-175}, 
abstract={A fundamental problem in detecting threats to security by monitoring computer usage is the high number of false positives that are created when analyzing a large data set for anomalous behavior. We address the problem by modeling user behavior at multiple scales so as to allow for the identification potential insider threats from users' logged activity by tracking users' activity over time. In this work, we apply a novel method for representing user activity at multiple temporal scales to a dataset that contains malicious behavior. We report our detection results and discuss how a layered detection method may be advantageous for the discovery of specific types of malicious behavior.}, 
keywords={security of data;anomalous behavior;computer usage monitoring;layered behavioral trace modeling;malicious behavior;multiple temporal scales;threat detection;Tracking}, 
doi={10.1109/ISI.2013.6578813}, 
month={June},}
@INPROCEEDINGS{6621365, 
author={M. Rossetti and F. Stella and M. Zanker}, 
booktitle={2013 24th International Workshop on Database and Expert Systems Applications}, 
title={Towards Explaining Latent Factors with Topic Models in Collaborative Recommender Systems}, 
year={2013}, 
pages={162-167}, 
abstract={Latent factor models have been proved to be the state of the art for the Collaborative Filtering approach in a Recommender System. However, latent factors obtained with mathematical methods applied to the user-item matrix can be hardly interpreted by humans. In this paper we exploit Topic Models applied to textual data associated with items to find explanations for latent factors. Based on the Movie Lens dataset and textual data about movies collected from Freebase we run a user study with over hundred participants to develop a reference dataset for evaluating different strategies towards more interpretable and portable latent factor models.}, 
keywords={collaborative filtering;data analysis;mathematical analysis;matrix algebra;recommender systems;MovieLens dataset;collaborative filtering approach;freebase;latent factor models;mathematical methods;recommender systems;reference dataset;textual data;topic models;user study;user-item matrix;Accuracy;Collaboration;Indexes;Motion pictures;Predictive models;Probability distribution;Recommender systems;Collaborative Filtering;Explanations;Recommender Systems}, 
doi={10.1109/DEXA.2013.26}, 
ISSN={1529-4188}, 
month={Aug},}
@INPROCEEDINGS{1279805, 
author={G. Gholmieh and G. H. Courellis and D. Song and Z. Wang and V. Z. Marmarelis and T. W. Berger}, 
booktitle={Proceedings of the 25th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (IEEE Cat. No.03CH37439)}, 
title={Characterization of the short-term plasticity of the dentate gyrus-CA3 system using nonlinear systems analysis}, 
year={2003}, 
volume={2}, 
pages={1929-1932 Vol.2}, 
abstract={Short-term plasticity (STP) have been traditionally studied using the paired impulse approach and the short impulse train approach. A new method for studying STP has been recently introduced [1], In this article, this method has been modified to take into account the varying amplitude of the input. It was then applied to the dentate gyrus-CA3 system. The system was studied at the population level using the population spike (PS) amplitude. Random impulse sequences (electrical shocks) with constant intensity were used to stimulate the afferents to the dentate gyrus (perforant path). Monosynaptic and disynaptic population spikes were recorded from the dentate gyrus and the CA3 regions respectively. The PS amplitude time series of the dentate gyrus formed the input dataset while the PS amplitude time series of the CAS region formed the output dataset. The data was analyzed using the Volterra Poisson approach leading to the estimation of the first and second order kernels, which formed comprehensive and quantitative descriptors of the nonlinear dynamics of STP in the CA3 hippocampal region.}, 
keywords={Poisson distribution;Volterra equations;bioelectric phenomena;brain;medical computing;neural nets;neurophysiology;nonlinear dynamical systems;operating system kernels;physiological models;Volterra Poisson approach;afferents;data analyzing;dentate gyrus-CA3 system;disynaptic population spike;electrical shock;first order kernel;hippocampal region;monosynaptic population spike;nonlinear dynamics;nonlinear system analysis;paired impulse approach;perforant path;population level;population spike amplitude;random impulse sequence;second order kernel;short impulse train approach;short-term plasticity characteristic;time series;Biomedical engineering;Data acquisition;Data analysis;Electric shock;Frequency;Kernel;Neuroscience;Nonlinear systems;Signal processing;User interfaces}, 
doi={10.1109/IEMBS.2003.1279805}, 
ISSN={1094-687X}, 
month={Sept},}
@INPROCEEDINGS{7159467, 
author={S. J. Briscilla}, 
booktitle={2015 International Conference on Circuits, Power and Computing Technologies [ICCPCT-2015]}, 
title={An efficient paradigm for achieving synchronization in hierarchical clustering}, 
year={2015}, 
pages={1-6}, 
abstract={Clustering is one of the important streams in data mining which is useful for discovering groups and identifying interesting distributions in the underlying data. For efficient clustering process in high dimensional datasets, synchronization algorithm is proposed. It can perform clustering by using the mathematical model called kuramato model. This model is widely used for the synchronization of oscillators. We regard each object in the dataset as phase oscillator and do the clustering process. This algorithm doesn't requires any input parameters which are difficult to estimate. It only requires the efficient interaction range as input for the clustering process. Since it is a dynamic clustering algorithm it does not requires the users to determine the predefined inputs like the number of clusters to be formed. Our proposed algorithm is scalable even when the dimensionality of the dataset increases. Hierarchical clustering algorithm can generate hierarchical tree like clusters which is used to predict the clusters of different types in high dimensional real time datasets.}, 
keywords={data mining;pattern clustering;trees (mathematics);data mining;dynamic clustering algorithm;group discovery;hierarchical clustering algorithm;hierarchical tree like clusters;high dimensional datasets;high dimensional real time datasets;kuramato model;mathematical model;oscillator synchronization;phase oscillator;synchronization algorithm;Cancer;Clustering algorithms;Glass;Heuristic algorithms;Oscillators;Synchronization;Windows;Dynamic clustering;interaction range;kuramato model;synchronization Algorithm}, 
doi={10.1109/ICCPCT.2015.7159467}, 
month={March},}
@INPROCEEDINGS{5708869, 
author={K. Forster and S. Monteleone and A. Calatroni and D. Roggen and G. Troster}, 
booktitle={2010 Ninth International Conference on Machine Learning and Applications}, 
title={Incremental kNN Classifier Exploiting Correct-Error Teacher for Activity Recognition}, 
year={2010}, 
pages={445-450}, 
abstract={Non-stationary data distributions are a challenge in activity recognition from body worn motion sensors. Classifier models have to be adapted online to maintain a high recognition performance. Typical approaches for online learning are either unsupervised and potentially unstable, or require ground truth information which may be expensive to obtain. As an alternative we propose a teacher signal that can be provided by the user in a minimally obtrusive way. It indicates if the predicted activity for a feature vector is correct or wrong. To exploit this information we propose a novel incremental online learning strategy to adapt a k-nearest-neighbor classifier from instances that are indicated to be correctly or wrongly classified. We characterize our approach on an artificial dataset with abrupt distribution change that simulates a new user of an activity recognition system. The adapted classifier reaches the same accuracy as a classifier trained specifically for the new data distribution. The learning based on the provided correct - error signal also results in a faster learning speed compared to online learning from ground truth. We validate our approach on a real world gesture recognition dataset. The adapted classifiers achieve an accuracy of 78.6% compared to the subject independent baseline of 68.3%.}, 
keywords={gesture recognition;learning (artificial intelligence);sensors;signal classification;activity recognition;body worn motion sensor;classifier model;correct-error teacher signal;feature vector;gesture recognition;incremental kNN classifier;incremental online learning;k-nearest-neighbor classifier;learning speed;nonstationary data distribution;Accuracy;Adaptation model;Artificial neural networks;Data models;Stability analysis;Training;Upper bound}, 
doi={10.1109/ICMLA.2010.72}, 
month={Dec},}
@INPROCEEDINGS{6514342, 
author={R. Agrawal and M. Phatak}, 
booktitle={2013 3rd IEEE International Advance Computing Conference (IACC)}, 
title={A novel algorithm for automatic document clustering}, 
year={2013}, 
pages={877-882}, 
abstract={Internet has become an indispensible part of today's life. World Wide Web (WWW) is the largest shared information source. Finding relevant information on the WWW is challenging. To respond to a user query, it is difficult to search through the large number of returned documents with the presence of today's search engines. There is a need to organize a large set of documents into categories through clustering. The documents can be a user query or simply a collection of documents. Document clustering is the task of combining a set of documents into clusters so that intra cluster documents are similar to each other than inter cluster documents. Partitioning and Hierarchical algorithms are commonly used for document clustering. Existing partitioning algorithms have the limitation that the number of clusters has to be given as input and the clustering result depends on this input. If the number of clusters is not known, results are not acceptable. In this paper, we have developed a novel algorithm which generates number of clusters automatically for any unknown text dataset and clusters the documents appropriately based on Cosine Similarity between them. We have also detected zero clustering issue in partitioning algorithm and solved it using our novel algorithm.}, 
keywords={document handling;pattern clustering;query processing;search engines;Internet;World Wide Web;automatic document clustering algorithm;cosine similarity;document collection;hierarchical algorithm;information source;partitioning algorithm;search engines;user query;zero clustering issue;Algorithm design and analysis;Clustering algorithms;Computational modeling;Feature extraction;Partitioning algorithms;Silicon;Vectors;Cosine similarity;Document clustering;TF-IDF;Threshold;Zero Clustering}, 
doi={10.1109/IAdCC.2013.6514342}, 
month={Feb},}
@INPROCEEDINGS{7396836, 
author={N. Li and L. J. Latecki}, 
booktitle={2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
title={Affinity Inference with Application to Recommender Systems}, 
year={2015}, 
volume={1}, 
pages={393-400}, 
abstract={We propose a novel framework for affinity inference and apply it to recommender systems. Given a set of objects and affinities between some pairs of them, we infer the relative value of the unknown affinities based on the transitive property of the affinity relationship. An inference chain is defined as any possible transitive inference process between two objects. In general, there are an infinite number of distinct inference chains between two objects. Each inference chain reveals these two objects are affinitive with a certain confidence, which depends on the individual affinities of the links and the length of the chain. We quantify and aggregate all these confidences as the relative value of the unknown affinity with an efficient method. We formulate collaborative filtering recommendation as an affinity inference problem. The given ratings are transformed into affinities between abstract objects of users and item-rating pairs. The unknown ratings are predicted based on the inferred affinities. The recommendations are made according to both the predicted ratings and the prediction confidences, which are also derived from the inferred affinities. Our approach achieves good prediction accuracy and significantly alleviates the cold-start problem on the standard MovieLens dataset. Moreover, experimental results show that our approach can effectively incorporate extra information to improve the predictions and recommendations.}, 
keywords={collaborative filtering;inference mechanisms;recommender systems;affinity inference problem;affinity relationship;cold-start problem;collaborative filtering recommendation;inference chain;inferred affinity;item-rating pair;predicted rating;prediction confidence;recommender system;standard MovieLens dataset;transitive inference process;transitive property;Collaboration;Inference algorithms;Markov processes;Matrix decomposition;Prediction algorithms;Predictive models;Recommender systems;affinity inference;recommender systems}, 
doi={10.1109/WI-IAT.2015.37}, 
month={Dec},}
@INPROCEEDINGS{7753615, 
author={H. Li and Peng Su and Zhizhen Chi and Jingjing Wang}, 
booktitle={2016 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)}, 
title={Image retrieval and classification on deep convolutional SparkNet}, 
year={2016}, 
pages={1-6}, 
abstract={Image retrieval and classification are hot topics in computer vision and have attracted great attention nowadays with the emergence of large-scale data. We propose a new scheme to use both deep learning models and large-scale computing platform and jointly learn powerful feature representations in image classification and retrieval. We achieve a superior performance on the ImageNet dataset, where the framework is easy to be embedded for daily user experience. First we conduct the classification task using deep convolutional neural networks with several novel techniques, including batch normalization and multi-crop testing to obtain a better performance. Then we transfer the network's knowledge to image retrieval task by comparing the feature codebook of the query image with those feature database extracted from the deep model. Such a search pipeline is implemented in a MapReduce framework on the Spark platform, which is suitable for large-scale and real-time data processing. At last, the system outputs to users some textual information of the predicted object searching from Internet as well as similar images from the retrieval stage, making our work a real application.}, 
keywords={Internet;computer vision;data handling;embedded systems;image classification;image retrieval;learning (artificial intelligence);neural nets;parallel processing;ImageNet dataset;Internet;MapReduce framework;Spark platform;computer vision;deep convolutional SparkNet;deep convolutional neural networks;feature database extraction;image classification;image retrieval;large-scale computing platform;large-scale data processing;multicrop testing;query image;real-time data processing;Convolutional codes;Feature extraction;Image retrieval;Machine learning;Sparks;Training}, 
doi={10.1109/ICSPCC.2016.7753615}, 
month={Aug},}
@INPROCEEDINGS{6375166, 
author={P. K. Panigrahi}, 
booktitle={2012 Fourth International Conference on Computational Intelligence and Communication Networks}, 
title={A Comparative Study of Supervised Machine Learning Techniques for Spam E-mail Filtering}, 
year={2012}, 
pages={506-512}, 
abstract={Unsolicited e-mail (Spam) has become a major issue for each e-mail user. In recent days it is very difficult to filter spam emails as these emails are written or generated in a very special way so that anti-spam filters cannot detect such emails. This Paper compares and discusses performance measures of certain categories of supervised machine learning techniques such as Bayes algorithms, lazy algorithms, tree algorithms, neural network, and support vector machines for classifying a spam e-mail corpus maintained by UCI Machine Learning Repository. The objective of this study is to consider the content of the emails, learn a finite dataset available and to develop a classification model that will able to predict whether an e-mail is spam or not.}, 
keywords={Bayes methods;Internet;information filtering;learning (artificial intelligence);neural nets;pattern classification;support vector machines;trees (mathematics);unsolicited e-mail;Bayes algorithm;UCI Machine Learning Repository;antispam filter;e-mail content;lazy algorithm;neural network;spam e-mail classification;spam e-mail filtering;supervised machine learning technique;support vector machine;tree algorithm;unsolicited e-mail;Accuracy;Classification algorithms;Electronic mail;Machine learning;Machine learning algorithms;Support vector machines;Training;Classification;Filtering;Machine Learning Algorithms;Spam-Email}, 
doi={10.1109/CICN.2012.14}, 
month={Nov},}
@INPROCEEDINGS{5992579, 
author={M. Magnani and L. Rossi}, 
booktitle={2011 International Conference on Advances in Social Networks Analysis and Mining}, 
title={The ML-Model for Multi-layer Social Networks}, 
year={2011}, 
pages={5-12}, 
abstract={In this paper we introduce a new model to represent an interconnected network of networks. This model is fundamental to reason about the real organization of on-line social networks, where users belong to and interact on different networks at the same time. In addition we extend traditional SNA measures to deal with this multiplicity of networks and we apply the model to a real dataset extracted from two microblogging sites.}, 
keywords={social networking (online);ML-model;dataset;microblogging sites;multilayer social networks;Context;Cultural differences;Facebook;Media;Twitter;Viruses (medical);Multi-layer networks;Social Network Analysis;Social Network Sites;centrality}, 
doi={10.1109/ASONAM.2011.114}, 
month={July},}
@INPROCEEDINGS{7840677, 
author={Y. Zheng and W. Wu and H. Zeng and N. Cao and H. Qu and M. Yuan and J. Zeng and L. M. Ni}, 
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
title={TelcoFlow: Visual exploration of collective behaviors based on telco data}, 
year={2016}, 
pages={843-852}, 
abstract={Collective behavior is an important concept defined to capture behavioral patterns emerged among the crowd spontaneously. In social science, people's behaviors can be regarded as temporal transitions between a set of typical states (e.g., home and work) which are always associated with certain locations. This fact leads to an interesting research topic in developing ways to explore people's collective behavior patterns through movement analysis, which is our focus in this paper. In recent years, massive volumes of spatiotemporal data generated by mobile phones, called telco data, bring an unprecedented opportunity to study collective behaviors in terms of large coverage and fine-grained resolution. However, distilling valuable collective behavior patterns from the large scale of telco data is not an easy task. The challenge is rooted in two aspects, including the data uncertainty as well as the lack of methods to characterize, compare and understand dynamic crowd behaviors, which triggers the use of visual analytics to take full advantage of machines' computational power as well as human's domain knowledge and cognitive abilities. In this paper, we propose TelcoFlow, a comprehensive visual analytics system which incorporates advanced quantitative analyses (e.g., statebased behavior model) and intuitive visualizations (e.g., an extended flow view embedded with state glyphs) to support an efficient and in-depth analysis of collective behaviors based on telco data. Case studies with a real-world dataset and expert interviews are carried out to demonstrate the effectiveness of our system for analysts to gain insights into collective behaviors and facilitate various analytical tasks.}, 
keywords={behavioural sciences computing;cognition;data visualisation;graphical user interfaces;smart phones;spatiotemporal phenomena;TelcoFlow;collective behavior patterns;crowd behavioral patterns;data uncertainty;dynamic crowd behaviors;extended flow view embedding;human cognitive abilities;human domain knowledge abilities;machine computational power;mobile phones;movement analysis;quantitative analysis;real-world dataset;social science;spatio-temporal data;state glyphs;state-based behavior model;telco data;temporal transitions;visual analytics;visual exploration;Analytical models;Data models;Electronic mail;Mobile handsets;Uncertainty;Visual analytics;collective behavior;movement;spatio-temporal analysis;telco data;visual analytics}, 
doi={10.1109/BigData.2016.7840677}, 
month={Dec},}
@INPROCEEDINGS{6714209, 
author={R. Prejbeanu and A. C. Olteanu and N. Tapus}, 
booktitle={2013 RoEduNet International Conference 12th Edition: Networking in Education and Research}, 
title={Crowdsourcing solution for mobile resource consumption analysis}, 
year={2013}, 
pages={1-6}, 
abstract={The outstanding increase in smartphone adoption rates across the globe in recent years has created the opportunity to analyze large amounts of data produced by mobile usage in the search for a means to improve the user experience. Several solutions exist that help the user understand the resource consumption of the mobile device. However, their estimates are rough and hardly accurate. We believe that leveraging crowdsourcing can lead in the future to better insights for the user over the mobile device. In this paper, we describe our current status in designing and implementing a centralized system for aggregation and statistical analysis of resource consumption traces from mobile devices. We evaluate the system using a real-world dataset covering hundreds of devices over three months.}, 
keywords={mobile computing;smart phones;statistical analysis;centralized system;crowdsourcing solution;mobile devices;mobile resource consumption analysis;mobile usage;real-world dataset;resource consumption traces;smartphone adoption rates;statistical analysis;system evaluation;Data models;Google;Mobile communication;Mobile handsets;Sociology;Statistical analysis;crowdsourcing;mobile devices;resource consumption;statistical analysis}, 
doi={10.1109/RoEduNet.2013.6714209}, 
ISSN={2068-1038}, 
month={Sept},}
@INPROCEEDINGS{7817095, 
author={M. B. Duggimpudi and A. Moursy and E. Ali and V. V. Raghavan}, 
booktitle={2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)}, 
title={An Ontology-Based Architecture for Providing Insights in Wireless Networks Domain}, 
year={2016}, 
pages={473-478}, 
abstract={Ontology-based approaches have been explored in several domains for knowledge representation and improving accuracy. However, ontology-based approaches for assisting a decision maker by delivering a concrete plan from analyzing the insights extracted from an ontology, have not received much attention. Insights-as-a-service is a technology that aids a decision maker by providing a concrete action plan, involving a comparative analysis of patterns derived from the data and the extraction of insights from such an analysis. In this paper, we propose an ontology-based architecture for mining insights within the Wireless Network Ontology (WNO), an ontology generated for the wireless network domain for delivering better wireless network performance. We present and illustrate: (i) the major components of the architecture together with the algorithms used for summarizing the network performance profiles in the form of rank tables, and (ii) how the insight rules (the action plan) are extracted from these tables. By utilizing the proposed approach, an actionable plan for assisting the decision maker can be obtained as domain knowledge is incorporated in the system. Experimental results on a wireless network dataset show that the proposed model provides an optimal action plan for a wireless network to improve its performance by encoding data-driven rules into the ontology and suggesting changes to its current network configuration.}, 
keywords={knowledge representation;ontologies (artificial intelligence);radio networks;telecommunication computing;domain knowledge;insights-as-a-service;knowledge representation;ontology-based architecture;wireless network ontology;wireless networks domain;Data mining;Graphical user interfaces;Inference algorithms;Ontologies;Semantics;Wireless networks;Domain knowledge;Insight;Networks;Ontology}, 
doi={10.1109/WI.2016.0078}, 
month={Oct},}
@INPROCEEDINGS{7364072, 
author={H. A. Cao and T. K. Wijaya and K. Aberer and N. Nunes}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={A collaborative framework for annotating energy datasets}, 
year={2015}, 
pages={2716-2725}, 
abstract={Targeting human activities responsible for the energy consumption instead of focusing solely on single appliance feedback for achieving energy efficiency in residential homes would link human behaviors to the resulting energy consumption. To this end, learning when appliances are in an active or idle state and the related user activity is crucial. Until smart appliances become widespread and can communicate their internal state, identifying when the residents interact with the appliances has to be determined from the available information that can be recorded from these devices. Developing and validating learning models require ground truth in the form of annotations to indicate when an appliance is active or idle. Launching data collection campaigns to incorporate these missing ground truth data involves careful planning before the roll-out of the experiment. Prohibitive costs for the hardware and time investment to monitor the deployed equipment are necessary for quality data. As such, publicly released datasets containing appliance-level data offer a basis for most researchers. This paper addresses these challenges by providing a collaborative web-based framework to retrofit labeling on existing datasets. The platform is publicly available, applies the wisdom of the crowd in the realm of energy research and leverages gamification techniques to encourage users' active contribution. The access to the platform and furthermore to the expert manually labeled dataset intends to enable future research and foster more collaboration in this area.}, 
keywords={Internet;domestic appliances;groupware;home computing;appliance-level data;collaborative Web-based framework;collaborative framework;energy consumption;energy dataset annotation;energy efficiency;gamification techniques;labeling retrofitting;prohibitive costs;residential homes;single appliance feedback;smart appliances;Collaboration;Crowdsourcing;Databases;Electronic mail;Home appliances;Labeling;Monitoring;Activity inference;Appliances states;Collaborative computing;Computer-supported cooperative work;Data mining;Datasets;Energy data analytics;Energy disaggregation;Ground truth acquisition;Information systems applications}, 
doi={10.1109/BigData.2015.7364072}, 
month={Oct},}
@INPROCEEDINGS{5286461, 
author={L. Galli and D. Loiacono and P. L. Lanzi}, 
booktitle={2009 IEEE Symposium on Computational Intelligence and Games}, 
title={Learning a context-aware weapon selection policy for Unreal Tournament III}, 
year={2009}, 
pages={310-316}, 
abstract={Modern computer games are becoming increasingly complex and only experienced players can fully master the game controls. Accordingly, many commercial games now provide aids to simplify the player interaction. These aids are based on simple heuristics rules and cannot adapt neither to the current game situation nor to the player game style. In this paper, we suggest that supervised methods can be applied effectively to improve the quality of such game aids. In particular, we focus on the problem of developing an automatic weapon selection aid for Unreal Tournament III, a recent and very popular first person shooter (FPS). We propose a framework to (i) collect a dataset from game sessions, (ii) learn a policy to automatically select the weapon, and (iii) deploy the learned models in the game to replace the default weapon-switching aid provided in the game distribution. Our approach allows the development of weapon-switching policies that are aware of the current game context and can also imitate a particular game style.}, 
keywords={computer games;learning (artificial intelligence);weapons;Unreal Tournament III;automatic weapon selection aid;computer games;context-aware weapon selection policy;first person shooter;game controls;supervised methods;weapon-switching policies;Application software;Bayesian methods;Computational intelligence;Context awareness;Machine learning;Military computing;Neural networks;Supervised learning;Testing;Weapons}, 
doi={10.1109/CIG.2009.5286461}, 
ISSN={2325-4270}, 
month={Sept},}
@INPROCEEDINGS{7395697, 
author={P. Mrazovic and M. Matskin and N. Dokoohaki}, 
booktitle={2015 IEEE International Conference on Data Mining Workshop (ICDMW)}, 
title={Trajectory-Based Task Allocation for Reliable Mobile Crowd Sensing Systems}, 
year={2015}, 
pages={398-406}, 
abstract={Mobile crowd sensing (MCS) is as a promising people-centric sensing paradigm which allows ordinary citizens to contribute sensing data using mobile communication devices. In this paper we study correlation between users' mobility and their role as contributors in MCS applications. We propose a new trajectory-based approach for task allocation in MCS environments and model participants' spatio-temporal competences by analyzing their mobile traces. By allocating MCS tasks only to participant who are familiar with the target location we significantly increase the reliability of contributed data and reduce total communication cost. We introduce novel metric to estimate participants' competence to conduct MCS tasks and propose fair ranking approach allowing newcomers to compete with experienced senior contributors. Additionally, we group similar expert contributors and thus open up new possibilities for physical collaboration between them. We evaluate our work using GeoLife trajectory dataset and the experimental results show the advantages of our approach.}, 
keywords={geography;mobile computing;GeoLife trajectory dataset;MCS;fair ranking approach;mobile communication devices;people-centric sensing paradigm;physical collaboration;reliable mobile crowd sensing systems;spatio-temporal competences;trajectory-based task allocation;Global Positioning System;Mobile communication;Reliability;Resource management;Sensors;Tensile stress;Trajectory;mobile crowd sensing;mobile crowdsourcing;mobility profiling;participatory sensing;task allocation}, 
doi={10.1109/ICDMW.2015.90}, 
month={Nov},}
@INPROCEEDINGS{7259394, 
author={W. Hou and Y. Huang and K. Zhang}, 
booktitle={2015 IEEE 14th International Conference on Cognitive Informatics Cognitive Computing (ICCI*CC)}, 
title={Research of micro-blog diffusion effect based on analysis of retweet behavior}, 
year={2015}, 
pages={255-261}, 
abstract={Research on the diffusion effect of micro-blog plays an important role in improving marketing efficiency, strengthening monitoring public opinion and accurately discovering hotspot etc. To solve the problems not taking users' differences into consideration in the previous research, this paper proposes an algorithm to predict scale and depth of retweet massages based on analysis of retweet behavior. With the combination of LR algorithm and nine related features extracted from micro-blog users themselves, their relationships and micro-blog contents, we proposes a prediction model of retweet behavior. Based on this model, we proposes an algorithm to predict the diffusion effect, which considers the character of information spreading along users and does statistical analysis of adjacent users iteratively. Experimental results on Sina micro-blog dataset show that the algorithm has a prediction accuracy of 87.1% and 81.6% in scale and depth respectively, which indicates the model works well.}, 
keywords={Web sites;data analysis;statistical analysis;LR algorithm;Sina microblog dataset;diffusion effect;marketing efficiency;microblog contents;prediction model;public opinion monitoring;retweet behavior;retweet massages;statistical analysis;Blogs;IEL;Behavior Prediction;Diffusion Depth;Logistic Regression;Micro-Blog;Retweet Scale}, 
doi={10.1109/ICCI-CC.2015.7259394}, 
month={July},}
@INPROCEEDINGS{7805743, 
author={K. Inuzuka and T. Hayashi and T. Takagi}, 
booktitle={2016 IEEE 9th International Workshop on Computational Intelligence and Applications (IWCIA)}, 
title={Recommendation incorporating transition of temporally intensive unity}, 
year={2016}, 
pages={21-26}, 
abstract={It is important to note that user preferences change over time. However, it is not guaranteed that user preferences change at a steady rate. For example, a person who intensively listens to music of the same artist might intensively listen to the music of a different artist after a few days. For this reason, it is effective to incorporate such preference changes into recommender systems. In this paper, we propose an approach that predicts user preferences with consideration of preference changes by learning the transition of the preference that is the temporally intensive unity of purchasing items as one preference. Our approach is composed of a Kalman filter and matrix factorization. We show through experiments using a real-world dataset that our approach outperforms competitive methods such as the first order Markov model.}, 
keywords={Kalman filters;matrix decomposition;recommender systems;Kalman filter;first order Markov model;matrix factorization;recommender systems;temporally intensive unity;Context;Hidden Markov models;Kalman filters;Music;Prediction algorithms;Recommender systems;Time series analysis;Kalman Filter;Matrix Factorization;Time-Aware Recommendation}, 
doi={10.1109/IWCIA.2016.7805743}, 
month={Nov},}
@INPROCEEDINGS{6718273, 
author={M. S. Vural and M. Gök and Z. Yetgin}, 
booktitle={2013 International Conference on Electronics, Computer and Computation (ICECCO)}, 
title={Generating incident-level artificial data using GIS based crime simulation}, 
year={2013}, 
pages={239-242}, 
abstract={Most crime analysis tools used to find criminals of a particular incident or, to find the interrelations among the crime incidents, possibly over a GIS (Geographical Information System) map. The development of these tools require access to incident-level crime data. Obtaining real data is very restricted if not possible due to the official regulations. In this paper, a parametric model is proposed to generate the incident-level crime datasets involving crimes, criminals and criminals' suspicious acquaintances where the parameters are used for fine tuned adaptation of the model. The motivation for this approach is that unsupervised approaches for crime analysis do not require fully realistic data set in order to develop decision making algorithms. The model is based on GIS by approximating the characteristics of the population in real-life. Then, results of various GIS related queries are demonstrated on the GIS map to enable the visual analysis of the incidents.}, 
keywords={cartography;criminal law;geographic information systems;police data processing;GIS based crime simulation;GIS map;crime analysis tools;crime incidents;criminals suspicious acquaintances;decision making algorithms;geographical information system map;incident visual analysis;incident-level artificial data;incident-level crime datasets;parametric model;unsupervised approaches;Cities and towns;Data models;Decision making;Geographic information systems;Sociology;Statistics;User interfaces;Clustering Algorithms;Crime Analyses;GIS Query;Realistic Dataset}, 
doi={10.1109/ICECCO.2013.6718273}, 
month={Nov},}
@INPROCEEDINGS{6126409, 
author={Jin Sun and H. Ling}, 
booktitle={2011 International Conference on Computer Vision}, 
title={Scale and object aware image retargeting for thumbnail browsing}, 
year={2011}, 
pages={1511-1518}, 
abstract={Many image retargeting algorithms, despite aesthetically carving images smaller, pay limited attention to image browsing tasks where tiny thumbnails are presented. When applying traditional retargeting methods for generating thumbnails, several important issues frequently arise, including thumbnail scales, object completeness and local structure smoothness. To address these issues, we propose a novel image retargeting algorithm, Scale and Object Aware Retargeting (SOAR), which has four components: (1) a scale dependent saliency map to integrate size information of thumbnails, (2) objectness (Alexe et al. 2010) for preserving object completeness, (3) a cyclic seam carving algorithm to guide continuous retarget warping, and (4) a thin-plate-spline (TPS) retarget warping algorithm that champions local structure smoothness. The effectiveness of the proposed algorithm is evaluated both quantitatively and qualitatively. The quantitative evaluation is conducted through an image browsing user study to measure the effectiveness of different thumbnail generating algorithms, followed by the ANOVA analysis. The qualitative study is performed on the RetargetMe benchmark dataset. In both studies, SOAR generates very promising performance, in comparison with state-of-the-art retargeting algorithms.}, 
keywords={image reconstruction;statistical analysis;ANOVA analysis;RetargetMe benchmark dataset;cyclic seam carving algorithm;image browsing tasks;local structure smoothness;object completeness preservation;objectness;scale and object aware image retargeting;scale dependent saliency map;thin-plate-spline retarget warping algorithm;thumbnail browsing;thumbnail scales;Accuracy;Algorithm design and analysis;Deformable models;Humans;Image segmentation;Pollution measurement;Visualization}, 
doi={10.1109/ICCV.2011.6126409}, 
ISSN={1550-5499}, 
month={Nov},}
@INPROCEEDINGS{5708561, 
author={A. Liu and Q. Li and L. Huang and S. Wen and C. Tang and M. Xiao}, 
booktitle={2010 IEEE Asia-Pacific Services Computing Conference}, 
title={Reputation-Driven Recommendation of Services with Uncertain QoS}, 
year={2010}, 
pages={115-122}, 
abstract={Service recommendation in a Web of services with uncertain QoS is a challenging problem. In this paper, we propose a reputation-based service recommendation framework. We formally define a service reputation model that analyzes the relations between uncertain QoS and reputation. We also devise a two-phase planning approach to constructing a composite service as the recommendation when none of existing services can fulfill the user's requirements alone. Furthermore, we design a utility difference based approach that can fairly distribute the overall rating of a composite service to its component services and theoretically prove its fairness. We evaluate the efficiency and fairness of our framework on a publicly available dataset: ICEBE05.}, 
keywords={Web services;distributed processing;quality of service;Web service;composite service;reputation-based service recommendation;service reputation model;uncertain QoS;utility difference based approach;Biological system modeling;Computational modeling;Computer science;Planning;Quality of service;Time factors;Web services;QoS;composition;reputation;web service}, 
doi={10.1109/APSCC.2010.25}, 
month={Dec},}
@INPROCEEDINGS{7743319, 
author={G. D. Sosa and S. Rodríguez and J. Guaje and J. Victorino and M. Mejía and L. S. Fuentes and A. Ramírez and H. Franco}, 
booktitle={2016 XXI Symposium on Signal Processing, Images and Artificial Vision (STSIVA)}, 
title={3D surface reconstruction of entomological specimens from uniform multi-view image datasets}, 
year={2016}, 
pages={1-8}, 
abstract={Modeling of 3D objects and scenes have become a common tool in different applied fields from simulation-based design in high-end engineering applications (aviation, civil structures, engine components, etc.) to entertainment (computer-based animation, video-game development, etc.). In Biology and related fields, 3D object modeling and reconstruction provide valuable tools to support the visualization, comparison and even morphometric analysis in both academical and applied tasks. Such computational tools, usually implemented as web-based virtual reality applications, significantly reduce the manipulation of fragile samples, preventing their damage and, even, their complete loss. On the other hand, they allow to take the morphological properties of physical specimens to the digital domain, giving support to common entomology tasks such as characterization, morphological taxonomy and teaching. This paper addresses the problem of producing reliable 3D point clouds from the surface of entomological specimens, based on a proved approach for multi view 3D reconstruction from high resolution pictures. Given the traditional issues of macro-photography for small sized objects (i.e. short depth of field, presence of subtle and complex structures, etc.), a pre-processing protocol, based on focus stacking, supported the generation of enhanced views obtained by an acquisition device specifically designed for this work. The proposed approach has been tested on a sample of six representative subjects from the Entomological Collection of the Centro de Biosistemas, Universidad Jorge Tadeo Lozano (Colombia). The resulting point clouds exhibit an overall good visual quality for the body structure the selected specimens, while file sizes are portable enough to support web based visualization.}, 
keywords={biology computing;data visualisation;image reconstruction;solid modelling;zoology;3D object modelling;3D surface reconstruction;data visualization;entomological specimen;multiview image dataset;Cameras;Lenses;Solid modeling;Stacking;Surface morphology;Surface reconstruction;Three-dimensional displays}, 
doi={10.1109/STSIVA.2016.7743319}, 
month={Aug},}
@INPROCEEDINGS{7023740, 
author={Y. Zhang and T. Yue and H. Wang and A. Wei}, 
booktitle={2014 IEEE 17th International Conference on Computational Science and Engineering}, 
title={Predicting the Quality of Experience for Internet Video with Fuzzy Decision Tree}, 
year={2014}, 
pages={1181-1187}, 
abstract={In this paper, we attempt to predict users' quality of experience (QoE) with the log data collected from the web sites of Internet video service providers. To this end, we first collect service log data in the wild from one of the Top 5 most popular providers in China. Then we do a series of data preprocessing to format the original semi-structured log data to structured. We calculate several key video quality metrics, such as join time and frame rate, and explore the distributions of each quality metric, as well as the relationship between individual quality metric and user engagement. Considering that user engagement may be a result of comprehensive effect of several metrics, we apply fuzzy decision tree (FDT), a kind of classification algorithms in the area of machine learning, to develop the predictive model of user QoE for Internet video. Finally, we compare the prediction accuracy of our model with the model developed using decision tree on several different datasets. Our model separately achieves about 20% and 10% improvement in prediction accuracy on the dataset of sessions with the same content type and the dataset of sessions with mobile access devices.}, 
keywords={Internet;Web sites;decision trees;fuzzy set theory;pattern classification;quality of experience;video on demand;China;FDT;Internet video service providers;QoE;Web sites;classification algorithms;fuzzy decision tree;machine learning;mobile access devices;semistructured log data;service log data;user engagement;user quality of experience prediction;video quality metrics;Decision trees;Internet;Measurement;Pragmatics;Predictive models;Streaming media;Video recording;Internet video;QoE;fuzzy decision tree;quality metrics}, 
doi={10.1109/CSE.2014.230}, 
month={Dec},}
@INPROCEEDINGS{1326806, 
author={S. Lippens and J. P. Martens and T. De Mulder}, 
booktitle={2004 IEEE International Conference on Acoustics, Speech, and Signal Processing}, 
title={A comparison of human and automatic musical genre classification}, 
year={2004}, 
volume={4}, 
pages={iv-233-iv-236 vol.4}, 
abstract={Recently there has been an increasing amount of work in the area of automatic genre classification of music in audio format. In addition to automatically structuring large music collections such classification can be used as a way to evaluate features for describing musical content. However the evaluation and comparison of genre classification systems is hindered by the subjective perception of genre definitions by users. In this work, we describe a set of experiments in automatic musical genre classification. An important contribution of this work is the comparison of the automatic results with human genre classifications on the same dataset. The results show that, although there is room for improvement, genre classification is inherently subjective and therefore perfect results can not be expected neither from automatic nor human classification. The experiments also show that features derived from an auditory model have similar performance with features based on mel-frequency cepstral coefficients (MFCC).}, 
keywords={audio signal processing;cepstral analysis;feature extraction;music;signal classification;MFCC;audio format music;auditory model;automatic musical genre classification;feature extraction;genre definition subjective perception;human musical genre classification;mel-frequency cepstral coefficients;music collection structuring;musical content description;Cepstral analysis;Classification algorithms;Computer science;Consumer electronics;Feature extraction;Humans;Information systems;Mel frequency cepstral coefficient;Psychoacoustic models;Signal processing algorithms}, 
doi={10.1109/ICASSP.2004.1326806}, 
ISSN={1520-6149}, 
month={May},}
@INPROCEEDINGS{7840871, 
author={E. Peltonen and E. Lagerspetz and P. Nurmi and S. Tarkoma}, 
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
title={Too big to mail: On the way to publish large-scale mobile analytics data}, 
year={2016}, 
pages={2374-2377}, 
abstract={The Carat project started in 2012 has collected over 1.5 TB of data from over 850,000 mobile users all over the world. The project uses Apache Thrift to transmit data, and Apache Spark to run data analysis tasks, and the gist of the Carat analysis method has been published. While the Carat application code is open source, the data is much harder to share because of its size and privacy concerns. This paper outlines the challenges in sharing such a large-scale dataset with detailed information about smart devices, applications, and their users, and presents some solutions to these challenges.}, 
keywords={data analysis;data privacy;mobile computing;public domain software;Apache Spark;Apache Thrift;Carat analysis method;Carat application code;Carat project;data analysis;large-scale dataset;large-scale mobile analytics data publishing;mobile users;smart devices;Androids;Batteries;Data models;Data privacy;Humanoid robots;IEEE 802.11 Standard;Mobile communication;Big Data;Energy-awareness;Mobile Analytics}, 
doi={10.1109/BigData.2016.7840871}, 
month={Dec},}
@INPROCEEDINGS{7403746, 
author={C. Liao and A. Squicciarini and C. Griffin and S. Rajtmajer}, 
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
title={A hybrid epidemic model for antinormative behavior in online social networks}, 
year={2015}, 
pages={1563-1564}, 
abstract={In this paper, we describe a novel approach to investigate negative behavior dynamics in online social networks as epidemic phenomena. We present a finite-state machine model for time-varying epidemic dynamics, and validate this model with experiments over a large dataset of Youtube commentaries, indicating how different epidemic patterns of behavior can be tied to specific interaction patterns among users. A full version of this paper is available on arXiv.org.}, 
keywords={behavioural sciences computing;epidemics;finite state machines;social networking (online);Youtube commentaries;antinormative behavior;finite-state machine model;hybrid epidemic model;negative behavior dynamics;online social networks;time-varying epidemic dynamics;Context;Hidden Markov models;Mathematical model;Smoothing methods;Videos;YouTube}, 
doi={10.1145/2808797.2809334}, 
month={Aug},}
@INPROCEEDINGS{7796821, 
author={D. Reinhardt and I. Manyugin}, 
booktitle={2016 IEEE 41st Conference on Local Computer Networks (LCN)}, 
title={OP4: An OPPortunistic Privacy-Preserving Scheme for Crowdsensing Applications}, 
year={2016}, 
pages={460-468}, 
abstract={Crowdsensing applications rely on volunteers to collect sensor readings using their mobile devices. Since the collected sensor readings are annotated with spatiotemporal information, the volunteers' privacy may be endangered. Existing privacy-preserving solutions often disclose the volunteers' location information to either a central third party or their peers. As a result, the volunteers need to trust these parties to respect their privacy. In this paper, we present a distributed approach based on the concept of multi-party computation, which does not require a trusted party and protects the location information against curious users. We evaluate the performance of our approach and show its feasibility by means of extensive simulations based on a real-world dataset. We further implement a proof-of-concept to test its performance under realistic conditions.}, 
keywords={data protection;mobile computing;OP4;crowdsensing applications;distributed approach;location information protection;mobile devices;multiparty computation;opportunistic privacy-preserving scheme;real-world dataset;sensor reading collection;spatiotemporal information;Computational modeling;Cryptography;Generators;Logic gates;Privacy;Sensors;Servers}, 
doi={10.1109/LCN.2016.75}, 
month={Nov},}
@ARTICLE{6867349, 
author={J. Yu and D. Tao and M. Wang and Y. Rui}, 
journal={IEEE Transactions on Cybernetics}, 
title={Learning to Rank Using User Clicks and Visual Features for Image Retrieval}, 
year={2015}, 
volume={45}, 
number={4}, 
pages={767-779}, 
abstract={The inconsistency between textual features and visual contents can cause poor image search results. To solve this problem, click features, which are more reliable than textual information in justifying the relevance between a query and clicked images, are adopted in image ranking model. However, the existing ranking model cannot integrate visual features, which are efficient in refining the click-based search results. In this paper, we propose a novel ranking model based on the learning to rank framework. Visual features and click features are simultaneously utilized to obtain the ranking model. Specifically, the proposed approach is based on large margin structured output learning and the visual consistency is integrated with the click features through a hypergraph regularizer term. In accordance with the fast alternating linearization method, we design a novel algorithm to optimize the objective function. This algorithm alternately minimizes two different approximations of the original objective function by keeping one function unchanged and linearizing the other. We conduct experiments on a large-scale dataset collected from the Microsoft Bing image search engine, and the results demonstrate that the proposed learning to rank models based on visual features and user clicks outperforms state-of-the-art algorithms.}, 
keywords={feature extraction;image retrieval;learning (artificial intelligence);Microsoft Bing image search engine;click features;clicked images;fast alternating linearization method;hypergraph regularizer term;image ranking model;image retrieval;image search;large-scale dataset;learning to rank;query images;rank framework;rank models;textual features;textual information;user clicks;visual consistency;visual contents;visual features;Approximation algorithms;Feature extraction;Laplace equations;Linear programming;Search engines;Training;Visualization;Click;Clink;hypergraph;learning to rank;learning to rank.}, 
doi={10.1109/TCYB.2014.2336697}, 
ISSN={2168-2267}, 
month={April},}
@INPROCEEDINGS{6113152, 
author={D. Garcia and F. Schweitzer}, 
booktitle={2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing}, 
title={Emotions in Product Reviews--Empirics and Models}, 
year={2011}, 
pages={483-488}, 
abstract={Online communities provide Internet users with means to overcome some information barriers and constraints, such as the difficulty to gather independent information about products and firms. Product review communities allow customers to share their opinions and emotions after the purchase of a product. We introduce a new dataset of product reviews from Amazon.com, with emotional information extracted by sentiment detection tools. Our statistical analysis of this data provides evidence for the existence of polemic reviews, as well as for the coexistence of positive and negative emotions inside reviews. We find a strong bias towards large values in the expression of positive emotions, while negative ones are more evenly distributed. We identified different time dynamics of the creation of reviews dependent on the presence of marketing and word of mouth effects. We define an agent-based model of the users of product review communities using a modeling framework for online emotions. This model can reproduce the scenarios of response to external influences, as well as some properties of the distributions of positive and negative emotions expressed in product reviews. This analysis and model can provide guidelines to manufacturers on how to increase customer satisfaction and how to measure the emotional impact of marketing campaigns through reviews data.}, 
keywords={Internet;Web sites;customer satisfaction;electronic commerce;information retrieval;multi-agent systems;purchasing;statistical analysis;Amazon.com;Internet users;agent-based model;customer satisfaction;data reviews;emotional impact;emotional information extraction;independent information;information barriers;information constraints;marketing campaigns;modeling framework;negative emotions;online community;online emotions;polemic reviews;positive emotions;product purchase;product review community;product reviews;sentiment detection tools;statistical analysis;time dynamics;word of mouth effects;Biological system modeling;Communities;Correlation;Equations;Internet;Mathematical model;Media;agent-based modeling;emotions;product reviews}, 
doi={10.1109/PASSAT/SocialCom.2011.219}, 
month={Oct},}
@INPROCEEDINGS{7098234, 
author={J. Patel and V. Jindal and I. L. Yen and F. Bastani and J. Xu and P. Garraghan}, 
booktitle={2015 IEEE Twelfth International Symposium on Autonomous Decentralized Systems}, 
title={Workload Estimation for Improving Resource Management Decisions in the Cloud}, 
year={2015}, 
pages={25-32}, 
abstract={In cloud computing, good resource management can benefit both cloud users as well as cloud providers. Workload prediction is a crucial step towards achieving good resource management. While it is possible to estimate the workloads of long-running tasks based on the periodicity in their historical workloads, it is difficult to do so for tasks which do not have such recurring workload patterns. In this paper, we present an innovative clustering based resource estimation approach which groups tasks that have similar characteristics into the same cluster. The historical workload data for tasks in a cluster are used to estimate the resources needed by new tasks based on the cluster(s) to which they belong. In particular, for a new task T, we measure T's initial workload and predict to which cluster(s) it may belong. Then, the workload information of the cluster(s) is used to estimate the workload of T. The approach is experimentally evaluated using Google dataset, including resource usage data of over half a million tasks. We develop a workload model based on the dataset which is then used to estimate the workload patterns of several randomly selected tasks from the trace log. The results confirm the effectiveness of this cluster-based method for estimating the resources required by each task.}, 
keywords={cloud computing;pattern clustering;resource allocation;Google dataset;cloud computing;clustering based resource estimation approach;historical workloads;resource management decisions;resource usage data;workload estimation;workload pattern estimation;workload prediction;Cloud computing;Clustering algorithms;Estimation;Google;Resource management;Servers;Time series analysis;Cloud computing;dynamic time warp distance;workload clustering;workload prediction}, 
doi={10.1109/ISADS.2015.17}, 
ISSN={1541-0056}, 
month={March},}
@INPROCEEDINGS{5614607, 
author={A. De and E. Diaz and V. V. Raghavan}, 
booktitle={2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology}, 
title={Search Engine Result Aggregation Using Analytical Hierarchy Process}, 
year={2010}, 
volume={3}, 
pages={300-303}, 
abstract={A metasearch engine queries search engines and collates information returned by them in one result set for the user. Metasearch can be external or internal. In external metasearch, result lists from external, independent search engines are merged. On the other hand, in an internal metasearch, result lists from using different search algorithms on the same corpus are aggregated. Thus result merging is a key function of metasearch. In this work, we propose a model for result merging that is based on the Analytic Hierarchy Process and compares documents and search engines in pair-wise comparison before merging. Our model has the capability to merge result lists based on ranks as well as scores, as returned by search engines. We use the LETOR 2 (LEarning TO Rank) dataset from Microsoft Research Asia for our experiments. When using document ranks, our model improves by 31.60% and 8.58% over the Borda-Fuse and Weighted Borda-Fuse models respectively. When using document scores the improvements are 42.92% and 18.03% respectively.}, 
keywords={query processing;search engines;LETOR 2 dataset;analytical hierarchy process;learning to rank;metasearch engine queries search engines;search engine result aggregation;Analytical models;Computational modeling;Engines;Fuses;Merging;Metasearch;Search engines;Information Retrieval;Web Intelligence}, 
doi={10.1109/WI-IAT.2010.256}, 
month={Aug},}
@INPROCEEDINGS{7275700, 
author={Kavinkumar V. and R. R. Reddy and R. Balasubramanian and Sridhar M. and Sridharan K. and D. Venkataraman}, 
booktitle={2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
title={A hybrid approach for recommendation system with added feedback component}, 
year={2015}, 
pages={745-752}, 
abstract={With the increasing E-Commerce and online shopping there is a need for recommendation systems which help the customers in decision making and to suggest potential goods of purchase. In domains such as automobiles there are many websites but most of them are not having enhanced recommendation systems to enable easy decision making. Thus we have taken the initiative of building a dataset with multiple parameters based on a survey of the communities needs using potential blogs and created a recommendation system using user based and item based collaborative filtering. In addition to the combined collaborative filtering techniques we propose a framework which includes a feedback analysis to improve the recommendation system. The enhanced model aids the customers in decision making. We have proposed the feedback system at two levels. One is external feedback where the comments are gathered from public platforms like social media and automobile websites. The other is internal feedback i.e. the feedback is taken from users who have been provided with recommended items. The opinions extracted from such varied comments broadens the system and results. Our proposed hybrid model with feedback analysis has improvised the current system by providing better suggestions to customers.}, 
keywords={collaborative filtering;electronic commerce;recommender systems;automobile Web sites;automobiles;decision making;e-commerce;external feedback;feedback analysis;feedback component;hybrid approach;internal feedback;item based collaborative filtering;online shopping;potential blogs;recommendation system;social media;Algorithm design and analysis;Automobiles;Collaboration;Feature extraction;Filtering;Media;Principal component analysis;Collaborative Filtering;Feedback analysis;Multiple Parameter;Sentiment}, 
doi={10.1109/ICACCI.2015.7275700}, 
month={Aug},}
@INPROCEEDINGS{6853805, 
author={V. Karappa and C. D. D. Monteiro and F. M. Shipman and R. Gutierrez-Osuna}, 
booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Detection of sign-language content in video through polar motion profiles}, 
year={2014}, 
pages={1290-1294}, 
abstract={Locating sign language (SL) videos on video sharing sites (e.g., YouTube) is challenging because search engines generally do not use the visual content of videos for indexing. Instead, indexing is done solely based on textual content (e.g., title, description, metadata). As a result, untagged SL videos do not appear in the search results. In this paper, we present and evaluate a classification approach to detect SL videos based on their visual content. The approach uses an ensemble of Haar-based face detectors to define regions of interest (ROI), and a background model to segment movements in the ROI. The two-dimensional (2D) distribution of foreground pixels in the ROI is then reduced to two 1D polar motion profiles by means of a polar-coordinate transformation, and then classified by means of an SVM. When evaluated on a dataset of user-contributed YouTube videos, the approach achieves 81% precision and 94% recall.}, 
keywords={Haar transforms;face recognition;indexing;sign language recognition;video signal processing;Haar-based face detectors;SVM;YouTube videos;foreground pixels;polar motion profiles;polar-coordinate transformation;regions of interest;search engines;sign-language detection;two-dimensional distribution;untagged SL videos;video sharing sites;Adaptation models;Assistive technology;Face;Face detection;Feature extraction;Gesture recognition;Hidden Markov models;content-based video retrieval;metadata extraction;sign language}, 
doi={10.1109/ICASSP.2014.6853805}, 
ISSN={1520-6149}, 
month={May},}
@INPROCEEDINGS{7379660, 
author={Y. J. Kim and B. N. Kang and D. Kim}, 
booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics}, 
title={Hidden Markov Model Ensemble for Activity Recognition Using Tri-Axis Accelerometer}, 
year={2015}, 
pages={3036-3041}, 
abstract={Recently, thanks to a variety of sensors equipped on smartphone, a lot of research about mobile activity recognition using accelerometer have been studied for context inference of mobile user and healthcare applications. Previous works, however, have a limitation in classifying some activities because of intra-class variations and inter-class similarities. To handle this problem, in this paper we propose a novel method to recognize activity of smart phone user based on hidden Markov model, where an ensemble method of hidden Markov models is proposed and used to recognize activity. To evaluate our method, we have carried out some experiments by using UCI Human Activity Recognition dataset, and as a result we have achieved about 83.51% accuracy when using two simple features, mean and standard deviation. It is a comparable result to other powerful discriminative methods such as support vector machine and multilayer perceptron.}, 
keywords={accelerometers;health care;hidden Markov models;smart phones;user interfaces;UCI human activity recognition dataset;context inference;healthcare applications;hidden Markov model ensemble;mobile activity recognition;mobile user;smartphone;tri-axis accelerometer;Accelerometers;Feature extraction;Hidden Markov models;Legged locomotion;Mobile communication;Pattern recognition;Training;hidden Markov model ensemble;mobile activity recognition;mobile context inference;temporal pattern classification}, 
doi={10.1109/SMC.2015.528}, 
month={Oct},}
@INPROCEEDINGS{7809722, 
author={Y. Alufaisan and M. Kantarcioglu and Y. Zhou}, 
booktitle={2016 IEEE 2nd International Conference on Collaboration and Internet Computing (CIC)}, 
title={Detecting Discrimination in a Black-Box Classifier}, 
year={2016}, 
pages={329-338}, 
abstract={Data mining techniques are playing an increasing role in making crucial decisions in our daily lives ranging from credit card approvals to employment decisions. Typically algorithms used to build decision models remain as a black-box to the end user. Therefore the process of decision making appears to be opaque. At the same time, increasing the transparency of a black-box decision making model allows us to discover hidden discrimination, and hold entities accountable. Although algorithmic transparency with respect to black box classifiers requires addressing many challenges, our main objective in this paper is to investigate whether a black-box classification model is biased against certain subgroups. Specifically, we study the indirect discrimination of hidden protected features. Protected features, such as race, gender, and religious beliefs, are those that are prohibited to be legally used for making decisions. Simply removing the protected features is not enough to eliminate discrimination because there could be strong correlations between protected features and non-protected features, such as race versus zip code. In this paper, we present two techniques to measure discrimination of a black-box model as a result of data bias or algorithmic weakness. Data bias is investigated further by introducing artificial bias to the dataset under consideration. Our experimental results demonstrate the effectiveness of our bias measures where bias comes from different sources.}, 
keywords={data mining;decision making;pattern classification;algorithmic transparency;algorithmic weakness;artificial bias;black-box classification model;black-box decision making model;data bias;data mining techniques;discrimination detection;Bayes methods;Correlation;Data mining;Data models;Decision making;Feature extraction;Itemsets;Data Mining;Discrimination;Machine Learning}, 
doi={10.1109/CIC.2016.051}, 
month={Nov},}
@INPROCEEDINGS{5234603, 
author={P. Thamrongrat and L. Preechaveerakul and W. Wettayaprasit}, 
booktitle={2009 2nd IEEE International Conference on Computer Science and Information Technology}, 
title={A novel Voting Algorithm of multi-class SVM for web page classification}, 
year={2009}, 
pages={327-331}, 
abstract={The increasing numbers of Web pages on the cyber world result to the less effectiveness of document retrieval that matches the need of users. The classification of Web pages is one of the solutions to solve this problem. This paper proposes VAMSVM_WPC model which is a novel voting algorithm for classifying the Web pages, which uses a multi-class SVM method. First, feature is generated from text and title, and then reduces the number of features by two feature selection techniques. Use these two types of features to give input to multi-class SVM. Finally, on the output of SVM, a voting algorithm is used to determine the category of the Web pages. Results on CMU benchmark dataset show that using text and title feature with 1vsAll_Voting Algorithm gives the highest F-measure value.}, 
keywords={Internet;information retrieval;pattern classification;support vector machines;text analysis;Web page classification;document retrieval;feature selection;multiclass support vector machine;text analysis;Artificial intelligence;Computer science;Equations;Laboratories;Performance gain;Support vector machine classification;Support vector machines;Testing;Voting;Web pages;feature selection;support vector machine;web page classification voting}, 
doi={10.1109/ICCSIT.2009.5234603}, 
month={Aug},}
@ARTICLE{7496849, 
author={L. Nie and L. Zhang and Y. Yan and X. Chang and M. Liu and L. Shao}, 
journal={IEEE Transactions on Cybernetics}, 
title={Multiview Physician-Specific Attributes Fusion for Health Seeking}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-12}, 
abstract={Community-based health services have risen as important online resources for resolving users health concerns. Despite the value, the gap between what health seekers with specific health needs and what busy physicians with specific attitudes and expertise can offer is being widened. To bridge this gap, we present a question routing scheme that is able to connect health seekers to the right physicians. In this scheme, we first bridge the expertise matching gap via a probabilistic fusion of the physician-expertise distribution and the expertise-question distribution. The distributions are calculated by hypergraph-based learning and kernel density estimation. We then measure physicians attitudes toward answering general questions from the perspectives of activity, responsibility, reputation, and willingness. At last, we adaptively fuse the expertise modeling and attitude modeling by considering the personal needs of the health seekers. Extensive experiments have been conducted on a real-world dataset to validate our proposed scheme.}, 
keywords={Adaptation models;Bridges;Computer science;Context;Internet;Medical services;Routing;Adaptive fusion;attitude modeling;community-based health services (CHSs);expertise modeling;question routing}, 
doi={10.1109/TCYB.2016.2577590}, 
ISSN={2168-2267}, 
month={},}
@INPROCEEDINGS{6676560, 
author={P. M. Dudas and M. d. Jongh and P. Brusilovsky}, 
booktitle={2013 17th International Conference on Information Visualisation}, 
title={A Semi-supervised Approach to Visualizing and Manipulating Overlapping Communities}, 
year={2013}, 
pages={180-185}, 
abstract={When evaluating a network topology, occasionally data structures cannot be segmented into absolute, heterogeneous groups. There may be a spectrum to the dataset that does not allow for this hard clustering approach and may need to segment using fuzzy/overlapping communities or cliques. Even to this degree, when group members can belong to multiple cliques, there leaves an ever present layer of doubt, noise, and outliers caused by the overlapping clustering algorithms. These imperfections can either be corrected by an expert user to enhance the clustering algorithm or to preserve their own mental models of the communities. Presented is a visualization that models overlapping community membership and provides an interactive interface to facilitate a quick and efficient means of both sorting through large network topologies and preserving the user's mental model of the structure.}, 
keywords={data visualisation;interactive systems;social sciences computing;sorting;interactive interface;network topology;overlapping communities manipulation;overlapping communities visualization;overlapping community membership;semisupervised approach;sorting;user mental model;overlapping communities;semi-supervised clustering;user-defined cliques;visualization}, 
doi={10.1109/IV.2013.23}, 
ISSN={1550-6037}, 
month={July},}
@INPROCEEDINGS{6761337, 
author={D. Christin and A. Reinhardt and M. Hollick}, 
booktitle={38th Annual IEEE Conference on Local Computer Networks}, 
title={On the efficiency of privacy-preserving path hiding for mobile sensing applications}, 
year={2013}, 
pages={818-826}, 
abstract={Current mobile sensing applications typically annotate the collected sensor readings with spatiotemporal information before reporting them to a central server. Such information can however endanger the users' privacy, as it reveals insights about their daily routines. Users must therefore trust the application administrators not to misuse the reported information. To diminish user dependence on administrators trustworthiness, we propose a privacy-preserving collaborative scheme, in which users exchange the collected sensor readings at opportunistic encounters. We model malicious administrators attempting to identify exchanged sensor readings based on spatial disparity by applying four state-of-the-art outlier detection algorithms. We thoroughly investigate the influence of different exchange patterns and the parameters of the algorithms on their performance based on a real-world dataset. The results for location traces of 20 users gathered during 14 days show that our algorithm achieves a high level of privacy protection.}, 
keywords={data privacy;mobile radio;telecommunication security;administrators trustworthiness;central server;malicious administrators;mobile sensing applications;outlier detection algorithms;privacy protection;privacy-preserving collaborative;privacy-preserving path hiding;sensor readings;spatiotemporal information;time 14 day;user privacy;Computational modeling;Conferences;Detection algorithms;Privacy;Sensors;Servers;Spatiotemporal phenomena;Mobile applications;security and privacy protection}, 
doi={10.1109/LCN.2013.6761337}, 
ISSN={0742-1303}, 
month={Oct},}
@INPROCEEDINGS{4035752, 
author={H. Kang and C. Plaisant and B. Lee and B. B. Bederson}, 
booktitle={2006 IEEE Symposium On Visual Analytics Science And Technology}, 
title={NetLens: Iterative Exploration of Content-Actor Network Data}, 
year={2006}, 
pages={91-98}, 
abstract={Networks have remained a challenge for information retrieval and visualization because of the rich set of tasks that users want to accomplish. This paper offers an abstract content-actor network data model, a classification of tasks, and a tool to support them. The NetLens interface was designed around the abstract content-actor network data model to allow users to pose a series of elementary queries and iteratively refine visual overviews and sorted lists. This enables the support of complex queries that are traditionally hard to specify. NetLens is general and scalable in that it applies to any dataset that can be represented with our abstract data model. This paper describes NetLens applying a subset of the ACM Digital Library consisting of about 4,000 papers from the CM I conference written by about 6,000 authors. In addition, we are now working on a collection of half a million emails, and a dataset of legal cases}, 
keywords={data models;data visualisation;digital libraries;graphical user interfaces;NetLens interface;abstract data model;content-actor network data;digital library;human-computer interaction;incremental data exploration;information retrieval;information visualization;iterative query refinement;network visualization;piccolo;Data models;Data visualization;Displays;Graphical user interfaces;Laboratories;Law;Legal factors;Software libraries;User interfaces;Visual analytics;Human-Computer Interaction;content-actor network data;digital library;incremental data exploration;information visualization;iterative query refinement;network visualization;piccolo;user interfaces}, 
doi={10.1109/VAST.2006.261426}, 
month={Oct},}
@INPROCEEDINGS{7880044, 
author={A. Capodieci and L. Mainetti and S. Carrozzo}, 
booktitle={2016 12th International Conference on Innovations in Information Technology (IIT)}, 
title={Semantic enterprise service bus for cultural heritage}, 
year={2016}, 
pages={1-8}, 
abstract={The problem of data flooding is becoming more and more important, and it is necessary to find a way to access relevant information and to collect information and data from different data sources. In the field of cultural heritage, users need to be able to find and extract information from databases of specific contexts (paintings, inscriptions, churches, old coins, etc.). The problem is that each of these databases has a different data model. A project created to solve this problem is Europeana, which aims to bring together data from different providers (museums, libraries or other entities) in a single dataset. This paper aims to propose an architecture for a heterogeneous data sources system for the integration of structured/semi-structured data sources, enabling the information content to be more accessible and searchable for users. The data integration model proposed in this paper aims to exploit the method of "schema matching", using ontology-based data integration techniques by automating the integration process.}, 
keywords={data integration;data structures;history;ontologies (artificial intelligence);Europeana;cultural heritage;data flooding;heterogeneous data sources system;information collection;ontology-based data integration;schema matching;semantic enterprise service bus;structured/semi-structured data sources;Cultural differences;Data integration;Data models;Europe;Ontologies;Semantics;Technological innovation;ESB;Semantic;Semantic Enterprise Service Bus}, 
doi={10.1109/INNOVATIONS.2016.7880044}, 
month={Nov},}
@INPROCEEDINGS{7044866, 
author={N. r. Kim and J. H. Lee}, 
booktitle={2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS)}, 
title={Group recommendation system: Focusing on home group user in TV domain}, 
year={2014}, 
pages={985-988}, 
abstract={Group recommendation system is an important research area, because there are many situations where a group user take an item such as watching the movie, listening the music and watching the TV contents with their family or friends. In order to suggest group recommendation, understanding of group user and domain is necessary. However, there are few analysis of group recommendation system using real-world dataset, because most researches use synthetic dataset. In this paper, we apply the various methods to real-world dataset, and provide some guides for group recommendation system.}, 
keywords={recommender systems;television applications;TV domain;group recommendation system;home group user;real-world dataset;synthetic dataset;Analytical models;Collaboration;Computers;History;Recommender systems;TV;TV domain;group modeling;group recommendation system;home group user}, 
doi={10.1109/SCIS-ISIS.2014.7044866}, 
month={Dec},}
@INPROCEEDINGS{5130259, 
author={B. Erdogan and N. G. Gencer}, 
booktitle={2009 14th National Biomedical Engineering Meeting}, 
title={Application of Wiener Deconvolution Model in P300 Spelling Paradigm}, 
year={2009}, 
pages={1-4}, 
abstract={Spelling paradigm first introduced by Farwell and Donchin, is one of the brain computer interface (BCI) applications that enables paralyzed people to communicate with their environment. In such a problem, user needs to focus on the characters which are randomly flashed row or column-wise on the computer screen in a small period of time. The accuracy in spelling words is the main problem in this scheme and the duration of the correct prediction is quite important. The purpose of this work is twofold: to analyze a user specific response to a spelling paradigm system considering the optimal frequency bands for P300 detection, and secondly to investigate the classification performance for the perception of row and columnwise flashings in the spelling system. The preprocessing is performed with Wiener deconvolution model (WDM) and optimal filters for user specific system is constructed. The proposed algorithm is applied to dataset IIb of BCI competition 2003 and the words for training and testing sets are predicted with 100% accuracy after first 4 trials, as compared to other winning algorithms (100% accuracy in 5 repetitions) of the competition. Furthermore, our classification results show that perception to row and column flashings may differ considerably.}, 
keywords={bioelectric phenomena;brain-computer interfaces;deconvolution;electroencephalography;filtering theory;handicapped aids;medical signal processing;neurophysiology;signal classification;BCI;EEG;P300 spelling paradigm system;Wiener deconvolution model;brain computer interface;classification performance;columnwise flashing;computer screen;optimal filters;optimal frequency bands;paralyzed people;rowwise flashing;signal preprocessing;Accuracy;Application software;Brain computer interfaces;Brain modeling;Deconvolution;Electroencephalography;Enterprise resource planning;Filters;Testing;Wavelength division multiplexing;Brain Computer Interface;P300;Spelling Paradigm;Wiener Deconvolution Model}, 
doi={10.1109/BIYOMUT.2009.5130259}, 
month={May},}
@INPROCEEDINGS{5486884, 
author={Fei Ding and Hui Cheng and Xia-meng Si and Yun Liu and Fei Xiong and Bo Shen}, 
booktitle={2010 2nd International Conference on Advanced Computer Control}, 
title={Read and reply behaviors in a BBS social network}, 
year={2010}, 
volume={4}, 
pages={571-576}, 
abstract={Understanding user' read and reply behaviors are important both for the study of online social network properties and the modeling of information dissemination and trend prediction. Our dataset is crawled from a famous Chinese BBS social network. By analyzing the read and reply datas of users, we observe that distributions of discussion size and user participation level follow the power-law distribution, agreeing with former empirical and analytical investigations. But the distribution of user interest does not exhibit a power-law property. Statistics from hot posts show that read and reply behaviors vary among different topic categories. For every 100 users interested in a post, one or two of them in average would join the discussion. In addition, investigation concerning time property of replies shows that decay functions with periodical fluctuations are suitable to describe the short-term evolution.}, 
keywords={Web sites;behavioural sciences computing;data analysis;information dissemination;social networking (online);BBS social network;data analysis;information dissemination modelling;online social network property;power law distribution;Blogs;Data analysis;Discussion forums;Fluctuations;Information systems;Laboratories;Power system modeling;Predictive models;Social network services;Statistical distributions;BBS;data analysis;online social networks;user behavior}, 
doi={10.1109/ICACC.2010.5486884}, 
month={March},}
@INPROCEEDINGS{5575952, 
author={V. Singh and S. Venkatesha and A. K. Singh}, 
booktitle={2010 IEEE International Conference on Granular Computing}, 
title={Geo-clustering of Images with Missing GeoTags}, 
year={2010}, 
pages={420-425}, 
abstract={Images with GPS coordinates are a rich source of information about a geographic location. Innovative user services and applications are being built using geotagged images taken from community contributed repositories like Flickr. Only a small subset of the images in these repositories is geotagged, limiting their exploration and effective utilization. We propose to use optional meta-data along with image content to geo-cluster all the images in a partly geotagged dataset. We formulate the problem as a graph clustering problem where edge weights are vectors of incomparable components. We develop probabilistic approaches to fuse the components into a single measure and then, discover clusters using an existing random walk method. Our empirical results strongly show that meta-data can be successfully exploited and merged together to achieve geo clustering of images missing geotags.}, 
keywords={data models;image processing;pattern clustering;probability;random processes;visual databases;community contributed repositories;geographic location;geotagged dataset;geotagged images;graph clustering problem;images geo-clustering;missing GeoTags;optional meta data;probabilistic approach;random walk method;Clustering algorithms;Image edge detection;Logistics;Markov processes;Probabilistic logic;Training;Visualization;clustering;geotag prediction;heterogeneous;images;missing data}, 
doi={10.1109/GrC.2010.76}, 
month={Aug},}
@INPROCEEDINGS{7379480, 
author={S. Song and Y. Meng and Z. Zheng}, 
booktitle={2015 IEEE International Conference on Systems, Man, and Cybernetics}, 
title={Recommending Hashtags to Forthcoming Tweets in Microblogging}, 
year={2015}, 
pages={1998-2003}, 
abstract={Over the last few years, microblogging is increasingly becoming an important platform for users to acquire information and publish some reviews or personal status. In microblogging, hash tags mean some topic words between two '#', such as some social events or some hot topics. Hash tags can highlight the topic of tweets, and make tweets be easily searched and understood by others. Therefore, many users like adding hash tags for their tweets. Existing hash tag recommendation methods always consider semantic similarity between hash tags and tweets as the only key factor. However, the hash tags' user acceptance degree and development tendency are two important factors for evaluate the recommendation probability of them. In this paper, we propose a model for recommending some related hash tags for users to choose one or more of them as content added into forthcoming tweets. The above three factors have been considered to complete this task, which are the semantic similarity between a hash tag and a tweet, the user acceptance degree of the hash tag, and the development tendency of the hash tag. Experimental results on a dataset from Sina Weibo, one of the largest Chinese microblogging websites, show usefulness of the proposed model for recommending hash tags to forthcoming tweets.}, 
keywords={information retrieval;recommender systems;social networking (online);Chinese microblogging Web site;Sina Weibo;hash tag development tendency;hashtags recommendation method;recommendation probability;semantic similarity;tweets;user acceptance degree;Data models;Semantics;Splines (mathematics);Tagging;Time series analysis;Training;Twitter;forthcoming tweets;hashtag development tendency detection;hashtag recommendation;microblogging;topic model;web service}, 
doi={10.1109/SMC.2015.348}, 
month={Oct},}
@INPROCEEDINGS{7906928, 
author={S. Mehnaz and E. Bertino}, 
booktitle={2016 14th Annual Conference on Privacy, Security and Trust (PST)}, 
title={Building robust temporal user profiles for anomaly detection in file system accesses}, 
year={2016}, 
pages={207-210}, 
abstract={Protecting sensitive data against malicious or compromised insiders is a big concern. In most cases, insiders have authorized access in file systems containing such data which they misuse or exfiltrate for financial profit. Moreover, external parties can compromise identity credentials of valid file system users by means of exploiting security vulnerabilities, phishing attacks etc. Therefore, in order to protect sensitive information from such attackers, security measures, e.g., access control and encryption are often combined with anomaly detection. Anomaly detection is based on the key observation that the access behavior of an attacker is significantly different from the regular access pattern of a benign user. However, due to the complexity of users' interactions with a file system, the modeling of user profiles is a challenging problem. As a result, most of the existing anomaly detection techniques suffer from poor user profiles that contribute to high false positive and high false negative rates. In this paper, we propose an approach that as a first step discovers the users' tasks (sets of file accesses that represent distinct file system activities) by applying frequent sequence mining on the access log. In the next step, our approach builds robust temporal user profiles by extensively analyzing the timestamp information of users' file system accesses and thus precisely models the relation between the users' tasks and their temporal properties using a multilevel temporal data structure. Finally, we evaluate the performance of our approach on a real dataset.}, 
keywords={data structures;anomaly detection technique;file system authorized access;financial profit;multilevel temporal data structure;regular access pattern;robust temporal user profile;sensitive data protection;Access control;Automata;Buildings;Data structures;Robustness;Time-frequency analysis}, 
doi={10.1109/PST.2016.7906928}, 
month={Dec},}
@INPROCEEDINGS{5643564, 
author={L. Gruber and S. Gauglitz and J. Ventura and S. Zollmann and M. Huber and M. Schlegel and G. Klinker and D. Schmalstieg and T. Höllerer}, 
booktitle={2010 IEEE International Symposium on Mixed and Augmented Reality}, 
title={The City of Sights: Design, construction, and measurement of an Augmented Reality stage set}, 
year={2010}, 
pages={157-163}, 
abstract={We describe the design and implementation of a physical and virtual model of an imaginary urban scene-the “City of Sights”- that can serve as a backdrop or “stage” for a variety of Augmented Reality (AR) research. We argue that the AR research community would benefit from such a standard model dataset which can be used for evaluation of such AR topics as tracking systems, modeling, spatial AR, rendering tests, collaborative AR and user interface design. By openly sharing the digital blueprints and assembly instructions for our models, we allow the proposed set to be physically replicable by anyone and permit customization and experimental changes to the stage design which enable comprehensive exploration of algorithms and methods. Furthermore we provide an accompanying rich dataset consisting of video sequences under varying conditions with ground truth camera pose. We employed three different ground truth acquisition methods to support a broad range of use cases. The goal of our design is to enable and improve the replicability and evaluation of future augmented reality research.}, 
keywords={augmented reality;image sequences;rendering (computer graphics);solid modelling;user interfaces;City of Sights;augmented reality stage set;collaborative AR design;digital blueprints sharing;ground truth acquisition methods;ground truth camera pose;imaginary urban scene;rendering test;user interface design;video sequence;Accuracy;Calibration;Cameras;Computational modeling;Solid modeling;Subspace constraints;Three dimensional displays}, 
doi={10.1109/ISMAR.2010.5643564}, 
month={Oct},}
@INPROCEEDINGS{7462601, 
author={Z. Yijie and X. Jun}, 
booktitle={2015 Sixth International Conference on Intelligent Systems Design and Engineering Applications (ISDEA)}, 
title={Competition Results Prediction Model Based on Athlete Ability Data Simulation and Analysis}, 
year={2015}, 
pages={222-225}, 
abstract={This paper offers formalized description of outcome prediction for sports competitions. It proposes a novel match prediction model and team model based study on existing technologies. The team model adopts improved Bayesian network. It adopt EM algorithm to learn player capability of each team to evaluate the ability value of player, according to integrate match situation. Then the scoring probability and team score expectation are computed, combined with the case of players in presence. At last the score estimation of round prediction by match model is acquired. During study of team model coefficients, we assume it obeys distribution of Logit and Probit and compare their effects. Experimental analysis adopts three seasons of real dataset in CBA league, and our model is compared to Bayesian network model. The results show our model is more accurate than existing model in match process prediction.}, 
keywords={belief networks;data analysis;probability;sport;Bayesian network model;CBA league;EM algorithm;Logit;Probit;athlete ability data simulation;formalized description;match prediction model;match process prediction;round prediction;score estimation;scoring probability;sports competitions;team model coefficients;team score expectation;Analytical models;Bayes methods;Computational modeling;Data models;Games;Predictive models;Random variables;Bayesian network;coefficient;result prediction;team model}, 
doi={10.1109/ISDEA.2015.64}, 
month={Aug},}
@INPROCEEDINGS{7009721, 
author={I. Ahmed and D. Guan and T. Chung}, 
booktitle={2014 International Conference on Machine Learning and Cybernetics}, 
title={A novel semi-supervised learning for SMS classification}, 
year={2014}, 
volume={2}, 
pages={856-861}, 
abstract={In this paper, we propose a novel semi-supervised methodology to detect spam or ham SMSs, using frequent item set mining algorithm Apriori, probabilistic model Naive Bayes and ensemble learning. This paper considers the unbalanced data set problem which means designing of two class SMS classifier using small number of ham and unlabeled dataset only. Using only a few labeled examples with Semi-supervised training is typically unreliable. However, by applying user-specified minimum support and minimum confidence on ham and unlabeled dataset, we gained significant accuracy on classifying SMSs, experimenting on UCI data Repository.}, 
keywords={data mining;electronic messaging;learning (artificial intelligence);pattern classification;Apriori algorithm;SMS classification;ensemble learning;frequent item set mining algorithm;naive Bayes model;semisupervised learning;Abstracts;Accuracy;Classification algorithms;Information security;Support vector machines;Apriori Algorithm;Ensemble Learning;Ham;Minimum confidence;Minimum support;Naive Bayes classifier;Short Message Service (SMS);spam}, 
doi={10.1109/ICMLC.2014.7009721}, 
ISSN={2160-133X}, 
month={July},}
@ARTICLE{7865955, 
author={L. l. Shi and L. Liu and Y. Wu and L. Jiang and J. Hardy}, 
journal={IEEE Access}, 
title={Event Detection and User Interest Discovering in Social Media Data Streams}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Social media plays an increasingly important role in people’s life. Microblogging is a form of social media which allows people to share and disseminate real-life events. Broadcasting events in microblogging networks can be an effective method of creating awareness, divulging important information and so on. However, many existing approaches at dissecting the information content primarily discuss the event detection model and ignore the user interest which can be discovered during event evolution. This leads to difficulty in tracking the most important events as they evolve including identifying the influential spreaders. There is further complication given that the influential spreaders interests will also change during event evolution. The influential spreaders play a key role in event evolution and this has been largely ignored in traditional event detection methods. To this end, we propose a user-interest model based event evolution model, named the HEE (Hot Event Evolution) model. This model not only considers the user interest distribution, but also uses the short text data in the social network to model the posts and the recommend methods to discovering the user interests. This can resolve the problem of data sparsity, as exemplified by many existing event detection methods, and improve the accuracy of event detection. A hot event automatic filtering algorithm is initially applied to remove the influence of general events, improving the quality and efficiency of mining the event. Then an automatic topic clustering algorithm is applied to arrange the short texts into clusters with similar topics. An improved user-interest model is proposed to combine the short texts of each cluster into a long text document simplifying the determination of the overall topic in relation to the interest distribution of each user during the evolution of important events. Finally a novel cosine measure based event similarity detection method is used to assess c- rrelation between events thereby detecting the process of event evolution. The experimental results on a real Twitter dataset demonstrate the efficiency and accuracy of our proposed model for both event detection and user interest discovery during the evolution of hot events.}, 
keywords={Data mining;Data models;Event detection;Filtering algorithms;Semantics;Twitter;Event Evolution;Microblogging;User Topic}, 
doi={10.1109/ACCESS.2017.2675839}, 
ISSN={2169-3536}, 
month={},}
@INPROCEEDINGS{6823925, 
author={Y. Zhang and X. Zhu and Q. Shen}, 
booktitle={2013 5th IEEE International Conference on Broadband Network Multimedia Technology}, 
title={A recommendation model based on collaborative filtering and factorization machines for social networks}, 
year={2013}, 
pages={110-114}, 
abstract={The communication of persons based on mobile Internet is more and more frequency as the widespread of social network media like Facebook, Weibo and so on. Faced with the vast amount of user groups and variety user features, the traditional single recommended algorithm turns to noneffective. In this paper, we use the collaborative filtering to select features in social network relationship which is used to build a vector space model. Also, we propose a distributed algorithm based on factorization machines and genetic algorithm, and use the vector space model as one part of input. We split the records into different sessions depend on the time sequence for the filter of train dataset. In the experiment part we proved that all the methods mentioned above remarkable improved the recommendation accuracy.}, 
keywords={collaborative filtering;distributed algorithms;genetic algorithms;mobile computing;recommender systems;social networking (online);vectors;Facebook;Weibo;collaborative filtering;distributed algorithm;factorization machines;genetic algorithm;mobile Internet;recommendation accuracy;recommendation model;social network media;social network relationship;time sequence;train dataset;user features;user groups;vector space model;Accuracy;Collaboration;Data models;Feature extraction;Filtering;Social network services;Vectors;Collaborative filtering;Factorization machines;Recommendation system;Social network}, 
doi={10.1109/ICBNMT.2013.6823925}, 
month={Nov},}
@ARTICLE{6839005, 
author={R. Mekuria and M. Sanna and E. Izquierdo and D. C. A. Bulterman and P. Cesar}, 
journal={IEEE Transactions on Multimedia}, 
title={Enabling Geometry-Based 3-D Tele-Immersion With Fast Mesh Compression and Linear Rateless Coding}, 
year={2014}, 
volume={16}, 
number={7}, 
pages={1809-1820}, 
abstract={3-D tele-immersion (3DTI) enables participants in remote locations to share, in real time, an activity. It offers users interactive and immersive experiences, but it challenges current media-streaming solutions. Work in the past has mainly focused on the efficient delivery of image-based 3-D videos and on realistic rendering and reconstruction of geometry-based 3-D objects. The contribution of this paper is a real-time streaming component for 3DTI with dynamic reconstructed geometry. This component includes both a novel fast compression method and a rateless packet protection scheme specifically designed towards the requirements imposed by real time transmission of live-reconstructed mesh geometry. Tests on a large dataset show an encoding speed-up up to ten times at comparable compression ratio and quality, when compared with the high-end MPEG-4 SC3DMC mesh encoders. The implemented rateless code ensures complete packet loss protection of the triangle mesh object and a delivery delay within interactive bounds. Contrary to most linear fountain codes, the designed codec enables real-time progressive decoding allowing partial decoding each time a packet is received. This approach is compared with transmission over TCP in packet loss rates and latencies, typical in managed WAN and MAN networks, and heavily outperforms it in terms of end-to-end delay. The streaming component has been integrated into a larger 3DTI environment that includes state of the art 3-D reconstruction and rendering modules. This resulted in a prototype that can capture, compress transmit, and render triangle mesh geometry in real-time in realistic internet conditions as shown in experiments. Compared with alternative methods, lower interactive end-to-end delay and frame rates over three times higher are achieved.}, 
keywords={Internet;codecs;delays;geometry;image coding;image reconstruction;media streaming;mesh generation;rendering (computer graphics);solid modelling;virtual reality;3D reconstruction modules;3DTI;Internet;MAN networks;WAN networks;delivery delay;dynamic reconstructed geometry;end-to-end delay;fast mesh compression method;frame rates;geometry-based 3D object reconstruction;geometry-based 3D teleimmersion;high-end MPEG-4 SC3DMC mesh encoders;image-based 3D videos;immersive experiences;interactive experiences;linear fountain codes;linear rateless coding;live-reconstructed mesh geometry;media-streaming solutions;packet loss protection;packet loss rates;partial decoding;rateless packet protection scheme;real-time progressive decoding;real-time streaming component;rendering modules;transmission over TCP;triangle mesh geometry;triangle mesh object;Encoding;Geometry;Image reconstruction;Real-time systems;Three-dimensional displays;Transform coding;Videos;3-D mesh streaming;3-D tele-immersion;Block codes;geometry processing;multimedia communication;multimedia systems;source coding}, 
doi={10.1109/TMM.2014.2331919}, 
ISSN={1520-9210}, 
month={Nov},}
@INPROCEEDINGS{6113930, 
author={R. Banjade and S. Maharjan}, 
booktitle={2011 Second Asian Himalayas International Conference on Internet (AH-ICI)}, 
title={Product recommendations using linear predictive modeling}, 
year={2011}, 
pages={1-4}, 
abstract={Recommendation systems apply statistical and knowledge discovery techniques to the problem of making product recommendations and they are achieving widespread success in E-Commerce these days. A successful recommendation system fulfils several purposes and the choice of the methodology significantly influences the quality of recommendations and other aspects including scalability. As the volume of data in the e-commerce is growing massively, the system should also be able to address the need to provide the recommendations either by in-memory calculations or offline calculations, both demanding the high performance. For a large number of customers and products, the linear regression with a proper model selection can provide significantly better results and performance. Recommendations engines are increasingly becoming a popular choice for solving the problem of content discovery enabling the user to find personally relevant content that they might not have known was available. In this paper, we consider linear regression technique for analyzing large-scale dataset for the purpose of useful recommendations to e-commerce customers by offline calculations of model results.}, 
keywords={customer services;data mining;electronic commerce;recommender systems;regression analysis;content discovery;e-commerce customers;in-memory calculation;knowledge discovery techniques;large scale dataset;linear predictive modeling;linear regression;model selection;offline calculations;product recommendations;recommendation engines;recommendation system;scalability;statistical techniques;Algorithm design and analysis;Analytical models;Data models;Linear regression;Prediction algorithms;Predictive models;Tuning;e-commerce;modeling;regression analysis;scalability;tuning}, 
doi={10.1109/AHICI.2011.6113930}, 
ISSN={1089-7801}, 
month={Nov},}
@INPROCEEDINGS{6406534, 
author={S. Wu and Z. Fang and J. Tang}, 
booktitle={2012 IEEE 12th International Conference on Data Mining Workshops}, 
title={Accurate Product Name Recognition from User Generated Content}, 
year={2012}, 
pages={874-877}, 
abstract={This paper presents the solution of the team "ISSSID" for the Consumer Products Contest #1(CPROD1) of ICDM 2012. The contest provides a dataset including hundreds of thousands of text items, a product catalog with over fifteen million products, and hundreds of manually annotated product mentions. The goal of the competition is to automatically recognize product mentions in the textual content and disambiguate which product(s) in the product catalog are referenced by the mentions. We propose a hybrid approach which combines the results obtained by several separately trained recognition models. Specifically, the approach uses a standard matching model, a rule template model, and a conditional random field model, and finally combines the results using a blending model. The proposed approach achieves the best performance in the contest.}, 
keywords={Catalogs;Data mining;Educational institutions;Lead;Semantics;Standards;Training data;CPROD1;Named Entity Recognition;Nature Language Processing}, 
doi={10.1109/ICDMW.2012.129}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{7349753, 
author={Y. Jia and X. Wang and R. S. Sedeh and Q. Wu and N. Cohen}, 
booktitle={2015 International Conference on Healthcare Informatics}, 
title={A Web-Based Application for Visualizing the CMS Referral Network}, 
year={2015}, 
pages={501-503}, 
abstract={In recent years, Centers for Medicare and Medicaid Services (CMS) have released a large scale physician referral dataset. Combining this dataset with others containing enriched information of healthcare providers, we developed a web-based application that visualizes the CMS referral network focusing on physicians' referrals to hospitals. The application can be used in many practical cases, providing evidence to facilitate decision making for many types of users such as patients, physicians, hospitals and governments.}, 
keywords={data visualisation;health care;hospitals;medical information systems;CMS referral network visualization;Centers for Medicare and Medicaid Services;Web-based application;decision making;health care providers;large-scale physician referral dataset;physician referrals;Data models;Data visualization;Hospitals;Organizations;Rails;Taxonomy;Data visualization;Referral network;Web-based application}, 
doi={10.1109/ICHI.2015.91}, 
month={Oct},}
@INPROCEEDINGS{6864328, 
author={P. A. Bizopoulos and A. I. Sakellarios and D. D. Koutsouris and D. Iliopoulou and L. K. Michalis and D. I. Fotiadis}, 
booktitle={IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)}, 
title={Randomly generated realistic vessel geometry using spline interpolation and 2D Perlin noise}, 
year={2014}, 
pages={157-160}, 
abstract={The importance of using artificial vessel walls for examining the effects of the geometry to the risk of plaque rupture, has grown. This has also become essential, mainly because of the lack of an extensive dataset of real vessel geometries that covers an adequate number of cases (e.g. variable amplitude of stenosis for the same vessel). In this paper, we propose a method for creating realistic artificial vessel geometry. The centerline is constructed using PseudoRandom Noise Generator (PRNG) and 3d spline interpolation and the artificial wall using a variation of the 2d Perlin Noise. The resulted geometry indicates that this method achieves realistic representations of vessel walls with user defined parameters.}, 
keywords={biomechanics;blood vessels;diseases;fracture;geometry;interpolation;noise generators;physiological models;random noise;splines (mathematics);2D Perlin noise variation;3D spline interpolation;PRNG method;artificial vessel geometry effects;artificial vessel walls;centerline construction;plaque rupture risk;pseudorandom noise generator;random vessel geometry generation;real vessel geometry dataset;realistic artificial vessel geometry;realistic vessel geometry generation;realistic vessel wall representations;user defined parameters;variable stenosis amplitude;Arteries;Computational modeling;Educational institutions;Geometry;Interpolation;Splines (mathematics);Three-dimensional displays}, 
doi={10.1109/BHI.2014.6864328}, 
ISSN={2168-2194}, 
month={June},}
@INPROCEEDINGS{6890535, 
author={Chenyi Lei and D. Liu}, 
booktitle={2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)}, 
title={Image annotation via social diffusion analysis with common interests}, 
year={2014}, 
pages={1-6}, 
abstract={Automatic annotation of images is of crucial importance in image retrieval and management systems. Most of the existing annotation methods rely on content-based approach to annotation, whose effectiveness is restricted due to the semantic gap between low-level features and semantic annotations, as well as the irrelevance between annotations and image content. Recently, social media analysis has been investigated for image annotation. Inspired by the abundant social diffusion records of images in online social networks, we propose a novel image annotation approach based on social diffusion analysis. We present a common-interest model to interpret social diffusion, i.e. different images have different social diffusion routes due to the preferences of users, and such preferences are represented as common interests of pairwise users rather than personalized interests. We propose an image annotation framework that consists of learning of common interests, feature extraction from social diffusion records, and automatic annotation by learning to rank. Experimental results on a real-world dataset show that our proposed approach outperforms content-based and user-preference-based annotation methods.}, 
keywords={feature extraction;image retrieval;social networking (online);automatic annotation;common interests;common-interest model;feature extraction;image annotation framework;image management system;image retrieval system;online social networks;semantic annotations;semantic gap;social diffusion analysis;social diffusion records;social media analysis;Abstracts;Positron emission tomography;Vectors;Common interests;image annotation;learning to rank;social diffusion analysis;social media}, 
doi={10.1109/ICMEW.2014.6890535}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{6335169, 
author={A. K. Gopalakrishna and T. Özçelebi and A. Liotta and J. J. Lukkien}, 
booktitle={2012 6th IEEE International Conference Intelligent Systems}, 
title={Exploiting machine learning for intelligent room lighting applications}, 
year={2012}, 
pages={406-411}, 
abstract={Research has shown that environment lighting influences the behavior of the employees in an office setting highly, making lighting configuration in an office space crucial. A breakout area may be used by the employees for various activities that need to be supported by different lighting conditions, e.g. informal meetings or personal retreat. The desired lighting conditions depend on user preferences and other contextual data observable in the environment. In this paper, we introduce a new method for building prediction models to provide intelligent lighting in our pilot breakout area. Based on a set of pre-defined features that are expected to have influence on the users' choice in selecting a desired lighting environment, we introduce a probabilistic model for generating synthetic data. We also discuss and compare the performances of various rule-based classification models on the synthetic data and find `DecisionTable' to be the most suitable model for our pilot implementation. We study the influence of the training set size (number of samples) on various classification models and the influences of individual features through simulations. We present empirical results based on the synthetic dataset and a roadmap for future research.}, 
keywords={building management systems;decision tables;learning (artificial intelligence);lighting;office environment;pattern classification;probability;decision table;employees behavior;environment lighting;informal meetings;intelligent room lighting applications;machine learning;office setting;office space;personal retreat;probabilistic model;rule-based classification models;synthetic data generation;user preferences;Accuracy;Classification algorithms;Data models;Lighting;Prediction algorithms;Predictive models;Training;adaptive office;classification models;intelligent lighting;synthetic data model}, 
doi={10.1109/IS.2012.6335169}, 
ISSN={1541-1672}, 
month={Sept},}
@INPROCEEDINGS{6425776, 
author={A. Tatar and P. Antoniadis and M. D. de Amorim and S. Fdida}, 
booktitle={2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining}, 
title={Ranking News Articles Based on Popularity Prediction}, 
year={2012}, 
pages={106-110}, 
abstract={News articles are a captivating type of online content that capture a significant amount of Internet users' interest. They are particularly consumed by mobile users and extremely diffused through online social platforms. As a result, there is an increased interest in promptly identifying the articles that will receive a significant amount of user attention. This task falls under the broad scope of content popularity prediction and has direct implications in various contexts such as caching strategies or online advertisement policies. In this paper we address the problem of predicting the popularity of news articles based on user comments. We formulate the prediction task into a ranking problem where the goal is not to infer the precise attention that a content will receive but to accurately rank articles based on their predicted popularity. To this end, we analyze the ranking performance of three prediction models using a dataset of articles covering a four-year period and published by 20minutes.fr, an important French online news platform. Our results indicate that prediction methods improve the ranking performance and we observed that for our dataset a simple linear prediction method outperforms more dedicated prediction methods.}, 
keywords={data mining;electronic publishing;information retrieval;social networking (online);Internet;content popularity prediction;linear prediction method;news article;online social platform;ranking problem;user comment;Accuracy;Adaptation models;Correlation;Mathematical model;Predictive models;Social network services;News articles;popularity;prediction;ranking}, 
doi={10.1109/ASONAM.2012.28}, 
month={Aug},}
@INPROCEEDINGS{5490321, 
author={T. Lefevre and B. Mory and R. Ardon and J. Sanchez-Castro and A. Yezzi}, 
booktitle={2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro}, 
title={Automatic Inferior Vena Cava segmentation in contrast-enhanced CT volumes}, 
year={2010}, 
pages={420-423}, 
abstract={This paper presents a novel robust automatic method for the segmentation of the Inferior Vena Cava (IVC) in the proximity of the liver. In clinical diagnosis and surgery planning, IVC segmentation is essential since it strongly impacts both liver volumetry accuracy and vascularity analysis. Given the anatomical variability, the lack of clear boundaries and complexity of the surrounding structures along the IVC, its segmentation remains a difficult and open problem. To cope with such challenging conditions, we developed an implicit representation of a generalized cylinder and op
@INPROCEEDINGS{6235609, 
author={A. Liu and J. Li and Z. J. Wang and M. J. McKeown}, 
booktitle={2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)}, 
title={An FDR-controlled, exploratory group modeling for assessing brain connectivity}, 
year={2012}, 
pages={558-561}, 
abstract={Recent development in the inference of brain connectivity from neuroimaging data such as functional magnetic resonance images (fMRI) provides better understanding of brain activities and functions. The group analysis of fMRI data usually focuses on functional connectivity, while exploratory graphical modeling of effective connectivity is generally designed for the single-subject case. In this paper, a new group-level network learning method, the group PCfdr algorithm, is proposed by combining group inference with exploratory graphical modeling for estimating brain connectivity, with inter-subject variances taken into consideration. The simulation results demonstrate that this group method can accurately recover underlying group-level structures and steadily control the false discovery rate at the user specified level. It is also applied to a real fMRI dataset of normal subjects and Parkinson's Disease patients to infer the brain connectivity at the group level.}, 
keywords={biomedical MRI;brain;diseases;neurophysiology;physiological models;brain connectivity;brain functions;exploratory graphical modeling;exploratory group modeling;fMRI dataset;false discovery rate;functional connectivity;functional magnetic resonance imaging;group PCfdr algorithm;group-level network learning method;neuroimaging data;parkinsons disease patients;underlying group-level structures;user specified level;Analytical models;Bayesian methods;Brain models;Correlation;Data models;Mathematical model;effective connectivity;exploratory graphical modeling;fMRI;group analysis}, 
doi={10.1109/ISBI.2012.6235609}, 
ISSN={1945-7928}, 
month={May},}
@INPROCEEDINGS{6992161, 
author={R. Zhao and H. Li and Y. Yang and Y. Liang}, 
booktitle={2014 Sixth International Conference on Wireless Communications and Signal Processing (WCSP)}, 
title={Privacy-preserving personalized search over encrypted cloud data supporting multi-keyword ranking}, 
year={2014}, 
pages={1-6}, 
abstract={Cloud computing is emerging as a revolutionary computing paradigm which provides a flexible and economic strategy for data management and resource sharing. Security and privacy become major concerns in the cloud scenario, for which Searchable Encryption (SE) technology is proposed to support efficient keyword based queries and retrieval of encrypted data. However, the absence of personalized search is still a typical shortage in existing SE schemes. In this paper, we focus on addressing personalized search over encrypted cloud data and propose a Privacy-preserving Personalized Search over Encrypted Cloud Data Supporting Multi-keyword Ranking(PPSE) scheme that supports Top-k retrieval in stringent privacy requirements. For the first time, we formulate the privacy issue and design goals for personalized search in SE. We introduce the Open Directory Project to construct a formal model for integrating preferential ranking with keyword search reasonably and automatically, which can help eliminate the ambiguity of any two search requests. In PPSE, we employ the vector space model and the secure kNN scheme to guarantee sufficient search accuracy and privacy protection. The tf-idf weight and the preference weight help to ensure that the search result will faithfully respect the user's interest. As a result, thorough security analysis and performance evaluation on experiments performed on the real-world dataset demonstrate that the PPSE scheme indeed accords with our proposed design goals.}, 
keywords={cloud computing;cryptography;data privacy;query processing;Open Directory Project;PPSE scheme;SE technology;encrypted cloud data supporting multikeyword ranking;flexible-economic data management strategy;flexible-economic resource sharing strategy;formal model;keyword search;keyword-based encrypted data query;keyword-based encrypted data retrieval;performance evaluation;preference weight;preferential ranking integration;privacy protection;privacy-preserving personalized search;real-world dataset;search accuracy;search request ambiguity elimination;secure kNN scheme;security analysis;tf-idf weight;top-k retrieval;user interest;vector space model;Cryptography;Data privacy;Dictionaries;Indexes;Servers;Vectors;Multi-keyword ranking;Personalized search;Searchable encryption}, 
doi={10.1109/WCSP.2014.6992161}, 
month={Oct},}
@INPROCEEDINGS{4216825, 
author={B. S. Suryavanshi and N. Shiri and S. P. Mudur}, 
booktitle={NAFIPS 2006 - 2006 Annual Meeting of the North American Fuzzy Information Processing Society}, 
title={Analysis of Fuzzy Clustering Techniques Used for Web Personalization}, 
year={2006}, 
pages={335-340}, 
abstract={Web personalization aims to provide content and services tailor-made to the needs of individual users usually from the knowledge gained through their (previous) interactions with the site. Typically, an access behavior model of users is learnt from the usage of the Web site which is then used to provide personalized recommendations to the current user(s). In this paper, we present a detailed qualitative as well as experimental analysis of various fuzzy clustering techniques used for mining usage profiles. We discuss their algorithmic strategies, requirement of input parameters, noise handling capacity, scalability to large datasets and similarity of partitions. We validate our claims through experiments using a large real life dataset}, 
keywords={Web design;fuzzy set theory;pattern clustering;Web personalization;Web site;access behavior model;fuzzy clustering;large datasets;noise handling capacity;Clustering algorithms;Computer science;Fuzzy sets;Marketing and sales;Partitioning algorithms;Recommender systems;Scalability;Software engineering;Uncertainty;Web sites}, 
doi={10.1109/NAFIPS.2006.365432}, 
month={June},}
@INPROCEEDINGS{6092665, 
author={D. Liu and S. Wu}, 
booktitle={2011 Fifth International Conference on Management of e-Commerce and e-Government}, 
title={A Query-Oriented Summarization System for XML Elements}, 
year={2011}, 
pages={222-225}, 
abstract={In document-center XML dataset, an element may contain so many text that users have to spend enough time to judge the elements returned by XML search engine are valuable or not. Query-orient XML summarization system aim to provide users a brief and readable substitution of the original retrieved elements according to the user's query, which can relieve user's reading burden effectively. In this work, we extract sentences from the results of XML search engine, and combine them as a summary. Experiments on the IEEE-CS datasets used in INEX show that, the query-oriented XML summary generated by our method is reasonable.}, 
keywords={Internet;XML;query processing;search engines;IEEE-CS datasets;XML elements;XML search engine;document center XML dataset;query oriented summarization system;Computational modeling;Electronic mail;Feature extraction;Search engines;Sections;XML;Content and Structure;Query Expansion;Query-Oriented;XML Text Summarization}, 
doi={10.1109/ICMeCG.2011.22}, 
month={Nov},}
@INPROCEEDINGS{5360645, 
author={J. Liu and M. Shang and D. Chen}, 
booktitle={2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery}, 
title={Personal Recommendation Based on Weighted Bipartite Networks}, 
year={2009}, 
volume={5}, 
pages={134-137}, 
abstract={Recently, network based recommendation algorithms have demonstrated much better performance than the standard collaborative filtering method, and most of which have been focused on the unweighted cases even in a multigraded rating system. However, these modifications from multigraded rating data to binary data may lose information, thus hinder the expressing of user's preference and finally misleading the recommendation systems. In this paper, we propose to use weighted bipartite user-object networks to model the recommender systems. The weight of the edge is directly the rate that a user giving on an object. We use a benchmark dataset, i.e., Moivelens dataset, to test the performance. The results show that weighted theme has higher recommendation accuracy than its unweighted counterpart.}, 
keywords={information filtering;recommender systems;collaborative filtering method;multigraded rating system;network based recommendation algorithms;personal recommendation;recommendation accuracy;recommendation systems;weighted bipartite networks;weighted bipartite user-object networks;Algorithm design and analysis;Benchmark testing;Collaboration;Computer science;Filtering algorithms;Fuzzy systems;Inference algorithms;Information filtering;Knowledge engineering;Recommender systems;Bipartite network;Collaborative filtering;Network-based inference;Personal recommendation}, 
doi={10.1109/FSKD.2009.469}, 
month={Aug},}
@INPROCEEDINGS{7545025, 
author={M. C. Hu and H. T. Wu and L. Y. Lo and T. Y. Pan and W. H. Cheng and K. L. Hua and T. Mei}, 
booktitle={2016 IEEE Second International Conference on Multimedia Big Data (BigMM)}, 
title={A Framework of Enlarging Face Datasets Used for Makeup Face Analysis}, 
year={2016}, 
pages={219-222}, 
abstract={There have been lots of research about face detection and face recognition. However, faces with makeup usually seriously affect the face recognition result. If we want to recognize the face with higher accuracy, it would be better to first know whether the input face is with makeup or not, and we can use corresponding makeup face or non-makeup face model to recognize it. Unfortunately, the current available datasets for face analysis do not include enough makeup and non-makeup image pairs of users. In this work, we propose a framework to efficiently increase pairs of makeup and non-makeup face images for the existing makeup face datasets. Patch-based features are extracted and support vector machine (SVM) is applied to classify whether a face image is with makeup. The technique of partial least squares (PLS) is then employed to authenticate whether a makeup photo and a non-makeup photo belong to the same person. By combining the makeup detection and the face authentication methods, we can successfully construct a larger face dataset that can be specifically used for applications of makeup face analysis.}, 
keywords={cryptography;face recognition;feature extraction;least mean squares methods;support vector machines;PLS;SVM;face authentication methods;face detection;face recognition;makeup face analysis;makeup face datasets;nonmakeup face model;nonmakeup photo;partial least squares;patch-based feature extraction;support vector machine;user non-makeup image pairs;Authentication;Face;Face detection;Face recognition;Feature extraction;Image color analysis;Support vector machines;face analysis;face authentication;makeup detection;support vector machine}, 
doi={10.1109/BigMM.2016.62}, 
month={April},}
@INPROCEEDINGS{6281297, 
author={R. Mead and A. Atrash and M. J. Mataric}, 
booktitle={2011 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
title={Recognition of spatial dynamics for predicting social interaction}, 
year={2011}, 
pages={201-202}, 
abstract={We present a user study and dataset designed and collected to analyze how humans use space in face-to-face interactions. In a proof-of-concept investigation into human spatial dynamics, a Hidden Markov Model (HMM) was trained over a subset of features to recognize each of three interaction cues-initiation, acceptance, and termination-in both dyadic and triadic scenarios; these cues are useful in predicting transitions into, during, and out of multi-party social encounters. It is shown that the HMM approach performed twice as well as a weighted random classifier, supporting the feasibility of recognizing and predicting social behavior based on spatial features.}, 
keywords={hidden Markov models;mobile robots;pattern classification;social sciences computing;HMM approach;face-to-face interactions;hidden Markov model;human spatial dynamics;multi-party social encounters;social behavior;social interaction;weighted random classifier;Calibration;Educational institutions;Floors;Hidden Markov models;Humans;Robots;Human-robot interaction;nonverbal behavior;proxemics;social signals;spatial dynamics}, 
doi={10.1145/1957656.1957731}, 
ISSN={2167-2121}, 
month={March},}
@INPROCEEDINGS{5541041, 
author={Jingfeng Guo and Jie Sun}, 
booktitle={2010 International Conference On Computer Design and Applications}, 
title={Link intensity prediction of online dating networks based on weighted information}, 
year={2010}, 
volume={5}, 
pages={V5-375-V5-378}, 
abstract={The main task of the existing link prediction is to predict link existence. However, this paper proposes a new method aiming at the online dating networks, which using weighted transactional information to predict link intensity. Three different supervised learning methods are used in detail, comparing the importance of attribute features, topological features, transactional features and global features based on users' profile information, as well as the influence of four different network graphs to the model performance. The experiment on Xiaonei dataset shows that the method used in this paper can be used to predict link intensity accurately, also illustrates that the global features have the greatest impact on the performance of model .}, 
keywords={learning (artificial intelligence);social networking (online);Xiaonei dataset;attribute features;link intensity prediction;network graphs;online dating networks;supervised learning methods;topological features;transactional features;weighted transactional information;Algorithm design and analysis;Computer networks;Computer science;Data mining;IP networks;Predictive models;Probability;Social network services;Sun;Supervised learning;link intensity;link prediction;transactional feature}, 
doi={10.1109/ICCDA.2010.5541041}, 
month={June},}
@INPROCEEDINGS{7545040, 
author={H. To and H. Park and S. H. Kim and C. Shahabi}, 
booktitle={2016 IEEE Second International Conference on Multimedia Big Data (BigMM)}, 
title={Incorporating Geo-Tagged Mobile Videos into Context-Aware Augmented Reality Applications}, 
year={2016}, 
pages={295-302}, 
abstract={In recent years, augmented-reality (AR) has been attracting extensive attentions from both the research community and industry as a new form of media, mixing virtual content into the physical world. However, the scarcity of the AR content and the lack of user contexts are major impediments to providing and representing rich and dynamic multimedia content on AR applications. In this study, we propose an approach to search and filter big multimedia data, specifically geo-tagged mobile videos, for context-aware AR applications. The challenge is to automatically search for interesting video segments out of a huge amount of user-generated mobile videos, which is one of the biggest multimedia data, to be efficiently incorporated into AR applications. We model the significance of video segments as AR content adopting camera shooting patterns defined in filming, such as panning, zooming, tracking and arching. Then, several efficient algorithms are proposed to search for such patterns using fine granular geospatial properties of the videos such as camera locations and viewing directions over time. Experiments with real-world geo-tagged video dataset show that the proposed algorithms effectively search for a large collection of user-generated mobile videos to identify top K significant video segments.}, 
keywords={augmented reality;mobile computing;multimedia computing;user interfaces;video signal processing;AR;context-aware augmented reality applications;dynamic multimedia content;geo-tagged mobile videos;user-generated mobile videos;video segments;virtual content;Browsers;Cameras;Context;Mobile communication;Multimedia communication;Streaming media;Videos;Augmented Reality;Geo-tagging;Mobile Video;Multimedia Content}, 
doi={10.1109/BigMM.2016.64}, 
month={April},}
@INPROCEEDINGS{7314702, 
author={L. Xue and W. Luan}, 
booktitle={2015 Ninth International Conference on Frontier of Computer Science and Technology}, 
title={Improved K-means Algorithm in User Behavior Analysis}, 
year={2015}, 
pages={339-342}, 
abstract={College Network Center has accumulated large number of user's online behavior data. We tried to analyze these data in order to grasp the user surfing habits and potential relationship, and then summarize an effective strategy for user online management. First of all, we taken a university as the example, and collected the user online data to extract related fields for sampling. In addition, we proposed a figure to present the number of Internet users in different time. After that, we use k-means algorithm to cluster the records, improved the algorithm by considering the element of user information. Finally, we apply the improved algorithm on a dataset and get the conclusion.}, 
keywords={Internet;behavioural sciences computing;pattern clustering;Internet users;cluster;k-means algorithm;university;user behavior analysis;user online management;user surfing habits;users online behavior data;Algorithm design and analysis;Biological system modeling;Classification algorithms;Clustering algorithms;Databases;Internet;Partitioning algorithms;k-means cluster;network managementt}, 
doi={10.1109/FCST.2015.15}, 
ISSN={2159-6301}, 
month={Aug},}
@INPROCEEDINGS{6008714, 
author={Y. Kang and Y. Zhou and Z. Zheng and M. R. Lyu}, 
booktitle={2011 IEEE 4th International Conference on Cloud Computing}, 
title={A User Experience-Based Cloud Service Redeployment Mechanism}, 
year={2011}, 
pages={227-234}, 
abstract={Cloud computing has attracted much interest recently from both industry and academic. Nowadays, more and more Internet applications are moving to the cloud environment. Making optimal deployment of cloud applications is critical for providing good performance to attract users. Optimizing user experience is usually required for cloud service deployment. However, it is a challenging task to know the user experience of end users, since there is generally no proactive connection between a user to the machine that will host the service instance. To attack this challenge, in this paper, we first propose a framework to model cloud features and capture user experience. Then based on the collected user connection information, we formulate the redeployment of service instances as k-median and max k-cover problems. We proposed several approximation algorithms to efficiently solve these problems. Comprehensive experiments are conducted by employing a real-world QoS dataset of service invocation. The experimental results show the effectiveness of our proposed redeployment approaches.}, 
keywords={approximation theory;cloud computing;quality of service;Internet applications;approximation algorithms;cloud computing;k-median problems;max k-cover problems;real-world QoS dataset;user experience-based cloud service redeployment mechanism;Approximation algorithms;Approximation methods;Cloud computing;Delay;Greedy algorithms;Heuristic algorithms}, 
doi={10.1109/CLOUD.2011.20}, 
ISSN={2159-6182}, 
month={July},}
@INPROCEEDINGS{7840690, 
author={H. Liao and Y. Li and T. Hu and J. Luo}, 
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
title={Inferring restaurant styles by mining crowd sourced photos from user-review websites}, 
year={2016}, 
pages={937-944}, 
abstract={When looking for a restaurant online, user uploaded photos often give people an immediate and tangible impression about a restaurant. Due to their informativeness, such user contributed photos are leveraged by restaurant review websites to provide their users an intuitive and effective search experience. In this paper, we present a novel approach to inferring restaurant types or styles (ambiance, dish styles, suitability for different occasions) from user uploaded photos on user-review websites. To that end, we first collect a novel restaurant photo dataset associating the user contributed photos with the restaurant styles from TripAdvior. We then propose a deep multi-instance multi-label learning (MIML) framework to deal with the unique problem setting of the restaurant style classification task. We employ a two-step bootstrap strategy to train a multi-label convolutional neural network (CNN). The multi-label CNN is then used to compute the confidence scores of restaurant styles for all the images associated with a restaurant. The computed confidence scores are further used to train a final binary classifier for each restaurant style tag. Upon training, the styles of a restaurant can be profiled by analyzing restaurant photos with the trained multi-label CNN and SVM models. Experimental evaluation has demonstrated that our crowd sourcing-based approach can effectively infer the restaurant style when there are a sufficient number of user uploaded photos for a given restaurant.}, 
keywords={catering industry;data mining;learning (artificial intelligence);neural nets;pattern classification;social networking (online);statistical analysis;support vector machines;SVM;TripAdvior;confidence scores;crowd sourcing-based approach;deep multiple instance multilabel learning;final binary classifier;intuitive effective search experience;multilabel CNN;multilabel convolutional neural network training;restaurant photo dataset;restaurant review Web sites;restaurant style classification;restaurant style tag;restaurant types;two-step bootstrap strategy;user contributed photos;user-review websites;Business;Data mining;Feature extraction;Neural networks;Support vector machines;Training;Urban areas;Restaurant styles;crowd sourcing;data mining;multi-instance multi-label learning;multi-label CNN;social media}, 
doi={10.1109/BigData.2016.7840690}, 
month={Dec},}
@ARTICLE{6714457, 
author={W. Qiu and J. Yuan and E. Ukwatta and Y. Sun and M. Rajchl and A. Fenster}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Prostate Segmentation: An Efficient Convex Optimization Approach With Axial Symmetry Using 3-D TRUS and MR Images}, 
year={2014}, 
volume={33}, 
number={4}, 
pages={947-960}, 
abstract={We propose a novel global optimization-based approach to segmentation of 3-D prostate transrectal ultrasound (TRUS) and T2 weighted magnetic resonance (MR) images, enforcing inherent axial symmetry of prostate shapes to simultaneously adjust a series of 2-D slice-wise segmentations in a “global” 3-D sense. We show that the introduced challenging combinatorial optimization problem can be solved globally and exactly by means of convex relaxation. In this regard, we propose a novel coherent continuous max-flow model (CCMFM), which derives a new and efficient duality-based algorithm, leading to a GPU-based implementation to achieve high computational speeds. Experiments with 25 3-D TRUS images and 30 3-D T2w MR images from our dataset, and 50 3-D T2w MR images from a public dataset, demonstrate that the proposed approach can segment a 3-D prostate TRUS/MR image within 5-6 s including 4-5 s for initialization, yielding a mean Dice similarity coefficient of 93.2% ± 2.0% for 3-D TRUS images and 88.5% ± 3.5% for 3-D MR images. The proposed method also yields relatively low intra- and inter-observer variability introduced by user manual initialization, suggesting a high reproducibility, independent of observers.}, 
keywords={axial symmetry;biomedical MRI;biomedical ultrasonics;duality (mathematics);graphics processing units;image segmentation;medical image processing;optimisation;2-D slice-wise segmentations;3D prostate TRUS-MR image;3D prostate transrectal ultrasound images;CCMFM;Dice similarity coefficient;GPU-based implementation;T2 weighted magnetic resonance images;axial symmetry;coherent continuous max-flow model;combinatorial optimization problem;convex relaxation;duality-based algorithm;eflicient convex optimization approach;global 3-D sense;global optimization-based approach;inherent axial symmetry;interobserver variability;intraobserver variability;prostate segmentation;Biopsy;Image segmentation;Imaging;Optimization;Shape;Silicon;Three-dimensional displays;Convex optimization;rotational symmetry;three-dimensional (3-D) prostate magnetic resonance imaging (MRI);three-dimensional (3-D) prostate transrectal ultrasound (TRUS);Algorithms;Analysis of Variance;Databases, Factual;Humans;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Male;Prostate;Prostatic Neoplasms;Reproducibility of Results}, 
doi={10.1109/TMI.2014.2300694}, 
ISSN={0278-0062}, 
month={April},}
@INPROCEEDINGS{5698678, 
author={S. Tripathi}, 
booktitle={2010 International Conference on Power, Control and Embedded Systems}, 
title={AQUIRE: Applied qualitative use of interrelated entities #x2014; Methodology for analysing qualitative data in HCI}, 
year={2010}, 
pages={1-6}, 
abstract={Industrial software user experience research tends to use more qualitative research in order to gain much knowledge about their users. There are number of analysis techniques originated from other domains like social science and psychology, available to analyse the qualitative dataset gathered during field studies. Human Computer Interaction researchers have tried and using some or combination of those techniques. Since these techniques have evolved basically in academia research, it offers a rigorous process and needs certain level of expertise in using them. In industrial software user experience research, where research projects are very much applied in nature and having shorter project life cycle, often putting difficulties before the practitioners in using that rigorous analysis process. This paper presents an innovative, simple to use and systematic way of analyzing qualitative data set in shorter period of time. The presented methodology uses applied qualitative use of interrelated entities gathered in terms of narrative data set for analysing the results.}, 
keywords={data analysis;human computer interaction;AQUIRE;HCI;applied qualitative use of interrelated entities;human computer interaction;industrial software user experience research;qualitative data analysis;usability;Context;Data analysis;Human computer interaction;Interviews;Probes;Tagging;Usability;Analytical model;Data analysis;Human Computer Interaction;Human factors;Man machine system;User centred design}, 
doi={10.1109/ICPCES.2010.5698678}, 
month={Nov},}
@INPROCEEDINGS{7372244, 
author={C. Cao and Q. Ni and Y. Zhai}, 
booktitle={2015 IEEE 27th International Conference on Tools with Artificial Intelligence (ICTAI)}, 
title={An Effective Recommendation Model Based on Communities and Trust Network}, 
year={2015}, 
pages={1029-1036}, 
abstract={Recommendation technology has experienced its great popularity for resource recommendation in web-based social networks. This paper proposed an effective recommendation model based on communities and trust network (CTNRM). In the proposed model, the neighbors of the recommended user are selected from the users in the same community with the recommended user, the users in the recommended user's circle of friends and the users in the recommended user's trusted community. Experiments on the Epinions dataset demonstrate the feasibility and effectiveness of CTNRM. The experimental results validate the effectiveness of our proposed recommendation model for improving the precision and rating coverage especially for inactive users.}, 
keywords={collaborative filtering;recommender systems;social networking (online);trusted computing;CTNRM;Epinions dataset;Web-based social networks;community detection;effective recommendation model;inactive users;rating coverage;recommended user circle of friends;recommended user neighbor selection;recommended user trusted community;resource recommendation technology;trust network;Collaboration;Computational modeling;Mathematical model;Prediction algorithms;Reliability;Social network services;Sparse matrices;collaborative filtering recommendation;community detection;the trusted community;trust network}, 
doi={10.1109/ICTAI.2015.147}, 
ISSN={1082-3409}, 
month={Nov},}
@INPROCEEDINGS{6675167, 
author={L. Zhou and C. Hansen}, 
booktitle={2013 IEEE Symposium on Large-Scale Data Analysis and Visualization (LDAV)}, 
title={Interactive rendering and efficient querying for large multivariate seismic volumes on consumer level PCs}, 
year={2013}, 
pages={117-118}, 
abstract={We present a volume visualization method that allows interactive rendering and efficient querying of large multivariate seismic volume data on consumer level PCs. The volume rendering pipeline utilizes a virtual memory structure that supports out-of-core multivariate multi-resolution data and a GPU-based ray caster that allows interactive multivariate transfer function design. A Gaussian mixture model representation is precomputed and nearly interactive querying is achieved by testing the Gaussian functions against user defined transfer functions on the GPU in the runtime. Finally, the method has been tested on a multivariate 3D seismic dataset which is larger than the size of the main memory of the testing machine.}, 
keywords={Gaussian processes;data structures;geophysics computing;graphics processing units;interactive systems;pipeline processing;query processing;rendering (computer graphics);seismology;transfer functions;virtual storage;GPU-based ray caster;Gaussian functions;Gaussian mixture model representation;consumer level PCs;interactive multivariate transfer function design;interactive rendering;large multivariate seismic volume data;multivariate 3D seismic dataset;nearly interactive querying;out-of-core multivariate multiresolution data;virtual memory structure;volume rendering pipeline;volume visualization method;Multivariate volume;Out-of-core Methods;Transfer Functions}, 
doi={10.1109/LDAV.2013.6675167}, 
month={Oct},}
@INPROCEEDINGS{838346, 
author={R. Conley and J. W. Lavrakas}, 
booktitle={IEEE 2000. Position Location and Navigation Symposium (Cat. No.00CH37062)}, 
title={Global implications on the removal of selective availability}, 
year={2000}, 
pages={506-513}, 
abstract={In March 1996, President Clinton issued a Presidential Decision Directive (PDD) that directed the Department of Defense (DoD) to discontinue the use of selective availability (SA) by the year 2006, with annual reviews of the DoD's status in preparing for the event beginning in 2000. Unless the DoD presents compelling evidence to counter the PDD, SA will transition to an “off” state within the next few years. The purpose of this paper is to explore the ramifications of removing SA from the standard positioning service (SPS). The paper summarizes expected GPS orbit and clock error behavior in the absence of SA, and how those errors map into signal-in-space (SIS) user range errors (UREs). This data is then used to assess expected instantaneous position solution errors on a global basis, using the current constellation. This analysis serves to demonstrate the core contribution of the GPS space and control segments to SPS accuracy. The GPS Modernization Program being executed by the GPS Joint Program Office (JPO) includes a new civil signal that will provide SPS users with a two-frequency ionosphere delay correction capability. However, it will be several years before this capability is available, so single-frequency model errors will still be part of the SPS user's error budget. The paper summarizes an analysis of measured errors in the single-frequency model based upon correlations between the single frequency model and two-frequency measurements taken over an extended period of time from the Federal Aviation Administration's (FAA) National Satellite Test Bed (NSTB). This error information is then added to our SIS URE dataset, and instantaneous position solutions including the single frequency error are generated. The paper then discusses the implications our findings have for the global positioning/navigation/timing (PNT) infrastructure. The paper provides a summary of how well GPS will support a wide assortment of user communities, and discusses the policy considerations implicit in the decision to discontinue the use of SA}, 
keywords={Global Positioning System;clocks;correlation methods;delays;error analysis;government policies;measurement errors;military systems;timing;Department of Defense;DoD;FAA;Federal Aviation Administration;GPS Joint Program Office;GPS Modernization Program;GPS clock error;GPS orbit error;National Satellite Test Bed;Presidential Decision Directive;SIS URE dataset;civil signal;instantaneous position solution errors;instantaneous position solutions;measured errors;positioning/navigation/timing infrastructure;selective availability removal;signal-in-space user range errors;single-frequency model errors;standard positioning service;two-frequency ionosphere delay correction;user error budget;Clocks;Counting circuits;Delay;Error analysis;Extraterrestrial measurements;FAA;Frequency measurement;Global Positioning System;Ionosphere;Time measurement}, 
doi={10.1109/PLANS.2000.838346}, 
month={},}
@ARTICLE{7423758, 
author={L. Zhou}, 
journal={IEEE Transactions on Multimedia}, 
title={On Data-Driven Delay Estimation for Media Cloud}, 
year={2016}, 
volume={18}, 
number={5}, 
pages={905-915}, 
abstract={It is well known that delay announcement is an economical and efficient way to improve the user satisfaction since the waiting time (delay) is an important performance metric for media cloud. However, how to accurately estimate the delay in an online-implementation manner is still an open and challenging problem. In this study, we study the data-driven delay estimation in a practical cloud media with heavy traffic, and propose an accurate estimation strategy only with a small amount of dataset. Importantly, we explicitly model the subjective announcement-dependent user response via an objective response function through the elaborate data analysis and model. On the theoretical end, the user response in terms of the estimated delay is characterized by the time window data-cleaning, where an appropriate dataset is set up through the window function analysis. On the technical end, we analyze the conditions for data-driven delay estimation, and prove that the proposed method is able to obtain a near-optimal solution within a finite time period. Extensive simulation results demonstrate the efficiency of the proposed delay estimation method.}, 
keywords={cloud computing;data analysis;delay estimation;multimedia computing;data analysis;data-driven delay estimation;media cloud;objective response function;subjective announcement-dependent user response;time window data-cleaning;window function analysis;Cloud computing;Data models;Delay estimation;Estimation;Media;Multimedia communication;Delay announcement;Media cloud;delay announcement;media cloud;user response;user satisfaction}, 
doi={10.1109/TMM.2016.2537782}, 
ISSN={1520-9210}, 
month={May},}
@INPROCEEDINGS{7177441, 
author={J. Cai and R. Hong and M. Wang and Q. Tian}, 
booktitle={2015 IEEE International Conference on Multimedia and Expo (ICME)}, 
title={Exploring feature space with semantic attributes}, 
year={2015}, 
pages={1-6}, 
abstract={Indexing is a critical step for searching digital images in a large database. To date, how to design discriminative and compact indexing strategy still remains a challenging issue, partly due to the well-known semantic gap between user queries and rich semantics in the large scale dataset. In this paper, we propose to construct a novel joint semantic-visual space by leveraging visual descriptors and semantic attributes, which aims to narrow down the semantic gap by taking both attribute and indexing into one framework. Such a joint space embraces the flexibility of conducting Coherent Semantic-visual Indexing, which employs binary codes to boost the retrieval speed with satisfying accuracy. To solve the proposed model effectively, three contributions are made in this submission. First, we propose an interactive optimization method to find the joint space of semantic and visual descriptors. Second, we prove the convergence property of our optimization algorithm, which guarantees our system will find a good solution in certain rounds. At last, we integrate the semantic-visual joint space system with spectral hashing, which can find an efficient solution to search up to million scale datasets. Experiments on two standard retrieval datasets i.e., Holidays1M and Oxford5K, show that the proposed method presents promising performance compared with the state-of-the-arts.}, 
keywords={file organisation;image retrieval;indexing;optimisation;binary codes;coherent semantic-visual indexing;convergence property;interactive optimization method;optimization algorithm;semantic attributes;semantic gap;semantic-visual joint space system;spectral hashing;visual descriptors;Binary codes;Feature extraction;Indexing;Joints;Semantics;Visualization;Wheels}, 
doi={10.1109/ICME.2015.7177441}, 
ISSN={1945-7871}, 
month={June},}
@INPROCEEDINGS{7195567, 
author={K. Lee and J. Park and J. Baik}, 
booktitle={2015 IEEE International Conference on Web Services}, 
title={Location-Based Web Service QoS Prediction via Preference Propagation for Improving Cold Start Problem}, 
year={2015}, 
pages={177-184}, 
abstract={With the popularity of service-oriented architecture, many web systems have been developed in form of composite services. Since the performance of these composite services highly depends on Quality of Service (QoS) of employed atomic web services, it is important to predict the QoS values of atomic web services with high accuracy. Although collaborative filtering based approaches have recently been proposed to predict the web service QoS values, they mostly face a cold start problem which causes unreliable prediction due to the highly sparse historical data, newly introduced users and web services. Furthermore, existing work only considers the case of newly introduced users. In this paper, we propose a Location-based Matrix Factorization technique via Preference Propagation (LMF-PP) to improve the cold start problem in web service QoS prediction domain. LMF-PP exploits the location information of entities (i.e., Users and web services) and employs the preference propagation to make the accurate QoS prediction even for the newly introduced entities and in the small amount of data (i.e., Highly sparse matrix). The performance of LMF-PP is compared with that of existing approaches on a real world dataset. The experimental results show that LMF-PP can outperform the existing approaches in not only a cold start environment but also a warm start environment.}, 
keywords={Web services;collaborative filtering;matrix decomposition;quality of service;service-oriented architecture;sparse matrices;LMF-PP;Web systems;atomic Web services;cold start problem;collaborative filtering based approach;composite service;entity location information;highly sparse historical data;highly sparse matrix;location-based Web service QoS prediction;location-based matrix factorization technique;preference propagation;quality of service;service-oriented architecture;Accuracy;Mathematical model;Predictive models;Quality of service;Reliability;Sparse matrices;Web services;Collaborative Filtering;Matrix Factorization;Preference Propagation;Web Service QoS Prediction}, 
doi={10.1109/ICWS.2015.33}, 
month={June},}
@INPROCEEDINGS{7516319, 
author={A. S. Anisyah and P. H. Rusmin and H. Hindersah}, 
booktitle={2015 4th International Conference on Interactive Digital Media (ICIDM)}, 
title={Route optimization movement of tugboat with A #x2217; tactical pathfinding in SPIN 3D simulation}, 
year={2015}, 
pages={1-5}, 
abstract={Ships as one of the favorite transport is now a tool of choice, especially for the transport of goods in container. A relatively low cost with a large capacity are the main reasons why the vessel is the best choice. It is of course also affect the port density of the traffic managers of trade. The concerned problem in this research focus on optimization route for tugboat. The goal is to calculate the total cost for fuel and total distance that tugboat explored. Using A* algorithm tactical pathfinding with navigation mesh will draw an optimization route for tugboat to berthing or leave from port. Two activity that mention before are the main responsibility for the tugboat which used as parameter to calculate the total distance that tugboat explored. In the end of this research will find out how the route affect the total of distance without reduce the service for the vessel berth in port. Dataset which used in this research are port of Tanjung Priok with some parameters for unwalkable area for A* agent. The obstacle in this simulation are the position of other ships in the queue, the position of other ships in the docks and obstacles in the port area such as coral and anything that can be dangerous for the ship docked. From this research, optimize path success to reduce the distance compare to simulation without optimize route.}, 
keywords={graph theory;graphical user interfaces;marine navigation;sea ports;search problems;A* agent;A* tactical pathfinding;SPIN 3D simulation;Tanjung Priok;navigation mesh;port density;ship position;total distance;total fuel cost;tugboat route optimization movement;vessel berth;Algorithm design and analysis;Containers;Marine vehicles;Navigation;Optimization;Ports (Computers);Solid modeling;A∗;Navigation Mesh;Tactical Pathfinding;pathfinding;port simulator}, 
doi={10.1109/IDM.2015.7516319}, 
month={Dec},}
@INPROCEEDINGS{5995332, 
author={V. Bychkovsky and S. Paris and E. Chan and F. Durand}, 
booktitle={CVPR 2011}, 
title={Learning photographic global tonal adjustment with a database of input / output image pairs}, 
year={2011}, 
pages={97-104}, 
abstract={Adjusting photographs to obtain compelling renditions requires skill and time. Even contrast and brightness adjustments are challenging because they require taking into account the image content. Photographers are also known for having different retouching preferences. As the result of this complexity, rule-based, one-size-fits-all automatic techniques often fail. This problem can greatly benefit from supervised machine learning but the lack of training data has impeded work in this area. Our first contribution is the creation of a high-quality reference dataset. We collected 5,000 photos, manually annotated them, and hired 5 trained photographers to retouch each picture. The result is a collection of 5 sets of 5,000 example input-output pairs that enable supervised learning. We first use this dataset to predict a user's adjustment from a large training set. We then show that our dataset and features enable the accurate adjustment personalization using a carefully chosen set of training photos. Finally, we introduce difference learning: this method models and predicts difference between users. It frees the user from using predetermined photos for training. We show that difference learning enables accurate prediction using only a handful of examples.}, 
keywords={image reconstruction;learning (artificial intelligence);visual databases;brightness adjustments;compelling renditions;input image pairs;learning photographic global tonal adjustment;output image pairs;photographs adjustment;supervised machine learning;users adjustment;Cameras;Ground penetrating radar;Histograms;Machine learning;Measurement;Supervised learning;Training}, 
doi={10.1109/CVPR.2011.5995332}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7471862, 
author={A. Chasapi and C. Kotropoulos and K. Pliakos}, 
booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Adaptive algorithms for hypergraph learning}, 
year={2016}, 
pages={1179-1183}, 
abstract={Social media sharing platforms enable image content as well as context information (e.g., user friendships, geo-tags assigned to images) to be jointly analyzed in order to achieve accurate image annotation or successful image recommendation. The context information is expressed frequently in terms of high-order relations, such as the relations among users, tags, and images. Hypergraphs can model the aforementioned high-order relations between their vertices (i.e., users, user social groups, tags, geo-tags, and images) by hyper-edges, whose influence can be assessed by properly estimating their weights. Here, an efficient adaptive hypergraph weight estimation is proposed for image tagging. In particular, both equality and inequality constraints enforced during hypergraph learning are taken into account and an efficient adaptation step selection using the Armijo rule is proposed. Experiments conducted on a dataset demonstrate the superior performance of the proposed approach compared to the state-of-the-art.}, 
keywords={adaptive estimation;identification technology;image recognition;Armijo rule;adaptation step selection;adaptive algorithms;adaptive hypergraph weight estimation;context information;high-order relations;hypergraph learning;image annotation;image content;image recommendation;image tagging;inequality constraints;social media sharing platforms;Adaptation models;Estimation;Media;Optimization;Social groups;Symmetric matrices;Tagging;Gradient search algorithms;Hypergraph learning;Image tagging}, 
doi={10.1109/ICASSP.2016.7471862}, 
month={March},}
@ARTICLE{6679018, 
author={A. Shivram and C. Ramaiah and V. Govindaraju}, 
journal={IET Biometrics}, 
title={A hierarchical Bayesian approach to online writer identification}, 
year={2013}, 
volume={2}, 
number={4}, 
pages={191-198}, 
abstract={With the explosive growth of the tablet form factor and greater availability of pen-based direct input, online writer identification is increasingly becoming critical for person identification, digital forensics as well as downstream applications such as intelligent and adaptive user environments, search, indexing and retrieval of handwritten documents. Extant research has approached writer identification by using writing styles as a discriminative function between writers. In contrast, the authors model writing styles as a shared component of an individual's handwriting. They develop a theoretical framework for this conceptualisation and model it by using a three-level hierarchical Bayesian model (Latent Dirichlet Allocation). In this text-independent, unsupervised model each writer's handwriting is modelled as a distribution over finite writing styles that are shared among writers. They test their model on a new online handwriting dataset IBM_UB_1 and also offer benchmark comparisons by using the IAM-OnDB database. Their experiments show comparable results to the current benchmarks and demonstrate the efficacy of explicitly modelling the shared writing styles.}, 
keywords={belief networks;digital forensics;handwriting recognition;unsupervised learning;IAM-OnDB database;digital forensics;extant research;handwritten documents;hierarchical Bayesian approach;online writer identification;person identification;text-independent;three-level hierarchical Bayesian model;unsupervised model;writing styles}, 
doi={10.1049/iet-bmt.2013.0017}, 
ISSN={2047-4938}, 
month={December},}
@INPROCEEDINGS{6909712, 
author={K. Duan and D. J. Crandall and D. Batra}, 
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Multimodal Learning in Loosely-Organized Web Images}, 
year={2014}, 
pages={2465-2472}, 
abstract={Photo-sharing websites have become very popular in the last few years, leading to huge collections of online images. In addition to image data, these websites collect a variety of multimodal metadata about photos including text tags, captions, GPS coordinates, camera metadata, user profiles, etc. However, this metadata is not well constrained and is often noisy, sparse, or missing altogether. In this paper, we propose a framework to model these "loosely organized" multimodal datasets, and show how to perform loosely-supervised learning using a novel latent Conditional Random Field framework. We learn parameters of the LCRF automatically from a small set of validation data, using Information Theoretic Metric Learning (ITML) to learn distance functions and a structural SVM formulation to learn the potential functions. We apply our framework on four datasets of images from Flickr, evaluating both qualitatively and quantitatively against several baselines.}, 
keywords={Web sites;learning (artificial intelligence);meta data;multimedia computing;random processes;support vector machines;Flickr;ITML;LCRF;distance function;information theoretic metric learning;latent conditional random field;loosely organized Web Images;loosely organized multimodal dataset;loosely supervised learning;multimodal learning;multimodal metadata collection;online image collection;photo sharing Website;potential function;structural SVM;Clustering algorithms;Equations;Global Positioning System;Mathematical model;Measurement;Training;Visualization;graphical models;multimodal image modeling;object recognition;semi-supervised learning}, 
doi={10.1109/CVPR.2014.316}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7801205, 
author={N. Gandhi and L. J. Armstrong and O. Petkar}, 
booktitle={2016 IEEE Technological Innovations in ICT for Agriculture and Rural Development (TIAR)}, 
title={Proposed decision support system (DSS) for Indian rice crop yield prediction}, 
year={2016}, 
pages={13-18}, 
abstract={Rice crop production provides more than 40% to overall crop production in India and is essential in ensuring food security. Its production is reliant on favorable climatic conditions. Improving the ability of farmers to predict crop productivity under different climatic scenarios, can assist farmers and other stakeholders in making important decisions in terms of agronomy and crop choice. This paper proposes a decision support system prototype for rice crop yield prediction for Maharashtra state, India. A Graphical User Interface (GUI) has been created in Java using NetBeans tool and Microsoft Office Access database for the ease of farmers and decision makers. The interface allows for the selection of the range of precipitation, minimum temperature, average temperature, maximum temperature and reference crop evapotranspiration and predicts the expected class of yield viz., low, moderate or high. The ranges of the parameters were calculated by using historic data from the study area. The classes for the yield were defined as low with 0.15 to 0.60 tonnes/hectare; moderate with 0.61 to 1.10 tonnes/hectare and high with 1.11 to 3.16 tonnes/hectare. The proposed prototype could be used for a bigger dataset and wider study area to predict the crop yield. This will provide a guide to the farmer to assist in decision making on potential crop yield for particular climatic scenario.}, 
keywords={Java;crops;decision making;decision support systems;graphical user interfaces;software tools;DSS;Indian rice crop yield prediction;Java;Maharashtra state;Microsoft Office Access database;NetBeans tool;agronomy;crop choice;crop productivity prediction;decision making;decision support system;food security;graphical user interface;reference crop evapotranspiration;Agriculture;Data visualization;Decision support systems;Graphical user interfaces;Predictive models;Production;Temperature distribution;DSS;crop yield prediction;decision support systems;graphical user interface;prototype}, 
doi={10.1109/TIAR.2016.7801205}, 
month={July},}
@INPROCEEDINGS{7403568, 
author={A. Soliman and L. Bahri and B. Carminati and E. Ferrari and S. Girdzijauskas}, 
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
title={DIVa: Decentralized identity validation for social networks}, 
year={2015}, 
pages={383-391}, 
abstract={Online Social Networks exploit a lightweight process to identify their users so as to facilitate their fast adoption. However, such convenience comes at the price of making legitimate users subject to different threats created by fake accounts. Therefore, there is a crucial need to empower users with tools helping them in assigning a level of trust to whomever they interact with. To cope with this issue, in this paper we introduce a novel model, DIVa, that leverages on mining techniques to find correlations among user profile attributes. These correlations are discovered not from user population as a whole, but from individual communities, where the correlations are more pronounced. DIVa exploits a decentralized learning approach and ensures privacy preservation as each node in the OSN independently processes its local data and is required to know only its direct neighbors. Extensive experiments using real-world OSN datasets show that DIVa is able to extract fine-grained community-aware correlations among profile attributes with average improvements up to 50% than the global approach.}, 
keywords={data mining;learning (artificial intelligence);social networking (online);DIVa;decentralized identity validation;decentralized learning approach;fine-grained community-aware correlation;mining technique;online social network;privacy preservation;real-world OSN dataset;user profile attribute;Association rules;Computational modeling;Correlation;Data privacy;Privacy;Security;Social network services;Community-aware Identity Validation;Decentralized Online Social Networks;Ensemble Learning;Privacy-preserving Learning}, 
doi={10.1145/2808797.2808861}, 
month={Aug},}
@INPROCEEDINGS{4570940, 
author={Kuo-Yang Cheng and Hsiao-Hsi Wang and Chia-Hsien Wen and Yaw-Ling Lin and Kuan-Ching Li and Cho-Li Wang}, 
booktitle={2008 First IEEE International Conference on Ubi-Media Computing}, 
title={Dynamic file replica location and selection strategy in data grids}, 
year={2008}, 
pages={484-489}, 
abstract={In this paper, we present the design of PU-DG optimizer toolbox (also known as PU-DG Optibox), which not only finds out the best strategy according to huge amount of simulation results but also proposes the min-max balancing workload method to upgrade the efficiency of execution in data grid environments. Data grid is one of key factors to build up large-scale dataset storage system and providing high performance computing capacity, by connecting scattered computing and storage resources located dispersedly in the grid. One major challenge in data grids is how to provide good and timely access to huge amount of data in distributed locations, given the high latency of interconnection networks. In this paper, we present the design framework of PU-DG Optibox for data grid environments. The proposed toolbox is a package containing a number of high-end techniques and methods running as middleware on top of data grid platforms, in order to optimize file downloads, by improving its efficiency and performance. The PU-DG Optibox provides users and developers possibilities for setting their own priority strategies. In addition, min-max balancing workload method is proposed to avoid that one computing node with lower network bandwidth to receive a job that has high complexity of job factor. Experimental results of techniques packaged in the proposed toolbox demonstrate its effectiveness.}, 
keywords={grid computing;middleware;minimax techniques;PU-DG Optibox;data grid selection strategy;dynamic file replica location;high-end techniques;interconnection networks;job factor complexity;large-scale dataset storage system;middlewares;min-max balancing workload method;optimizer toolbox;performance computing capacity;Computational modeling;Delay;Design optimization;Grid computing;High performance computing;Joining processes;Large-scale systems;Multiprocessor interconnection networks;Packaging;Scattering}, 
doi={10.1109/UMEDIA.2008.4570940}, 
month={July},}
@INPROCEEDINGS{845647, 
author={G. Holmes and L. Trigg}, 
booktitle={Neural Information Processing, 1999. Proceedings. ICONIP '99. 6th International Conference on}, 
title={A diagnostic tool for tree based supervised classification learning algorithms}, 
year={1999}, 
volume={2}, 
pages={514-519 vol.2}, 
abstract={The process of developing applications of machine learning and data mining that employ supervised classification algorithms includes the important step of knowledge verification. Interpretable output is presented to a user so that they can verify that the knowledge contained in the output makes sense for the given application. As the development of an application is an iterative process it is quite likely that a user would wish to compare models constructed at various times or stages. One crucial stage where comparison of models is important is when the accuracy of a model is being estimated, typically using some form of cross-validation. This stage is used to establish an estimate of how well a model will perform on unseen data. This is vital information to present to a user, but it is also important to show the degree of variation between models obtained from the entire dataset and models obtained during cross-validation. In this way it can be verified that the cross-validation models are at least structurally aligned with the model garnered from the entire dataset. This paper presents a diagnostic tool for the comparison of tree-based supervised classification models. The method is adapted from work on approximate tree matching and applied to decision trees. The tool is described together with experimental results on standard datasets}, 
keywords={data mining;decision trees;knowledge verification;learning (artificial intelligence);pattern classification;approximate tree matching;cross-validation;data mining;decision trees;diagnostic tool;interpretable output;iterative process;knowledge verification;machine learning;tree based supervised classification learning algorithms;Boosting;Classification algorithms;Classification tree analysis;Data mining;Decision trees;Error analysis;Iterative algorithms;Machine learning;Machine learning algorithms;Training data}, 
doi={10.1109/ICONIP.1999.845647}, 
month={},}
@ARTICLE{4579299, 
author={M. Yasukawa and M. Kitsuregawa and K. Taniguchi and T. Koike}, 
journal={IEEE Systems Journal}, 
title={PVES: Powered Visualizer for Earth Environmental Science}, 
year={2008}, 
volume={2}, 
number={3}, 
pages={390-400}, 
abstract={In this paper, we propose a powered visualizer for Earth environmental science (PVES) which can accommodate three-dimensional (3D) datasets. Though a data integration system called the information fusion reactor for Earth environmental science (IFRES) is being developed at the Institute of Industrial Science at the University of Tokyo, PVES is a part of the IFRES contribution to the global Earth observation system of systems (GEOSS). Three key functions are implemented. The first is a rather naive function that allows users to visualize 3D raw data through virtual reality modeling language. Second, the user can specify an arbitrary curve over the 3D dataset and then visualize its cross section. This has been proven to be very powerful for 3D analyses of flow phenomena. Third, users can easily specify various kinds of related data in IFRES to overlay on the cross section. This function also helps users to understand the flow phenomena deeply through the fusion of information, atmospheric infrared sounder (AIRS) data and its reanalysis data are provided as examples of applicable data in this paper; AIRS data is a satellite sensor product, and reanalysis data is a type of model outputs. We also present some observations extracted with the PVES and confirm effectiveness and usefulness of PVES.}, 
keywords={data visualisation;environmental science computing;sensor fusion;virtual reality languages;Information Fusion Reactor for Earth Environmental Science;PVES;atmospheric infrared sounder;data integration system;global Earth observation system of systems;powered visualizer for Earth environmental science;satellite sensor product;three-dimensional datasets;virtual reality modeling language;Information fusion;three-dimensional (3-D) data;virtual reality;visualization}, 
doi={10.1109/JSYST.2008.925980}, 
ISSN={1932-8184}, 
month={Sept},}
@INPROCEEDINGS{6726381, 
author={L. Wang and L. Yang}, 
booktitle={2013 IEEE 9th International Conference on Mobile Ad-hoc and Sensor Networks}, 
title={Depth Mapping Using the Hierarchical Reconstruction of Multiple Sequence}, 
year={2013}, 
pages={485-489}, 
abstract={One method was proposed for recovering structure model of the object by using monocular vision equipment. The fundamental idea can be summed up as sequential reconstruction under hierarchical method. First of all, by dividing the single observation path into multiple, more details of structure can be observed. After outliers removing, these sequences can be aligned with the referenced one which can be specified by users. Finally, after surface fitting, depth map of the object can be densely retrieved. The experiment shows that the depth map can be retrieved much densely than traditional method from the same dataset.}, 
keywords={image reconstruction;image sequences;surface fitting;depth mapping;hierarchical method;hierarchical reconstruction;monocular vision equipment;multiple sequence;outliers removing;sequential reconstruction;single observation path;surface fitting;Cameras;Conferences;Geometry;Image reconstruction;Reconstruction algorithms;Solid modeling;Surface reconstruction;depth mapping;monocular vision;outliers removing;sequences alignment}, 
doi={10.1109/MSN.2013.84}, 
month={Dec},}
@INPROCEEDINGS{1617434, 
author={Zhiyong Huang and C. S. Jensen and Hua Lu and Beng Chin Ooi}, 
booktitle={22nd International Conference on Data Engineering (ICDE'06)}, 
title={Skyline Queries Against Mobile Lightweight Devices in MANETs}, 
year={2006}, 
pages={66-66}, 
abstract={Skyline queries are well suited when retrieving data according to multiple criteria. While most previous work has assumed a centralized setting this paper considers skyline querying in a mobile and distributed setting, where each mobile device is capable of holding only a portion of the whole dataset; where devices communicate through mobile ad hoc networks; and where a query issued by a mobile user is interested only in the user’s local area, although a query generally involves data stored on many mobile devices due to the storage limitations. We present techniques that aim to reduce the costs of communication among mobile devices and reduce the execution time on each single mobile device. For the former, skyline query requests are forwarded among mobile devices in a deliberate way, such that the amount of data to be transferred is reduced. For the latter, specific optimization measures are proposed for resource-constrained mobile devices. We conduct extensive experiments to show that our proposal performs efficiently in real mobile devices and simulated wireless ad hoc networks.}, 
keywords={Computational modeling;Computer science;Costs;Information retrieval;Mobile ad hoc networks;Mobile communication;Mobile handsets;Proposals;Telephone sets;Wireless communication}, 
doi={10.1109/ICDE.2006.142}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{6183579, 
author={D. Penney and J. Chen and D. H. Laidlaw}, 
booktitle={2012 IEEE Pacific Visualization Symposium}, 
title={Effects of illumination, texture, and motion on task performance in 3D tensor-field streamtube visualizations}, 
year={2012}, 
pages={97-104}, 
abstract={We present results from a user study of task performance on streamtube visualizations, such as those used in three-dimensional (3D) vector and tensor field visualizations. This study used a tensor field sampled from a full-brain diffusion tensor magnetic resonance imaging (DTI) dataset. The independent variables include illumination model (global illumination and OpenGL-style local illumination), texture (with and without), motion (with and without), and task. The three spatial analysis tasks are: (1) a depth-judgment task: determining which of two marked tubes is closer to the user's viewpoint, (2) a visual-tracing task: marking the endpoint of a tube, and (3) a contact-judgment task: analyzing tube-sphere penetration. Our results indicate that global illumination did not improve task completion time for the tasks we measured. Global illumination reduced the errors in participants' answers over local OpenGLstyle rendering for the visual-tracing task only when motion was present. Motion contributed to spatial understanding for all tasks, but at the cost of longer task completion time. A high-frequency texture pattern led to longer task completion times and higher error rates. These results can help in the design of lighting model, such as flow or diffusion-tensor field visualizations and identify situations when the lighting is more efficient and accurate.}, 
keywords={data analysis;data visualisation;lighting;rendering (computer graphics);user interfaces;3D tensor-field streamtube visualization;3D vector field visualization;OpenGL-style local illumination;OpenGLstyle rendering;contact-judgment task;depth-judgment task;full-brain diffusion tensor magnetic resonance imaging;global illumination;high-frequency texture pattern;illumination effect;motion effect;task completion time;task performance;texture effect;tube endpoint marking;tube-sphere penetration analysis;user study;user viewpoint;visual-tracing task;Accuracy;Complexity theory;Electron tubes;Error analysis;Lighting;Rendering (computer graphics);Visualization}, 
doi={10.1109/PacificVis.2012.6183579}, 
ISSN={2165-8765}, 
month={Feb},}
@INPROCEEDINGS{7218596, 
author={W. Sun and X. Liu and W. Lou and Y. T. Hou and H. Li}, 
booktitle={2015 IEEE Conference on Computer Communications (INFOCOM)}, 
title={Catch you if you lie to me: Efficient verifiable conjunctive keyword search over large dynamic encrypted cloud data}, 
year={2015}, 
pages={2110-2118}, 
abstract={Encrypted data search allows cloud to offer fundamental information retrieval service to its users in a privacy-preserving way. In most existing schemes, search result is returned by a semi-trusted server and usually considered authentic. However, in practice, the server may malfunction or even be malicious itself. Therefore, users need a result verification mechanism to detect the potential misbehavior in this computation outsourcing model and rebuild their confidence in the whole search process. On the other hand, cloud typically hosts large outsourced data of users in its storage. The verification cost should be efficient enough for practical use, i.e., it only depends on the corresponding search operation, regardless of the file collection size. In this paper, we are among the first to investigate the efficient search result verification problem and propose an encrypted data search scheme that enables users to conduct secure conjunctive keyword search, update the outsourced file collection and verify the authenticity of the search result efficiently. The proposed verification mechanism is efficient and flexible, which can be either delegated to a public trusted authority (TA) or be executed privately by data users. We formally prove the universally composable (UC) security of our scheme. Experimental result shows its practical efficiency even with a large dataset.}, 
keywords={cloud computing;cryptography;trusted computing;computation outsourcing model;data users;dynamic encrypted cloud data;efficient verifiable conjunctive keyword search;encrypted data search scheme;file collection size;public trusted authority;result verification mechanism;semitrusted server;universally composable security;Conferences;Cryptography;Indexes;Keyword search;Polynomials;Servers}, 
doi={10.1109/INFOCOM.2015.7218596}, 
ISSN={0743-166X}, 
month={April},}
@ARTICLE{6727447, 
author={J. P. R. Caicedo and J. Verrelst and J. Muñoz-Marí and J. Moreno and G. Camps-Valls}, 
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
title={Toward a Semiautomatic Machine Learning Retrieval of Biophysical Parameters}, 
year={2014}, 
volume={7}, 
number={4}, 
pages={1249-1259}, 
abstract={Biophysical parameters such as leaf chlorophyll content (LCC) and leaf area index (LAI) are standard vegetation products that can be retrieved from Earth observation imagery. This paper introduces a new machine learning regression algorithms (MLRAs) toolbox into the scientific Automated Radiative Transfer Models Operator (ARTMO) software package. ARTMO facilitates retrieval of biophysical parameters from remote observations in a MATLAB graphical user interface (GUI) environment. The MLRA toolbox enables analyzing the predictive power of various MLRAs in a semiautomatic and systematic manner, and applying a selected MLRA to multispectral or hyperspectral imagery for mapping applications. It contains both linear and nonlinear state-of-the-art regression algorithms, in particular linear feature extraction via principal component regression (PCR), partial least squares regression (PLSR), decision trees (DTs), neural networks (NNs), kernel ridge regression (KRR), and Gaussian processes regression (GPR). The performance of multiple implemented regression strategies has been evaluated against the SPARC dataset (Barrax, Spain) and simulated Sentinel-2 (8 bands), CHRIS (62 bands) and HyMap (125 bands) observations. In general, nonlinear regression algorithms (NN, KRR, and GPR) outperformed linear techniques (PCR and PLSR) in terms of accuracy, bias, and robustness. Most robust results along gradients of training/validation partitioning and noise variance were obtained by KRR while GPR delivered most accurate estimations. We applied a GPR model to a hyperspectral HyMap flightline to map LCC and LAI. We exploited the associated uncertainty intervals to gain insight in the per-pixel performance of the model.}, 
keywords={Gaussian processes;decision trees;feature extraction;geophysical image processing;graphical user interfaces;hyperspectral imaging;learning (artificial intelligence);least squares approximations;mathematics computing;neural nets;principal component analysis;regression analysis;vegetation mapping;CHRIS observation;Gaussian processes regression;MATLAB graphical user interface;SPARC dataset;biophysical parameters;decision trees;hyperspectral HyMap flightline;hyperspectral imagery;kernel ridge regression;leaf area index;leaf chlorophyll content;linear feature extraction;machine learning regression algorithms;multispectral imagery;neural networks;noise variance;nonlinear regression algorithms;partial least squares regression;partitioning;principal component regression;scientific Automated Radiative Transfer Models Operator software package;semiautomatic machine learning retrieval;simulated Sentinel-2 observation;vegetation;Artificial neural networks;Biological system modeling;Graphical user interfaces;Ground penetrating radar;Kernel;Remote sensing;Training;Biophysical parameter retrieval;CHRIS;HyMap;Sentinel-2 (S2);graphical user interface (GUI) toolbox;leaf area index (LAI);leaf chlorophyll content (LCC);machine learning;nonparametric regression}, 
doi={10.1109/JSTARS.2014.2298752}, 
ISSN={1939-1404}, 
month={April},}
@INPROCEEDINGS{6649676, 
author={W. Qiu and Z. Zheng and X. Wang and X. Yang and M. R. Lyu}, 
booktitle={2013 IEEE International Conference on Services Computing}, 
title={Reputation-Aware QoS Value Prediction of Web Services}, 
year={2013}, 
pages={41-48}, 
abstract={QoS value prediction of Web services is an important research issue for service recommendation, selection and composition. Collaborative Filtering (CF) is one of the most widely used methods which employs QoS values contributed by similar users to make predictions. Therefore, historical QoS values contributed by different users can have great impacts on prediction results. However, existing Web service QoS value prediction approaches did not take data credibility into consideration, which may impact the prediction accuracy. To address this problem, we propose a reputation-aware QoS value prediction approach, which first calculates the reputation of each user based on their contributed values, and then takes advantage of reputation-based ranking to exclude the values contributed by untrustworthy users. CF QoS prediction approach is finally used to predict the missing QoS values based on the purified dataset. Experimental results show that our approach has higher prediction accuracy than other approaches.}, 
keywords={Web services;collaborative filtering;quality of service;CF;CF QoS prediction approach;Web services;collaborative filtering;data credibility;historical QoS values;prediction accuracy;quality of service;reputation-aware QoS value prediction;reputation-based ranking;service composition;service recommendation;service selection;Accuracy;Collaboration;Equations;Mathematical model;Prediction algorithms;Quality of service;Web services;QoS value prediction;Web service;collaborative filtering;reputation-based ranking}, 
doi={10.1109/SCC.2013.43}, 
month={June},}
@ARTICLE{7307182, 
author={L. Spyrou and Y. Blokland and J. Farquhar and J. Bruhn}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={Optimal Multitrial Prediction Combination and Subject-Specific Adaptation for Minimal Training Brain Switch Designs}, 
year={2016}, 
volume={24}, 
number={6}, 
pages={700-709}, 
abstract={Brain-Computer Interface (BCI) systems are traditionally designed by taking into account user-specific data to enable practical use. More recently, subject independent (SI) classification algorithms have been developed which bypass the subject specific adaptation and enable rapid use of the system. A brain switch is a particular BCI system where the system is required to distinguish from two separate mental tasks corresponding to the on-off commands of a switch. Such applications require a low false positive rate (FPR) while having an acceptable response time (RT) until the switch is activated. In this work, we develop a methodology that produces optimal brain switch behavior through subject specific (SS) adaptation of: a) a multitrial prediction combination model and b) an SI classification model. We propose a statistical model of combining classifier predictions that enables optimal FPR calibration through a short calibration session. We trained an SI classifier on a training synchronous dataset and tested our method on separate holdout synchronous and asynchronous brain switch experiments. Although our SI model obtained similar performance between training and holdout datasets, 86% and 85% for the synchronous and 69% and 66% for the asynchronous the between subject FPR and TPR variability was high (up to 62%). The short calibration session was then employed to alleviate that problem and provide decision thresholds that achieve when possible a target FPR=1% with good accuracy for both datasets.}, 
keywords={brain-computer interfaces;calibration;BCI system;FPR variability;SI classification model;TPR variability;brain-computer interface systems;false positive rate;holdout asynchronous brain switch experiments;holdout datasets;minimal training brain switch designs;optimal FPR calibration;optimal multitrial prediction;response time;statistical model;subject-independent classification algorithms;subject-specific adaptation;user-specific data;Adaptation models;Brain modeling;Calibration;Predictive models;Silicon;Switches;Training;Adaptive;brain switch;brain–computer interface (BCI);false positive rate;response time;subject independent}, 
doi={10.1109/TNSRE.2015.2494378}, 
ISSN={1534-4320}, 
month={June},}
@INPROCEEDINGS{7820426, 
author={B. Wang and J. Huang and H. Zheng and H. Wu}, 
booktitle={2016 12th International Conference on Computational Intelligence and Security (CIS)}, 
title={Semi-Supervised Recursive Autoencoders for Social Review Spam Detection}, 
year={2016}, 
pages={116-119}, 
abstract={As spam hampers the productivity and performance of social media and causes erosion in the user base and thus associated financial loss, a semi-supervised recursive autoencoders model is applied to social review spams detection problem in this paper. The model is based on semi supervised recursive autoencoders, which learns vector representations of phrases and full sentences as well as their hierarchical structure from the text. This model exploits hierarchical structure and uses compositional semantics to understand meanings, without requiring any language-specific lexica, parsers or knowledge base. Experiments conducted on real dataset show that the approach can effectively detect the social review spams.}, 
keywords={learning (artificial intelligence);social networking (online);text analysis;unsolicited e-mail;vectors;compositional semantics;financial loss;full sentences;hierarchical structure;phrases;semisupervised recursive autoencoders;social media;social review spam detection;vector representations;Feature extraction;Knowledge based systems;Pragmatics;Productivity;Semantics;Social network services;Training;Autoencoders;Review;Semi-supervised;Spam Detection}, 
doi={10.1109/CIS.2016.0035}, 
month={Dec},}
@INPROCEEDINGS{7312772, 
author={P. A. Legg}, 
booktitle={2015 IEEE Symposium on Visualization for Cyber Security (VizSec)}, 
title={Visualizing the insider threat: challenges and tools for identifying malicious user activity}, 
year={2015}, 
pages={1-7}, 
abstract={One of the greatest challenges for managing organisational cyber security is the threat that comes from those who operate within the organisation. With entitled access and knowledge of organisational processes, insiders who choose to attack have the potential to cause serious impact, such as financial loss, reputational damage, and in severe cases, could even threaten the existence of the organisation. Security analysts therefore require sophisticated tools that allow them to explore and identify user activity that could be indicative of an imminent threat to the organisation. In this work, we discuss the challenges associated with identifying insider threat activity, along with the tools that can help to combat this problem. We present a visual analytics approach that incorporates multiple views, including a user selection tool that indicates anomalous behaviour, an interactive Principal Component Analysis (iPCA) tool that aids the analyst to assess the reasoning behind the anomaly detection results, and an activity plot that visualizes user and role activity over time. We demonstrate our approach using the Carnegie Mellon University CERT Insider Threat Dataset to show how the visual analytics workflow supports the Information-Seeking mantra.}, 
keywords={principal component analysis;security of data;CERT insider threat dataset;Carnegie Mellon University;anomaly detection;iPCA tool;information-seeking mantra;malicious user activity;organisational cyber security;principal component analysis;security analyst;visual analytics workflow;Data visualization;Electronic mail;Feature extraction;Principal component analysis;Security;Visual analytics;Insider threat;behavioural analysis;model visualization}, 
doi={10.1109/VIZSEC.2015.7312772}, 
month={Oct},}
@INPROCEEDINGS{4673259, 
author={G. Liu and S. P. Huang and X. K. Zhu}, 
booktitle={2008 International Conference on Risk Management Engineering Management}, 
title={User Acceptance of Internet Banking in an Uncertain and Risky Environment}, 
year={2008}, 
pages={381-386}, 
abstract={Internet is expected to be especially beneficial to the banking industry. As more and more financial institutions are finding ways to utilize Internet technologies to launch online banking services, an important issue is to understand what factors will impact the decisions of customers in adopting such services. Based on the UTAUT model, the D&M IS success model, and the concept of trust, this paper presents an integrated model to investigate user acceptance of Internet banking in an uncertain and risky environment. Empirically, we analyze an extensive dataset, a sample of 413 respondents; and the results strongly support our model. A momentous conclusion is that user acceptance of Internet banking is highly influenced by perceived risk and perceived uncertainty.}, 
keywords={Internet;bank data processing;electronic money;risk management;D&M IS success model;Internet banking industry;Internet technology;customer decisions;financial institutions;risky environment;uncertain environment;Banking;Data analysis;Electronic mail;Environmental economics;Research and development management;Risk management;Space technology;Testing;Uncertainty;Web and internet services;Internet banking;perceived risk;perceived uncertainty;trust perception;user acceptance}, 
doi={10.1109/ICRMEM.2008.82}, 
month={Nov},}
@INPROCEEDINGS{6376735, 
author={X. Wu and L. L. Liang and W. J. Wang and Q. Peng}, 
booktitle={2012 International Conference on Audio, Language and Image Processing}, 
title={Principal object detection towards product image search}, 
year={2012}, 
pages={866-871}, 
abstract={Online shopping is an attractive, convenient, and efficient shopping way for billions of web users. A disappointing fact is that it is usually difficult for users to find the products fitting their needs purely based on text search. Content-based product image search becomes a promising way to solve this problem. However, the presence of natural backgrounds and fashion models significantly affect the feature matching, which makes product image search a challenging task. To clean the background and minimize the influence of noises, in this paper, a graph-based principal object detection algorithm is proposed to extract the product items while removing backgrounds and noises. A product image retrieval system is then constructed to verify the effectiveness of the proposed approach. Experiments on a large scale dataset with 1.36 million product images crawled from Taobao demonstrate the proposed approach significantly improves the retrieval performance.}, 
keywords={Internet;content-based retrieval;graph theory;image retrieval;object detection;retail data processing;Taobao;Web users;content-based product image search;crawled product images;graph-based principal object detection algorithm;online shopping;product image retrieval system;product image search;text search;Image color analysis;Image edge detection;Image retrieval;Image segmentation;Object detection;Search problems;Visualization}, 
doi={10.1109/ICALIP.2012.6376735}, 
month={July},}
@ARTICLE{7762039, 
author={L. Han and S. Luo and H. Wang and L. Pan and X. Ma and T. Zhang}, 
journal={IEEE Journal of Biomedical and Health Informatics}, 
title={An Intelligible Risk Stratification Model based on Pairwise and Size Constrained KMeans}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Having a system to stratify individuals according to risk is key to clinical disease prevention, this allows individuals identified at different risk tiers benefit from further investigation and intervention. But the same risk score estimated for two different persons doesn’t mean they need the same further investigation or represent the similarity health condition between two persons. Meanwhile, users still does not know a prior what most of the risk tiers are, and how many tiers should be found in risk stratification. In this paper, the proposed pairwise and size constrained Kmeans (PSCKmeans) method simultaneously integrates the limited supervised information and the size constraints to screen the high-risk population based on similarity measurement, and gets a feasible and balanced stratification solution to avoid cluster with few points. Results on CHNS public dataset and follow-up dataset show that proposed PSCKmeans method can naturally grade the risk of diabetes into four tiers, and achieve 73.8%, 85.1% and 0.95 sensitivity, specificity and RME on testing data. The proposed method compares favorably with 8 previous semi-supervised clustering methods, it demonstrates that semi-supervised clustering by unifying multiple forms of constraints can guide a good partition that is more relevant for the domain and find new categories through prior knowledge. Finally, This risk stratification model can provide a tool for risk stratification of clinical disease and be used for further intervention for people with similar health condition.}, 
keywords={Diabetes;Diseases;Informatics;Iris;Measurement;Statistics;Training;Pairwise Constraints;Risk Assessment;Semi-supervised Clustering;Size Constraints;Type 2 Diabetes}, 
doi={10.1109/JBHI.2016.2633403}, 
ISSN={2168-2194}, 
month={},}
@ARTICLE{7898399, 
author={Y. A. Chen and J. C. Wang and Y. H. Yang and H. H. Chen}, 
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
title={Component Tying for Mixture Model Adaptation in Personalization of Music Emotion Recognition}, 
year={2017}, 
volume={25}, 
number={7}, 
pages={1409-1420}, 
abstract={Personalizing a music emotion recognition model is needed because the perception of music emotion is highly subjective, but it is a time-consuming process. In this paper, we consider how to expedite the personalization process that begins with a general model trained offline using a general user base and progressively adapts the model to a music listener using the emotion annotations of the listener. Specifically, we focus on reducing the number of user annotations needed for the personalization. We investigate and evaluate four component tying methods: single group tying, quadrantwise tying, hierarchical tying, and random tying. These methods aim to exploit the available annotations by identifying related model parameters on-the-fly and updating them jointly. In the evaluation, we use the AMG1608 dataset, which contains the clip-level valence-arousal emotion ratings of 1608 30-s music clips annotated by 665 listeners. Also, we use the acoustic emotion Gaussians model as the general model that uses a mixture of Gaussian components to learn the mapping between the acoustic feature space and the emotion space. The results show that the model adaptation with component tying requires only 10–20 personal annotations to obtain the same level of prediction accuracy as the baseline model adaptation method that uses 50 personal annotations without component tying.}, 
keywords={Adaptation models;Emotion recognition;Load modeling;Multiple signal classification;Music;Predictive models;Component tying;mixture model;model adaptation;music emotion recognition;personalization}, 
doi={10.1109/TASLP.2017.2693565}, 
ISSN={2329-9290}, 
month={July},}
@INPROCEEDINGS{6735341, 
author={R. Interdonato and S. Romeo and A. Tagarelli and G. Karypis}, 
booktitle={2013 IEEE 25th International Conference on Tools with Artificial Intelligence}, 
title={A Versatile Graph-Based Approach to Package Recommendation}, 
year={2013}, 
pages={857-864}, 
abstract={An emerging trend in research on recommender systems is the design of methods capable of recommending packages instead of single items. The problem is challenging due to a variety of critical aspects, including context-based and user-provided constraints for the items constituting a package, but also the high sparsity and limited accessibility of the primary data used to solve the problem. Most existing works on the topic have focused on a specific application domain (e.g., travel package recommendation), thus often providing ad-hoc solutions that cannot be adapted to other domains. By contrast, in this paper we propose a versatile package recommendation approach that is substantially independent of the peculiarities of a particular application domain. A key aspect in our framework is the exploitation of prior knowledge on the content type models of the packages being generated that express what the users expect from the recommendation task. Packages are learned for each package model, while the recommendation stage is accomplished by performing a PageRank-style method personalized w.r.t. the target user's preferences, possibly including a limited budget. Our developed method has been tested on a TripAdvisor dataset and compared with a recently proposed method for learning composite recommendations.}, 
keywords={graph theory;recommender systems;PageRank-style method;TripAdvisor dataset;composite recommendation;context-based constraints;package model;primary data;recommendation stage;recommendation task;recommender systems;recommending packages;travel package recommendation;user-provided constraints;versatile graph-based approach;versatile package recommendation;Collaboration;Computational modeling;Context modeling;Damping;Recommender systems;Smoothing methods;Vectors}, 
doi={10.1109/ICTAI.2013.130}, 
ISSN={1082-3409}, 
month={Nov},}
@INPROCEEDINGS{5982707, 
author={Y. K. Huang and L. F. Lin}, 
booktitle={2011 7th International Wireless Communications and Mobile Computing Conference}, 
title={Continuous within query in road networks}, 
year={2011}, 
pages={1176-1181}, 
abstract={In recent years, the research community introduced various methods for processing spatio-temporal queries. Continuous Within query (and CWQ) is an important type of spatio-temporal queries with many real applications. A CWQ can be used to find the moving objects whose distances to the moving query object are less than or equal to a user-given distance dε at each time instant. In this paper, we study how to efficiently process the CWQ in road networks, where the criterion for determining the CWQ result is the road distance between objects. We propose an efficient algorithm, namely the continuous within query (CWQ) algorithm, combined with a road distance model to answer the CWQ. Extensive experiments using real road network dataset demonstrate the effectiveness and the efficiency of the proposed algorithm.}, 
keywords={image motion analysis;query processing;road traffic;traffic information systems;CWQ;continuous within query algorithm;moving query object;real road network dataset;research community;road distance model;spatio-temporal query;Data models;Data structures;Joining processes;Mobile communication;Neodymium;Roads;Spatial databases;Continuous Within query;road distance;road networks;spatio-temporal queries}, 
doi={10.1109/IWCMC.2011.5982707}, 
ISSN={2376-6492}, 
month={July},}
@INPROCEEDINGS{4635899, 
author={Tao Zhu and W. W. Y. Ng and J. W. T. Lee and Bin-Bin Sun and J. Wang and D. S. Yeung}, 
booktitle={2008 International Conference on Wavelet Analysis and Pattern Recognition}, 
title={L-gem based co-training for CBIR with relevance feedback}, 
year={2008}, 
volume={2}, 
pages={873-879}, 
abstract={Relevance feedback has been developed for several years and becomes an effective method for capturing userpsilas concepts to improve the performance of content-based image retrieval (CBIR). In contrast to fully labeled training dataset in supervised learning, semi-supervised learning and active learning deal with training dataset with only a small portion of labeled samples. This is more realistic because one could easily find thousands of unlabeled images from the Internet. How to make use of such unlabeled resources on the Internet is an important research topic. Co-training method is to expand the number of labeled samples in semi-supervised learning by swapping training samples between two classifiers. In this work, we propose to apply the localized generalization error model (L-GEM) to co-training. Two radial basis function neural networks (RBFNN) with different features split is adopted in the co-training and the unlabeled samples with lowest L-GEM value is added to the co-training in next iteration. In the CBIR system, we output those positive images with lowest L-GEM value as the highest confident image and output those images with highest L-GEM to ask for user labeling. Higher the L-GEM value of a sample is, the less confident is the classifier to predict its image class. Experimental results show that the proposed method could effectively improve the image retrieval results.}, 
keywords={content-based retrieval;image retrieval;learning (artificial intelligence);radial basis function networks;relevance feedback;CBIR;Internet;L-GEM;co-training method;content-based image retrieval;localized generalization error model;radial basis function neural networks;relevance feedback;semi-supervised learning;Content based retrieval;Feedback;Image analysis;Image retrieval;Internet;Labeling;Pattern analysis;Performance analysis;Semisupervised learning;Wavelet analysis;Co-Training;Content-Based Image Retrieval (CBIR);Localized Generalization Error Model (L-GEM);Radial-basis Function Neural Networks (RBFNN)}, 
doi={10.1109/ICWAPR.2008.4635899}, 
ISSN={2158-5695}, 
month={Aug},}
@INPROCEEDINGS{4258717, 
author={S. Appavu and R. Rajaram}, 
booktitle={2007 IEEE Intelligence and Security Informatics}, 
title={Association Rule Mining for Suspicious Email Detection: A Data Mining Approach}, 
year={2007}, 
pages={316-323}, 
abstract={Email has been an efficient and popular communication mechanism as the number of Internet user's increase. In many security informatics applications it is important to detect deceptive communication in email. This paper proposes to apply Association Rule Mining for Suspected Email Detection. (Emails about Criminal activities).Deception theory suggests that deceptive writing is characterized by reduced frequency of first person pronouns and exclusive words and elevated frequency of negative emotion words and action verbs . We apply this model of deception to the set of Email dataset, then applied Apriori algorithm to generate the rules The rules generated are used to test the email as deceptive or not. In particular we are interested in detecting emails about criminal activities. After classification we must be able to differentiate the emails giving information about past criminal activities(Informative email) and those acting as alerts(warnings) for the future criminal activities. This differentiation is done using the features considering the tense used in the emails. Experimental results show that simple Associative classifier provides promising detection rates.}, 
keywords={Internet;classification;computer crime;data mining;electronic mail;Apriori algorithm;Internet;association rule mining;classification;criminal activity;data mining;deception theory;deceptive writing;suspicious email detection;Association rules;Character generation;Data mining;Data security;Educational institutions;Electronic mail;Frequency;Informatics;Internet;Testing;Apriori algorithm;Association Rule Mining;Data Mining;Deceptive Theory;Tense}, 
doi={10.1109/ISI.2007.379491}, 
month={May},}
@INPROCEEDINGS{4535823, 
author={Q. Lu and S. Luo}, 
booktitle={2008 2nd International Conference on Bioinformatics and Biomedical Engineering}, 
title={Primary Research of Digital Atlas of Human Anatomy on Virtual Reality}, 
year={2008}, 
pages={2442-2445}, 
abstract={Immersion, interaction and imagination are the core of virtual reality, and solving the problem of the interaction between the user and the operating interface is very difficult. Using Virtual Chinese Human (VCH) dataset and virtual reality technology in this paper, we present a method to build a three- dimensional digital atlas of human anatomy based on VirtoolsDev3.5. The interfaces for skeleton, cardiovascular system and alimentary system are implemented, in the aspects of anatomy labeling, section analysis, and cardiovascular circulation. Through these interfaces, users are able to use mouse and keyboard to interact with the system. All digital models are based on the original data set of the first female virtual human Section Image Set. The animation of heart function evaluation, circulation, and electrocardiogram is designed to display the position and movement of dynamic process of human heart. A webpage version of the atlas is also available for data sharing.}, 
keywords={biomechanics;cardiovascular system;computer animation;electrocardiography;haemodynamics;medical computing;user interfaces;virtual reality;VirtoolsDev3.5;Virtual Chinese Human dataset;alimentary system;anatomy labeling aspects;cardiovascular circulation;cardiovascular system;digital models;electrocardiogram;heart function evaluation;human anatomy;human heart dynamic process;skeleton;three-dimensional digital atlas research;virtual reality technology;Animation;Cardiology;Cardiovascular system;Heart;Human anatomy;Keyboards;Labeling;Mice;Skeleton;Virtual reality}, 
doi={10.1109/ICBBE.2008.943}, 
ISSN={2151-7614}, 
month={May},}
@INPROCEEDINGS{6424424, 
author={A. Shivram and C. Ramaiah and U. Porwal and V. Govindaraju}, 
booktitle={2012 International Conference on Frontiers in Handwriting Recognition}, 
title={Modeling Writing Styles for Online Writer Identification: A Hierarchical Bayesian Approach}, 
year={2012}, 
pages={387-392}, 
abstract={With the explosive growth of the tablet form factor and greater availability of pen-based direct input, writer identification in online environments is increasingly becoming critical for a variety of downstream applications such as intelligent and adaptive user environments, search, retrieval, indexing and digital forensics. Extant research has approached writer identification by using writing styles as a discriminative function between writers. In contrast, we model writing styles as a shared component of an individualâs handwriting. We develop a theoretical framework for this conceptualization and model this using a three level hierarchical Bayesian model (Latent Dirichlet Allocation). In this text-independent, unsupervised model each writerâs handwriting is modeled as a distribution over finite writing styles that are shared amongst writers. We test our model on a novel online/offline handwriting dataset IBM UB 1 which is being made available to the public. Our experiments show comparable results to current benchmarks and demonstrate the efficacy of explicitly modeling shared writing styles.}, 
keywords={Bayes methods;document image processing;handwriting recognition;text analysis;unsupervised learning;adaptive user environments;digital forensics;finite writing styles;hierarchical Bayesian approach;indexing;information retrieval;information searching;intelligent user environments;latent Dirichlet allocation;offline handwriting dataset IBM UB 1;online environments;online handwriting dataset IBM UB 1;online writer identification;pen-based direct input;shared writing styles;tablet form factor;text-independent model;unsupervised model;writer handwriting;Adaptation models;Bayesian methods;Data models;Resource management;Support vector machines;Vocabulary;Writing}, 
doi={10.1109/ICFHR.2012.235}, 
month={Sept},}
@INPROCEEDINGS{4408865, 
author={J. Wu and A. Osuntogun and T. Choudhury and M. Philipose and J. M. Rehg}, 
booktitle={2007 IEEE 11th International Conference on Computer Vision}, 
title={A Scalable Approach to Activity Recognition based on Object Use}, 
year={2007}, 
pages={1-8}, 
abstract={We propose an approach to activity recognition based on detecting and analyzing the sequence of objects that are being manipulated by the user. In domains such as cooking, where many activities involve similar actions, object-use information can be a valuable cue. In order for this approach to scale to many activities and objects, however, it is necessary to minimize the amount of human-labeled data that is required for modeling. We describe a method for automatically acquiring object models from video without any explicit human supervision. Our approach leverages sparse and noisy readings from RFID tagged objects, along with common-sense knowledge about which objects are likely to be used during a given activity, to bootstrap the learning process. We present a dynamic Bayesian network model which combines RFID and video data to jointly infer the most likely activity and object labels. We demonstrate that our approach can achieve activity recognition rates of more than 80% on a real-world dataset consisting of 16 household activities involving 33 objects with significant background clutter. We show that the combination of visual object recognition with RFID data is significantly more effective than the RFID sensor alone. Our work demonstrates that it is possible to automatically learn object models from video of household activities and employ these models for activity recognition, without requiring any explicit human labeling.}, 
keywords={belief networks;image sequences;learning (artificial intelligence);object detection;object recognition;radiofrequency identification;RFID tagged object;activity recognition;dynamic Bayesian network model;object detection;object model learning;object-use information;visual object recognition;Bayesian methods;Character recognition;Data mining;Educational institutions;Humans;Information resources;Object detection;Object recognition;RFID tags;Radiofrequency identification}, 
doi={10.1109/ICCV.2007.4408865}, 
ISSN={1550-5499}, 
month={Oct},}
@INPROCEEDINGS{6633612, 
author={A. Canossa and J. B. Martinez and J. Togelius}, 
booktitle={2013 IEEE Conference on Computational Inteligence in Games (CIG)}, 
title={Give me a reason to dig Minecraft and psychology of motivation}, 
year={2013}, 
pages={1-8}, 
abstract={Recently both game industry professionals and academic researchers have started focusing on player-generated behavioral data as a mean to gather insights on player psychology through datamining. Although some research has already proven solid correlations between in-game behavior and personality, most techniques focus on extracting knowledge from in-game behavior data alone. This paper posits that triangulating exclusively behavioral datasets with established theoretical frameworks serving as hermeneutic grids, may help extracting additional meaning and information. The hermeneutic grid selected for this study is the Reiss Motivation Profiler and it is applied to behavioral data gathered from Minecraft players.}, 
keywords={behavioural sciences computing;computer games;data mining;human factors;psychology;Minecraft players;Reiss Motivation Profiler;academic researchers;behavioral dataset;data mining;game industry professionals;hermeneutic grid;in-game behavior data;knowledge extraction;motivation psychology;personality;player psychology;player-generated behavioral data;Correlation;Data models;Educational institutions;Games;Materials;Psychology;Sociology}, 
doi={10.1109/CIG.2013.6633612}, 
ISSN={2325-4270}, 
month={Aug},}
@INPROCEEDINGS{6375215, 
author={S. Sonawane and S. Karsoliya and P. Saurabh and B. Verma}, 
booktitle={2012 Fourth International Conference on Computational Intelligence and Communication Networks}, 
title={Self Configuring Intrusion Detection System}, 
year={2012}, 
pages={757-761}, 
abstract={With the rapid expansion of computer networks during the past few years, security has become a crucial issue for modern computer systems. A good way to identify malicious use is through monitoring unusual user activity. To identify these malicious activities various data-mining and machine learning techniques have been deployed for intrusion detection. The manual tuning process required by current systems depends on the system operators in working out the tuning solution and in integrating it into the detection model. This paper proposes Self Configuring Intrusion Detection System (SCIDS) to make tuning automatically. The key idea is to use the binary SLIPPER as a basic module, which is a rule learner based on confidence-rated boosting. This system is evaluated using the NSL KDD intrusion detection dataset. An experimental result shows the SCIDS system with SLIPPER algorithm gives better performance in terms of detection rate, false alarm rate, total misclassification cost and cost per example on NSL-KDD dataset than that of on KDD.}, 
keywords={data mining;learning (artificial intelligence);pattern classification;security of data;NSL KDD intrusion detection dataset;SCIDS system;SLIPPER algorithm;automatic tuning;binary SLIPPER;computer network;confidence-rated boosting;data mining technique;detection rate;false alarm rate;machine learning technique;malicious activity;malicious use identification;misclassification cost;modern computer system;rule learning;self-configuring intrusion detection system;tuning process;unusual user activity monitoring;Classification algorithms;Data models;Intrusion detection;Prediction algorithms;Training;Tuning;Confidence value;Intrusion;anomaly detection;attacks;false prediction;misuse detection;tuning}, 
doi={10.1109/CICN.2012.181}, 
month={Nov},}
@INPROCEEDINGS{6460981, 
author={J. Peng and J. Wang and D. Kong}, 
booktitle={Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)}, 
title={A new convex variational model for liver segmentation}, 
year={2012}, 
pages={3754-3757}, 
abstract={Due to intensity overlapping, blurred edges and complex backgrounds with clutter features, liver segmentation is still a challenging task. In this paper, we address it with a constrained convex variational model, which can definitely avoid leakage through anatomical knowledge from users. A novel heuristic intensity model is proposed to suppress irrelevant strong edges and constrain the segmentation. Both global and local region appearance information are integrated to model higher level features such as local context. As a result, weak liver boundaries and fine structures can be stably delineated according to the information from neighborhood and nearby layers. No precise prior segmentation is needed and few seeds without shape restriction, about three seeds, are adequate to capture fine structures. The initialization is also very easy. Moreover, an accelerated primal-dual algorithm is proposed to efficiently and globally optimize the model. Our method is validated on MICCAI dataset and produces a high score of 80.6. It can be used to segment other abdominal organs.}, 
keywords={convex programming;edge detection;feature extraction;image segmentation;liver;medical image processing;MICCAI dataset;abdominal organs;anatomical knowledge;blurred edges;clutter features;complex backgrounds;constrained convex variational model;global region appearance information;heuristic intensity model;higher level features;intensity overlapping;irrelevant strong edges;liver boundaries;liver segmentation;local region appearance;local region appearance information;primal-dual algorithm;shape restriction;Computational modeling;Computed tomography;Image edge detection;Image segmentation;Liver;Mathematical model;Shape}, 
ISSN={1051-4651}, 
month={Nov},}
@ARTICLE{6798763, 
author={E. Ramasso and R. Gouriveau}, 
journal={IEEE Transactions on Reliability}, 
title={Remaining Useful Life Estimation by Classification of Predictions Based on a Neuro-Fuzzy System and Theory of Belief Functions}, 
year={2014}, 
volume={63}, 
number={2}, 
pages={555-566}, 
abstract={Various approaches for prognostics have been developed, and data-driven methods are increasingly applied. The training step of these methods generally requires huge datasets to build a model of the degradation signal, and estimate the limit under which the degradation signal should stay. Applicability and accuracy of these methods are thereby closely related to the amount of available data, and even sometimes requires the user to make assumptions on the dynamics of health states evolution. Following that, the aim of this paper is to propose a method for prognostics and remaining useful life estimation that starts from scratch, without any prior knowledge. Assuming that remaining useful life can be seen as the time between the current time and the instant where the degradation is above an acceptable limit, the proposition is based on a classification of prediction strategy (CPS) that relies on two factors. First, it relies on the use of an evolving real-time neuro-fuzzy system that forecasts observations in time. Secondly, it relies on the use of an evidential Markovian classifier based on Dempster-Shafer theory that enables classifying observations into the possible functioning modes. This approach has the advantage to cope with a lack of data using an evolving system, and theory of belief functions. Also, one of the main assets is the possibility to train the prognostic system without setting any threshold. The whole proposition is illustrated and assessed by using the CMAPPS turbofan dataset. RUL estimates are shown to be very close to actual values, and the approach appears to accurately estimate the failure instants, even with few learning data.}, 
keywords={Markov processes;fuzzy neural nets;inference mechanisms;pattern classification;remaining life assessment;CMAPPS turbofan dataset;CPS;Dempster-Shafer theory;RUL estimates;belief functions;classification;data-driven methods;degradation signal;evidential Markovian classifier;learning data;neuro-fuzzy system;prediction strategy;prognostic system;remaining useful life estimation;Data models;Degradation;Estimation;Hidden Markov models;Prediction algorithms;Predictive models;Training;Belief functions;Takagi-Sugeno systems;classification of prediction;prognostics}, 
doi={10.1109/TR.2014.2315912}, 
ISSN={0018-9529}, 
month={June},}
@INPROCEEDINGS{6117125, 
author={S. Uppoor and M. Fiore}, 
booktitle={2011 IEEE Vehicular Networking Conference (VNC)}, 
title={Large-scale urban vehicular mobility for networking research}, 
year={2011}, 
pages={62-69}, 
abstract={Simulation is the tool of choice for the large-scale performance evaluation of upcoming telecommunication networking paradigms that involve users aboard vehicles, such as next-generation cellular networks for vehicular access, pure vehicular ad hoc networks, and opportunistic disruption-tolerant networks. The single most distinguishing feature of vehicular networks simulation lies in the mobility of users, which is the result of the interaction of complex macroscopic and microscopic dynamics. Notwithstanding the improvements that vehicular mobility modeling has undergone during the past few years, no car traffic trace is available today that captures both macroscopic and microscopic behaviors of drivers over a large urban region, and does so with the level of detail required for networking research. In this paper, we present a realistic synthetic dataset of the car traffic over a typical 24 hours in a 400-km2 region around the city of Köln, in Germany. We outline how our mobility description improves today's existing traces and show the potential impact that a comprehensive representation of vehicular mobility can have one the evaluation of networking technologies.}, 
keywords={cellular radio;mobility management (mobile radio);next generation networks;road traffic;vehicular ad hoc networks;car traffic trace;networking research;next-generation cellular networks;opportunistic disruption-tolerant networks;pure vehicular ad hoc networks;telecommunication networking paradigms;time 24 hour;urban vehicular mobility;vehicular access;vehicular mobility modeling;Cities and towns;Layout;Microscopy;Roads;Topology;Urban areas;Vehicles}, 
doi={10.1109/VNC.2011.6117125}, 
ISSN={2157-9857}, 
month={Nov},}
@INPROCEEDINGS{6976495, 
author={Xiaotian Jin and Defeng Guo and Hongjian Liu}, 
booktitle={2014 IEEE Workshop on Advanced Research and Technology in Industry Applications (WARTIA)}, 
title={Enhanced stock prediction using social network and statistical model}, 
year={2014}, 
pages={1199-1203}, 
abstract={As a fundamental part of capital market, stock investment has great significance in optimizing capital allocation, funding as well as increasing the value of assets. To predict and evaluate the stock price also has important practical significance for investors due to the high income and high-risk characteristics of stock investments. However, the stock price is usually random fluctuation which affecting by speculative factors, thus make it is almost impossible to make accurate prediction. Considering conformist mentality is essential for accurately predict stock price. Recently, with expansive development of social networks (e.g. Facebook, Tweet or Sina weibo, etc.), huge users publish and exchange a variety of financial-related information with other users. Therefore, social network is an important approach to comprehend conformist mentality. In view of the above analysis, this paper proposed a new stock prediction method based on social networks and regression model, and we also used real dataset of NASDAQ market and Twitter to verify the proposed model. Experimental results show that the proposed method can accurately predict stock price.}, 
keywords={data analysis;investment;pricing;regression analysis;social networking (online);stock markets;Facebook;NASDAQ market;Sina weibo;Tweet;Twitter;capital allocation optimization;capital market;conformist mentality;financial-related information;random fluctuation;real dataset;regression model;social networks;speculative factors;statistical model;stock investment;stock prediction enhancement;stock price;Analytical models;Autoregressive processes;Hidden Markov models;Indexes;Predictive models;Social network services;Support vector machines;NASDAQ;Twitter;regression model;social network;stock prediction}, 
doi={10.1109/WARTIA.2014.6976495}, 
month={Sept},}
@INPROCEEDINGS{7383074, 
author={X. Guo and H. Gao and H. Wang}, 
booktitle={2015 10th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)}, 
title={Image Clustering Based on the Human Intelligence}, 
year={2015}, 
pages={366-373}, 
abstract={Current clustering algorithms mainly base on calculating distance between items that provides similarity information. But the distance cannot reflect all the correct information between items, which may lead to significant errors. We study the problem of seeking pairwise constraints with crowdsourcing in order to improve clustering results. Crowdsourcing is an emerging and powerful paradigm, which enables the use of background knowledge collecting from users, and image clustering is a relevant and appropriate use case. We propose a framework bringing in human intelligence during the clustering process. The key point of the framework is to choose best questions to perform on the crowdsourcing platform, gather pairwise constraints, and melt the existing algorithm and human input together. As the computation is extensive, we also provide some heuristic optimal methods, including natural transitive relations, to reduce the number of HITs of asking people. We evaluate the framework on real image dataset. The experiment result demonstrates the algorithm achieves a fairly good performance comparing to the other state-of-theart methods, and the optimized strategies significantly reduce the number of HIT.}, 
keywords={image classification;learning (artificial intelligence);optimisation;outsourcing;pattern clustering;HIT;crowdsourcing platform;heuristic optimal method;human intelligence;image clustering;machine learning;pairwise constraint;Clustering algorithms;Clustering methods;Crowdsourcing;Data mining;Data models;Training;Crowdsourcing;Image Clustering;Pairwise Constraints;Transitivity}, 
doi={10.1109/ISKE.2015.39}, 
month={Nov},}
@INPROCEEDINGS{7314643, 
author={H. Dong and J. Wang and H. Lin and B. Xu and Z. Yang}, 
booktitle={2015 Ninth International Conference on Frontier of Computer Science and Technology}, 
title={Predicting Best Answerers for New Questions: An Approach Leveraging Distributed Representations of Words in Community Question Answering}, 
year={2015}, 
pages={13-18}, 
abstract={Community Question Answering (CQA) sites are becoming an increasingly important source of information where users can share knowledge on various topics. Although these sites provide opportunities for users to seek for help or provide answers, they also bring new challenges. One of the challenges is most new questions posted everyday cannot be routed to the appropriate users who can answer them in CQA. That is to say, experts cannot receive questions that match their expertise. Therefore new questions cannot be answered in time. In this paper, we propose an approach which based on distributed representations of words to predict the best answerer for a new question on CQA sites. Our approach considers both user activity and user authority. The user activity and user authority are based on the previous questions answered by the user. We have applied our model on the dataset downloaded from StackOverflow, one of the biggest CQA sites. The results show that our approach performs better than the TF-IDF and Language Model based methods.}, 
keywords={question answering (information retrieval);CQA sites;StackOverflow;best answerer prediction;community question answering sites;distributed representations;distributed word representations;user activity;user authority;Computational modeling;History;Information retrieval;Knowledge discovery;Measurement;Natural language processing;Semantics;CQA;activity;authority;distributed representations of words}, 
doi={10.1109/FCST.2015.56}, 
ISSN={2159-6301}, 
month={Aug},}
@INPROCEEDINGS{6890313, 
author={X. Nie and W. Feng and L. Wan and H. Dai and C. M. Pun}, 
booktitle={2014 IEEE International Conference on Multimedia and Expo (ICME)}, 
title={Intrinsic image decomposition by hierarchical L0 sparsity}, 
year={2014}, 
pages={1-6}, 
abstract={This paper presents a hierarchical approach to single image intrinsic decomposition based on non-local L0 sparsity. In contrast to previous studies using heuristic methods to well-define the ill-posed problem, our approach is able to effectively construct sparse, non-local and multiscale reflectance dependencies in an unsupervised manner, thus is less dependent on the chromaticity feature and more accurately captures the global reflectance correlations. Besides, we impose homogenous smoothness prior and scale constraint in our model to further improve the decomposition accuracy. We formulate the decomposition as a quadratic minimization problem, which can be efficiently solved in closed form. Extensive experiments show that our approach can successfully extract the shading and reflectance components from a single image, and outperforms state-of-the-art methods on benchmark dataset. Besides, our approach can achieve comparable results with user-assisted methods on natural scenes.}, 
keywords={image processing;minimisation;chromaticity feature;decomposition accuracy;global reflectance correlations;hierarchical L0 sparsity;ill-posed problem;intrinsic image decomposition;multiscale reflectance dependencies;nonlocal L0 sparsity;quadratic minimization problem;reflectance components;shading components;unsupervised manner;Benchmark testing;Dictionaries;Educational institutions;Image decomposition;Minimization;Sparse matrices;Vectors;Intrinsic image decomposition;L0 sparsity;hierarchical approach;non-local prior}, 
doi={10.1109/ICME.2014.6890313}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{7856498, 
author={M. T. Uddin and M. J. A. Patwary and T. Ahsan and M. S. Alam}, 
booktitle={2016 International Conference on Innovations in Science, Engineering and Technology (ICISET)}, 
title={Predicting the popularity of online news from content metadata}, 
year={2016}, 
pages={1-5}, 
abstract={Popularity prediction of online news aims to predict the future popularity of news article prior to its publication estimating the number of shares, likes, and comments. Yet, popularity prediction is a challenging task due to various issues including difficulty to measure the quality of content and relevance of content to users; prediction difficulty of complex online interactions and information cascades; inaccessibility of context outside the web; local and geographic conditions; social network properties. This paper focuses on popularity prediction of online news by predicting whether users share an article or not, and how many users share the news adopting before publication approach. This paper proposes the gradient boosting machine for popularity prediction using features that are known before publication of articles. The proposed model shows around 1.8% improvement over previously applied techniques on a benchmark dataset. This model also indicates that features extracted from articles keywords, publication day, and the data channel are highly influential for popularity prediction.}, 
keywords={data mining;electronic publishing;learning (artificial intelligence);meta data;social networking (online);text analysis;article keywords;article publication;benchmark dataset;complex online interactions;content metadata;content quality measurement;content relevance measurement;data channel;geographic conditions;gradient boosting machine;information cascades;local conditions;online news popularity prediction;publication day;social network properties;Computational modeling;Decision trees;Feature extraction;Metadata;Prediction algorithms;Predictive models;Social network services;Before Publication Approach;Machine Learning;Popularity Prediction;Social Media Contents;Text Mining}, 
doi={10.1109/ICISET.2016.7856498}, 
month={Oct},}
@INPROCEEDINGS{6930212, 
author={X. y. Tian and G. Yu and P. y. Li}, 
booktitle={2014 International Conference on Management Science Engineering 21th Annual Conference Proceedings}, 
title={Spammer detection on Sina Micro-Blog}, 
year={2014}, 
pages={82-87}, 
abstract={The popularity of Sina Micro-Blog has made it a great target for spammers. The spreading spammers reduce the political, economic and social value of the Sina Micro-Blog and leads to a bad user experience, which attracts many researchers' attention. Here we introduce the background of the spammer detection and present a study of spammer detection on Micro-Blog. We collected a dataset that includes 1,900 users and more than 340,000 tweets. And different from previous studies which mainly focus on shallow machine learning algorithms, we also used the deep learning model to build a classifier which can be used to discriminate spammers from legitimate users. However, different from deep learning's good performance in other areas, the experimental results suggest that the deep learning model can't learn more information from the user profiles and tweets than the Support Vector Machine although it outperform the Naïve Bayes. The experimental results suggests that we can build a classifier which can correctly discriminate most of spammers from the normal ones, and the deep learning model is not necessarily better than the shallow ones.}, 
keywords={learning (artificial intelligence);pattern classification;security of data;social networking (online);support vector machines;unsolicited e-mail;Sina microblog;classifier;deep learning model;naive Bayes algorithm;shallow machine learning algorithms;spammer detection;spreading spammers;support vector machine;tweets;user experience;user profiles;Accuracy;Feature extraction;Kernel;Machine learning algorithms;Support vector machines;Twitter;deep learning;micro-blog;social network;spammer detection}, 
doi={10.1109/ICMSE.2014.6930212}, 
ISSN={2155-1847}, 
month={Aug},}
@INPROCEEDINGS{7396606, 
author={L. Yu and Y. Zhifan and P. Nie and X. Zhao and Y. Zhang}, 
booktitle={2015 12th Web Information System and Application Conference (WISA)}, 
title={Multi-source Emotion Tagging for Online News}, 
year={2015}, 
pages={49-52}, 
abstract={With the rapid growth of social media and online news services, users nowadays can respond to online news by rating subjective emotions such as happiness, surprise or anger actively. Once the user ratings is over a certain range, it begins to show up a tendency of what most people think and feel, which can help us understand the preferences and perspectives of most users, and help news providers to provide users with more positive news. Thus it has become a pregnant research problem to tag emotion automatically. This paper tackles the task of emotion tagging for online news with multi-source including news article and comment, as emotion is not only tagged after reading news article, but also can be incorporated in comment with what they feel. In this paper, a novel classification model are proposed with two layer logistic regression. The new approach get outputs from basic classifiers and combine them in a new classifier, making a more accurate prediction when compared with a single source method. An extensive set of experimental results on a real dataset from a popular online news service demonstrate the effectiveness of the proposed approach.}, 
keywords={electronic publishing;pattern classification;regression analysis;sentiment analysis;social networking (online);classification model;multisource emotion tagging;news article;news comment;online news services;social media;subjective emotion rating;two-layer logistic regression;Blogs;Dictionaries;Logistics;Media;Robustness;Tagging;Training;Meta Classification;Multiple Source;Online News;Sentiment Tagging}, 
doi={10.1109/WISA.2015.24}, 
month={Sept},}
@INPROCEEDINGS{5074612, 
author={A. G. Klein and D. H. Johnson and W. A. Sethares and H. Lee and C. R. Johnson and E. Hendriks}, 
booktitle={2008 42nd Asilomar Conference on Signals, Systems and Computers}, 
title={Algorithms for Old Master painting canvas thread counting from x-rays}, 
year={2008}, 
pages={1229-1233}, 
abstract={The task of determining the weave density in the canvas support of Old Master paintings is introduced as a period extraction problem. Because of the way paintings were commonly prepared and preserved, the threads in the horizontal and vertical directions in the canvas support can be counted from x-ray images of the painting. Current procedures are tedious, time-consuming, and (usually) insufficiently documented. This paper describes the design of an algorithm for counting threads from x-rays that uses the Fourier Transform of the Radon Transform of a portion of the image with some crude, but appropriate, decision-making. The algorithm is presented as a sequence of refinements based on a simple mathematical model of the available image data: high resolution x-rays of paintings by Vincent van Gogh from the collection of the Van Gogh Museum. Over 900 spot counts were performed manually by a student team at Cornell using a graphical user interface created for this project. These manual counts provide a dataset for evaluating performance of the algorithm. A major goal is to convince art historians of the viability of automated (and semi-automated) counting procedures.}, 
keywords={Fourier transforms;Radon transforms;X-ray imaging;art;feature extraction;image resolution;Fourier transform;Radon transform;Vincent van Gogh paintings;canvas thread counting;graphical user interface;high resolution x-rays;old master paintings;period extraction problem;simple mathematical model;x-ray images;Algorithm design and analysis;Art;Decision making;Fourier transforms;Graphical user interfaces;Image resolution;Mathematical model;Painting;X-rays;Yarn}, 
doi={10.1109/ACSSC.2008.5074612}, 
ISSN={1058-6393}, 
month={Oct},}
@INPROCEEDINGS{6946573, 
author={J. Jasiewicz and I. Sobkowiak-Tabaka}, 
booktitle={2014 IEEE Geoscience and Remote Sensing Symposium}, 
title={Mapping past human activity using low representative site location datasets and elevation data}, 
year={2014}, 
pages={910-913}, 
abstract={Archaeological maps based on the location of sites are strongly biased by the degree of archaeological recognition and inform little about the real pattern of past human activities, especially on areas poorly covered by surveys. Continuous maps and spatial models, independent of the degree of archaeological recognition of the area, can used as a tool for explanation of the patterns of past human activity [1]. There are several methods (see: [2, 3, 4, 1] for details) which couple information about the location of archaeological remnants and variables derived from natural datasets and social and economic variables. These methods use Geographic Information Science technology and statistical algorithms and result in maps of past human activity. Correct models require the user to know the importance of variables what is difficult to proceed on insensibly contrasted areas like temperate lowlands (Jasiewicz, Hildebrandt-Radke 2009). and requires a large amount of representative data so its application is limited only to well recognized areas where data representativeness is not questionable - which does not occur very often. Furthermore, archaeological remnants tend to be clustered also that some areas were examined more thoroughly than others. This leads to the problem of data imbalance. The term refers to any dataset that exhibits an radically unequal distribution between its classes [5, 6].}, 
keywords={archaeology;geographic information systems;terrain mapping;Geographic Information Science technology;archaeological maps;archaeological recognition;continuous maps;economic variables;elevation data;low representative site location datasets;past human activity mapping;social variables;spatial models;statistical algorithms;Analytical models;Data models;Educational institutions;Indexes;Lakes;Spatial databases;Standards;Archaeological datasets;CART;machine learning;predictive modeling}, 
doi={10.1109/IGARSS.2014.6946573}, 
ISSN={2153-6996}, 
month={July},}
@INPROCEEDINGS{1264978, 
author={A. Walker and B. Pham and A. Maeder}, 
booktitle={10th International Multimedia Modelling Conference, 2004. Proceedings.}, 
title={A Bayesian framework for automated dataset retrieval in geographic information systems}, 
year={2004}, 
pages={138-144}, 
abstract={Existing geographic information systems (GIS) are intended for expert users and consequently, do not provide any machine intelligence to assist users. This paper presents a Bayesian framework that will incorporate expert knowledge in order to retrieve all relevant datasets given an initial user query. The framework uses a spatial model that combines relational, non-spatial and spatial data. This spatial model allows efficient access of relational linkages for a Bayesian network, and thus improves support for complex and vague queries. The Bayesian network assigns causal probabilities to these relational linkages in order to define expert knowledge of related datasets in the GIS. In addition, the framework will learn which datasets are best suited for particular query input through feedback supplied by the user. This contribution will increase the performance and efficiency of knowledge extraction from GIS by allowing users to focus on interpreting data, instead of focusing on finding which data is relevant to their analysis. The initial user query can be vague and the framework will still be capable of retrieving relevant datasets via the linkages discovered in the Bayesian network.}, 
keywords={belief networks;data models;geographic information systems;information retrieval;knowledge based systems;query processing;Bayesian framework;Bayesian network;automated dataset retrieval;causal probabilities;expert knowledge;geographic information systems;knowledge extraction;machine intelligence;nonspatial data;query processing;relational data;spatial data;Bayesian methods;Couplings;Data analysis;Data mining;Data visualization;Feedback;Geographic Information Systems;Information retrieval;Machine intelligence;Web server}, 
doi={10.1109/MULMM.2004.1264978}, 
month={Jan},}
@INPROCEEDINGS{7830808, 
author={X. Shi and Q. Zhou and X. Qu and F. Lin and H. Xu}, 
booktitle={2016 9th International Symposium on Computational Intelligence and Design (ISCID)}, 
title={Visual Analysis of Station Usage Patterns in Public Bicycle System}, 
year={2016}, 
volume={2}, 
pages={132-135}, 
abstract={Public Bicycle System(PBS) is a new kind of urban transportation infrastructure. Understanding the usage patterns of stations in PBS is crucial for bicycle dispatch and traffic resource optimization. In this paper, we propose a visual analytic system to explore the station usage patterns. A filter model is proposed firstly, which is used to filter out desired records under different conditions. Map view and station analysis view are presented to assist exploration in the context of geography and rental information. In the station analysis view, a table-like component is designed to sort and demonstrate data intuitively based on the filter model, which helps the users to inspect single/multiple stations hire patterns under different time granularity uniformly. We exemplify our approach with a real PBS dataset through case studies.}, 
keywords={data analysis;data visualisation;public transport;road traffic;road vehicles;PBS;bicycle dispatch;filter model;geography information;map view;public bicycle system;rental information;station analysis view;station usage patterns;traffic resource optimization;urban transportation infrastructure;visual analysis;visual analytic system;Analytical models;Bicycles;Couplings;Data models;Data visualization;Urban areas;Visualization;Public Bicycle System;Transportation Information Visualization;visual analytics}, 
doi={10.1109/ISCID.2016.2039}, 
month={Dec},}
@ARTICLE{7019836, 
author={C. Fang and J. Liu and Z. Lei}, 
journal={China Communications}, 
title={Parallelized user clicks recognition from massive HTTP data based on dependency graph model}, 
year={2014}, 
volume={11}, 
number={12}, 
pages={13-25}, 
abstract={With increasingly complex website structure and continuously advancing web technologies, accurate user clicks recognition from massive HTTP data, which is critical for web usage mining, becomes more difficult. In this paper, we propose a dependency graph model to describe the relationships between web requests. Based on this model, we design and implement a heuristic parallel algorithm to distinguish user clicks with the assistance of cloud computing technology. We evaluate the proposed algorithm with real massive data. The size of the dataset collected from a mobile core network is 228.7GB. It covers more than three million users. The experiment results demonstrate that the proposed algorithm can achieve higher accuracy than previous methods.}, 
keywords={Web sites;data mining;graph theory;hypermedia;parallel algorithms;transport protocols;Web technologies;Web usage mining;Website structure;cloud computing;dependency graph model;heuristic parallel algorithm;massive HTTP data;mobile core network;parallelized user clicks recognition;Algorithm design and analysis;Big data;Computational modeling;Data mining;Data models;Data preprocessing;Internet;Parallel algorithms;cloud computing;graph model;massive data;web usage mining}, 
doi={10.1109/CC.2014.7019836}, 
ISSN={1673-5447}, 
month={Dec},}
@INPROCEEDINGS{5581261, 
author={J. Lim and S. Han and B. Choi and C. Lee and B. Chung}, 
booktitle={2010 2nd International Conference on Information Technology Convergence and Services}, 
title={Classifying of Objectionable Contents with Various Audio Signal Features}, 
year={2010}, 
pages={1-5}, 
abstract={The rapid development of multimedia related technologies and internet infrastructure have made general users can create, edit, and post their contents and can easily access any content that they desire. But it also leads to the harmful side effects that are creation and uncontrolled distribution of objectionable contents. Especially it is very serious for pornographic contents that are more than about 70% of objectionable contents. The objectionable contents mean the pornographic contents in this paper. Most of the related studies are focused on image-based approaches and there are few studies based on audio-based approaches. In this paper, we try to classify objectionable contents based on various audio signal features. The audio signal features used in this paper are perceptual features that are spectral properties, MFCC based feature set and TDMFCC based feature set. For the reasonable results, we define the audio-based objectionable contents model and then construct dataset according to the defined model. For training and classifying dataset of two classes, objectionable and non-objectionable class, SVM classifier is used. TDMFCC based feature set has a good performance of accurate rate with SVM classifier, about 95%, and the results show that it is very effective to detect and classify the objectionable contents based on audio features.}, 
keywords={audio signal processing;feature extraction;multimedia systems;signal classification;spectral analysis;support vector machines;Internet infrastructure;MFCC based feature set;SVM classifier;TDMFCC based feature set;audio signal features;audio-based approach;audio-based objectionable content model;image-based approach;multimedia related technology;objectionable content classification;spectral property;Data models;Discrete cosine transforms;Feature extraction;Mel frequency cepstral coefficient;Support vector machines;Training;Videos}, 
doi={10.1109/ITCS.2010.5581261}, 
month={Aug},}
@INPROCEEDINGS{7532867, 
author={S. Tong and Y. P. Loh and X. Liang and T. Kumada}, 
booktitle={2016 IEEE International Conference on Image Processing (ICIP)}, 
title={Visual attention inspired distant view and close-up view classification}, 
year={2016}, 
pages={2787-2791}, 
abstract={The images of distant view and close-up view indicate a photographers' attention which can be further utilized for user behavior analysis and scene evaluation. As images may compose arbitrary contexts, distant view and close-up view classification becomes non-trivial. In this work, we found two cues can represent human visual attention, i.e. focus cue and scale cue. We model the focus cue in frequency domain using the Discrete Wavelet Transform, and employ signal distribution as the focus feature. For the scale cue, we model it by defining a spatial size and a conceptual size in the image using the Edge Box and Convolution Neural Network. By integrating these two models, a robust scheme is proposed for this non-trivial task. Experiments on a newly retrieved dataset, which has 2137 natural images, show the classification accuracy achieves up to 97.3%.}, 
keywords={discrete wavelet transforms;feature selection;image classification;neural nets;close-up view classification;convolution neural network;discrete wavelet transform;edge box;focus feature;human visual attention;photographers attention;scene evaluation;signal distribution;user behavior analysis;visual attention inspired distant view;Discrete wavelet transforms;Feature extraction;Frequency-domain analysis;Histograms;Image edge detection;Visualization;Distant and close-up view;focus cue;scale cue;visual attention}, 
doi={10.1109/ICIP.2016.7532867}, 
month={Sept},}
@INPROCEEDINGS{7899607, 
author={N. Rai}, 
booktitle={2016 23rd International Conference on Pattern Recognition (ICPR)}, 
title={Bi-modal regression for Apparent Personality trait Recognition}, 
year={2016}, 
pages={55-60}, 
abstract={The task of the ChaLearn Apparent Personality Analysis: First Impressions Challenge is to rate/quantify personality traits of users in short video sequences. Although the validity of personality judgments from short interactions is questionable, studies show the possibility of predicting attributed traits (First Impressions) using facial [15] and acoustic [13] features. The challenge introduces a newly constructed dataset which consists of manually annotated videos collected from YouTube. In this paper, we present our approach for predicting traits by combining multiple modality specific models. Our models include Deep Networks which focus on leveraging visual information in the given faces, Networks focusing on supplementary information from the background and models using acoustic features. We also discuss another approach for modeling traits as a combination of global and trait-specific variables. We explore methods for extracting fixed length descriptors of videos based on frame-level predictions. We also experiment with various methods for fusing model predictions. We observe that fusion achieves a considerable gain in accuracy over the best stand-alone model, possibly due to utilizing information from all modalities. The proposed method achieves an accuracy gain of approximately 18% above the provided challenge baseline.}, 
keywords={image sequences;regression analysis;YouTube;bimodal regression;chalearn apparent personality analysis;deep networks;personality trait recognition;video sequences;Acoustics;Computational modeling;Facial features;Feature extraction;Predictive models;Training;Visualization}, 
doi={10.1109/ICPR.2016.7899607}, 
month={Dec},}
@INPROCEEDINGS{7359767, 
author={S. Katragadda and H. Karnati and M. Pusala and V. Raghavan and R. Benton}, 
booktitle={2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
title={Detecting adverse drug effects using link classification on twitter data}, 
year={2015}, 
pages={675-679}, 
abstract={Adverse drug events (ADEs) are among the leading causes of death in the United States. Although many ADEs are detected during pharmaceutical drug development and the FDA approval process, all of the possible reactions cannot be identified during this period. Currently, post-consumer drug surveillance relies on voluntary reporting systems, such as the FDA's Adverse Event Reporting System (AERS). With an increase in availability of medical resources and health related data online, interest in medical data mining has grown rapidly. This information coupled with online conversations of people which involve discussions about their health provide a substantial resource for the identification of ADEs. In this work, we propose a method to identify adverse drug effects from tweets by modeling it as a link classification problem in graphs. Drug and symptom mentions are extracted from the tweet history of each user and a drug-symptom graph is built, where nodes represent either drugs or symptoms and edges are labelled positive or negative, for desired or adverse drug effects respectively. A link classification model is then used to identify negative edges i.e. adverse drug effects. We test our model on 864 users using 10-fold cross validation with Sider's dataset as ground truth. Our model was able to achieve an F-Score of 0.77 compared to the best baseline model with an F-Score of 0.58.}, 
keywords={data mining;drugs;graph theory;learning (artificial intelligence);pattern classification;social networking (online);text analysis;10-fold cross validation;ADE;AERS;F-score value;FDA adverse event reporting system;FDA approval process;Sider dataset;Twitter data;United States;adverse-drug effect detection;drug symptom graph;graph edges;graph nodes;ground truth;health related data;link classification problem;medical data mining;medical resources;online conversations;pharmaceutical drug development;postconsumer drug surveillance;tweets;voluntary reporting systems;Drugs;Pipelines;Adverse Drug Reaction;Graph Classification;Machine Learning;Pharmacovigilance;Social Media;Text Mining}, 
doi={10.1109/BIBM.2015.7359767}, 
month={Nov},}
@ARTICLE{5686924, 
author={X. Y. Wei and Y. G. Jiang and C. W. Ngo}, 
journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
title={Concept-Driven Multi-Modality Fusion for Video Search}, 
year={2011}, 
volume={21}, 
number={1}, 
pages={62-73}, 
abstract={As it is true for human perception that we gather information from different sources in natural and multi-modality forms, learning from multi-modalities has become an effective scheme for various information retrieval problems. In this paper, we propose a novel multi-modality fusion approach for video search, where the search modalities are derived from a diverse set of knowledge sources, such as text transcript from speech recognition, low-level visual features from video frames, and high-level semantic visual concepts from supervised learning. Since the effectiveness of each search modality greatly depends on specific user queries, prompt determination of the importance of a modality to a user query is a critical issue in multi-modality search. Our proposed approach, named concept-driven multi-modality fusion (CDMF), explores a large set of predefined semantic concepts for computing multi-modality fusion weights in a novel way. Specifically, in CDMF, we decompose the query-modality relationship into two components that are much easier to compute: query-concept relatedness and concept-modality relevancy. The former can be efficiently estimated online using semantic and visual mapping techniques, while the latter can be computed offline based on concept detection accuracy of each modality. Such a decomposition facilitates the need of adaptive learning of fusion weights for each user query on-the-fly, in contrast to the existing approaches which mostly adopted predefined query classes and/or modality weights. Experimental results on TREC video-retrieval evaluation 2005-2008 dataset validate the effectiveness of our approach, which outperforms the existing multi-modality fusion methods and achieves near-optimal performance (from oracle fusion) for many test queries.}, 
keywords={query processing;video retrieval;high-level semantic visual concepts;knowledge sources;low-level visual features;multi-modality fusion;speech recognition;supervised learning;text transcript;user query;video frames;video retrieval;video search;Airplanes;Context;Context modeling;Correlation;Semantics;Training;Visualization;Concept-driven fusion;multi-modality;semantic concept;video search}, 
doi={10.1109/TCSVT.2011.2105597}, 
ISSN={1051-8215}, 
month={Jan},}
@INPROCEEDINGS{7363978, 
author={S. Biookaghazadeh and Y. Xu and S. Zhou and M. Zhao}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={Enabling scientific data storage and processing on big-data systems}, 
year={2015}, 
pages={1978-1984}, 
abstract={Big-data systems are increasingly important for solving the data-driven problems in many science domains including geosciences. However, existing big-data systems cannot support the self-describing data formats such as NetCDF which are commonly used by scientific communities for data distribution and sharing. This limitation presents a serious hurdle to the further adoption of big-data systems by science domains and prevents scientific users from leveraging these systems to improve their productivity. This paper presents a solution to this problem by enabling big-data systems to directly store and process scientific data. Specifically, it enables Hadoop to efficiently store NetCDF data on HDFS and process them in MapReduce using convenient APIs. It also enables Hive to support standard queries on NetCDF data, transparently to users. The paper also presents an evaluation of the proposed solution using several representative queries on a typical geoscientific dataset. The results show that the proposed approach achieves substantial speedup (up to 20 times) and space saving (83% reduction), compared to the traditional approach which has to convert NetCDF data to CSV format for Hadoop and Hive to use them.}, 
keywords={Big Data;application program interfaces;data mining;storage management;API;Hadoop;Hive;MapReduce;NetCDF;big-data system;data-driven problem;geosciences;scientific data storage;Big data;Computational modeling;Data models;Distributed databases;File systems;Geology;Meteorology;Hadoop;NetCDF;Scientific data;big data}, 
doi={10.1109/BigData.2015.7363978}, 
month={Oct},}
@INPROCEEDINGS{7443679, 
author={A. Karar and A. Kaur}, 
booktitle={2015 Annual IEEE India Conference (INDICON)}, 
title={Latent fingerprint recognition and categorization using Multiphase Watershed Segmentation}, 
year={2015}, 
pages={1-6}, 
abstract={Latent fingerprints play a very important role in forensic applications to recognize the criminals. The latent fingerprints still face many issues due to the large amount of distortion. The sole purpose of this article is to improve the matching accuracy of latent fingerprints which are of bad quality. In this research work, we propose Multiphase Watershed Segmentation algorithm to refine the features collected from the poor quality fingerprint impression. Firstly all fingerprints of fine quality are acquired from the user dataset. Then we introduce noise models into image like Gaussian disturbance to test the matching process performance. The image is enhanced with the help of anisotropic filter. Weighted equalization of pores, line patterns and minutiae features are extracted and estimated. These are used to match from database to query using multiphase Watershed recognition algorithm. This algorithm is based on feature space calculation to discover and describe the primary feature in images. The features are vigorous to most of the image variations. The analyses are implemented in both frequency and spatial channels.}, 
keywords={feature extraction;filtering theory;fingerprint identification;image enhancement;image matching;image segmentation;police data processing;Gaussian disturbance;anisotropic filter;criminals recognition;feature estimation;feature extraction;feature space calculation;fingerprint impression;fingerprint quality;forensic applications;frequency channel;image enhancement;image variations;latent fingerprint categorization;latent fingerprint recognition;latent fingerprints matching accuracy;line pattern feature;matching process performance;minutiae feature;multiphase watershed segmentation algorithm;noise models;pores feature;spatial channel;user dataset;Algorithm design and analysis;Feature extraction;Filtering algorithms;Fingerprint recognition;Image edge detection;Image matching;Image segmentation;Anisotropic Filter;Crime Scene Investigation (CSI);Mean Square Error (MSR);Peak Signal To Noise Ratio (PSNR);Support Vector Machine (SVM;Watershed Segmentation}, 
doi={10.1109/INDICON.2015.7443679}, 
month={Dec},}
@INPROCEEDINGS{7403534, 
author={M. F. Chiang and E. P. Lim and J. W. Low}, 
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
title={On mining lifestyles from user trip data}, 
year={2015}, 
pages={145-152}, 
abstract={Large cities today are facing major challenges in planning and policy formulation to keep their growth sustainable. In this paper, we aim to gain useful insights about people living in a city by developing novel models to mine user lifestyles represented by the users' activity centers. Two models, namely ACMM and ACHMM, have been developed to learn the activity centers of each user using a large dataset of bus and subway train trips performed by passengers in Singapore. We show that ACHMM and ACMM yield similar accuracies in location prediction task. We also propose methods to automatically predict "home", "work" and "others" labels of locations visited by each user. Through validating with human-labeled home and work locations, we show that the accuracy of location label assignment is surprisingly very good even using an unsupervised method. With the location labels assigned, we further derive interesting insights of urban lifestyles at both individual and population levels.}, 
keywords={data mining;town and country planning;ACHMM;Singapore;human-labeled home;location label assignment;unsupervised method;urban lifestyles;user activity centers;user lifestyle mining;user trip data;Cities and towns;Hidden Markov models;Legged locomotion;Public transportation;Semantics;Sociology;Statistics}, 
doi={10.1145/2808797.2808906}, 
month={Aug},}
@ARTICLE{7572141, 
author={S. Koldijk and M. A. Neerincx and W. Kraaij}, 
journal={IEEE Transactions on Affective Computing}, 
title={Detecting work stress in offices by combining unobtrusive sensors}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Employees often report the experience of stress at work. In the SWELL project we investigate how new context aware pervasive systems can support knowledge workers to diminish stress. The focus of this paper is on developing automatic classiers to infer working conditions and stress related mental states from a multimodal set of sensor data (computer logging, facial expressions, posture and physiology). We address two methodological and applied machine learning challenges: 1) Detecting work stress using several (physically) unobtrusive sensors, and 2) Taking into account individual dierences. A comparison of several classication approaches showed that, for our SWELL-KW dataset, neutral and stressful working conditions can be distinguished with 90% accuracy by means of SVM. Posture yields most valuable information, followed by facial expressions. Furthermore, we found that the subjective variable `mental eort' can be better predicted from sensor data than e.g. `perceived stress'. A comparison of several regression approaches showed that mental eort can be predicted best by a decision tree (correlation of 0.82). Facial expressions yield most valuable information, followed by posture. We nd that especially for estimating mental states it makes sense to address individual dierences. When we train models on particular subgroups of similar users, (in almost all cases) a specialized model performs equally well or better than a generic model.}, 
keywords={Computational modeling;Computers;Context;Sensors;Stress;Stress measurement;Support vector machines;Machine learning;computer logging;facial expressions;individual dierences;mental state inference;physiology;posture;stress}, 
doi={10.1109/TAFFC.2016.2610975}, 
ISSN={1949-3045}, 
month={},}
@INPROCEEDINGS{7406451, 
author={H. Morimitsu and R. M. Cesar and I. Bloch}, 
booktitle={2015 IEEE International Conference on Computer Vision Workshop (ICCVW)}, 
title={Attributed Graphs for Tracking Multiple Objects in Structured Sports Videos}, 
year={2015}, 
pages={751-759}, 
abstract={In this paper we propose a novel approach for tracking multiple object in structured sports videos using graphs. The objects are tracked by combining particle filter and frame description with Attributed Relational Graphs. We start by learning a probabilistic structural model graph from annotated images and then use it to evaluate and correct the current tracking state. Different from previous studies, our approach is also capable of using the learned model to generate new hypotheses of where the object is likely to be found after situations of occlusion or abrupt motion. We test the proposed method on two datasets: videos of table tennis matches extracted from YouTube and badminton matches from the ACASVA dataset. We show that all the players are successfully tracked even after they occlude each other or when there is a camera cut.}, 
keywords={graph theory;image filtering;object tracking;particle filtering (numerical methods);probability;sport;video signal processing;ACASVA dataset;YouTube;attributed relational graphs;badminton matches;frame description;image annotation;object tracking;particle filter;probabilistic structural model graph;structured sports videos;table tennis matches;Cognition;Computational modeling;Data models;Object tracking;Target tracking;Videos}, 
doi={10.1109/ICCVW.2015.102}, 
month={Dec},}
@INPROCEEDINGS{7082013, 
author={J. Li and C. Sun and J. Lv}, 
booktitle={2014 Seventh International Symposium on Computational Intelligence and Design}, 
title={CTMF: Context-Aware Trust-Based Matrix Factorization with Implicit Trust Network}, 
year={2014}, 
volume={2}, 
pages={387-390}, 
abstract={Trust-aware recommender system can provide more accurate rating predictions than traditional recommender system by taking the trust relationships between users into consideration. Yet the state-of-the-art improved trust-aware collaborative filtering approach only considers the user-based implicit trust network model and the influence of trust information on rating prediction, ignoring the null value problem of local trust and the situation that people in different contextual conditions have different trust networks. The existing context-aware matrix factorization methods only consider the influence of contextual information on rating prediction, which are faced with the sparse initial rating matrix issue. To solve all the problems above, we propose two context-aware trust-based matrix factorization approaches to take both user-based implicit trust network model and item-based implicit trust network model into account and fully capture the influence of both context and trust information on rating. Experimental results on one real world dataset show that the two proposed approaches outperform the improved trust-aware approach and the existing context-aware matrix factorization methods in prediction performance.}, 
keywords={collaborative filtering;matrix decomposition;network theory (graphs);recommender systems;trusted computing;ubiquitous computing;CTMF;context-aware trust-based matrix factorization;contextual information;item-based implicit trust network model;null value problem;rating prediction;sparse initial rating matrix;trust information;trust-aware collaborative filtering approach;trust-aware recommender system;user-based implicit trust network model;Collaboration;Context;Context modeling;Recommender systems;Sparse matrices;Training;collaborative filtering;context-aware trust-based recommender systems;item-based implicit trust network;matrix factorization;user-based implicit trust network}, 
doi={10.1109/ISCID.2014.176}, 
month={Dec},}
@INPROCEEDINGS{6040707, 
author={Z. Xu and L. Ru and L. Xiang and Q. Yang}, 
booktitle={2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology}, 
title={Discovering User Interest on Twitter with a Modified Author-Topic Model}, 
year={2011}, 
volume={1}, 
pages={422-429}, 
abstract={This paper focuses on the problem of discovering users' topics of interest on Twitter. While previous efforts in modeling users' topics of interest on Twitter have focused on building a "bag-of-words" profile for each user based on his tweets, they overlooked the fact that Twitter users usually publish noisy posts about their lives or create conversation with their friends, which do not relate to their topics of interest. In this paper, we propose a novel framework to address this problem by introducing a modified author-topic model named twitter-user model. For each single tweet, our model uses a latent variable to indicate whether it is related to its author's interest. Experiments on a large dataset we crawled using Twitter API demonstrate that our model outperforms traditional methods in discovering user interest on Twitter.}, 
keywords={social networking (online);Twitter API;Twitter-user model;bag-of-words profile;modified author topic model;user interest discovery;Aggregates;Encyclopedias;Inference algorithms;Internet;Neodymium;Twitter;Twitter;topic model;twitter-user model;user interest}, 
doi={10.1109/WI-IAT.2011.47}, 
month={Aug},}
@INPROCEEDINGS{7545050, 
author={M. Yamamoto and T. Yamasaki and K. Aizawa}, 
booktitle={2016 IEEE Second International Conference on Multimedia Big Data (BigMM)}, 
title={Service Annotation and Profiling by Review Analysis}, 
year={2016}, 
pages={357-364}, 
abstract={With the increase in the number of user reviews on user review sites, useful tools for extracting good and bad points of services so that users can easily and intuitively understand the quality of the services are required. If the annotations are selected from the pre-defined list, there can always be missing keywords. Supervised annotation approaches would suffer from the same problem. In this paper, we present an unsupervised method for extracting unique aspects of services and user opinions on these aspects from plain user reviews and apply it to service annotation using Yelp's Academic Dataset. Our method is simple and easy to extend to other languages and domains. By using only the term frequency (TF), general aspects such as whether the food or service is good can be extracted. Further, what is praised particularly to the specific service can be extracted by using the term frequency and inverse document frequency (TF-IDF).}, 
keywords={Web sites;document handling;user interfaces;TF-IDF;Yelp academic dataset;inverse document frequency;profiling;review analysis;service annotation;term frequency;user opinions;user review sites;Business;Cameras;Data mining;Electronic mail;Feature extraction;Mathematical model;Recommender systems}, 
doi={10.1109/BigMM.2016.89}, 
month={April},}
@INPROCEEDINGS{6982067, 
author={E. M. Rochd and M. Quafafou}, 
booktitle={2014 IEEE 11th International Conference on e-Business Engineering}, 
title={A Top-N Recommender Model with Partially Predefined Structure}, 
year={2014}, 
pages={112-119}, 
abstract={Recommender systems can retrieve appropriate results based on users behavioral patterns and preferences. They may be built based on multi-label learning approaches, as each customer transaction may be labeled with several results that interest him/her. It is therefore useful to model the correlations between labels while controlling complexity of the learning algorithm. This paper presents a generative probabilistic model for online resources (products/URLs) recommendation, by capturing the complex local correspondence between the user's queries and the resources he/she has actually viewed. The structure of our model is partially defined and it is completed according to the observed data. Consequently, several links between observed and/or latent random variables are induced from the training dataset before starting the estimation of parameters. Experiments conducted on real data show the effectiveness of our approach.}, 
keywords={learning (artificial intelligence);parameter estimation;probability;query processing;random processes;recommender systems;Top-N recommender model;complex local correspondence;customer transaction;generative probabilistic model;latent random variables;learning algorithm;multilabel learning approaches;online resource recommendation;parameter estimation;partially predefined structure;recommender systems;user behavioral patterns;user behavioral preferences;user queries;Context;Correlation;Data models;Mathematical model;Probabilistic logic;Testing;Training;E-commerce;Information Retrieval;Local Influence;Recommendation;Topic Models;User Behavior}, 
doi={10.1109/ICEBE.2014.29}, 
month={Nov},}
@INPROCEEDINGS{7288082, 
author={J. Ri and H. Lu and Z. Gan and G. Choe and S. Ri}, 
booktitle={2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)}, 
title={Prediction of trust relationship based on subjective logic}, 
year={2015}, 
pages={1006-1011}, 
abstract={In social network, trust relationship between two users depends on the trustors opinion about the trustee, and the opinion contains uncertainty, but previous researches did not pay much attention to the uncertainty. Subjective logic is suitable for modeling and analyzing situations involving uncertainty and incomplete knowledge. This paper proposes a method of predicting trust relationship based on subjective logic. Regarding to users typical activities in social network, four types of subjective opinion are defined reflecting different aspects, and non-linear functions are used to map observed values to belief extent. The experimental results on the extended Epinions dataset show that the proposed method can improve the accuracy of trust relationship prediction.}, 
keywords={nonlinear functions;social networking (online);trusted computing;extended Epinions dataset;nonlinear function;situation analysis;situation modeling;social network;subjective logic;subjective opinion;trust relationship prediction;trustor opinion;Accuracy;Analytical models;Computer science;Correlation;Predictive models;Social network services;Uncertainty}, 
doi={10.1109/CYBER.2015.7288082}, 
month={June},}
@INPROCEEDINGS{7367282, 
author={S. Aygün and S. Okyay}, 
booktitle={2015 IEEE 3rd Workshop on Advances in Information, Electronic and Electrical Engineering (AIEEE)}, 
title={Improving the pearson similarity equation for recommender systems by age parameter}, 
year={2015}, 
pages={1-6}, 
abstract={Recent years, collaborative filtering systems have become so much popular due to excessive developments in technology. Rooting from the machine learning, recommender systems are in the center of the knowledge engineering that are used frequently in many computer science related fields. Therefore, this paper tries to utilize time information inferred from users' ages, which is going to be employed in the recommender systems during the similarity calculation between users. Adding more parameters, in other words, inserting detailed information of the users' preferences into the calculations will stimulate the resolution of fulfilling results thanks to new criterion. Specifically, it is proposed a conversion between two particular dates which are relevant to the user's personal data. 10-year interval is considered as a transition between generations, thus generation gap between users can make sense positively or negatively in terms of the amount of gap. Additionally, for the prerequisite of being a peer, it is defined a 3-year long difference can be a boundary to say that two users can be in the same wavelength. From the inspiration of this idea, a new mathematical addition into the well-known similarity calculation is presented in this paper. Eventually, this further time detail is taken into account via inverse proportion in similarity calculation and time incorporation approach for recommender system is emphasized. All in all, proposed method is to be tested with a real valued Movie Lens-MLP dataset on the platform of MatLab.}, 
keywords={human computer interaction;recommender systems;MatLab;Movie Lens-MLP;Pearson similarity equation;age parameter;generation gap;recommender systems;Collaboration;Data mining;Mathematical model;Proposals;Recommender systems;Standards;Pearson similarity;age parametrized recommender system;collaborative filtering;movie recommendation;recommender systems}, 
doi={10.1109/AIEEE.2015.7367282}, 
month={Nov},}
@INPROCEEDINGS{7509326, 
author={M. Agrawal and R. L. Velusamy}, 
booktitle={2016 IEEE Students' Conference on Electrical, Electronics and Computer Science (SCEECS)}, 
title={R-SALSA: A spam filtering technique for social networking sites}, 
year={2016}, 
pages={1-7}, 
abstract={Now a days, Social media is instrumental for expeditious communication among users across the globe. The escalation in the growth of Social media tools such as LinkedIn, Google+, MySpace, Pinterest, Facebook, Instagram, Twitter, Yammer, Weibo, Hyves, etc., led to rise in the volume of unsolicited messages and spamming activities in past few years. Enormous volume of spamming activities has caused severe problem in essential communication. Spam messages may be generated by automated spam bots or users. In vision of these circumstances, there has been much research effort toward doing spam filtering based on supervised approaches. Motivated by the fact, steady nature of supervised approach requires model retraining to identify new variety of spam messages. An unsupervised approach namely Reliability based Stochastic Approach for Link-Structure Analysis (R-SALSA) algorithm has been proposed in this paper for classifying a message being Spam or benign. The dataset collected from popular Netherland's social media named Hyves is used to test proposed algorithm. It has been evaluated with different performance based metrics namely true positive rate, false positive rate, accuracy, and it is found to be performing better than previously proposed unsupervised author-reporter model. The proposed algorithm achieved 9.17% accuracy in spam identification when compared with Hyper Link Induced Topic Search (HITS) and 2.49% accuracy in spam identification when compared with SALSA based method.}, 
keywords={e-mail filters;information filtering;pattern classification;social networking (online);stochastic processes;unsolicited e-mail;Hyves;Netherland social media;R-SALSA;false positive rate;message classification;performance based metrics;reliability based stochastic approach for link-structure analysis algorithm;social networking sites;spam filtering technique;spam identification;spam messages;spamming activities;supervised approach;true positive rate;unsupervised approach;unsupervised author-reporter model;Algorithm design and analysis;Classification algorithms;Filtering;Media;Reliability;Social network services;Unsolicited electronic mail;Markov chain;Page Rank;SALSA;SVM;Social media;Social spam;botnet}, 
doi={10.1109/SCEECS.2016.7509326}, 
month={March},}
@INPROCEEDINGS{5702461, 
author={J. Ostermann}, 
booktitle={28th Picture Coding Symposium}, 
title={3D information coding}, 
year={2010}, 
pages={19-19}, 
abstract={There are several technologies available for the transmission and presentation of 3D information to the human user. The appropriate selection of technology depends to a large extent on the application as well as on the maturity of the required technology. 3D video was established in niche markets, including professional applications (e.g., scientific visualization) and entertainment (IMAX cinemas, 3D gaming) some time ago and since 2008, it became main stream in digital cinemas with the consumer market getting ready for 3D video by introducing stereo TVs and related technology. Depending on the application, different presentations and data formats are required. For scientific visualization, 3D data formats are used. The 3D data is rendered at the server or it is transmitted to the receiver or client which renders the appropriate view as determined by the user. The correct display of the objects on monoscopic and stereoscopic displays is possible. 3D games typically require the 3D data at the client in order to enable low-latency interaction with the data. The most common 3D data sets consist of 3D geometry and 2D texture data. In case a 3D object is recorded from all allowed viewing angles, this dataset of images can be used for image-based rendering where always one of the recorded images is shown on the display. Image-based rendering may be combined with 3D geometry and animation. The estimation of 3D geometry of natural scenes is a challenging task. Therefore most applications rely on synthetic or manually created 3D data. For presentation of 3D movies, stereo displays requiring glasses to separate left and right views as well as autostereoscopic displays requiring no glasses exist. Independent of the viewer position and orientation, a stereo display presents one view to the left eye and one view to the right eye. These displays just need to receive two video streams typically recorded with a stereo camera. MPEG developed video coding standards to support these d- - isplays. The latest standard is MVC, an extension of AVC for coding of stereo sequences using frame reordering. Due to the sudden popularity of stereoscopic video, service providers face the challenge of transmitting stereo using the existing video distribution and presentation infrastructure. One solution is referred to as Frame-packing (FP) because both video frames are copied typically side by side into one frame prior to encoding. Hence, the resulting stereo sequence has only half of the regular resolution. The set-top box decodes FP into one regular frame and signals FP to the display which upconverts each half of the decoded frame to full size for the left and right view. In MPEG-2 FP is signaled as frame packing using private data (USA, Japan) or a newly developed extension in the MPEG-2 Transport Stream. AVC, which is mainly used for HDTV distribution, uses SEI messages to signal FP. MPEG is currently working on an extension to AVC that enhances FP to full resolution stereo while still keeping the compatibility to AVC with FP. In the real world, the view of a scene depends on the head position and orientation of the viewer. Hence, the images presented to the two eyes should be changed depending on the eye position. To a limited extend, auto-stereoscopic displays support this feature by displaying simultaneously several views with a lens in front of the screen assuring that always two appropriate views are visible to the eyes of a viewer. Starting with subjective tests in 2011, MPEG targets this 3DV application by coding several video streams and a depth map of the scene enabling the display to create the appropriate views by means of a view synthesis algorithm. The main challenges are the reliable estimation of a depth map, which may be manually supported for non-real-time applications, as well as the view-synthesis algorithm.}, 
keywords={stereo image processing;video coding;3D data formats;3D information coding;3D video;AVC;MPEG-2 transport stream;frame-packing;image-based rendering;scientific visualization;stereo display;stereo sequences;stereoscopic video;video coding standards;video distribution;view-synthesis algorithm}, 
doi={10.1109/PCS.2010.5702461}, 
month={Dec},}
@INPROCEEDINGS{6406354, 
author={Y. Altshuler and N. Aharony and M. Fire and Y. Elovici and A. Pentland}, 
booktitle={2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing}, 
title={Incremental Learning with Accuracy Prediction of Social and Individual Properties from Mobile-Phone Data}, 
year={2012}, 
pages={969-974}, 
abstract={As truly ubiquitous wearable computers, mobile phones are quickly becoming the primary source for social, behavioral, and environmental sensing and data collection. Today's smart phones are equipped with increasingly more sensors and accessible data types that enable the collection of literally dozens of signals regarding the phone, its user, and their environment. A great deal of research effort in academia and industry is put into mining this data for higher level sense-making, such as understanding user context, inferring social networks, learning individual features, and so on. In many cases this analysis work is the result of exploratory forays and trial-and-error. Adding to the challenge, the devices themselves are limited platforms, hence data collection campaign must be carefully designed in order to collect the signals in the appropriate frequency, avoiding the exhausting the the device's limited battery and processing power. Currently however, there is no structured methodology for the design of mobile data collection and analysis initiatives. In this work we investigate the properties of learning and inference of real world data collected via mobile phones over time. In particular, we analyze how the ability to predict individual parameters and social links is incrementally enhanced with the accumulation of additional data. To do so we use the Friends and Family dataset, containing rich data signals gathered from the smart phones of 140 adult members of an MIT based young-family residential community for over a year, and is one of the most comprehensive mobile phone datasets gathered in academia to date. We develop several models for predicting social and individual properties from sensed mobile phone data over time, including detection of life-partners, ethnicity, and whether a person is a student or not. Finally, we propose a method for predicting the maximal learning accuracy possible for the learning task at hand, based on an initial set of measure- ents. This has various practical implications, such as better design of mobile data collection campaigns, or evaluating of planned analysis strategies.}, 
keywords={data mining;learning (artificial intelligence);smart phones;social sciences computing;Friends and Family dataset;accuracy prediction;behavioral sensing;data collection;data mining;environmental sensing;ethnicity detection;incremental learning;individual property prediction;life-partner detection;mobile phone;smart phone;social network;social property prediction;social sensing;ubiquitous wearable computer;Accuracy;Communities;Sensors;Smart phones;Social network services;Incremental Learning;Machine Learning;Mobile Networks}, 
doi={10.1109/SocialCom-PASSAT.2012.102}, 
month={Sept},}
@ARTICLE{5742970, 
author={D. Tuia and M. Volpi and L. Copa and M. Kanevski and J. Munoz-Mari}, 
journal={IEEE Journal of Selected Topics in Signal Processing}, 
title={A Survey of Active Learning Algorithms for Supervised Remote Sensing Image Classification}, 
year={2011}, 
volume={5}, 
number={3}, 
pages={606-617}, 
abstract={Defining an efficient training set is one of the most delicate phases for the success of remote sensing image classification routines. The complexity of the problem, the limited temporal and financial resources, as well as the high intraclass variance can make an algorithm fail if it is trained with a suboptimal dataset. Active learning aims at building efficient training sets by iteratively improving the model performance through sampling. A user-defined heuristic ranks the unlabeled pixels according to a function of the uncertainty of their class membership and then the user is asked to provide labels for the most uncertain pixels. This paper reviews and tests the main families of active learning algorithms: committee, large margin, and posterior probability-based. For each of them, the most recent advances in the remote sensing community are discussed and some heuristics are detailed and tested. Several challenging remote sensing scenarios are considered, including very high spatial resolution and hyperspectral image classification. Finally, guidelines for choosing the good architecture are provided for new and/or unexperienced user.}, 
keywords={geophysical image processing;image classification;learning (artificial intelligence);remote sensing;active learning algorithms;hyperspectral image classification;remote sensing community;suboptimal dataset;supervised remote sensing image classification;user-defined heuristic;Entropy;Machine learning;Pixel;Remote sensing;Support vector machines;Training;Uncertainty;active learning;hyperspectral;image classification;support vector machine (SVM);training set definition;very high resolution (VHR)}, 
doi={10.1109/JSTSP.2011.2139193}, 
ISSN={1932-4553}, 
month={June},}
@INPROCEEDINGS{5284234, 
author={A. Khrabrov and G. Cybenko}, 
booktitle={2009 International Conference on Computational Science and Engineering}, 
title={A Language of Life: Characterizing People Using Cell Phone Tracks}, 
year={2009}, 
volume={4}, 
pages={495-501}, 
abstract={Mobile devices can produce continuous streams of data which are often specific to the person carrying them. We show that cellphone tracks from the MIT Reality dataset can be used to reliably characterize individual people. This is done by treating each person's data as a separate language by building a standard n-gram language model for each "author". We then compute the perplexities of an unlabelled sample as based on each person's language model. The sample is assigned to the user yielding the lowest perplexity score. This technique achieves 85% accuracy and can also be used for clustering. We also show how language models can also be used for predicting movement and propose metrics to measure the accuracy of the predictions. Finally, we develop an alternative method for identifying individuals bycounting the subsequences in a sample which are unique to their authors. This is done by building a generalized suffix tree of the training set and counting each subsequence from a sample which is unique for some person as evidence towards identifying that person as the author. We present the identification and prediction as a part of a HUMBLE human behavior modeling framework, outline general modeling goals, and show how our methods help. Our results suggest that people's medium-scale movement behavioral patterns, at the granularity of cell tower footprints, can be used to characterize individuals.}, 
keywords={mobile handsets;social aspects of automation;cell phone tracks;generalized suffix tree;mobile device;standard n-gram language model;Bioinformatics;Cellular phones;Context modeling;Data engineering;Educational institutions;Humans;Intrusion detection;Poles and towers;Predictive models;Trajectory;human behavior modeling;mobility patterns;n-gram models;sensor data}, 
doi={10.1109/CSE.2009.366}, 
month={Aug},}
@INPROCEEDINGS{6973611, 
author={H. Gao and J. Jiang and L. Zhang and L. Yuchao and D. Li}, 
booktitle={2013 International Conference on Information Science and Cloud Computing Companion}, 
title={Cloud Model: Detect Unsupervised Communities in Social Tagging Networks}, 
year={2013}, 
pages={317-323}, 
abstract={In the big data era, detecting unsupervised communities in a given dataset, analyzing the evolution of the unsupervised communities, tracing the interests of users are very important. For instance, we can capture user's interest and provide personalized information. In order to detect unsupervised communities in social tagging networks, this paper uses similarity cloud properties of cloud model to solve the different community analysis, classification, and describe the evolutions of unsupervised communities quantitatively and users' dynamic interests in unsupervised communities problems. Cloud model is used in this paper. By introducing similarity cloud properties of cloud model, cloud model can detect the unsupervised communities, describe the evolutions of unsupervised communities quantitatively, and users' dynamic interests in unsupervised communities. For illustration, the proposed model is applied to Delicious dataset to detect unsupervised communities and one month is used as time slice to study the evolutions of the unsupervised communities. Empirical results show that the unsupervised community in social tagging in network, using Similarity cloud properties of cloud model can effectively detect different unsupervised communities, and describe the evolutions of unsupervised communities quantitatively. Similarity cloud properties based cloud model can effectively detect unsupervised community in social tagging network, and quantitatively describe the evolutions of the community and community user' dynamic interest. Hence, CBUCD model is an efficient solution for detecting unsupervised community and analyzing evolutions.}, 
keywords={Internet;social networking (online);CBUCD model;cloud model;cloud properties;delicious dataset;personalized information;social tagging networks;unsupervised communities detection;users dynamic interests;Analytical models;Communities;Numerical models;Pragmatics;Tagging;Time-frequency analysis;Uncertainty;Cloud Model;Social Network;Social Tagging;Unsupervised Community}, 
doi={10.1109/ISCC-C.2013.56}, 
month={Dec},}
@INPROCEEDINGS{7837961, 
author={A. Painsky and S. Rosset}, 
booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
title={Compressing Random Forests}, 
year={2016}, 
pages={1131-1136}, 
abstract={Ensemble methods are considered among the state-of-the-art predictive modeling approaches. Applied to modern big data, these methods often require a large number of sub-learners, where the complexity of each learner typically grows with the size of the dataset. This phenomenon results in an increasing demand for storage space, which may be very costly. This problem mostly manifests in a subscriber based environment, where a user-specific ensemble needs to be stored on a personal device with strict storage limitations (such as a cellular device). In this work we introduce a novel method for lossless compression of tree-based ensemble methods, focusing on Random Forests. Our suggested method is based on probabilistic modeling of the ensemble's trees, followed by model clustering via Bregman divergence. This allows us to find a minimal set of models that provides an accurate description of the trees, and at the same time is small enough to store and maintain. Our compression scheme demonstrates high compression rates on a variety of modern datasets. Importantly, our scheme enables predictions from the compressed format and a perfect reconstruction of the original ensemble.}, 
keywords={Big Data;data compression;learning (artificial intelligence);probability;Bregman divergence;big data;cellular device;lossless compression;model clustering;predictive modeling approaches;probabilistic modeling;random forests compression;storage limitations;storage space;tree-based ensemble methods;user-specific ensemble;Dictionaries;Encoding;Mathematical model;Periodic structures;Probabilistic logic;Probability distribution;Vegetation;Compression;Entropy Coding;Random Forest}, 
doi={10.1109/ICDM.2016.0148}, 
month={Dec},}
@ARTICLE{7539401, 
author={H. Malik and S. Mishra}, 
journal={IEEE Transactions on Industry Applications}, 
title={Application of Gene Expression Programming (GEP) in Power Transformers Fault Diagnosis Using DGA}, 
year={2016}, 
volume={52}, 
number={6}, 
pages={4556-4565}, 
abstract={Accurate diagnosis of incipient faults in oil-filled power transformers is important in preventive maintenance of transformers. Dissolved gas analysis (DGA) is an effective tool to diagnose incipient transformer faults. The majority of the methods reported in literature to analyze DGA results lay more emphasis on user experience rather than mathematical formulation/justification. Furthermore, sometimes DGA results for a certain fault do not belong to any of the IEC/IEEE standard and cannot be categorized/diagnosed. To address these issues, we propose a new approach for DGA interpretation using gene expression programming (GEP). The proposed approach is employed for analysis of 552 DGA samples collected from transformers of Himachal Pradesh State Electricity Board, India, in conjunction with samples extracted from reliable literature. We use the aforementioned dataset to test and validate our proposed GEP model. We also compare the performance of our approach against other artificial intelligence-based techniques such as artificial neural network, fuzzy-logic, and support vector machine. Results and comparison against other soft computing approaches show relative superiority of GEP-based DGA interpretation in terms of classification accuracy.}, 
keywords={fault diagnosis;power transformers;DGA;GEP;Himachal Pradesh state electricity board;India;dissolved gas analysis;gene expression programming;oil-filled power transformers;power transformers fault diagnosis;preventive maintenance;Discharges (electric);Gases;Oil insulation;Oils;Partial discharges;Power transformers;Support vector machines;Artificial intelligence (AI);dissolved gas analysis (DGA);gene expression programming (GEP);incipient fault classification;power transformer}, 
doi={10.1109/TIA.2016.2598677}, 
ISSN={0093-9994}, 
month={Nov},}
@INPROCEEDINGS{7801499, 
author={A. Shafaei and J. J. Little}, 
booktitle={2016 13th Conference on Computer and Robot Vision (CRV)}, 
title={Real-Time Human Motion Capture with Multiple Depth Cameras}, 
year={2016}, 
pages={24-31}, 
abstract={Commonly used human motion capture systems require intrusive attachment of markers that are visually tracked with multiple cameras. In this work we present an efficient and inexpensive solution to markerless motion capture using only a few Kinect sensors. Unlike the previous work on 3d pose estimation using a single depth camera, we relax constraints on the camera location and do not assume a co-operative user. We apply recent image segmentation techniques to depth images and use curriculum learning to train our system on purely synthetic data. Our method accurately localizes body parts without requiring an explicit shape model. The body joint locations are then recovered by combining evidence from multiple views in real-time. We also introduce a dataset of ~6 million synthetic depth frames for pose estimation from multiple cameras and exceed state-of-the-art results on the Berkeley MHAD dataset.}, 
keywords={image motion analysis;image segmentation;image sensors;pose estimation;3D pose estimation;Kinect sensors;curriculum learning;image segmentation technique;multiple depth cameras;real-time human motion capture;single depth camera;Cameras;Pipelines;Pose estimation;Sensors;Shape;Three-dimensional displays;Training;depth sensors;human motion capture}, 
doi={10.1109/CRV.2016.25}, 
month={June},}
@INPROCEEDINGS{7146518, 
author={K. K. Rachuri and T. Hossmann and C. Mascolo and S. Holden}, 
booktitle={2015 IEEE International Conference on Pervasive Computing and Communications (PerCom)}, 
title={Beyond location check-ins: Exploring physical and soft sensing to augment social check-in apps}, 
year={2015}, 
pages={123-130}, 
abstract={Smartphone sensing research has been advancing at a brisk pace. Yet, current social networking services often only take advantage of location sensing: applications like Foursquare use the phone's GPS and Wi-Fi radios to infer the user's location to simplify checking-in to a place. However, smartphone sensing could be exploited to considerably expand the spectrum of information a user can share with a few clicks with friends: not only the location of an event but activities such as “cooking dinner” or “waiting for a bus” can be predicted and suggested to the user to ease the check-in process. In this paper we show how mobile phone sensing can be used in this sense. For this prediction process to be accurate however, sensors need to be sampled often, with a considerable impact on the phone battery. To alleviate this issue, we explore streams of phone usage data (soft sensors), such as application usage, messages, and phone calls for predicting the user's activity in a more efficient fashion for augmenting mobile social check-in apps. We have deployed our application and collected a dataset of over 2700 check-ins to 48 activities from 20 users. Our analysis shows a prediction accuracy of 75% when offering 5 check-in suggestions to users. Furthermore, we show that when using only soft sensors we can achieve very similar performance to that obtained with real sensors, thereby significantly reducing the impact on the phone battery. This finding might have a potentially high impact on smartphone based activity check-in apps.}, 
keywords={Global Positioning System;mobile computing;smart phones;social networking (online);wireless LAN;Foursquare;GPS radio;LBSN;Wi-Fi radio;brisk pace;location based social network;location sensing;mobile phone sensing;phone battery;smart phone sensing;social check-in application;soft sensing;soft sensor;Accuracy;Batteries;Context;Predictive models;Sensor phenomena and characterization;Software}, 
doi={10.1109/PERCOM.2015.7146518}, 
month={March},}
@INPROCEEDINGS{7086616, 
author={A. Devgun}, 
booktitle={2014 International Conference on Electronics, Communication and Computational Engineering (ICECCE)}, 
title={A machine learning adaptive approach to remove impurities over Bigdata}, 
year={2014}, 
pages={220-225}, 
abstract={A Bigdata is the vast information storage collected from various locations and sources. Bigdata is defined as centralized repository with a standard structural specification. But the information driven from various sources are not always appropriate for this structure. This kind of information suffers from number of associated impurities. These impurities include incompleteness, duplicate information, lack of association between dataset attributes etc. To represent this information in organized and structured form, there is the requirement of some algorithmic approach that can identify these impurities and accept the validated data. In this present work, a two stage mode is defined under machine learning approach to transformed unstructured data to structured form. In first stage of this model, a fuzzy based model is defined to analyze this user data. The analysis is performed here under the impurity type analysis and the association analysis. The fuzzy rule is implied here to identify the degree of impurity and the associativity. Once the analysis is performed, the final stage of work is the transformation approach. During this stage, the transformation of this unstructured data to structured data is performed. An ontology driven work is defined to define such mapping. The mapping is here performed under the domain constructs and the data constructs. The work is implemented in java environment. The obtained results from system shows the reliable and robust information mapping so that the effective information tracking over the dataset is obtained.}, 
keywords={Big Data;Java;fuzzy set theory;learning (artificial intelligence);ontologies (artificial intelligence);storage management;Big Data;Java environment;algorithmic approach;association analysis;associativity degree;centralized repository;dataset attributes;duplicate information;fuzzy based model;fuzzy rule;impurities identification;impurities removal;impurity degree;impurity type analysis;incompleteness impurities;information mapping;information storage;information tracking;machine learning adaptive approach;ontology driven work;standard structural specification;structured form;unstructured data transformation;user data;Analytical models;Big data;File systems;Impurities;Machine learning algorithms;Reliability;Servers;Bigdata;Fuzzy Effective;Impurities;machine learning;structured analysis}, 
doi={10.1109/ICECCE.2014.7086616}, 
month={Nov},}
@INPROCEEDINGS{7090405, 
author={Q. Wang and Y. Xu and Y. L. Chen and Y. Wang and X. Wu}, 
booktitle={2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)}, 
title={Dynamic hand gesture early recognition based on Hidden Semi-Markov Models}, 
year={2014}, 
pages={654-658}, 
abstract={Real-time performance can be greatly improved, if the early recognition is implemented. In this paper, a dynamic hand gesture early recognition system is proposed. The system can recognize the gesture before it is completed. Our method is based on the Hidden Semi-Markov Models. Three-dimensional information of the gesture trajectory collected by leapmotion is the main data we used. Experiments on the dataset which we established demonstrate the effectiveness of our method.}, 
keywords={gesture recognition;hidden Markov models;NUI;gesture trajectory;hand gesture early recognition;hidden semiMarkov model;natural user interface;Arrays;Conferences;Gesture recognition;Hidden Markov models;Training;Trajectory}, 
doi={10.1109/ROBIO.2014.7090405}, 
month={Dec},}
@ARTICLE{5166506, 
author={X. Lei and P. Yang and D. Yao}, 
journal={IEEE Transactions on Neural Systems and Rehabilitation Engineering}, 
title={An Empirical Bayesian Framework for Brain #x2013;Computer Interfaces}, 
year={2009}, 
volume={17}, 
number={6}, 
pages={521-529}, 
abstract={Current brain-computer interface (BCI) systems suffer from high complex feature selectors in comparison to simple classifiers. Meanwhile, neurophysiological and experimental information are hard to be included in these two separate phases. In this paper, based on the hierarchical observation model, we proposed an empirical Bayesian linear discriminant analysis (BLDA), in which the neurophysiological and experimental priors are considered simultaneously; the feature selection, weighted differently, and classification are performed jointly, thus it provides a novel systematic algorithm framework which can utilize priors related to feature and trial in the classifier design in a BCI. BLDA was comparatively evaluated by two simulations of a two-class and a four-class problem, and then it was applied to two real four-class motor imagery BCI datasets. The results confirmed that BLDA is superior in accuracy and robustness to LDA, regularized LDA, and SVM.}, 
keywords={belief networks;brain-computer interfaces;electroencephalography;feature extraction;medical signal processing;neurophysiology;pattern classification;statistical analysis;brain-computer interface;empirical Bayesian framework;empirical Bayesian linear discriminant analysis;experimental method;feature selection;four-class motor imagery BCI dataset;hierarchical observation model;neurophysiological method;systematic algorithm framework;Bayesian framework;brain–computer interface (BCI);linear discriminant analysis (LDA);restricted maximum likelihood;Adult;Algorithms;Artificial Intelligence;Bayes Theorem;Electroencephalography;Evoked Potentials, Motor;Humans;Male;Motor Cortex;Pattern Recognition, Automated;User-Computer Interface;Young Adult}, 
doi={10.1109/TNSRE.2009.2027705}, 
ISSN={1534-4320}, 
month={Dec},}
@INPROCEEDINGS{7866142, 
author={Y. Nie and C. An and J. Huang and Z. Yan and Y. Han}, 
booktitle={2016 IEEE First International Conference on Data Science in Cyberspace (DSC)}, 
title={A Bidirectional LSTM Model for Question Title and Body Analysis in Question Answering}, 
year={2016}, 
pages={307-311}, 
abstract={Community Question Answering(CQA) services become popular recently. In most existing CQA service, the question posted by real user is usually consist by two parts: question title and body. And there is a semantic relation between them. It is necessary to analyze the relation between title and body. In this paper, we propose a deep neural network based method to analyze and quantify the relation between question title and body. The proposed method employs Bidirectional LSTM to read the title and body separately. And finally our model outputs a relevance score to measure the semantic relation between the question title and body. We evaluate our model on the Yahoo!Answers dataset and the experimental results show our method is effective.}, 
keywords={natural language processing;question answering (information retrieval);recurrent neural nets;CQA;bidirectional LSTM model;body analysis;community question answering;deep neural network;natural language processing;question title;Analytical models;Computational modeling;Computer architecture;Knowledge discovery;Neural networks;Semantics;Training}, 
doi={10.1109/DSC.2016.72}, 
month={June},}
@INPROCEEDINGS{6618600, 
author={Boyuan Wang and Lei Li and Xin Lin}, 
booktitle={2013 16th International Symposium on Wireless Personal Multimedia Communications (WPMC)}, 
title={Evaluating quality of Web2.0 UGC based on user authority and topic distribution}, 
year={2013}, 
pages={1-6}, 
abstract={User Generated Content (UGC) could be composed and published by every user. As a result, UGC quality may not be well-guaranteed. We put forward a synthetic evaluation method for UGC quality of content via user authority and topic distribution. We consider various features of users, including basic registration information, relative static online social relations and dynamic interactions between users. We adopt Link Analysis (LA) based on user credibility and Generalized Regression Neural Network (GRNN) to rate user authority and then analyze the probabilistic topic distribution of authors and UGC based on textual content using Author-Topic Model (AT). We've also integrated the author's actual behavior during the composing of UGC to compute the contribution degree of user authority. Finally, we combined user authority and contribution degree to evaluate overall quality of a multi-author UGC. Experiments with real dataset collected from TianYa have shown that the proposed method could evaluate UGC reasonably.}, 
keywords={Internet;neural nets;probability;regression analysis;AT model;GRNN;LA;TianYa dataset collection;Web2.0 UGC quality evaluation;author-topic model;dynamic interaction;generalized regression neural network;link analysis;probabilistic topic distribution;registration information;relative static online social relation;synthetic evaluation method;textual content;user authority;user credibility;user generated content;Continuous wavelet transforms;Fans;AT;GRNN;LA;Social Network Analysis}, 
ISSN={1347-6890}, 
month={June},}
@INPROCEEDINGS{6785891, 
author={P. Zhang and X. Wang and B. Li}, 
booktitle={2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)}, 
title={On predicting Twitter trend: Factors and models}, 
year={2013}, 
pages={1427-1429}, 
abstract={In this paper, we predict hashtag trend in Twitter network with two basic issues under investigation, i.e. trend factors and prediction models. To address the first issue, we consider different content and context factors by designing features from tweet messages, network topology, user behavior, etc. To address the second issue, we adopt prediction models that have different combinations of the two basic model properties, i.e. linearity and state-space. Experiments on large Twitter dataset show that both content and context factors can help trend prediction. However, the most relevant factors are derived from user behaviors on the specific trend. Non-linear models are significantly better than their linear counterparts, which can be further slightly improved by the adoption of state-space models.}, 
keywords={social networking (online);Twitter dataset;Twitter trend prediction;content factors;context factors;hashtag trend prediction;network topology;nonlinear models;prediction models;state-space models;trend factors;tweet messages;user behavior;Biological system modeling;Context;Indexes;Market research;Predictive models;Radio frequency;Twitter;Twitter;information diffusion;trend prediction}, 
doi={10.1145/2492517.2492576}, 
month={Aug},}
@INPROCEEDINGS{1017511, 
author={Hyewon Seo and L. Yahia-Cherif and T. Goto and N. Magnenat-Thalmann}, 
booktitle={Proceedings of Computer Animation 2002 (CA 2002)}, 
title={GENESIS: generation of E-population based on statistical information}, 
year={2002}, 
pages={81-85}, 
abstract={Simulating virtual environments populated with virtual but realistic crowds requires dozens of different face and body geometries. We present GENESIS (GENeration of E-population based on Statistical Information), an application that allows automatic generation of desired population models. The aim of this application is to generate any population group that is statistically calculated to satisfy given properties. We divide the population generation module into the face part and the body part. Each of them is based on a database, which is an organization of information collected and processed from the real population dataset. Upon the specification of the population parameters by the user, GENESIS transforms them onto a number Of queries to the underlying databases, and process the results from the database in a statistical manner, allowing dynamic generation of desired number of individuals while satisfying the given properties of on the overall population}, 
keywords={computer animation;virtual reality;E-Population;GENESIS;body geometries;database;dynamic generation;face geometries;statistical information;virtual environments simulation;Animation;Application software;Biological system modeling;Computational geometry;Computational modeling;Computer simulation;Humans;Relational databases;Solid modeling;Virtual environment}, 
doi={10.1109/CA.2002.1017511}, 
ISSN={1087-4844}, 
month={},}
@INPROCEEDINGS{7532433, 
author={H. Zhang and M. Xu}, 
booktitle={2016 IEEE International Conference on Image Processing (ICIP)}, 
title={Modeling temporal information using discrete fourier transform for recognizing emotions in user-generated videos}, 
year={2016}, 
pages={629-633}, 
abstract={With the widespread of user-generated Internet videos, emotion recognition in those videos attracts increasing research efforts. However, most existing works are based on framelevel visual features and/or audio features, which might fail to model the temporal information, e.g. characteristics accumulated along time. In order to capture video temporal information, in this paper, we propose to analyse features in frequency domain transformed by discrete Fourier transform (DFT features). Frame-level features are firstly extract by a pre-trained deep convolutional neural network (CNN). Then, time domain features are transferred and interpolated into DFT features. CNN and DFT features are further encoded and fused for emotion classification. By this way, static image features extracted from a pre-trained deep CNN and temporal information represented by DFT features are jointly considered for video emotion recognition. Experimental results demonstrate that combining DFT features can effectively capture temporal information and therefore improve emotion recognition performance. Our approach has achieved a state-of-the-art performance on the largest video emotion dataset (VideoEmotion-8 dataset), improving accuracy from 51.1% to 55.6%.}, 
keywords={computer vision;discrete Fourier transforms;emotion recognition;feature extraction;image capture;neural nets;CNN;DFT features;VideoEmotion-8 dataset;audio features;discrete Fourier transform;emotion classification;emotion recognition performance;frequency domain transformation;pre-trained deep convolutional neural network;static image feature extraction;temporal information modeling;user-generated Internet videos;user-generated videos;video emotion dataset;video emotion recognition;video temporal information capture;visual features;Discrete Fourier transforms;Emotion recognition;Encoding;Feature extraction;Frequency-domain analysis;Time-domain analysis;Videos;convolutional neural network;discrete Fourier transform;video emotion recognition}, 
doi={10.1109/ICIP.2016.7532433}, 
month={Sept},}
@INPROCEEDINGS{6975560, 
author={Q. Kong and W. Mao and D. Zeng and L. Wang}, 
booktitle={2014 IEEE Joint Intelligence and Security Informatics Conference}, 
title={Predicting Popularity of Forum Threads for Public Events Security}, 
year={2014}, 
pages={99-106}, 
abstract={Web user's online interactive behavior with others often makes some user generated contents popular. The modeling and prediction of the popularity of online content are an important research issue for many key application domains. In this paper, we focus on one form of user generated content, forum threads, and their popularity prediction for public events security. To predict the popularity of forum threads, we first define the popularity prediction problem, and identify the dynamic factors that affect the popularity of forum threads. Based on the information of dynamic evolution at the early stage, we propose a popularity prediction algorithm which makes use of the locality property and combines multiple dynamic factors. The proposed algorithm is further evaluated using the Tianya forum dataset on the discussions of various public events. The experimental results show that, compared to the baseline methods, our method achieves relatively better performance in predicting the popularity of forum threads on public events security.}, 
keywords={Internet;human factors;social networking (online);Tianya forum dataset;Web user online interactive behavior;dynamic factors;forum thread popularity prediction;online content popularity modeling;online content popularity prediction;public event security;user generated contents;Classification algorithms;Heuristic algorithms;Hidden Markov models;Instruction sets;Message systems;Prediction algorithms;Predictive models;modeling and prediction of popularity evolution;popularity of online content;social media analytics}, 
doi={10.1109/JISIC.2014.24}, 
month={Sept},}
@INPROCEEDINGS{7866136, 
author={J. Li and X. Li and M. Shi and M. Zhou and L. Lai}, 
booktitle={2016 IEEE First International Conference on Data Science in Cyberspace (DSC)}, 
title={On Improving a Microblog Ranking}, 
year={2016}, 
pages={268-274}, 
abstract={Microblog ranking is a hot research topic in recent years. Most of the related works apply TF-IDF metric for calculating content similarity while neglecting their semantic similarity. And most existing search engines which retrieve the microblog list by string matching the search keywords is not competent to provide a reliable list for users when dealing with polysemy and synonym. Besides, treating all the users with same authority for all topics is intuitively not ideal. In this paper, a comprehensive strategy for microblog ranking is proposed. First, we extend the conventional TF-IDF based content similarity with exploiting knowledge from WordNet. Then, we further incorporate a new feature for microblog ranking that is the topical relation between search keyword and its retrieval. Author topical authority is also incorporated into the ranking framework as an important feature for microblog ranking. Gradient Boosting Decision Tree(GBDT), then is employed to train the ranking model with multiple features involved. We conduct thorough experiments on a large-scale real-world Twitter dataset and demonstrate that our proposed approach outperform a number of existing approaches in discovering higher quality and more related microblogs.}, 
keywords={Web sites;decision trees;gradient methods;information retrieval;search engines;search problems;semantic Web;GBDT;TF-IDF metric;WordNet;gradient boosting decision tree;microblog ranking;search keywords;semantic similarity;string matching;Gaussian distribution;Measurement;Real-time systems;Reliability;Search engines;Semantics;Twitter}, 
doi={10.1109/DSC.2016.92}, 
month={June},}
@INPROCEEDINGS{4648841, 
author={K. Assaleh and T. Shanableh and M. Fanaswala and H. Bajaj and F. Amin}, 
booktitle={2008 5th International Symposium on Mechatronics and Its Applications}, 
title={Vision-based system for continuous Arabic Sign Language recognition in user dependent mode}, 
year={2008}, 
pages={1-5}, 
abstract={Existing work on Arabic Sign Language recognition focuses on finger spelling and isolated gestures. In this work we extend vision-based existing solutions to recognition of continuous signing. As such we have collected and labeled the first video-based continuous Arabic Sign Language dataset. We intend to make the collected dataset available for the research community. The proposed solution extracts the motion from the video-based sentences by means of thresholding the forward prediction error between consecutive images. Such prediction errors are then transformed into the frequency domain and Zonal coded. We use Hidden Markov Models for model training and classification. The experimental results show an average word recognition rate of 94%, keeping in the mind the use of a high perplexity vocabulary and unrestrictive grammar.}, 
keywords={computer vision;feature extraction;frequency-domain analysis;gesture recognition;hidden Markov models;image classification;image motion analysis;image segmentation;learning (artificial intelligence);natural languages;video signal processing;Zonal coding;finger spelling;forward prediction error;frequency domain analysis;hidden Markov model;image classification;image thresholding;isolated gesture;machine learning;motion extraction;user dependent mode;video-based continuous Arabic sign language recognition;vision-based system;Application software;Computer science;Feature extraction;Fingers;Frequency domain analysis;Handicapped aids;Hidden Markov models;Mechatronics;Vectors;Vocabulary}, 
doi={10.1109/ISMA.2008.4648841}, 
month={May},}
@INPROCEEDINGS{7830348, 
author={Y. C. Ho and X. Liu and J. Y. J. Hsu and T. S. Huang}, 
booktitle={2016 9th International Symposium on Computational Intelligence and Design (ISCID)}, 
title={Consensus Oriented Recommendation}, 
year={2016}, 
volume={1}, 
pages={294-297}, 
abstract={Recommender systems are useful tools that help people to filter and explore massive information. While most recommender systems focus on providing recommendations for individuals, people's minds are easily altered and dominated by crowds, especially in a socialized environment. In addition to fulfill personalized intentions, more considerate recommendations, which maximize satisfactions of both individuals and common interests within crowds, are expected in various daily-life scenarios: e.g., scenic spots recommendation to help trip planning making for a group of friends, and movie/TV program recommendation for family members. In this paper, we aim at advancing the group recommendation and propose a novel approach which predicts user preferences with the consideration of "group consensus". We combine observations from real-world group discussions with the model learning and conduct several experiments on a real-world dataset. The results show that the proposed approach benefits both individual and group recommendation and surpasses the state-of-the-art approach in terms of individual preference prediction.}, 
keywords={decision making;recommender systems;TV program recommendation;consensus oriented recommendation;group consensus;group recommendation;individual preference prediction;personalized intentions;recommender systems;spot recommendation;trip planning making;Decision making;Frequency modulation;Mathematical model;Motion pictures;Predictive models;Recommender systems;Training;Collaborative Filtering;Consensus Decision-making;Group Recommendation;Recommender System}, 
doi={10.1109/ISCID.2016.1074}, 
month={Dec},}
@INPROCEEDINGS{6928655, 
author={Y. Zhang and L. Wang and L. Hu and X. Wang and M. Chen}, 
booktitle={10th International Conference on Heterogeneous Networking for Quality, Reliability, Security and Robustness}, 
title={COMER: Cloud-based medicine recommendation}, 
year={2014}, 
pages={24-30}, 
abstract={With the development of e-commerce, a growing number of people prefer to purchase medicine online for the sake of convenience. However, it is a serious issue to purchase medicine blindly without necessary medication guidance. In this paper, we propose a novel cloud-based medicine recommendation, which can recommend users with top-N related medicines according to symptoms. Firstly, we cluster the drugs into several groups according to the functional description information, and design a basic personalized medicine recommendation based on user collaborative filtering. Then, considering the shortcomings of collaborative filtering algorithm, such as computing expensive, cold start, and data sparsity, we propose a cloud-based approach for enriching end-user Quality of Experience (QoE) of medicine recommendation, by modeling and representing the relationship of the user, symptom and medicine via tensor decomposition. Finally, the proposed approach is evaluated with experimental study based on a real dataset crawled from Internet.}, 
keywords={drugs;filtering theory;medical diagnostic computing;quality of experience;COMER;QoE;cloud-based medicine recommendation;cold start;computing expensive;data sparsity;functional description information;personalized medicine recommendation;quality of experience;tensor decomposition;user collaborative filtering;Clustering algorithms;Collaboration;Drugs;Mathematical model;Medical diagnostic imaging;Tensile stress;Vectors;Cloud;Clustering;Collaborative Filtering;Medicine Recommendation;QoE;Tensor Decomposition}, 
doi={10.1109/QSHINE.2014.6928655}, 
month={Aug},}
@INPROCEEDINGS{5967058, 
author={M. Pańka and M. Chlebiej and K. Benedyczak and P. Bała}, 
booktitle={2011 Proceedings of the 34th International Convention MIPRO}, 
title={Visualization of multidimensional data on distributed mobile devices using interactive video streaming techniques}, 
year={2011}, 
pages={246-251}, 
abstract={Remote visualization of large datasets has been a challenge for distributed systems for a long time. This challenge is getting even bigger when visualization refers to devices with limited capabilities, like CPU and GPU power, number of RAM or screen size. In this paper we present a distributed system we have developed for interactive visualization of remote datasets on variety of modern mobile devices, including laptops, tablets and phones. In our system all the data are rendered on dedicated servers, compressed on-the-fly using a video codec and pushed to client as a single video stream. Based on this model we have taken off most of the computational power from client's devices, leaving them with a video decompression. We were also able to achieve very high frame rates and video quality, dynamically adapted to device capabilities and current network bandwidth of a client. Our system can be used with almost any kind of data, including 2D, 3D and even animated 3D data. All of them are being processed in real time based on user inputs, with minor latency, allowing interactive visualization. At the end of this paper we also present some preliminary results of system performance gained using sample, multidimensional medical datasets.}, 
keywords={data compression;data visualisation;interactive video;mobile computing;video codecs;video coding;video streaming;CPU;GPU;RAM;animated 3D data;distributed mobile device;interactive remote dataset visualization;interactive video streaming technique;laptop;mobile phone;multidimensional data visualization;multidimensional medical datasets;tablet;video codec;video decompression;video quality;Data visualization;Encoding;Mobile handsets;Real time systems;Servers;Streaming media;Three dimensional displays}, 
month={May},}
@INPROCEEDINGS{6011895, 
author={L. Li and Z. Wu and Z. J. Zha and S. Jiang and Q. Huang}, 
booktitle={2011 IEEE International Conference on Multimedia and Expo}, 
title={Matching Content-based Saliency Regions for partial-duplicate image retrieval}, 
year={2011}, 
pages={1-6}, 
abstract={In traditional partial-duplicate image retrieval, images are commonly represented using the Bag-of-Visual-Words (BOV) model built from image local features, such as SIFT. Actually, there is only a small similar portion between partial-duplicate images so that such representation on the whole image is not adequate for the partial-duplicate image retrieval task. In this paper, we propose a novel perspective to retrieval partial-duplicate images with Contented-based Saliency Region (CSR). CSRs are such sub-regions with abundant visual content and high visual attention in the image. The content of CSR is represented with the BOV model while saliency analysis is employed to ensure the high visual attention of CSR. Each CSR is regarded as an independent unit to be retrieved in the dataset. To effectively retrieve the CSRs, we design a relative saliency ordering constraint, which captures a weak saliency relative layout among interest points in the CSR. Comparison experiments with four state-of-the-art methods on the standard partial-duplicate image dataset clearly verify the effectiveness of our scheme. Further, our approach can provide a more diverse retrieval result, which facilitates the interaction of portable-device users.}, 
keywords={Content-based Saliency Region;Partial-Duplicate Image Retrieval;Relative Saliency Ordering Constraint}, 
doi={10.1109/ICME.2011.6011895}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{6909799, 
author={E. Ahmed and S. Cohen and B. Price}, 
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Semantic Object Selection}, 
year={2014}, 
pages={3150-3157}, 
abstract={Interactive object segmentation has great practical importance in computer vision. Many interactive methods have been proposed utilizing user input in the form of mouse clicks and mouse strokes, and often requiring a lot of user intervention. In this paper, we present a system with a far simpler input method: the user needs only give the name of the desired object. With the tag provided by the user we do a text query of an image database to gather exemplars of the object. Using object proposals and borrowing ideas from image retrieval and object detection, the object is localized in the target image. An appearance model generated from the exemplars and the location prior are used in an energy minimization framework to select the object. Our method outperforms the state-of-the-art on existing datasets and on a more challenging dataset we collected.}, 
keywords={feature selection;image retrieval;image segmentation;interactive systems;object detection;visual databases;appearance model;computer vision;energy minimization framework;image database;image retrieval;interactive methods;interactive object segmentation;mouse clicks;mouse strokes;object detection;object localization;object proposals;semantic object selection;text query;user input;user intervention;Computational modeling;Image retrieval;Image segmentation;Proposals;Search problems;Semantics;Image Retrieval;Interactive Segmentation;Object Segmentation;Semantic Selection}, 
doi={10.1109/CVPR.2014.403}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{6111477, 
author={L. Ding and A. Yilmaz and R. Yan}, 
journal={IEEE Transactions on Image Processing}, 
title={Interactive Image Segmentation Using Dirichlet Process Multiple-View Learning}, 
year={2012}, 
volume={21}, 
number={4}, 
pages={2119-2129}, 
abstract={Segmenting semantically meaningful whole objects from images is a challenging problem, and it becomes especially so without higher level common sense reasoning. In this paper, we present an interactive segmentation framework that integrates image appearance and boundary constraints in a principled way to address this problem. In particular, we assume that small sets of pixels, which are referred to as seed pixels, are labeled as the object and background. The seed pixels are used to estimate the labels of the unlabeled pixels using Dirichlet process multiple-view learning, which leverages 1) multiple-view learning that integrates appearance and boundary constraints and 2) Dirichlet process mixture-based nonlinear classification that simultaneously models image features and discriminates between the object and background classes. With the proposed learning and inference algorithms, our segmentation framework is experimentally shown to produce both quantitatively and qualitatively promising results on a standard dataset of images. In particular, our proposed framework is able to segment whole objects from images given insufficient seeds.}, 
keywords={image segmentation;learning (artificial intelligence);probability;Dirichlet process;boundary constraints;dirichlet process multiple view learning;image appearance;interactive image segmentation;unlabeled pixels;Computational modeling;Image color analysis;Image segmentation;Labeling;Logistics;Training;Vectors;Dirichlet processes;image segmentation;probabilistic models;Algorithms;Artificial Intelligence;Image Enhancement;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface}, 
doi={10.1109/TIP.2011.2181398}, 
ISSN={1057-7149}, 
month={April},}
@INPROCEEDINGS{7816646, 
author={M. Dadvar and A. Niamir}, 
booktitle={2016 27th International Workshop on Database and Expert Systems Applications (DEXA)}, 
title={Adopting MaxEnt to Identification of Bullying Incidents in Social Networks}, 
year={2016}, 
pages={186-189}, 
abstract={Bullying is a widespread problem in cyberspace and social networks. Therefore, in the recent years many studies have been dedicated to cyberbullying. Lack of appropriate dataset, due to variety of reasons, is one of the major obstacles faced in most studies. In this work we suggest that to overcome some of these barriers a model should be employed which is minimally affected by prevalence and small sample size. To this end we adopted the use of the Maximum Entropy method (MaxEnt) to identify the bully users in YouTube. The final results were compared with the commonly used methods. All models provided reasonable prediction of the bullying incidents. MaxEnt models had the highest discrimination capacity of bullying posts and the lowest sensitivity towards prevalence. We demonstrate that MaxEnt can be successfully adopted to cyberbullying studies with imbalanced datasets.}, 
keywords={maximum entropy methods;social networking (online);MaxEnt model;YouTube;cyberbullying;cyberspace;maximum entropy method;social networks;Calibration;Data models;Entropy;Predictive models;Social network services;Support vector machines;Vegetation;Cyberbullying;Maximum Entropy;Prevalence;Sample Size;Sentiment Analsysis;Social Networks;Text Retreival;YouTube}, 
doi={10.1109/DEXA.2016.048}, 
month={Sept},}
@INPROCEEDINGS{6889819, 
author={M. Damasceno and A. M. P. Canuto}, 
booktitle={2014 International Joint Conference on Neural Networks (IJCNN)}, 
title={An empirical analysis of ensemble systems in cancellable behavioural biometrics: A touch screen dataset}, 
year={2014}, 
pages={2661-2668}, 
abstract={This paper presents an experimental analysis of a revocable biometric verification problem using ensemble systems. Behavioural Biometric-based systems are a future emergent area on identification, verification and access control systems of users. However, there is still progress to be done in this field, specially related to system security and acceptable results for practical use. Cancellable Biometrics is a alternative solution to the security problem of biometric data. This technique consists of applying transformation functions to biometric data in order to protect the original characteristics of biometric template. In this case, if biometric template has compromised, a new representation of original biometric data can be generated. Although cancellable biometrics were proposed to solve privacy concerns, this concept raises new issues, becoming the authentication problem more complex and difficult to solve. Thus, more effective authentication structures are needed to perform these tasks. This work aims to investigate the use of ensemble systems in cancellable behavioural biometric system used by million people (touchscreen devices). Apart this, we also present an empirical analysis, comparing the ensemble structures with single classification algorithms.}, 
keywords={biometrics (access control);pattern classification;touch sensitive screens;biometric template;cancellable behavioural biometrics;ensemble structures;ensemble systems;revocable biometric verification problem;single classification algorithms;touch screen dataset;Authentication;Bioinformatics;Biological system modeling;Iris recognition;Vectors}, 
doi={10.1109/IJCNN.2014.6889819}, 
ISSN={2161-4393}, 
month={July},}
@INPROCEEDINGS{6130354, 
author={A. S. Voulodimos and A. D. Doulamis and D. I. Kosmopoulos and T. A. Varvarigou}, 
booktitle={2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)}, 
title={Video summarization guiding evaluative rectification for industrial activity recognition}, 
year={2011}, 
pages={950-957}, 
abstract={In this paper we present a video summarization method that extracts key-frames from industrial surveillance videos, thus dramatically reducing the number of frames without significant loss of semantic content. We propose to use the produced summaries as training set for neural network based Evaluative Rectification. Evaluative Rectification is a method that exploits an expert user's feedback regarding the correctness of an activity recognition framework on part of the data in order to enhance future classification results. The size of the training sample set usually depends on the topology of the network and on the complexity of the environment and activities observed. However, as is shown by the experiments conducted in a real-world industrial activity recognition dataset, using a much smaller but representative sample stemming from our summarization technique leads to significantly higher accuracy rates than those attained by a same size but randomly chosen set. To obtain comparable improvement in accuracy without the summarization technique, the experiments show that a far larger training sample set is needed, therefore requiring significantly increased human resources and computational cost.}, 
keywords={feature extraction;image recognition;neural nets;production engineering computing;video signal processing;activity recognition framework;evaluative rectification method;industrial activity recognition;industrial surveillance video;key-frame extraction;neural network;video summarization;Biological neural networks;Complexity theory;Hidden Markov models;Training;Vectors;Visualization;Welding}, 
doi={10.1109/ICCVW.2011.6130354}, 
month={Nov},}
@ARTICLE{5730497, 
author={J. Huang and X. Yang and X. Fang and W. Lin and R. Zhang}, 
journal={IEEE Transactions on Multimedia}, 
title={Integrating Visual Saliency and Consistency for Re-Ranking Image Search Results}, 
year={2011}, 
volume={13}, 
number={4}, 
pages={653-661}, 
abstract={In this paper, we propose a new algorithm for image re-ranking in web image search applications. The proposed method focuses on investigating the following two mechanisms: 1) Visual consistency. In most web image search cases, the images that closely related to the search query are visually similar. These visually consistent images which occur most frequently in the first few web pages will be given higher ranks. 2) Visual saliency. From visual aspect, it is obvious that salient images would be easier to catch users' eyes, and it is observed that these visually salient images in the front pages are often relevant to the user's query. By integrating the above two mechanisms, our method can efficiently re-rank the images from search engines and obtain a more satisfactory search result. Experimental results on a real-world web image dataset demonstrate that our approach can effectively improve the performance of image retrieval.}, 
keywords={Internet;image retrieval;query processing;search engines;Web image search;image retrieval;image search result re-ranking;real-world Web image dataset;search engines;visual consistency;visual saliency;Equations;Feature extraction;Image color analysis;Image edge detection;Mathematical model;Search engines;Visualization;Random walk;re-ranking;visual consistency;visual saliency}, 
doi={10.1109/TMM.2011.2127463}, 
ISSN={1520-9210}, 
month={Aug},}
@ARTICLE{6856208, 
author={C. Jiang and Y. Chen and K. J. R. Liu}, 
journal={IEEE Transactions on Signal Processing}, 
title={Evolutionary Dynamics of Information Diffusion Over Social Networks}, 
year={2014}, 
volume={62}, 
number={17}, 
pages={4573-4586}, 
abstract={Current social networks are of extremely large-scale generating tremendous information flows at every moment. How information diffuses over social networks has attracted much attention from both industry and academics. Most of the existing works on information diffusion analysis are based on machine learning methods focusing on social network structure analysis and empirical data mining. However, the network users' decisions, actions, and socio-economic interactions are generally ignored by most of existing works. In this paper, we propose an evolutionary game theoretic framework to model the dynamic information diffusion process in social networks. Specifically, we derive the information diffusion dynamics in complete networks, uniform degree, and nonuniform degree networks, with the highlight of two special networks, the Erdös-Rényi random network and the Barabási-Albert scale-free network. We find that the dynamics of information diffusion over these three kinds of networks are scale-free and all the three dynamics are same with each other when the network scale is sufficiently large. To verify our theoretical analysis, we perform simulations for the information diffusion over synthetic networks and real-world Facebook networks. Moreover, we also conduct an experiment on a Twitter hashtags dataset, which shows that the proposed game theoretic model can well fit and predict the information diffusion over real social networks.}, 
keywords={complex networks;data mining;evolutionary computation;game theory;learning (artificial intelligence);social networking (online);socio-economic effects;Barabasi-Albert scale-free network;Erdos-Renyi random network;Twitter hashtag dataset;data mining;dynamic information diffusion process;evolutionary dynamics;evolutionary game theoretic framework;game theoretic model;information diffusion analysis;information diffusion dynamics;machine learning methods;network user decisions;real-world Facebook networks;social network structure analysis;social networks;socio-economic interactions;synthetic networks;Analytical models;Diffusion processes;Games;Sociology;Statistics;Twitter;Evolutionary game;game theory;information diffusion;information spreading;social networks}, 
doi={10.1109/TSP.2014.2339799}, 
ISSN={1053-587X}, 
month={Sept},}
@ARTICLE{5729356, 
author={C. Aperjis and R. Johari and M. J. Freedman}, 
journal={IEEE/ACM Transactions on Networking}, 
title={Bilateral and Multilateral Exchanges for Peer-Assisted Content Distribution}, 
year={2011}, 
volume={19}, 
number={5}, 
pages={1290-1303}, 
abstract={Users of the BitTorrent file-sharing protocol and its variants are incentivized to contribute their upload capacity in a bilateral manner: Downloading is possible in return for uploading to the same user. An alternative is to use multilateral exchange to match user demand for content to available supply at other users in the system. We provide a formal comparison of peer-to-peer system designs based on bilateral exchange with those that enable multilateral exchange via a price-based market mechanism to match supply and demand. First, we compare the two types of exchange in terms of the equilibria that arise. A multilateral equilibrium allocation is Pareto-efficient, while we demonstrate that bilateral equilibrium allocations are not Pareto-efficient in general. We show that Pareto efficiency represents the “gap” between bilateral and multilateral equilibria: A bilateral equilibrium allocation corresponds to a multilateral equilibrium allocation if and only if it is Pareto-efficient. Our proof exploits the fact that Pareto efficiency implies reversibility of an appropriately constructed Markov chain. Second, we compare the two types of exchange through the expected percentage of users that can trade in a large system, assuming a fixed file popularity distribution. Our theoretical results as well as analysis of a BitTorrent dataset provide quantitative insight into regimes where bilateral exchange may perform quite well even though it does not always give rise to Pareto-efficient equilibrium allocations.}, 
keywords={Markov processes;Pareto distribution;peer-to-peer computing;pricing;supply and demand;BitTorrent file-sharing protocol;Markov chain;Pareto efficiency;bilateral equilibrium allocations;bilateral exchanges;downloading;fixed file popularity distribution;multilateral equilibrium allocation;multilateral exchanges;peer-assisted content distribution;peer-to-peer system design;price-based market mechanism;supply and demand;upload capacity;Biological system modeling;Convergence;Markov processes;Optimization;Peer to peer computing;Protocols;Resource management;Asymptotic analysis;Markov processes;market equillibria;peer-to-peer systems;random graphs}, 
doi={10.1109/TNET.2011.2114898}, 
ISSN={1063-6692}, 
month={Oct},}
@INPROCEEDINGS{6785694, 
author={S. Y. Bhat and M. Abulaish}, 
booktitle={2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)}, 
title={Community-based features for identifying spammers in Online Social Networks}, 
year={2013}, 
pages={100-107}, 
abstract={The popularity of Online Social Networks (OSNs) is often faced with challenges of dealing with undesirable users and their malicious activities in the social networks. The most common form of malicious activity over OSNs is spamming wherein a bot (fake user) disseminates content, malware/viruses, etc. to the legitimate users of the social networks. The common motives behind such activity include phishing, scams, viral marketing and so on which the recipients do not indent to receive. It is thus a highly desirable task to devise techniques and methods for identifying spammers (spamming accounts) in OSNs. With an aim of exploiting social network characteristics of community formation by legitimate users, this paper presents a community-based framework to identify spammers in OSNs. The framework uses community-based features of OSN users to learn classification models for identification of spamming accounts. The preliminary experiments on a real-world dataset with simulated spammers reveal that proposed approach is promising and that using community-based node features of OSN users can improve the performance of classifying spammers and legitimate users.}, 
keywords={computer crime;pattern classification;social networking (online);unsolicited e-mail;OSNs;bot;classification models;community formation;community-based features;content dissemination;fake user;legitimate user classification;malicious activity;malware;online social networks;phishing;scams;spammer classification;spammer identification;spamming account identification;viral marketing;viruses;Communities;Equations;Feature extraction;Mathematical model;Social network services;Unsolicited electronic mail;Community-Based feature identification;Social network analysis;Social network security;Spammer Detection}, 
doi={10.1145/2492517.2492567}, 
month={Aug},}
@INPROCEEDINGS{7033517, 
author={M. Fahimi and M. V. Jahan and M. N. Torshiz}, 
booktitle={2014 International Congress on Technology, Communication and Knowledge (ICTCK)}, 
title={Discovery of the triadic frequent closed patterns based on Hidden Markov Model in folksonomy}, 
year={2014}, 
pages={1-5}, 
abstract={With rise of web 2.0, its associated user-centric applications have attracted a lot of users. Folksonomy plays an important role in these systems, which is made of labeling data. Discovery triadic frequent closed patterns is an important tool in knowledge discovery in folksonomy. The huge volume of data and the number of dimensions in these systems, including users, tags and resources are challenging for data mining. In this paper, a method for discovering all triadic frequent closed patterns based on Hidden Markov Model in folksonomy is proposed. By extracting useful data from dataset, the proposed method emprises to build Hidden Markov Model on the two dimensions, then with inference from created hidden model discover triadic frequent closed patterns through applying third dimension on the results. In fact, extracting useful data in the first step and using viterbi based algorithm, for inference, regularly are pruned dataset and are causes for triadic frequent closed patterns to be discovered more quickly. Testing on a real data set taken from "Del.icio.us" website and comparing the results with the same algorithm in the field of folksonomy called "Trias" show that the proposed method in terms of the time, can extract all triadic frequent closed patterns more effectively.}, 
keywords={data mining;hidden Markov models;pattern classification;Trias folksonomy;Web 2.0;data extraction;data labeling;data mining;hidden Markov model;knowledge discovery;triadic frequent closed pattern discovery;viterbi based algorithm;Algorithm design and analysis;Data mining;Educational institutions;Hidden Markov models;Itemsets;Knowledge discovery}, 
doi={10.1109/ICTCK.2014.7033517}, 
month={Nov},}
@INPROCEEDINGS{6204982, 
author={Seyed Mohammad Bidoki and Seyed Mohammad Reza Moosavi}, 
booktitle={2012 International Conference on Information Retrieval Knowledge Management}, 
title={IDUF: An active learning based scenario for relevance feedback query expansion}, 
year={2012}, 
pages={244-248}, 
abstract={In usual Information Retrieval (IR) systems, the user query is represented in the form of a keyword set. Information resources are retrieved according to their similarities to this query. Consequently if query is not declared with appropriate terms, retrieved results would not be satisfactory. Therefore query refinement procedures are incorporated to improve the efficiency of the IR systems. In this paper, an active learning approach has been proposed for query expansion (QE) according to user feedbacks. A novel document selection procedure is used to acquire user feedbacks. In this procedure, firstly, the whole set of documents are classified according to existing feedbacks. Then a set of documents which are classified with low certainty and do not produce redundant information are selected as informative documents to get user feedbacks. In this scenario, the number of feedbacks is equal to customary relevance feedback methods but retrieval system would gain more useful information. Experimental results on Reuters-21578 full-text dataset demonstrate considerable improvement in the performance of retrieval system. It is shown experimentally that the proposed method can effectively employ user's feedback in discovering the favorable hidden concepts too.}, 
keywords={information resources;learning (artificial intelligence);pattern classification;query processing;relevance feedback;text analysis;IR system;active learning approach;document classification;document selection procedure;information resource;information retrieval system;informative document;query expansion;query refinement procedure;relevance feedback;user feedback;Computational modeling;Context;Information retrieval;Information services;Radio frequency;Text categorization;Vectors;Batch-Mode Active Learning;Query Expansion;Relevance Feedback;Reuters-21578;Text Classification;Text Retrieval}, 
doi={10.1109/InfRKM.2012.6204982}, 
month={March},}
@INPROCEEDINGS{7501689, 
author={R. P. Barnwal and N. Ghosh and S. K. Ghosh and S. K. Das}, 
booktitle={2016 IEEE International Conference on Smart Computing (SMARTCOMP)}, 
title={Enhancing Reliability of Vehicular Participatory Sensing Network: A Bayesian Approach}, 
year={2016}, 
pages={1-8}, 
abstract={Participatory sensing (PS) is an emerging socio-technological paradigm in which citizens voluntarily participate and contribute to a distributed information system using applications installed in their hand-held devices. It can be found in a number of real-life applications, viz. traffic monitoring, air/sound pollution, garbage monitoring, social networking, commodity pricing, and so on. In these systems, information sensed by the user helps the peers in decision making. Present work considers vehicular participatory sensing systems, where registered user senses (perceives) the traffic incident and submits its report(s) to a PS application server. PS application server in turn, broadcasts those reports as alerts to its subscribers. To promote the participation, the PS systems used to have incentive schemes for the participants. However, a common problem in participatory sensing is the generation of false reports either due to wrong perception of an event or to maliciously increase the degree of participation to gain undue incentives. Such false reports make the usage of the PS system unreliable and vulnerable to the illusion attack. This work proposes a novel approach to make PS applications more reliable by identifying and filtering out the falsely reported event through automated confidence assignment based on a probabilistic model. Waze traffic alerts have been used as the dataset to validate the proposed filtering mechanism. Finally, simulation-based experiments and performance evaluation have been done to demonstrate that the proposed approach is relatively accurate.}, 
keywords={probability;road traffic;traffic information systems;Bayesian approach;PS application server;PS paradigm;Waze traffic alerts;distributed information system;filtering mechanism;probabilistic model;socio-technological paradigm;vehicular participatory sensing network;Estimation;Monitoring;Reliability;Roads;Sensors;Servers;Vehicles}, 
doi={10.1109/SMARTCOMP.2016.7501689}, 
month={May},}
@INPROCEEDINGS{6217920, 
author={F. Jiang and S. S. H. Ling and J. I. Agbinya}, 
booktitle={7th International Conference on Broadband Communications and Biomedical Applications}, 
title={A nature inspired anomaly detection system using multiple detection engines}, 
year={2011}, 
pages={200-205}, 
abstract={The rapid growth of computer networks presents challenges to the single detection engine based system, which has been insufficient in meeting end-users' requirements in the large-scale distributed complex network. In this paper, multiple detection engines with multi-layered intrusion detection mechanisms are proposed. The principle is to coordinate the results from each single-engine intrusion alert system, by seamlessly integrating with the multiple layered distributed service-oriented structure. An improved hidden Markov model (HMM) is created for the detection engine which is capable of the immunology-based self/nonself discrimination. The classifications of normal and abnormal behaviour of system calls are further examined by an advanced fuzzy-based inference process called HPSOWM. Considering a real benchmark dataset from the public domain, our experimental results show that the proposed scheme can greatly shorten the training time of HMM and reduce the false positive rate significantly. The proposed HPSOWM especially works for the efficient classification of unknown behaviors and malicious attacks.}, 
keywords={computer network security;fuzzy reasoning;hidden Markov models;particle swarm optimisation;service-oriented architecture;wavelet transforms;HMM;HPSOWM;advanced fuzzy-based inference process;computer networks;false positive rate;hidden Markov model;hybrid particle swarm optimization with wavelet mutation approach;immunology-based self-nonself discrimination;large-scale distributed complex network;multilayered intrusion detection mechanisms;multiple detection engines;multiple layered distributed service-oriented structure;nature inspired anomaly detection system;single detection engine based system;single-engine intrusion alert system;Biological system modeling;Computational modeling;Engines;Hidden Markov models;Immune system;Intrusion detection;Training}, 
doi={10.1109/IB2Com.2011.6217920}, 
month={Nov},}
@ARTICLE{7420734, 
author={F. Patrona and A. Iosifidis and A. Tefas and N. Nikolaidis and I. Pitas}, 
journal={IEEE Transactions on Multimedia}, 
title={Visual Voice Activity Detection in the Wild}, 
year={2016}, 
volume={18}, 
number={6}, 
pages={967-977}, 
abstract={The visual voice activity detection (V-VAD) problem in unconstrained environments is investigated in this paper. A novel method for V-VAD in the wild, exploiting local shape and motion information appearing at spatiotemporal locations of interest for facial video segment description and the bag of words model for facial video segment representation, is proposed. Facial video segment classification is subsequently performed using the state-of-the-art classification algorithms. Experimental results on one publicly available V-VAD dataset denote the effectiveness of the proposed method, since it achieves better generalization performance in unseen users, when compared to the recently proposed state-of-the-art methods. Additional results on a new unconstrained dataset provide evidence that the proposed method can be effective even in such cases in which any other existing method fails.}, 
keywords={face recognition;feedforward neural nets;image classification;image motion analysis;image representation;image segmentation;learning (artificial intelligence);object detection;video signal processing;V-VAD dataset;V-VAD method;bag-of-words model;classification algorithms;facial video segment classification;facial video segment description;facial video segment representation;motion information;shape information;visual voice activity detection;Feature extraction;Motion segmentation;Shape;Spatiotemporal phenomena;Speech;Speech recognition;Visualization;Action Recognition;Action recognition;Bag of Words model;Voice Activity Detection in the wild;bag of words model;kernel extreme learning machine;space-time interest points;voice activity detection in the wild}, 
doi={10.1109/TMM.2016.2535357}, 
ISSN={1520-9210}, 
month={June},}
@ARTICLE{6777335, 
author={C. Jiang and Y. Chen and K. J. R. Liu}, 
journal={IEEE Journal of Selected Topics in Signal Processing}, 
title={Graphical Evolutionary Game for Information Diffusion Over Social Networks}, 
year={2014}, 
volume={8}, 
number={4}, 
pages={524-536}, 
abstract={Social networks have become ubiquitous in our daily life, as such they have attracted great research interests recently. A key challenge is that it is of extremely large-scale with tremendous information flow, creating the phenomenon of “Big Data.” Under such a circumstance, understanding information diffusion over social networks has become an important research issue. Most of the existing works on information diffusion analysis are based on either network structure modeling or empirical approach with dataset mining. However, the information diffusion is also heavily influenced by network users' decisions, actions and their socio-economic connections, which is generally ignored in existing works. In this paper, we propose an evolutionary game theoretic framework to model the dynamic information diffusion process in social networks. Specifically, we analyze the framework in uniform degree and non-uniform degree networks and derive the closed-form expressions of the evolutionary stable network states. Moreover, the information diffusion over two special networks, Erdös-Rényi random network and the Barabási-Albert scale-free network, are also highlighted. To verify our theoretical analysis, we conduct experiments by using both synthetic networks and real-world Facebook network, as well as real-world information spreading dataset of Memetracker. Experiments shows that the proposed game theoretic framework is effective and practical in modeling the social network users' information forwarding behaviors.}, 
keywords={Big Data;data mining;evolutionary computation;game theory;social networking (online);Barabási-Albert scale-free network;Big Data;Erdos-Rényi random network;Facebook network;Memetracker;closed-form expressions;dataset mining;dynamic information diffusion process;evolutionary stable network states;graphical evolutionary game theory framework;information diffusion analysis;network structure modeling;nonuniform degree networks;real-world information spreading dataset;social network user information forwarding behaviors;synthetic networks;uniform degree networks;Biological system modeling;Diffusion processes;Games;Social network services;Sociology;Statistics;Tin;Evolutionary game;game theory;information diffusion;information spreading;social networks}, 
doi={10.1109/JSTSP.2014.2313024}, 
ISSN={1932-4553}, 
month={Aug},}
@INPROCEEDINGS{7034849, 
author={A. Javed and H. Larijani and A. Ahmadinia and R. Emmanuel}, 
booktitle={2014 IEEE Fourth International Conference on Big Data and Cloud Computing}, 
title={Comparison of the Robustness of RNN, MPC and ANN Controller for Residential Heating System}, 
year={2014}, 
pages={604-611}, 
abstract={In this paper, a novel random neural network (RNN) controller is proposed to maintain a comfortable indoor environment in a single storey residential building having four rooms fitted with radiators for heating. This controller considers the effect of outside temperature and solar radiations on the building and is capable of maintaining a comfortable indoor environment on the basis of a PMV-based set point. The RNN controller is trained with a 30 day dataset from the living room of the building and the performance of the controller is evaluated by testing the controller in all four rooms of the building for 100 days. It is found that the RNN controller is not only capable of maintaining comfortable indoor environment as suggested by PMV-based set point but can also adjust the room temperature to a lower set point (not included in the training set) required by the user for unoccupied rooms. The RNN controller is further compared with similar artificial neural network (ANN) controller and model predictive control (MPC) controller. The results show that for maintaining comfortable indoor environment, the performance of the RNN controller is approximately equivalent to the MPC controller for the set points not covered in the training set, while ANN controller failed to maintain accurate comfortable environment for the operating points not covered in the training phase.}, 
keywords={heat systems;neurocontrollers;predictive control;space heating;temperature control;ANN controller;MPC;PMV-based set point;RNN;model predictive control;random neural network controller;residential heating system;single storey residential building;Artificial neural networks;Biological neural networks;Buildings;Heating;Neurons;Predictive control;Artificial neural networks;Model Predictive Control;Random neural networks;building energy management system;control;modelling}, 
doi={10.1109/BDCloud.2014.20}, 
month={Dec},}
@INPROCEEDINGS{1283438, 
author={M. A. Chupa and R. J. Moorhead and J. A. Mogill and J. F. Shriver and D. W. Irby and P. M. Flynn}, 
booktitle={Oceans 2003. Celebrating the Past ... Teaming Toward the Future (IEEE Cat. No.03CH37492)}, 
title={EnVis/Hum: high-resolution ocean model visualization and display}, 
year={2003}, 
volume={2}, 
pages={1030-1031 Vol.2}, 
abstract={High-resolution computational ocean models now in production provide excellent resolution of important physical features, but the sheer size of the model output makes effective visualization challenging. We present a combination of two systems: En Vis, a suite of batch-mode visualization tools, and Hum, a parallel pipelined motion picture program. Since computational models typically run on compute platforms that do not provide any hardware acceleration for graphics, En Vis employs software rendering techniques. It creates high-resolution RGB images from an input dataset, and uses a variety of surface shading techniques that make small-scale features much more apparent to researchers than do conventional flat-shaded methods. In addition to surface shading, En Vis also uses arbitrary-resolution lookup tables for coloring scalar fields, providing visual discrimination even in regions containing small variations in the field. En Vis also enables compositing of other image data such as terrain over masked portions of a model output image. In order to provide interactive frame rates for researchers exploring large global datasets, Hum employs a multithreaded architecture implementing pipelines composed of image loading, decoding, caching and display functionality. Other threads manage user interaction on multiple displays, where interactive pan and zoom features are provided. These allow a researcher to efficiently examine both large-and small-scale features. Hum's implementation of speculative prefetching keeps its imaging pipelines full, caching recently displayed image data for later redisplay, and automating prediction of which frames are likely to be required in the future. Where available, Hum can display stereo pairs of images, and can also overlay vector glyphs at a user-selectable display density. To place the use of En Vis and Hum in the context of a specific ocean modeling workflow, we present visualization examples using output from the Navy Layered Ocean Mo- > - > del at the Naval Research Laboratory. Visualization of ocean model results is an important tool which helps us to assess the realism of the ocean model, identify and diagnose problems, and learn more about the ocean and how it behaves. At the time of Hum's initial development in early 1999, NRL was running a 1/16 degree global model with a resolution of 4096/spl times/2304, which is significantly finer than a standard computer monitor. Hum was used to display and animate the 1/16/spl deg/ global images generated by EnVis. Existing applications were very limited in viewing theses large images, and it became apparent that a new animation tool was needed to work with theses large image sequences, which easily exceed local workstation memory. Global model resolutions have now increased to 1/32 degree with a resolution of 8192/spl times/4608 on each layer, reconfirming the need for these tools. Performance characteristics of En Vis rendering and Hum animations are supplied for typical model and hardware configurations.}, 
keywords={data visualisation;geophysics computing;oceanographic techniques;EnVis;Hum;NRL;Naval Research Laboratory;Navy Layered Ocean Model;arbitrary-resolution lookup table;batch-mode visualization tool;caching;computational model;decoding;display functionality;global model resolution;high-resolution RGB image;high-resolution ocean model visualization;image loading;image stereo pairs;input dataset;interactive pan feature;model output;multiple display;multithreaded architecture;ocean modeling workflow;parallel pipelined motion picture program;physical feature;prediction automation;scalar field coloring;small-scale feature;software rendering technique;surface shading technique;terrain data;user interaction;user-selectable display density;vector glyphs;visual discrimination;visualization example;zoom feature;Animation;Computational modeling;Context modeling;Displays;Hardware;Oceans;Pipelines;Rendering (computer graphics);Sea surface;Visualization}, 
doi={10.1109/OCEANS.2003.178478}, 
month={Sept},}
@INPROCEEDINGS{5193810, 
author={Z. Yao and Q. Zhang}, 
booktitle={2009 International Joint Conference on Computational Sciences and Optimization}, 
title={Item-Based Clustering Collaborative Filtering Algorithm under High-Dimensional Sparse Data}, 
year={2009}, 
volume={1}, 
pages={787-790}, 
abstract={This paper proposes a novel algorithm named item-based clustering recommendation algorithm (IBCRA) for reducing the poor recommendation quality due to the data sparsity and high dimension. Specifically, on the basis of high-dimensions data clustering algorithms, the IBCRA uses the rating data sparse difference and item categories in the rating dataset to construct a measuring formula for calculating dataset difference, where the formula is used for item clustering in user-item rating array. Then the IBCRA calculates item similarity and searches for k-nearest neighbors of target item based on the outcome of item clustering. Finally it predicts the ratings for those no rating items in dataset and so generates recommendations. The experimental results show, in perspective of the accuracy and speed of convergence, the IBCRA has improved the recommendation quality in collaborative filtering recommendation. Therefore, it can be used to recommend the products in e-commerce recommending systems.}, 
keywords={groupware;information filtering;information filters;pattern clustering;IBCRA;collaborative filtering algorithm;e-commerce;high-dimensional sparse data;item-based clustering recommendation algorithm;k-nearest neighbor method;user-item rating array;Clustering algorithms;Collaborative work;Conference management;Economic forecasting;Filtering algorithms;International collaboration;Partitioning algorithms;Predictive models;Recommender systems;Scalability;Collaborative Filter Recommendation;e-commerce;intelligent algorithm}, 
doi={10.1109/CSO.2009.239}, 
month={April},}
@INPROCEEDINGS{7417638, 
author={A. Shimoda and K. Ishibashi and K. Sato and M. Tsujino and T. Inoue and M. Shimura and T. Takebe and K. Takahashi and T. Mori and S. Goto}, 
booktitle={2015 IEEE Global Communications Conference (GLOBECOM)}, 
title={Inferring Popularity of Domain Names with DNS Traffic: Exploiting Cache Timeout Heuristics}, 
year={2015}, 
pages={1-6}, 
abstract={Popularity ranking of Internet services is an important metric for network operators, because it enables mid- to-long term planning of their network facilities and root cause analysis for unexpected traffic. The service-oriented traffic monitoring is much helpful to infer the popularity, hence it has been gathering much attention from both researchers and practitioners. Lately, service identification of a given flow has become very difficult due to the rapid growth of CDNs and/or encrypted traffic, while some research works employed preceding DNS traffic as a hint. However, because of its cache mechanism, the DNS message count deviates from the actual number of flows, which can greatly degrade the ranking reliability. We propose a theoretical model for inferring the user's number of accesses per domain name by exploiting the characteristics of the DNS message count. To the best of our knowledge, this paper is the first attempt to formulate the effect of user's stub resolvers; previous studies were focused on analyzing the effect of cache servers. We evaluated the precision of our model with a real dataset of traffic of thousands of users. By analyzing the top-50 domain names by the number of users, we can infer the number of flows within a 24% error rate on average in 42 out of 50 FQDNs.}, 
keywords={Internet;cache storage;inference mechanisms;telecommunication network planning;telecommunication traffic;DNS message count;DNS traffic;Internet services;cache timeout heuristics;domain names;long term planning;mid-term planning;network facilities;network operators;popularity inference;popularity ranking;service identification;service-oriented traffic monitoring;stub resolvers;Analytical models;Calibration;Internet;Monitoring;Regression analysis;Servers;Synchronization}, 
doi={10.1109/GLOCOM.2015.7417638}, 
month={Dec},}
@INPROCEEDINGS{5283040, 
author={C. Biancalana and A. Micarelli}, 
booktitle={2009 International Conference on Computational Science and Engineering}, 
title={Social Tagging in Query Expansion: A New Way for Personalized Web Search}, 
year={2009}, 
volume={4}, 
pages={1060-1065}, 
abstract={Social networks and collaborative tagging systems are rapidly gaining popularity as primary means for sorting and sharing data: users tag their bookmarks in order to simplify information dissemination and later lookup. Social Bookmarking services are useful in two important respects: first, they can allow an individual to remember the visited URLs, and second, tags can be made by the community to guide users towards valuable content. In this paper we focus on the latter use: we present a novel approach for personalized web search using query expansion. We further extend the family of well-known co-occurence matrix technique models by using a new way of exploring social tagging services. Our approach shows its strength particularly in the case of disambiguation of word contexts. We show how to design and implement such a system in practice and conduct several experiments on a real web-dataset collected from Regione Lazio Portal. To the best of our knowledge this is the first study centered on using social bookmarking and tagging techniques for personalization of web search and its evaluation in a real-world scenario.}, 
keywords={Internet;matrix algebra;query processing;social networking (online);Regione Lazio Portal;URL;data sharing;data sorting;matrix technique models;personalized Web search;query expansion;real Web-dataset;social bookmarking;social networks;social tagging;Artificial intelligence;Automation;Collaboration;Computer science;Information retrieval;Laboratories;Sorting;Tagging;Uniform resource locators;Web search}, 
doi={10.1109/CSE.2009.492}, 
month={Aug},}
@INPROCEEDINGS{6267895, 
author={Z. Gong and S. Lakshminarasimhan and J. Jenkins and H. Kolla and S. Ethier and J. Chen and R. Ross and S. Klasky and N. F. Samatova}, 
booktitle={2012 IEEE 26th International Parallel and Distributed Processing Symposium}, 
title={Multi-level Layout Optimization for Efficient Spatio-temporal Queries on ISABELA-compressed Data}, 
year={2012}, 
pages={873-884}, 
abstract={The size and scope of cutting-edge scientific simulations are growing much faster than the I/O subsystems of their runtime environments, not only making I/O the primary bottleneck, but also consuming space that pushes the storage capacities of many computing facilities. These problems are exacerbated by the need to perform data-intensive analytics applications, such as querying the dataset by variable and spatio-temporal constraints, for what current database technologies commonly build query indices of size greater than that of the raw data. To help solve these problems, we present a parallel query-processing engine that can handle both range queries and queries with spatio-temporal constraints, on B-spline compressed data with user-controlled accuracy. Our method adapts to widening gaps between computation and I/O performance by querying on compressed metadata separated into bins by variable values, utilizing Hilbert space-filling curves to optimize for spatial constraints and aggregating data access to improve locality of per-bin stored data, reducing the false positive rate and latency bound I/O operations (such as seek) substantially. We show our method to be efficient with respect to storage, computation, and I/O compared to existing database technologies optimized for query processing on scientific data.}, 
keywords={Hilbert spaces;data compression;database indexing;input-output programs;natural sciences computing;optimisation;parallel databases;query processing;software performance evaluation;splines (mathematics);B-spline compressed data;Hilbert space-filling curves;I-O subsystems;ISABELA-compressed data;data access aggregation;database technologies;multilevel layout optimization;parallel query-processing engine;query indices;scientific simulations;spatio-temporal queries;Bandwidth;Computational modeling;Indexes;Layout;Organizations;Query processing;Splines (mathematics)}, 
doi={10.1109/IPDPS.2012.83}, 
ISSN={1530-2075}, 
month={May},}
@INPROCEEDINGS{5720335, 
author={A. B. Mattos and J. P. Mena-Chalco and R. M. Cesar Jr. and L. Velho}, 
booktitle={2010 23rd SIBGRAPI Conference on Graphics, Patterns and Images}, 
title={3D Linear Facial Animation Based on Real Data}, 
year={2010}, 
pages={271-278}, 
abstract={In this paper we introduce a Facial Animation system using real three-dimensional models of people, acquired by a 3D scanner. We consider a dataset composed by models displaying different facial expressions and a linear interpolation technique is used to produce a smooth transition between them. One-to-one correspondences between the meshes of each facial expression are required in order to apply the interpolation process. Instead of focusing in the computation of dense correspondence, some points are selected and a triangulation is defined, being refined by consecutive subdivisions, that compute the matchings of intermediate points. We are able to animate any model of the dataset, given its texture information for the neutral face and the geometry information for all the expressions along with the neutral face. This is made by computing matrices with the variations of every vertex when changing from the neutral face to the other expressions. The knowledge of the matrices obtained in this process makes it possible to animate other models given only the texture and geometry information of the neutral face. Furthermore, the system uses 3D reconstructed models, being capable of generating a three-dimensional facial animation from a single 2D image of a person. Also, as an extension of the system, we use artificial models that contain expressions of visemes, that are not part of the expressions of the dataset, and their displacements are applied to the real models. This allows these models to be given as input to a speech synthesis application in which the face is able to speak phrases typed by the user. Finally, we generate an average face and increase the displacements between a subject from the dataset and the average face, creating, automatically, a caricature of the subject.}, 
keywords={computational geometry;computer animation;face recognition;image matching;image reconstruction;image scanners;image texture;interpolation;solid modelling;speech synthesis;user interfaces;2D image;3D linear facial animation;3D reconstruction;3D scanner;artificial model;facial expression;geometry information;linear interpolation technique;neutral face;speech synthesis;texture information;three dimensional model;Computational modeling;Face;Facial animation;Geometry;Interpolation;Three dimensional displays;3D Reconstruction;Computer Graphics;Facial Animation}, 
doi={10.1109/SIBGRAPI.2010.44}, 
ISSN={1530-1834}, 
month={Aug},}
@INPROCEEDINGS{7344804, 
author={A. Abdel-Hafez and Y. Xu and A. J⊘sang}, 
booktitle={2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
title={An accurate rating aggregation method for generating item reputation}, 
year={2015}, 
pages={1-8}, 
abstract={Many websites presently provide the facility for users to rate items quality based on user opinion. These ratings are used later to produce item reputation scores. The majority of websites apply the mean method to aggregate user ratings. This method is very simple and is not considered as an accurate aggregator. Many methods have been proposed to make aggregators produce more accurate reputation scores. In the majority of proposed methods the authors use extra information about the rating providers or about the context (e.g. time) in which the rating was given. However, this information is not available all the time. In such cases these methods produce reputation scores using the mean method or other alternative simple methods. In this paper, we propose a novel reputation model that generates more accurate item reputation scores based on collected ratings only. Our proposed model embeds statistical data, previously disregarded, of a given rating dataset in order to enhance the accuracy of the generated reputation scores. In more detail, we use the Beta distribution to produce weights for ratings and aggregate ratings using the weighted mean method. Experiments show that the proposed model exhibits performance superior to that of current state-of-the-art models.}, 
keywords={Internet;data handling;statistical distributions;Beta distribution;Web sites;aggregate ratings;item quality rating;item reputation;rating aggregation method;statistical data;weighted mean method;Aggregates;Computational modeling;Gaussian distribution;Probability distribution;Reliability;Shape;Uncertainty;Beta Distribution;Ratings Aggregation;Reputation Model;Weighted Mean}, 
doi={10.1109/DSAA.2015.7344804}, 
month={Oct},}
@INPROCEEDINGS{7395783, 
author={Q. Li and M. Gu and K. Zhou and X. Sun}, 
booktitle={2015 IEEE International Conference on Data Mining Workshop (ICDMW)}, 
title={Multi-Classes Feature Engineering with Sliding Window for Purchase Prediction in Mobile Commerce}, 
year={2015}, 
pages={1048-1054}, 
abstract={Mobile devices become more and more prevalent in recent years, especially in young groups. The rapid progress of mobile devices promotes the development of M-Commerce business. The purchase on mobile terminals accounts for a considerable percentage in the total trading volume of E-Commerce and begins to draw the attention of E-Commerce corporation. Alibaba held a Mobile Recommendation Algorithm Competition aiming to recommend appropriate items for mobile users at the right time and place. The dataset provided by Alibaba consists of about 6 billion operation logs made by 5 million Taobao users towards over 150 million items spanning a period of one month. Compared with traditional scenarios in purchase predicting, the competition raised three challenges: (1)The dataset is too large to be processed in personal computers, (2)Some days with great discounts provided by Taobao Marketplace are within the period of dataset, (3)Positive samples are too few compared to the dimension of features. In this paper we study the problem of predicting the purchase behaviour of M-Commerce users, by exploring the solution for Alibaba's Mobile Recommendation Algorithm Competition. We first deeply study the habit of customers and filter many outliers. After that we adopt the method of "sliding window" to supply positive samples of training dataset and smooth the burst of sales near Dec 12th. We design a feature engineering framework to extract 6 categories of features that aim to capture the buying potential of user-item pairs. Our features exploit the interaction of user-item pair, user's shopping habit and item' attraction for users. Then we apply Gradient Boost Decision Trees (GBDT) as the training model. In the end, we combine outputs of individual GBDT together by Logistic Regression to get the final predictions. Our solution achieves 8.66% F1 score, and ranks the third place in the final round.}, 
keywords={decision trees;gradient methods;mobile commerce;regression analysis;Alibaba mobile recommendation algorithm competition;e-commerce;feature engineering framework;gradient boost decision trees;logistic regression;m-commerce business;mobile commerce;multiclasses feature engineering;purchase prediction;sliding window;user shopping habit;user-item pair;Business;Computational efficiency;Crawlers;Feature extraction;Mobile communication;Testing;Training}, 
doi={10.1109/ICDMW.2015.172}, 
month={Nov},}
@INPROCEEDINGS{6913349, 
author={Q. Liu and B. Ribeiro and A. H. Sung and D. Suryakumar}, 
booktitle={2014 IIAI 3rd International Conference on Advanced Applied Informatics}, 
title={Mining the Big Data: The Critical Feature Dimension Problem}, 
year={2014}, 
pages={499-504}, 
abstract={In mining massive datasets, often two of the most important and immediate problems are sampling and feature selection. Proper sampling and feature selection contributes to reducing the size of the dataset while obtaining satisfactory results in model building. Theoretically, therefore, it is interesting to investigate whether a given dataset possesses a critical feature dimension, or the minimum number of features that is required for a given learning machine to achieve "satisfactory" performance. (Likewise, the critical sampling size problem concerns whether, for a given dataset, there is a minimum number of data points that must be included in any sample for a learning machine to achieve satisfactory performance.) Here the specific meaning of "satisfactory" performance is to be defined by the user. This paper addresses the complexity of both problems in one general theoretical setting and shows that they have the same complexity and are highly intractable. Next, an empirical method is applied in an attempt to find the approximate critical feature dimension of datasets. It is demonstrated that, under generally reasonable assumptions pertaining to feature ranking algorithms, the critical feature dimension are successfully discovered by the empirical method for a number of datasets of various sizes. The results are encouraging in achieving significant feature size reduction and point to a promising way in dealing with big data. The significance of the existence of crucial dimension in datasets is also explained.}, 
keywords={Big Data;data mining;feature selection;learning (artificial intelligence);Big Data mining;feature dimension problem;feature ranking algorithms;feature selection;learning machine;Accuracy;Classification algorithms;Complexity theory;Data mining;Educational institutions;Electromagnetic interference;Vectors;critical dimension;data mining;dimension reduction;feature ranking;machine learning}, 
doi={10.1109/IIAI-AAI.2014.105}, 
month={Aug},}
@INPROCEEDINGS{7518234, 
author={X. Li and S. Wei and G. Sun}, 
booktitle={2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)}, 
title={A Scheme for Activity Trajectory Dataset Publishing with Privacy Preserved}, 
year={2015}, 
pages={247-254}, 
abstract={Facilitated by ubiquitous computing techniques and the wide development of location-based service (LBS) and smart card systems, users' trajectory data, which is usually attached with user activities, are recorded more and more easily. Inappropriate publishing these trajectory data may jeopardize users' privacy. However, existing works on privacy-preserving trajectory data publishing fail to consider the activity information. In this paper, we propose a scheme under an existing privacy framework for solving the problem that how to preserve users' privacy when publishing an activity trajectory dataset. In this scheme, it should be ensured that the posterior probability about users' privacy inferred by adversaries is not too much larger than the prior one. We assume that user behaviors could be described by a Markov model and that the adversary has a knowledge of both the user model and our privacy preserving mechanism. The adversary could infer users' predefined sensitive information by Bayesian inference. Then we propose a dataset publishing algorithm, PPPAT, which preserves user privacy. It iterates on all the events of a given user to decide whether they could be published or not. Finally, we evaluate our algorithm by conducting experiments on two datasets, compared with several baselines. The results verify the effectiveness of our algorithm and the utility of outputted activity trajectory dataset.}, 
keywords={Markov processes;inference mechanisms;publishing;security of data;ubiquitous computing;Bayesian inference;LBS;Markov model;activity trajectory dataset;activity trajectory dataset publishing;location based service;privacy framework;privacy preserved;privacy preserving trajectory data publishing;smart card systems;trajectory data;ubiquitous computing techniques;Data privacy;Hospitals;Markov processes;Privacy;Publishing;Smart cards;Trajectory;activity trajectory;event;privacy;utility}, 
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.57}, 
month={Aug},}
@INPROCEEDINGS{7784118, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={Answer Quality Prediction Joint Textual and Non-Textual Features}, 
year={2016}, 
pages={144-148}, 
abstract={Community question answering (CQA) is a popular online service for people to ask and answer questions. But along with the increasing of user generating contents, the quality of answers provided by different users varies widely. So the quality of the answer caused wide attention. In this paper, we propose an answer quality prediction model to evaluate the answer quality considering both aspects of textual and non-textual features. We firstly employ Bidirectional long Short-Term Memory (BLSTM) based RNN model to evaluate textual quality of the answers. And we extract 11 features of the answers to evaluate the non-textual quality of answers. Finally, we jointly consider the score of answers textual and non-textual qualities. We evaluate our model in a benchmark dataset and the experimental results show that our model outperforms other existing approaches.}, 
keywords={Bidirectional long short term memory;answer quality prediction;community question answering}, 
doi={10.1109/WISA.2016.9}, 
month={Sept},}
@INPROCEEDINGS{6121795, 
author={F. Clarizia and F. Colace and M. De Santo and L. Greco and P. Napoletano}, 
booktitle={2011 11th International Conference on Intelligent Systems Design and Applications}, 
title={A new text classification technique using small training sets}, 
year={2011}, 
pages={1038-1043}, 
abstract={Text classification methods have been evaluated on supervised classification tasks of large datasets showing high accuracy. Nevertheless, due to the fact that these classifiers, to obtain a good performance on a test set, need to learn from many examples, some difficulties may be found when they are employed in real contexts. In fact, most users of a practical system do not want to carry out labeling tasks for a long time only to obtain a better level of accuracy. They obviously prefer algorithms that have high accuracy, but do not require a large amount of manual labeling tasks. In this paper we propose a new supervised method for single-label text classification, based on a mixed Graph of Terms, that is capable of achieving a good performance, in term of accuracy, when the size of the training set is 1% of the original. The mixed Graph of Terms can be automatically extracted from a set of documents following a kind of term clustering technique weighted by the probabilistic topic model. The method has been tested on the top 10 classes of the ModApte split from the Reuters-21578 dataset and learned on 1% of the original training set. Results have confirmed the discriminative property of the graph and have confirmed that the proposed method is comparable with existing methods learned on the whole training set.}, 
keywords={graph theory;pattern classification;pattern clustering;text analysis;ModApte;Reuters-21578;documents;graph of terms;manual labeling tasks;single label text classification;small training sets;supervised classification tasks;term clustering technique;text classification technique;Accuracy;Feature extraction;Intelligent systems;Probabilistic logic;Semantics;Training;Vectors;Text classification;probabilistic topic model;term extraction}, 
doi={10.1109/ISDA.2011.6121795}, 
ISSN={2164-7143}, 
month={Nov},}
@INPROCEEDINGS{7732116, 
author={M. Wasid and V. Kant and R. Ali}, 
booktitle={2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
title={Frequency-based similarity measure for context-aware recommender systems}, 
year={2016}, 
pages={627-632}, 
abstract={Collaborative Filtering (CF), the widely used and most successful technique in the area of Recommender Systems, provides useful recommendations to users based on their similar users. Computing similarity among the users efficiently is the major step in CF. Further, it has been observed from literature that the context into CF provides more accurate and relevant recommendations for users but it is hard to represent and model contextual factors directly into the system. In this paper, we have incorporated the contextual information into user profile as an additional feature through a proposed novel frequency count method. After extending the user profiles, items are recommended based on similar profiles computed through a novel similarity measure. To evaluate the performance of our proposed recommendation strategy, several experiments are conducted on the popular LDOS-CoMoDa dataset.}, 
keywords={collaborative filtering;recommender systems;ubiquitous computing;LDOS-CoMoDa dataset;collaborative filtering;context-aware recommender systems;contextual information;frequency count method;frequency-based similarity measure;Computational modeling;Context;Context modeling;Frequency measurement;Motion pictures;Recommender systems;Collaborative Filtering;Context-Awareness;Recommender Systems;Web personalization}, 
doi={10.1109/ICACCI.2016.7732116}, 
month={Sept},}
@INPROCEEDINGS{5645857, 
author={W. S. Al-Sharafat}, 
booktitle={2010 2nd International Conference on Computer Technology and Development}, 
title={Evolutionary methods for detecting network intrusions}, 
year={2010}, 
pages={354-358}, 
abstract={Intrusion detection (ID) is the process of monitoring the events occurring in a computer system or network and analyzing them for signs of intrusions, defined as attempts to compromise the confidentiality, integrity, availability, or to bypass the security mechanisms of a computer or network. Internet services, and the number of Internet users increases every day this makes networks as a window for malicious users to do their damage becomes very great and lucrative. The objective of this paper is to incorporate different methods to detect and classify intrusion from normal network packet. Among several evolutionary techniques, Steady State Genetic-based Machine Leaning Algorithm (SSGBML) will be used to detect intrusions with Zeroth Level Classifier system (ZCS) are investigated here. Steady State Genetic Algorithm (SSGA) is used as a discovery mechanism instead of Simple Genetic Algorithm ( SGA). SGA replaces all old rules with new produced rule preventing old good rules from participating in the next rule generation. In contrast, SSGA gives a chance for previous rules to participate in new generations. ZCS is used to play the role of detector by matching incoming environment message with classifiers to determine whether the current message is normal or intrusion and receiving feedback from environment. The experiments and evaluations of the proposed method were performed with the KDD 99 intrusion detection dataset.}, 
keywords={Internet;computer network security;genetic algorithms;learning (artificial intelligence);pattern classification;Internet service;KDD 99 intrusion detection dataset;Zeroth level classifier system;computer network;discovery mechanism;event monitoring;evolutionary method;network intrusion detection;steady state genetic-based machine leaning algorithm;Algorithm design and analysis;Artificial neural networks;Computational modeling;Detectors;Fires;Monitoring;Probes;Network Intrusion Detection;SGA;SSGA;SSGBML;ZCS;component}, 
doi={10.1109/ICCTD.2010.5645857}, 
month={Nov},}
@INPROCEEDINGS{4449120, 
author={C. H. Hobbs}, 
booktitle={OCEANS 2007}, 
title={Considerations in Marine Sand Mining and Beach Nourishment}, 
year={2007}, 
pages={1-10}, 
abstract={Beach nourishment restores beaches and enhances both their recreational value and their utility in shore protection. Increasing population in coastal areas and competition for land use has reduced the availability of terrestrial borrow pits while older sand pits have been exhausted. This situation has driven the search for borrow or mining sites offshore. Presently, almost all the marine sand mining along the Atlantic and Gulf coasts of the U.S. is to obtain material for publicly funded, beach nourishment. Much of this activity is in waters subject to federal jurisdiction. The demand for construction aggregate in metropolitan and developing coastal areas suggests that there might be an additional demand on offshore sand resources. Marine sand mining, as any type of dredging, disrupts habitat and can disturb transitory fishes and marine mammals. By altering the configuration of the sea floor, sand mining modifies wave transformation and, possibly, nearshore currents. Removing a large quantity of offshore sand and placing it in the nearshore zone alters habitat and sediment-transport processes. For over a decade, there has been a series of studies concerned with the potential environmental consequences of offshore sand mining and subsequent beach nourishment. A more recent set of projects developed and tested a suite of protocols for monitoring dredged and nourished areas. The field study testing the draft monitoring protocol was conducted in the active, sand-mining region of Sandbridge Shoal, offshore of Virginia Beach, Virginia. Results indicate that repopulation of dredged areas is enhanced by leaving patches of undisturbed bottom within the dredged region. No negative impacts on macrobenthos or demersal fishes were noted. Changes in wave transformation resulting from modifying the bottom topography are relatively small and, as the sand-mining sites tend to be in waters greater than 10 m deep, usually only occur during storms. In some instances, dredging shoals ma- decrease inshore wave height by diminishing the concentration of wave rays caused by refraction on the shoal. Shore-based radar may offer advantages for monitoring waves in the near shore. Agencies involved with individual sand-mining programs should consider participating in consortia managing local, coastal observing systems. Location and elevation/depth (x,y,z) data with sufficient metadata is the basic dataset necessary for determining and monitoring changes in the shore and seafloor surface. The data can be acquired as profiles or, especially in nearshore regions, swath-bathymetry surfaces. Profiles should be spaced to capture changes in topography. On-shore and off-shore measurements should be synoptic or nearly so. Although there are valid and strong scientific and management reasons for implementing a standardized, minimum monitoring protocol for marine sand mining, there appears to be little will for such a program. The obvious problem is cost coupled with the question "who pays?" Costs continue beyond the actual monitoring project. If data are in an electronic format, someone must maintain the computer system and the web site. In the case of beach nourishment projects, although the locality or agency sponsoring the project would carry the immediate expenses, eventually the cost would work down to the tax payer. For commercial mining of construction aggregate, the cost would wind up with the "end user." There are additional problems. The question "why monitor?" must be answered. If the data are collected solely to satisfy a monitoring requirement and are stored, unused the exercise cannot be justified. Widely and easily accessible monitoring data would help determine common factors contributing to the performance of sand mining and beach nourishment projects and would be beneficial in improving the predictive models.}, 
keywords={bathymetry;mining;sand;sediments;beach nourishment;coastal USA;dredging;land use;marine sand mining;offshore borrow site;offshore mining site;sand pits;seabottom topography;seafloor surface changes;shore surface changes;terrestrial borrow pits;Aggregates;Costs;Marine animals;Monitoring;Protocols;Sea floor;Sea measurements;Sea surface;Surface topography;Testing}, 
doi={10.1109/OCEANS.2007.4449120}, 
ISSN={0197-7385}, 
month={Sept},}
@INPROCEEDINGS{7507182, 
author={H. G. Elmongui and H. Morsy and R. Mansour}, 
booktitle={2015 IEEE/ACS 12th International Conference of Computer Systems and Applications (AICCSA)}, 
title={Inference models for Twitter user's home location prediction}, 
year={2015}, 
pages={1-8}, 
abstract={Twitter has emerged as one of the most powerful micro-blogging services for real-time sharing of information on the web. A large base of Twitter users tend to post short messages of 140 characters (Tweets) reflecting a variety of topics. Location-based-services (LBSs) may be built on top of microblogs to provide for targeted advertisement, news recommendation, or even microblogs personalization. Knowing the user's home location would empower such LBSs. In this paper, we propose prediction models to infer the users' home location based on their social graph and tweets content. The problem is non trivial as the tweets are short and not many people like to share their location for privacy concerns. Our extensive performance evaluation on a publicly available dataset demonstrates the effectiveness of the proposed models. The proposed models outperform the competitive state-of-the-art home location inference techniques that are based on the social graph, tweet content, and both by a relative gain in the F-measure of up to 37.71%, 29%, and 9.06%, respectively.}, 
keywords={data privacy;geographic information systems;inference mechanisms;social networking (online);F-measure;LBS;Twitter user home location prediction model;advertisement;home location inference techniques;inference models;location-based-services;microblogging services;microblogs personalization;news recommendation;privacy concerns;real-time information sharing;social graph;tweets content;Computational modeling;Predictive models;Real-time systems;Twitter;Urban areas}, 
doi={10.1109/AICCSA.2015.7507182}, 
month={Nov},}
@INPROCEEDINGS{6480220, 
author={W. Duan and Q. Cao and Y. Yu and S. Levy}, 
booktitle={2013 46th Hawaii International Conference on System Sciences}, 
title={Mining Online User-Generated Content: Using Sentiment Analysis Technique to Study Hotel Service Quality}, 
year={2013}, 
pages={3119-3128}, 
abstract={This study aims to look beyond the quantitative summary to provide a more comprehensive view of online user-generated content. We obtain a unique and extensive dataset of online user reviews for hotels across various review sites and over a long time periods. We use the sentiment analysis technique to decompose user reviews into five dimensions to measure hotel service quality. Those dimensions are then incorporated into econometrics models to examine their effect in shaping users' overall evaluation and content generating behavior. The results suggest that different dimensions of user reviews have significantly differential impact in forming user evaluation and driving content generation.}, 
keywords={Biological system modeling;Educational institutions;Industries;Mathematical model;Media;Reliability;User-generated content;User-generated content;online user reviews;sentiment analysis;service quality}, 
doi={10.1109/HICSS.2013.400}, 
ISSN={1530-1605}, 
month={Jan},}
@INPROCEEDINGS{6680916, 
author={S. Gambs and M. O. Killijian and M. N. d. P. Cortez}, 
booktitle={2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications}, 
title={De-anonymization Attack on Geolocated Data}, 
year={2013}, 
pages={789-797}, 
abstract={With the advent of GPS-equipped devices, a massive amount of location data is being collected, raising the issue of the privacy risks incurred by the individuals whose movements are recorded. In this work, we focus on a specific inference attack called the de-anonymization attack, by which an adversary tries to infer the identity of a particular individual behind a set of mobility traces. More specifically, we propose an implementation of this attack based on a mobility model called Mobility Markov Chain (MMC). A MMC is built out from the mobility traces observed during the training phase and is used to perform the attack during the testing phase. We design two distance metrics quantifying the closeness between two MMCs and combine these distances to build de-anonymizers that can re-identify users in an anonymized geolocated dataset. Experiments conducted on real datasets demonstrate that the attack is both accurate and resilient to sanitization mechanisms such as downsampling.}, 
keywords={Global Positioning System;Markov processes;data privacy;geography;mobile computing;GPS-equipped devices;Global Positioning System;MMC;anonymized geolocated dataset;de-anonymization attack;distance metrics;downsampling;inference attack;mobility Markov chain;mobility traces;privacy risks;sanitization mechanisms;testing phase;training phase;Geology;Markov processes;Measurement;Semantics;Testing;Training;Vectors;Privacy;de-anonymization;geolocation;inference attack}, 
doi={10.1109/TrustCom.2013.96}, 
ISSN={2324-898X}, 
month={July},}
@INPROCEEDINGS{6676744, 
author={J. Zhu and Z. Zheng and M. R. Lyu}, 
booktitle={2013 IEEE Sixth International Conference on Cloud Computing}, 
title={DR2: Dynamic Request Routing for Tolerating Latency Variability in Online Cloud Applications}, 
year={2013}, 
pages={589-596}, 
abstract={Application latency is one significant user metric for evaluating the performance of online cloud applications. However, as applications are migrated to the cloud and deployed across a wide-area network, the application latency usually presents high variability over time. Among lots of subtleties that influence the latency, one important factor is relying on the Internet for application connectivity, which introduces a high degree of variability and uncertainty on user-perceived application latency. As a result, a key challenge faced by application designers is how to build consistently low-latency cloud applications with the large number of geo-distributed and latency-varying cloud components. In this paper, we propose a dynamic request routing framework, DR2, by taking full advantage of redundant components in the clouds to tolerate latency variability. In practice, many functionally-equivalent components have been already deployed redundantly for load balancing and fault tolerance, thus resulting in low additional overhead for DR2. To evaluate the performance of our approach, we conduct a set of experiments based on two large-scale real-world datasets and a synthetic dataset. The results show the effectiveness and efficiency of our approach.}, 
keywords={cloud computing;resource allocation;software fault tolerance;software performance evaluation;wide area networks;DR2;Internet;application connectivity;dynamic request routing framework;fault tolerance;functionally-equivalent components;geo-distributed cloud components;large-scale real-world datasets;latency variability tolerance;latency-varying cloud components;load balancing;online cloud applications;performance evaluation;redundant components;synthetic dataset;user-perceived application latency;wide-area network;Adaptation models;Cloud computing;Data models;Prediction algorithms;Routing;Servers;Cloud computing;component selection;latency prediction;latency variability tolerating;request routing}, 
doi={10.1109/CLOUD.2013.59}, 
ISSN={2159-6182}, 
month={June},}
@ARTICLE{6933883, 
author={P. Tran and A. Sue and P. Wong and Q. Li and P. Carter}, 
journal={IEEE Transactions on Biomedical Engineering}, 
title={Development of HEATHER for Cochlear Implant Stimulation Using a New Modeling Workflow}, 
year={2015}, 
volume={62}, 
number={2}, 
pages={728-735}, 
abstract={The current conduction pathways resulting from monopolar stimulation of the cochlear implant were studied by developing a human electroanatomical total head reconstruction (namely, HEATHER). HEATHER was created from serially sectioned images of the female Visible Human Project dataset to encompass a total of 12 different tissues, and included computer-aided design geometries of the cochlear implant. Since existing methods were unable to generate the required complexity for HEATHER, a new modeling workflow was proposed. The results of the finite-element analysis agree with the literature, showing that the injected current exits the cochlea via the modiolus (14%), the basal end of the cochlea (22%), and through the cochlear walls (64%). It was also found that, once leaving the cochlea, the current travels to the implant body via the cranial cavity or scalp. The modeling workflow proved to be robust and flexible, allowing for meshes to be generated with substantial user control. Furthermore, the workflow could easily be employed to create realistic anatomical models of the human head for different bioelectric applications, such as deep brain stimulation, electroencephalography, and other biophysical phenomena.}, 
keywords={CAD;bioelectric phenomena;brain;cochlear implants;electrocardiography;image reconstruction;medical image processing;mesh generation;physiological models;HEATHER;basal end;bioelectric applications;biophysical phenomena;cochlear implant stimulation;cochlear walls;computer-aided design geometries;conduction pathways;cranial cavity;deep brain stimulation;electroencephalography;female Visible Human Project dataset;finite-element analysis;human electroanatomical total head reconstruction;human head;implant body;injected current;mesh generation;modeling workflow;modiolus;monopolar stimulation;realistic anatomical models;scalp;substantial user control;tissues;Brain models;Computational modeling;Electrodes;Finite element analysis;Head;Solid modeling;Bioelectric finite-element analysis;Cochlear implant;bioelectric finite element analysis;cochlear implant;current conduction;human head model;Adult;Brain;Cochlea;Cochlear Implants;Computer Simulation;Electromagnetic Fields;Female;Head;Humans;Models, Biological;Scattering, Radiation;Software}, 
doi={10.1109/TBME.2014.2364297}, 
ISSN={0018-9294}, 
month={Feb},}
@INPROCEEDINGS{7297433, 
author={Z. H. Chen and C. T. Tsai and S. M. Yuan and S. H. Chou and J. Chern}, 
booktitle={2015 8th International Conference on Ubi-Media Computing (UMEDIA)}, 
title={Big data: Open data and realty website analysis}, 
year={2015}, 
pages={84-88}, 
abstract={Currently, people obtained housing information from relatives, friends, sale representatives from model home, Realty networks, or government. It is difficult to get the housing-related information from these resources and hard to compare. Therefore, we proposed a public inquiry website called Housing Price Analysis using information from public websites to do statistical analyses based on the total ratio of "average value" and "standard deviation". People can use this website to get better understanding of price trend in certain area. This website will compare all records on public realty websites and government system based on same area with similar size of houses, number of rooms, age of house, etc. By using this system, users can get better understanding the trend of housing prices.}, 
keywords={Big Data;Web sites;pricing;property market;statistical analysis;average value;big data;housing price analysis;housing-related information;model home;open data;price trend;public inquiry Website;realty Website analysis;standard deviation;statistical analyses;Buildings;Correlation;Databases;Government;Servers;Big Data;Dataset;Open Datal}, 
doi={10.1109/UMEDIA.2015.7297433}, 
month={Aug},}
@INPROCEEDINGS{7881507, 
author={M. A. U. H. Tahir and S. Asghar and A. Zafar and S. Gillani}, 
booktitle={2016 International Conference on Computational Science and Computational Intelligence (CSCI)}, 
title={A Hybrid Model to Detect Phishing-Sites Using Supervised Learning Algorithms}, 
year={2016}, 
pages={1126-1133}, 
abstract={Since last decades, online technologies have revolutionized the modern computing world. However, as a result, security threats are increasing rapidly. A huge community is using the online services even from chatting to banking is done via online transactions. Customers of web technologies face various security threats and phishing is one of the most important threat that needs to be address. Therefore, the security mechanism must be enhanced. The attacker uses phishing attack to get victims credential information like bank account number, passwords or any other information by mimicking a website of an enterprise, and the victim is unaware of phishing website. In literature, several approaches have been proposed for detection and filtering phishing attack. However, researchers are still searching for such a solution that can provide better results to secure users from phishing attack. Phishing websites have certain characteristics and patterns and to identify those features can help us to detect phishing. To identify such features is a classification task and can be solved using data mining techniques. In this paper, we are presenting a hybrid model for classification to overcome phishing-sites problem. To evaluate this model, we have used the dataset from UCI repository which contains 30 attributes and 11055 instances. The experimental results showed that our proposed hybrid model outperforms in terms of high accuracy and less error rate.}, 
keywords={Internet;computer crime;data mining;learning (artificial intelligence);pattern classification;UCI repository;Web technologies;classification task;data mining techniques;hybrid model;online services;online technologies;online transactions;phishing attack detection;phishing attack filtering;phishing-site detection;security mechanism;security threats;supervised learning algorithms;victims credential information;Classification algorithms;Data mining;Error analysis;Picture archiving and communication systems;Security;Training;Classification;Combines Multiple Models;Hybrid Model;Phishing-sites;Security Threat}, 
doi={10.1109/CSCI.2016.0214}, 
month={Dec},}
@INPROCEEDINGS{7795857, 
author={M. G. Campana and F. Delmastro and R. Bruno}, 
booktitle={2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)}, 
title={A machine-learned ranking algorithm for dynamic and personalised car pooling services}, 
year={2016}, 
pages={1856-1862}, 
abstract={Car pooling is expected to significantly help in reducing traffic congestion and pollution in cities by enabling drivers to share their cars with travellers with similar itineraries and time schedules. A number of car pooling matching services have been designed in order to efficiently find successful ride matches in a given pool of drivers and potential passengers. However, it is now recognised that many non-monetary aspects and social considerations, besides simple mobility needs, may influence the individual willingness of sharing a ride, which are difficult to predict. To address this problem, in this study we propose GoTogether, a recommender system for car pooling services that leverages on learning-to-rank techniques to automatically derive the personalised ranking model of each user from the history of her choices (i.e., the type of accepted or rejected shared rides). Then, GoTogether builds the list of recommended rides in order to maximise the success rate of the offered matches. To test the performance of our scheme we use real data from Twitter and Foursquare sources in order to generate a dataset of plausible mobility patterns and ride requests in a metropolitan area. The results show that the proposed solution quickly obtain an accurate prediction of the personalised user's choice model both in static and dynamic conditions.}, 
keywords={intelligent transportation systems;learning (artificial intelligence);road traffic control;Foursquare sources;GOTOGETHER;Twitter data;dynamic car pooling services;machine-learned ranking algorithm;personalised car pooling services;traffic congestion;Automobiles;Data models;Feature extraction;Heuristic algorithms;Systems architecture;Urban areas}, 
doi={10.1109/ITSC.2016.7795857}, 
month={Nov},}
@INPROCEEDINGS{5466978, 
author={U. Steinhoff and B. Schiele}, 
booktitle={2010 IEEE International Conference on Pervasive Computing and Communications (PerCom)}, 
title={Dead reckoning from the pocket - An experimental study}, 
year={2010}, 
pages={162-170}, 
abstract={Modern mobile phones enable absolute positioning based on GPS or WiFi. However, incremental positioning based on dead reckoning is an interesting source of complementary information, e.g., for indoor positioning or for filling in reception gaps. In the literature however, reasonable dead reckoning accuracies have been reported for fixed and typically a priori known device placements only, e.g., on the hip. Therefore, this paper contributes an experimental study of several published as well as novel approaches for dead reckoning in a scenario with unconstrained placement of a device in the user's trouser pockets. Utilizing the movement of a sensor in a trouser pocket due to body motion, we estimate the user's walking direction and steps robustly for arbitrary placements in the pocket and without additional body-worn sensors. We evaluate these methods on a large dataset of 23 traces of 8 different persons, and compare different approaches.}, 
keywords={Global Positioning System;mobile communication;wireless LAN;GPS;WiFi;dead reckoning;incremental positioning;mobile phones;trouser pocket;Access control;Automatic control;Communication system control;Computational modeling;Dead reckoning;Prototypes;Radio broadcasting;Road vehicles;Smart phones;Social network services;dead reckoning;pedestrian positioning;wearable computing}, 
doi={10.1109/PERCOM.2010.5466978}, 
month={March},}
@INPROCEEDINGS{7837873, 
author={M. Wan and J. McAuley}, 
booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)}, 
title={Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion Question Answering Systems}, 
year={2016}, 
pages={489-498}, 
abstract={Product review websites provide an incredible lens into the wide variety of opinions and experiences of different people, and play a critical role in helping users discover products that match their personal needs and preferences. To help address questions that can't easily be answered by reading others' reviews, some review websites also allow users to pose questions to the community via a question-answering (QA) system. As one would expect, just as opinions diverge among different reviewers, answers to such questions may also be subjective, opinionated, and divergent. This means that answering such questions automatically is quite different from traditional QA tasks, where it is assumed that a single 'correct' answer is available. While recent work introduced the idea of question-answering using product reviews, it did not account for two aspects that we consider in this paper: (1) Questions have multiple, often divergent, answers, and this full spectrum of answers should somehow be used to train the system, and (2) What makes a 'good' answer depends on the asker and the answerer, and these factors should be incorporated in order for the system to be more personalized. Here we build a new QA dataset with 800 thousand questions-and over 3.1 million answers-and show that explicitly accounting for personalization and ambiguity leads both to quantitatively better answers, but also a more nuanced view of the range of supporting, but subjective, opinions.}, 
keywords={Web sites;question answering (information retrieval);QA dataset;QA system;QA tasks;opinion question answering systems;personalization;product review Web sites;Cameras;Data mining;Information retrieval;Knowledge discovery;Lenses;Predictive models;Standards}, 
doi={10.1109/ICDM.2016.0060}, 
month={Dec},}
@INPROCEEDINGS{7280731, 
author={J. Costa and C. Silva and M. Antunes and B. Ribeiro}, 
booktitle={2015 International Joint Conference on Neural Networks (IJCNN)}, 
title={The impact of longstanding messages in micro-blogging classification}, 
year={2015}, 
pages={1-8}, 
abstract={Social networks are making part of the daily routine of millions of users. Twitter is among Facebook and Instagram one of the most used, and can be seen as a relevant source of information as users share not only daily status, but rapidly propagate news and events that occur worldwide. Considering the dynamic nature of social networks, and their potential in information spread, it is imperative to find learning strategies able to learn in these environments and cope with their dynamic nature. Time plays an important role by easily out-dating information, being crucial to understand how informative can past events be to current learning models and for how long it is relevant to store previously seen information, to avoid the computation burden associated with the amount of data produced. In this paper we study the impact of longstanding messages in micro-blogging classification by using different training time-window sizes in the learning process. Since there are few studies dealing with drift in Twitter and thus little is known about the types of drift that may occur, we simulate different types of drift in an artificial dataset to evaluate and validate our strategy. Results shed light on the relevance of previously seen examples according to different types of drift.}, 
keywords={learning (artificial intelligence);pattern classification;social networking (online);Facebook;Instagram;Twitter;daily status;events propagation;information source;information spread;learning models;learning process;learning strategies;longstanding messages;microblogging classification;news propagation;social networks;training time-window sizes;Tagging;Twitter}, 
doi={10.1109/IJCNN.2015.7280731}, 
ISSN={2161-4393}, 
month={July},}
@INPROCEEDINGS{7487632, 
author={H. Hu and G. Kantor}, 
booktitle={2016 IEEE International Conference on Robotics and Automation (ICRA)}, 
title={Instance selection for efficient and reliable camera calibration}, 
year={2016}, 
pages={4335-4340}, 
abstract={The popularity of cameras for perception is enabled in part by powerful intrinsic calibration routines, commonly requiring a user to manually collect images of a known calibration target. The manual nature of this process produces training data that is unevenly spread in the camera relative pose space. If we desire camera parameters that perform well on average over the entire relative pose space, training on such a dataset results in poor performance. To address this, we show that reasoning about the training data distribution to select a more uniformly-spread subset of images produces more accurate and stable calibrations with fewer images. Our approach can be used easily with most camera calibration algorithms. We demonstrate in large-scale physical experiments the effect of non-uniform training data and show that our approach outperforms baselines in reprojection error and parameter variance.}, 
keywords={calibration;cameras;parameter estimation;unsupervised learning;camera calibration;instance selection;large-scale physical experiments;nonuniform training data;parameter variance;reprojection error;training data distribution;Calibration;Cameras;Kernel;Mathematical model;Robot vision systems;Training;Training data}, 
doi={10.1109/ICRA.2016.7487632}, 
month={May},}
@INPROCEEDINGS{7557494, 
author={A. Abdullah and X. Li}, 
booktitle={2016 IEEE International Conference on Services Computing (SCC)}, 
title={Agent-Based Model to Web Service Composition}, 
year={2016}, 
pages={523-530}, 
abstract={Traditional centralized Web Service Composition (WSC) approaches suffer from critical problems such as performance bottleneck and single point of failure. However, the analogy between Service and Agent Computing paradigms suggests that incorporating both technologies will likely lead to a more effective hybrid service model. In this work, we adopt an agent-based approach to WSC, in which Service Dependency Graph (SDG) is constructed as an AND/OR graph, distributed among the agent community members. Upon receiving a user composition request, agents perform internal reasoning and corporate through a communication protocol attempting to find a solution. Experiments are conducted on a public dataset and results show the effectiveness of the proposed approach in terms of communication cost.}, 
keywords={Web services;graph theory;software agents;AND/OR graph;SDG;WSC approach;Web service composition;agent computing paradigm;agent-based model;communication protocol;service computing paradigm;service dependency graph;Cognition;Computational modeling;Connectors;Knowledge based systems;Protocols;Receivers;Web services}, 
doi={10.1109/SCC.2016.74}, 
month={June},}
@INPROCEEDINGS{5935306, 
author={N. Cao and C. Wang and M. Li and K. Ren and W. Lou}, 
booktitle={2011 Proceedings IEEE INFOCOM}, 
title={Privacy-preserving multi-keyword ranked search over encrypted cloud data}, 
year={2011}, 
pages={829-837}, 
abstract={With the advent of cloud computing, data owners are motivated to outsource their complex data management systems from local sites to the commercial public cloud for great flexibility and economic savings. But for protecting data privacy, sensitive data has to be encrypted before outsourcing, which obsoletes traditional data utilization based on plaintext keyword search. Thus, enabling an encrypted cloud data search service is of paramount importance. Considering the large number of data users and documents in the cloud, it is necessary to allow multiple keywords in the search request and return documents in the order of their relevance to these keywords. Related works on searchable encryption focus on single keyword search or Boolean keyword search, and rarely sort the search results. In this paper, for the first time, we define and solve the challenging problem of privacy-preserving multi-keyword ranked search over encrypted cloud data (MRSE). We establish a set of strict privacy requirements for such a secure cloud data utilization system. Among various multi-keyword semantics, we choose the efficient similarity measure of “coordinate matching”, i.e., as many matches as possible, to capture the relevance of data documents to the search query. We further use “inner product similarity” to quantitatively evaluate such similarity measure. We first propose a basic idea for the MRSE based on secure inner product computation, and then give two significantly improved MRSE schemes to achieve various stringent privacy requirements in two different threat models. Thorough analysis investigating privacy and efficiency guarantees of proposed schemes is given. Experiments on the real-world dataset further show proposed schemes indeed introduce low overhead on computation and communication.}, 
keywords={cloud computing;cryptography;data privacy;query processing;Boolean keyword search;cloud computing;cloud data utilization system;coordinate matching measurement;data encryption;data privacy;inner product similarity measurement;multikeyword semantics;plaintext keyword search;privacy-preserving multikeyword ranked search;single keyword search;Cloud computing;Data privacy;Encryption;Indexes;Privacy;Servers}, 
doi={10.1109/INFCOM.2011.5935306}, 
ISSN={0743-166X}, 
month={April},}
@INPROCEEDINGS{7849737, 
author={F. Liu and X. Xu and C. Qing}, 
booktitle={2016 IEEE International Conference on Consumer Electronics-China (ICCE-China)}, 
title={Temporal order information for complex action recognition}, 
year={2016}, 
pages={1-4}, 
abstract={Exploiting simple actions to recognize complex actions instead of using complex actions as training samples can save labor expenses and time consumption. Each complex action is composed of a sequence of simple actions and different manners of combinations of simple actions can form different complex actions. Thus, in this paper, we focus on temporal order information (TOI), which can be used to improve the performance of complex action recognition. To utilize temporal information, a TOI model is proposed for complex action recognition so that users can utilize TOI for extracting features. Classifiers learned from simple actions are used to obtain classification scores that are concatenated as the complex actions final features. The proposed approach is evaluated on two complex action datasets: an Olympic Sports dataset and a YouTube Action dataset. The experiment results showed the effectiveness of the proposed method for complex action recognition.}, 
keywords={feature extraction;image classification;image motion analysis;image sequences;Olympic Sports dataset;TOI;YouTube Action dataset;action sequence;complex action recognition;feature extraction;temporal order information;Computer vision;Conferences;Feature extraction;Histograms;Pattern recognition;Trajectory;YouTube;classification scores;complex action recognition;feature extraction;simple actions;temporal order information}, 
doi={10.1109/ICCE-China.2016.7849737}, 
month={Dec},}
@INPROCEEDINGS{7545841, 
author={A. A. Farhan and J. Lu and J. Bi and A. Russell and B. Wang and A. Bamis}, 
booktitle={2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)}, 
title={Multi-view Bi-clustering to Identify Smartphone Sensing Features Indicative of Depression}, 
year={2016}, 
pages={264-273}, 
abstract={Depression is a major public health issue with direct and significant effects on both physical and mental health. In this study, we analyze smartphone sensing data to find differential behavioral features that are correlated with depression measures such as patient health questionnaire (PHQ-9). Our approach uses an innovative multi-view bi-clustering algorithm. It takes multiple views of sensing data as input to identify homogeneous behavioral groups and simultaneously the key sensing features that characterize the different groups. Using a publicly available dataset, we discover that these behavioral groups with differential sensing features are highly discriminative of PHQ-9 scores that are self reported by the study subjects. For instance, the group comprising less active users in the sensed activities corresponds to overall higher PHQ-9 scores. We then employ the key sensing features that distinguish the different groups to create predictive models to predict the group assignment of individuals. We verify the generalizability of these models using the support vector machine classifier. Cross validation studies show that our classifiers can classify individuals into the correct subgroups with an overall accuracy of 87%.}, 
keywords={health care;mobile computing;pattern clustering;smart phones;statistical analysis;support vector machines;SVM classifier;depression;multiview biclustering;public health;smart phone sensing feature;support vector machine classifier;Data mining;Feature extraction;Global Positioning System;Market research;Mood;Sensors;Support vector machines;Depression;Human Behavior;Machine Learning;Smartphones}, 
doi={10.1109/CHASE.2016.27}, 
month={June},}
@INPROCEEDINGS{5423029, 
author={Y. Mallya and P. Kumar}, 
booktitle={2010 IEEE 2nd International Advance Computing Conference (IACC)}, 
title={Shape constrained mesh editing for delineating anatomical structures in volumetric datasets}, 
year={2010}, 
pages={97-100}, 
abstract={Organ delineation from volumetric dataset is often encountered problem in medical imaging. Numerous 3D polygonal surface mesh model based segmentation algorithms have been reported in this area. These algorithms aim for a fully automated solution for segmenting anatomical structures in volumetric image datasets. But attraction to false boundaries results in inaccurate segmentation, which can happen because of poor model initialization or weak image feature response at correct locations. To be acceptable in clinical practice, it is crucial for a segmentation approach to include the possibility of integrating input from the user when automatic segmentation fails to provide the desired accuracy. This work presents a new 3D triangular surface mesh editing technique to correct the results of automatic segmentation. The method directly projects subset of mesh vertices that are closest to a user drawn line, representing the true boundary of the anatomical structures. Its neighboring vertices that are within user defined depth get deformed based on shape constraints resulting in a smooth edited surface. The shape constraint is formulated to maintain the distribution of the vertices in the region of edited mesh in correspondence with the initial surface model. The result of the study shows that the manually edited surface results in accurate (1.52+/- 0.5mm) anatomical boundary on the edited image plane while preserving the shape of the surface in the neighboring region.}, 
keywords={image segmentation;medical image processing;delineating anatomical structures;image segmentation;medical imaging;organ delineation;shape constrained mesh editing;shape constraints;vertices;volumetric datasets;Anatomical structure;Animation;Application software;Biomedical imaging;Computer graphics;Equations;Image segmentation;Shape;Solid modeling;Technological innovation;Mesh editing;Model Based Segmentation;Organ delineation;Shape constraints}, 
doi={10.1109/IADCC.2010.5423029}, 
month={Feb},}
@INPROCEEDINGS{7838273, 
author={H. Bashashati and R. K. Ward and A. Bashashati and A. Mohamed}, 
booktitle={2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)}, 
title={Neural Network Conditional Random Fields for Self-Paced Brain Computer Interfaces}, 
year={2016}, 
pages={939-943}, 
abstract={The task of classifying EEG signals for self-paced Brain Computer Interface (BCI) applications is extremely challenging. This difficulty in classification of self-paced data stems from the fact that the system has no clue about the start time of a control task and the data contains a large number of periods during which the user has no intention to control the BCI. Therefore, to improve the performance of the BCI, it is imperative to exploit the characteristics of the EEG data as much as possible. For motor imagery based self-paced BCIs, during motor imagery task the EEG signal of each subject goes through several internal state changes. Applying appropriate classifiers that can exploit the temporal correlation in EEG data can enhance the performance of the BCI. In this paper, we propose an algorithm which is able to capture the temporal correlation of the EEG signal. We compare the performance of our algorithm that is based on neural network conditional random fields to two well-known dynamic classifiers, the Hidden Markov Models and Conditional Random Fields and to the static classifier, Support Vector Machines. We compare these methods using the data from SM2 dataset, and we show that our algorithm yields results that are considerably superior to the other approaches in terms of the Area Under the Curve (AUC) of the BCI system.}, 
keywords={brain-computer interfaces;correlation methods;electroencephalography;hidden Markov models;medical signal processing;neural nets;signal classification;support vector machines;BCI;EEG data;EEG signal classification;SM2 dataset;hidden Markov models;motor imagery based self-paced BCI;neural network conditional random fields;self-paced brain computer interfaces;support vector machines;temporal correlation;Brain modeling;Electroencephalography;Feature extraction;Heuristic algorithms;Hidden Markov models;Support vector machines;Training}, 
doi={10.1109/ICMLA.2016.0169}, 
month={Dec},}
@INPROCEEDINGS{6025460, 
author={Y. Ke-xin and Z. Jian-qi}, 
booktitle={2011 International Conference on Mechatronic Science, Electric Engineering and Computer (MEC)}, 
title={Simulation on email worms propagation}, 
year={2011}, 
pages={299-301}, 
abstract={Email is one of the most convenient and indispensable communication mediums in our life. However, virus and email worms quickly evolved the ability to spread through the Internet by various means such as email, and exploiting vulnerabilities, etc. This paper presents a novel email worm propagation simulation algorithm based on the user's behavior and the network topology. With Enron Email Dataset, we construct a simulation environment for the study of email worm spread, and the results show that the presented algorithm in this paper can describe the propagation of email worms accurately.}, 
keywords={electronic mail;invasive software;user interfaces;E-mail worm propagation;Enron E-mail dataset;Internet;user behavior;Educational institutions;Electronic mail;Grippers;Network topology;Security;Software;Topology;Email worm propagation;Network topology;User behavior model}, 
doi={10.1109/MEC.2011.6025460}, 
month={Aug},}
@INPROCEEDINGS{6832008, 
author={P. Yang and X. Gui and F. Tian and J. Yao and J. Lin}, 
booktitle={2013 IEEE 10th International Conference on High Performance Computing and Communications 2013 IEEE International Conference on Embedded and Ubiquitous Computing}, 
title={A Privacy-Preserving Data Obfuscation Scheme Used in Data Statistics and Data Mining}, 
year={2013}, 
pages={881-887}, 
abstract={Many applications are benefited from data sharing, especially data statistics and data mining. But as the shared data may contain private information of data owner, it has a high risk of revealing data owner's privacy. Data obfuscation is proposed to gain a balance between data privacy and data usability. But it is hard for the present obfuscation schemes to remain the usability of data in a fine-grained level. Besides, the original data can't be retrieved from the obfuscated data. To address the above issues, we proposed a data obfuscation scheme that adds an accurate "noise" to the original data to protect the privacy while keeping the numeral characteristics of data unchanged in different levels. Besides, the scheme can also lower the impact on data mining. Furthermore, by allocating different keys to users, different users have different permissions to access to data. To achieve this, our scheme comes in four steps. Firstly, an improved cloud model is proposed to generate an accurate "noise". Next, an obfuscation algorithm is propose to add noise to the original data. Then, an initial scheme for dataset obfuscation is proposed, including the grouping and key allocating processes. In the final step, a fine-grained grouping scheme based on similarity is proposed. The experiments show that our scheme obfuscates date correctly, efficiently, and securely.}, 
keywords={cloud computing;data mining;data protection;statistical analysis;data mining;data owner privacy;data privacy protection;data sharing;data statistics;data usability;dataset obfuscation;fine-grained grouping scheme;fine-grained level;grouping allocating processes;improved cloud model;key allocating processes;numeral characteristics;privacy-preserving data obfuscation scheme;Data privacy;Encryption;Generators;Noise;Usability;data mining;numeral characteristics;obfuscation;privacy-preserving}, 
doi={10.1109/HPCC.and.EUC.2013.126}, 
month={Nov},}
@INPROCEEDINGS{4641320, 
author={M. Perttunen and M. V. Kleek and O. Lassila and J. Riekki}, 
booktitle={2008 The Second International Conference on Mobile Ubiquitous Computing, Systems, Services and Technologies}, 
title={Auditory Context Recognition Using SVMs}, 
year={2008}, 
pages={102-108}, 
abstract={We study auditory context recognition for context-aware mobile computing systems. Auditory contexts are recordings of a mixture of sounds, or ambient audio, from mobile users' everyday environments. Fortraining a classifier, a set of recordings from different environments are segmented and labeled. The segments are windowed into overlapping frames for feature extraction. While previous work in auditory context recognition has often treated the problem as a sequence classification task and used HMM-based classifiers to recognize a sequence of consecutive MFCCs of frames, we compute averaged Mel-spectrum over the segments and train a SVM-based classifier. Our scheme outperforms an already reported HMM-based scheme. This result is achieved using the same dataset. We also show that often the feature sets used by previous work are affected by attenuation, limiting their applicability in practice. Furthermore, we study the impact of segment duration on recognition accuracy.}, 
keywords={feature extraction;hidden Markov models;mobile computing;support vector machines;HMM-based classifiers;Mel-spectrum computation;SVM-based classifier;auditory context recognition;context-aware mobile computing systems;feature extraction;recognition accuracy;Audio recording;Context-aware services;Feature extraction;Hidden Markov models;Humans;Layout;Mobile computing;Pervasive computing;Portable computers;Speech;audio;classification;pervasive computing;ubiquitous computing}, 
doi={10.1109/UBICOMM.2008.21}, 
month={Sept},}
@INPROCEEDINGS{5995514, 
author={S. Mittal and S. Anand and P. Meer}, 
booktitle={CVPR 2011}, 
title={Generalized projection based M-estimator: Theory and applications}, 
year={2011}, 
pages={2689-2696}, 
abstract={We introduce a robust estimator called generalized projection based M-estimator (gpbM) which does not require the user to specify any scale parameters. For multiple inlier structures, with different noise covariances, the estimator iteratively determines one inlier structure at a time. Unlike pbM, where the scale of the inlier noise is estimated simultaneously with the model parameters, gpbM has three distinct stages-scale estimation, robust model estimation and inlier/outlier dichotomy. We evaluate our performance on challenging synthetic data, face image clustering upto ten different faces from Yale Face Database B and multi-body projective motion segmentation problem on Hopkins155 dataset. Results of state-of-the-art methods are presented for comparison.}, 
keywords={estimation theory;face recognition;image denoising;image segmentation;Hopkins155 dataset;Yale Face Database B;face image clustering;generalized projection based M-estimator;gpbM;inlier/outlier dichotomy;multi body projective motion segmentation problem;multiple inlier structures;noise covariances;robust model estimation;stages scale estimation;Bismuth;Computational modeling;Covariance matrix;Estimation;Kernel;Noise;Robustness}, 
doi={10.1109/CVPR.2011.5995514}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{1647609, 
author={Shu-Yen Wan and Chian-Hung Hou}, 
booktitle={19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06)}, 
title={Synergy of Symmetric Region Grow and Active Contour in Reconstruction of a 3D Rat}, 
year={2006}, 
pages={436-441}, 
abstract={The general goal of this work is modeling and reconstruction of a three-dimensional (3D) rat from a series of two-dimensional (2D) images captured from an inexpensive digital camera. We proposed a hybrid segmentation method that incorporates symmetric region grow (symRG) and active contour modeling (ACM) to robustly extract regions of interest (ROIs), such as organs, spines, and vessels. symRG is employed to enhance the segmentation performance while the edge information passed from the ACM can help prevent over-segmentation. We built a component-based software platform that includes the symRG and ACM components as well as the other image enhancement, post-segmentation processing, surface rendering components allowing the user to dynamically compose a streamlined 3D rat reconstruction procedure or script. The example dataset in this paper include 284 slices of 2D rat whole-body images. Separate scripts were used to model and visualize the body, heart, lung, stomach, and head. Few user-imposed parameters were required and the whole processing , from loading series of 2D images towards 3D rendition to demonstrate the results, is within two minutes}, 
keywords={biomedical optical imaging;cardiology;image enhancement;image reconstruction;image segmentation;lung;medical image processing;2D rat whole-body images;3D rat;active contour modeling;digital camera;head;heart;hybrid segmentation method;image enhancement;image reconstruction;lung;organs;post-segmentation processing;spines;stomach;surface rendering;symmetric region grow;two-dimensional images;vessels;Active contours;Data mining;Digital cameras;Image enhancement;Image reconstruction;Image segmentation;Rendering (computer graphics);Robustness;Streaming media;Surface reconstruction}, 
doi={10.1109/CBMS.2006.153}, 
ISSN={1063-7125}, 
month={},}
@INPROCEEDINGS{7877974, 
author={Y. Wang and Y. Guo and Y. Chen}, 
booktitle={2016 IEEE 13th International Conference on Signal Processing (ICSP)}, 
title={Accurate and early prediction of user lifespan in an online video-on-demand system}, 
year={2016}, 
pages={969-974}, 
abstract={Online video on demand (VoD) service is prevailing. Prediction of user lifespan in a VoD system benefits the service providers to characterize churn risk of users and manage to retain them. A systematical study on this problem is desired but absent in literature. We address this problem based on a large-scale dataset of user watching behavior from PPTV, one of the largest online VoD systems in China. The dataset is measured for 27 weeks and involves more than 10 million users. We analyze user watching behavior and preference in their lifespans and have some interesting observations. During user lifespans, unlike some user activity metrics such as the visiting frequency, the number of views and the finishing ratio that vary following inverted U-shaped curves, a user's preference for popular video contents, named the Popular Video Preference (PVP), decreases with time. As many users left the system very quickly, e.g. after only one week, it is necessary to make early prediction of whether a user will have a long lifespan based on short instead of long behavior history. We propose to apply machine learning methods to make this prediction based on user first-week behavior records. Experimental results show that the most relevant feature is the visit frequency; the PVP feature helps to improve the F1-score of prediction by 8.8% and reaches 0.74 at the best. Our proposed model and the PVP metric are helpful for VoD service providers to predict user lifespan and take measures to retain users at their early stage in the system.}, 
keywords={behavioural sciences;learning (artificial intelligence);prediction theory;telecommunication computing;video on demand;China;F1-score;PPTV;PVP;VoD service;inverted U-shaped curves;machine learning methods;online video-on-demand system;popular video contents;popular video preference;user lifespan prediction;user watching behavior;Correlation;Finishing;History;Learning systems;Measurement;Smart phones;TV;Data mining;Lifespan;Machine learning;User behavior analysis;Video}, 
doi={10.1109/ICSP.2016.7877974}, 
ISSN={2164-5221}, 
month={Nov},}
@INPROCEEDINGS{4587621, 
author={K. Wnuk and S. Soatto}, 
booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Filtering Internet image search results towards keyword based category recognition}, 
year={2008}, 
pages={1-8}, 
abstract={In this work we aim to capitalize on the availability of Internet image search engines to automatically create image training sets from user provided queries. This problem is particularly difficult due to the low precision of image search results. Unlike many existing dataset gathering approaches, we do not assume a category model based on a small subset of the noisy data or an ad-hoc validation set. Instead we use a nonparametric measure of strangeness [8] in the space of holistic image representations, and perform an iterative feature elimination algorithm to remove the most strange examples from the category. This is the equivalent of keeping only features that are found to be consistent with others in the class. We show that applying our method to image search data before training improves average recognition performance, and demonstrate that we obtain comparative precision and recall results to the current state of the art, all the while maintaining a significantly simpler approach. In the process we also extend the strangeness-based feature elimination algorithm to automatically select good threshold values and perform filtering of a single class when the background is given.}, 
keywords={Internet;computer vision;information filtering;search engines;Internet image search engines;dataset gathering;filtering Internet image search results;holistic image representations;iterative feature elimination algorithm;keyword based category recognition;Computer science;Filtering algorithms;Image recognition;Information filtering;Information filters;Internet;Noise reduction;Search engines;Testing;Training data}, 
doi={10.1109/CVPR.2008.4587621}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{6151510, 
author={M. M. Uddin and M. T. Hassan and A. Karim}, 
booktitle={2011 IEEE 14th International Multitopic Conference}, 
title={Personalized versus non-personalized tag recommendation: A suitability study on three social networks}, 
year={2011}, 
pages={56-61}, 
abstract={Tag recommendation systems are either personalized or non-personalized. Personalized tag recommendation utilizes a user's tagging behavior from her tagging history for predictions. Whereas non-personalized recommendation systems recommend what is popular and relevant to the user. In this study, we have analyzed the role of personal tagging history in recommending tags. The experiments are done on three folksonomy datasets: Delicious, Flickr and Bibsonomy. Important results for three popular tag recommendation algorithms: PITF, FolkRank and Adapted PageRank are reported in terms of prediction quality. It is found that users' history usage preferences change across all data sets; hence overall prediction quality of personalized recommendation system may suffer. We discover a generic life cycle of folksonomy users on the basis of their history usage. We propose this life cycle can be used to improve an overall prediction performance of a recommendation system across all folksonomies.}, 
keywords={recommender systems;social networking (online);user interfaces;Adapted PageRank algorithm;Bibsonomy dataset;Delicious dataset;Flickr dataset;FolkRank algorithm;PITF algorithm;history usage;nonpersonalized tag recommendation;personalized tag recommendation;social networks;tag recommendation system;usage preference;user tagging behavior;Adaptation models;Lead;Prediction algorithms;Folksonomy;Personalization;Tag Recommendation}, 
doi={10.1109/INMIC.2011.6151510}, 
month={Dec},}
@INPROCEEDINGS{7074191, 
author={G. A. Liu and J. H. L. Hansen}, 
booktitle={2011 19th European Signal Processing Conference}, 
title={A systematic strategy for robust automatic dialect identification}, 
year={2011}, 
pages={2138-2141}, 
abstract={Automatic dialect Classification is very important for speech based human computer interface and customer electronic products. Although many studies have been performed in ideal environment, little work has been done in noisy or small data corpus, both of which are very critical for the survival of a dialect identification system. This paper investigates a series of strategies to address the question of small and noisy dataset dialect classification task. A novel hierarchical universal background model is proposed to address the question of limited training dataset. To address the noisy question, we initiate the use of perceptual minimum variance distortionless response (PMVDR), combining with shifted delta cepstral (SDC) algorithm. Rotation forest is also explored to further improve the system performance. Finally, compared with the baseline system, the proposed best system shows relative gains of 31:8% and 28:7%, in the worse noise and clean condition on a small data set, respectively.}, 
keywords={cepstral analysis;electronic products;human computer interaction;pattern classification;speech processing;speech recognition;speech-based user interfaces;PMVDR;SDC algorithm;automatic dialect classification;clean condition;customer electronic products;hierarchical universal background model;noisy dataset dialect classification task;perceptual minimum variance distortionless response;robust automatic dialect identification;shifted delta cepstral algorithm;speech based human computer interface;training dataset;worse noise;Feature extraction;Mel frequency cepstral coefficient;Noise;Noise measurement;Robustness;Speech;Training}, 
ISSN={2076-1465}, 
month={Aug},}
@INPROCEEDINGS{6619090, 
author={J. Xu and M. D. Collins and V. Singh}, 
booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Incorporating User Interaction and Topological Constraints within Contour Completion via Discrete Calculus}, 
year={2013}, 
pages={1886-1893}, 
abstract={We study the problem of interactive segmentation and contour completion for multiple objects. The form of constraints our model incorporates are those coming from user scribbles (interior or exterior constraints) as well as information regarding the topology of the 2-D space after partitioning (number of closed contours desired). We discuss how concepts from discrete calculus and a simple identity using the Euler characteristic of a planar graph can be utilized to derive a practical algorithm for this problem. We also present specialized branch and bound methods for the case of single contour completion under such constraints. On an extensive dataset of ~1000 images, our experiments suggest that a small amount of side knowledge can give strong improvements over fully unsupervised contour completion methods. We show that by interpreting user indications topologically, user effort is substantially reduced.}, 
keywords={graph theory;image segmentation;tree searching;unsupervised learning;2D space;Euler characteristic;discrete calculus;interactive segmentation;planar graph;single contour completion;specialized branch and bound methods;topological constraints;unsupervised contour completion methods;user interaction;Calculus;Detectors;Image edge detection;Image segmentation;Optimization;Spirals;Vectors;contour completion;discrete calculus;interactive segmentation;topology}, 
doi={10.1109/CVPR.2013.246}, 
ISSN={1063-6919}, 
month={June},}
@ARTICLE{7464876, 
author={T. A. Hoang and E. P. Lim}, 
journal={IEEE Transactions on Knowledge and Data Engineering}, 
title={Microblogging Content Propagation Modeling Using Topic-Specific Behavioral Factors}, 
year={2016}, 
volume={28}, 
number={9}, 
pages={2407-2422}, 
abstract={When a microblogging user adopts some content propagated to her, we can attribute that to three behavioral factors, namely, topic virality, user virality, and user susceptibility. Topic virality measures the degree to which a topic attracts propagations by users. User virality and susceptibility refer to the ability of a user to propagate content to other users, and the propensity of a user adopting content propagated to her, respectively. In this paper, we study the problem of mining these behavioral factors specific to topics from microblogging content propagation data. We first construct a three dimensional tensor for representing the propagation instances. We then propose a tensor factorization framework to simultaneously derive the three sets of behavioral factors. Based on this framework, we develop a numerical factorization model and another probabilistic factorization variant. We also develop an efficient algorithm for the models' parameters learning. Our experiments on a large Twitter dataset and synthetic datasets show that the proposed models can effectively mine the topic-specific behavioral factors of users and tweet topics. We further demonstrate that the proposed models consistently outperforms the other state-of-the-art content based models in retweet prediction over time.}, 
keywords={data mining;learning (artificial intelligence);probability;social networking (online);tensors;Twitter dataset;content based models;microblogging content propagation modeling;model parameters learning;numerical factorization model;probabilistic factorization variant;propagation instance representation;retweet prediction;synthetic datasets;tensor factorization framework;three dimensional tensor;topic virality;topic-specific behavioral factor mining;user propensity;user susceptibility;user virality;Data models;Numerical models;Predictive models;Receivers;Tensile stress;Twitter;Content propagation;microblogging;susceptibility;user behavior;virality}, 
doi={10.1109/TKDE.2016.2562628}, 
ISSN={1041-4347}, 
month={Sept},}
@INPROCEEDINGS{6526103, 
author={C. Ye and K. Zheng and C. She}, 
booktitle={Proceedings of 2012 2nd International Conference on Computer Science and Network Technology}, 
title={Application layer ddos detection using clustering analysis}, 
year={2012}, 
pages={1038-1041}, 
abstract={Many methods were designed in previous literatures to protect systems from IP and TCP layers distributed denial of service attacks instead of the application layer. However, they will not work well any more when encountering with application layer distributed denial of service. We will introduce clustering method to analysis application layer ddos in this paper. To capture users' browsing behavior, we cluster users' sessions. We consider bots' browsing behavior as abnormally behavior. That is, different from normal human behavior. We first extract four features from session to cluster users sessions-average size of objects requested in the session, request rate, average popularity of all objects in the session, average transition probability. Then, we use large amount of legitimate request sequence to get normal user browsing behavior models. Finally, conduct simulation experiments with attack dataset to validate the models.}, 
keywords={IP networks;computer network security;IP layer;TCP layer;application layer ddos detection;attack dataset;average transition probability;clustering analysis;distributed denial of service attack;user browsing behavior model;application;browsing behavior;cluster;ddos}, 
doi={10.1109/ICCSNT.2012.6526103}, 
month={Dec},}
@INPROCEEDINGS{7312081, 
author={S. Zhu and Y. Han and Y. Wei}, 
booktitle={2015 International Conference on Intelligent Networking and Collaborative Systems}, 
title={Controlling Outsourcing Data in Cloud Computing with Attribute-Based Encryption}, 
year={2015}, 
pages={257-261}, 
abstract={In our IT society, cloud computing is clearly becoming one of the dominating infrastructures for enterprises as long as end users. As more cloud based services available to end users, their oceans of data are outsourced in the cloud as well. Without any special mechanisms, the data may be leaked to a third party for unauthorized use. Most presented works of cloud computing put these emphases on computing utility or new types of applications. But in the view of cloud users, such as traditional big companies, data in cloud computing systems is tend to be out of control and privacy fragile. So most of data they outsourced is less important. A mechanism to guarantee the ownership of data is required. In this paper, we analyzed a couple of recently presented scalable data management models to describe the storage patterns of data in cloud computing systems. Then we defined a new tree-based dataset management model to solve the storage and sharing problems in cloud computing. A couple of operation strategies including data encryption, data boundary maintenance, and data proof are extracted from the view of different entities in the cloud. The behaviors of different users are controlled by view management on the tree. Based on these strategies, a flexible data management mechanism is designed in the model to guarantee entity privacy, data availability and secure data sharing.}, 
keywords={cloud computing;cryptography;data privacy;outsourcing;trees (mathematics);attribute-based encryption;cloud computing system;data availability;data management model;data outsourcing;data sharing security;entity privacy;tree-based dataset management model;Access control;Cloud computing;Computational modeling;Data models;Data privacy;Encryption;Cloud Computing;Data Privacy;Database Management;Outsourcing Data}, 
doi={10.1109/INCoS.2015.29}, 
month={Sept},}
@INPROCEEDINGS{6804448, 
author={T. Ahmed and A. Srivastava}, 
booktitle={Electrical, Electronics and Computer Science (SCEECS), 2014 IEEE Students' Conference on}, 
title={A data-centric and machine based approach towards fixing the cold start problem in web service recommendation}, 
year={2014}, 
pages={1-6}, 
abstract={Web services are independent, modular and autonomous pieces of software offering functionality that help an organization achieve reduced development cost. However, the issue of selecting a web service from a set of similar services is non-trivial. Recently, service recommendation via collaborative filtering has started to receive attention in the research community as a criterion for selection. However, a problem with such a method is `cold start', wherein some of the users must invoke certain web services, so that other services can be recommended. In this paper, we propose a data driven approach towards web service recommendation to solve the cold start problem. To demonstrate the viability of the proposed technique in real time applications, we experiment with a real world dataset consisting of 150 web services invoked by 100 users around the globe. Moreover, as a proof of concept, we have developed a web based prototype capable of predicting QoS values and recommending services in real time. We present and discuss our findings in the result section.}, 
keywords={Web services;collaborative filtering;quality of service;recommender systems;QoS values;Web based prototype;Web service recommendation;autonomous software pieces;cold start problem;collaborative filtering;data driven approach;data-centric approach;machine based approach;Accuracy;Business;Computational modeling;Frequency modulation;Logic gates;Quality of service;World Wide Web;QoS;Service Selection;Web Services}, 
doi={10.1109/SCEECS.2014.6804448}, 
month={March},}
@INPROCEEDINGS{7785743, 
author={C. Grant and D. Z. Wang and M. Wick}, 
booktitle={2016 IEEE 17th International Conference on Information Reuse and Integration (IRI)}, 
title={Query-Driven Sampling for Collective Entity Resolution}, 
year={2016}, 
pages={208-217}, 
abstract={Entity Resolution is the process of determining records (mentions) in a database that correspond to the same real-world entity. Traditional pairwise ER methods can lead to inconsistencies and low accuracy due to localized decisions. Leading ER systems solve this problem by collectively resolving all records using a probabilistic graphical model and Markov chain Monte Carlo (MCMC) inference. However, for large datasets, this is an extremely expensive process. One key observation is that such exhaustive ER process incurs a huge up-front cost, which is wasteful in practice because most users are interested in only a small subset of entities. In this paper, we advocate pay-as-you-go entity resolution by developing a number of query-driven collective ER techniques. We introduce two classes of SQL queries that involve ER operators - selection-driven ER and join-driven ER. We implement novel variations of the MCMC Metropolis-Hastings algorithm to generate biased samples and selectivity-based scheduling algorithms to support the two classes of ER queries. Finally, we show that query-driven ER algorithms can converge and return results within minutes over a database populated with the extraction from a newswire dataset containing 71 million mentions.}, 
keywords={Markov processes;Monte Carlo methods;SQL;query processing;MCMC inference;MCMC metropolis-hastings algorithm;Markov chain Monte Carlo inference;SQL queries;collective entity resolution;join-driven ER;pairwise ER methods;probabilistic graphical model;query-driven sampling;selection-driven ER;selectivity-based scheduling algorithms;Convergence;Databases;Erbium;Graphical models;Markov processes;Probabilistic logic;Random variables;entity resolution;mcmc;metropolis hastings;query-driven}, 
doi={10.1109/IRI.2016.34}, 
month={July},}
@INPROCEEDINGS{6637501, 
author={H. Mehta and V. S. Dixit and P. Bedi}, 
booktitle={2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)}, 
title={Weighted difference entropy based similarity measure at two levels in a recommendation framework}, 
year={2013}, 
pages={2076-2083}, 
abstract={Presenting small chunks of interesting information to a target user from the large pool of information available on World Wide Web are the primary task of a recommender system. Memory based Collaborative Filtering generates recommendations based on preferences of those users whose past preferences are similar to the current preferences of the target user. These users collectively form the Nearest Neighbor Set (NN Set) of the target user. The better the selection of NN Set, the better is the generation of recommendations. In the proposed scheme, the user ratings (preferences) on the available items are divided into two levels. Level I consist of ratings on popular items and Level II consist of ratings on unpopular items. The proposed similarity measure, “TSimD(UX1,UX2)”, between two users is based on weighted difference entropy. Modified memory based Collaborative Filtering calculates the proposed similarity measure at both the levels to improve the selection of users in the NN Set of the target user. It selects those users in the NN Set who are more similar to the target user with respect to items at Level II as compared to similarity between them with respect to items at Level I. The results on Movie Lens dataset depicted that the proposed similarity measure had higher accuracy than the Weighted Difference Entropy based similarity measure which worked on the entire preferences of the users. The proposed similarity measure was also compared with other similarity measures (like Cosine, Pearson Correlation, Spearman and Rating Frequency based Similarity Measure) which were also obtained at two levels.}, 
keywords={Internet;Web sites;collaborative filtering;entropy;recommender systems;relevance feedback;set theory;Movie Lens dataset;NN set selection;TSimD(UX1,UX2);World Wide Web;information pool;modified memory based collaborative filtering;recommendation framework;recommendation generation;recommender system;target user nearest neighbor set;user preferences;user ratings;user selection;weighted difference entropy based similarity measure;Correlation;Entropy;Equations;Frequency measurement;Mathematical model;Training;Weight measurement;Entropy;Memory based Collaborative Filtering;User Rating Preferences}, 
doi={10.1109/ICACCI.2013.6637501}, 
month={Aug},}
@INPROCEEDINGS{7403603, 
author={A. Papaoikonomou and M. Kardara and T. Varvarigou}, 
booktitle={2015 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
title={Trust inference in online social networks}, 
year={2015}, 
pages={600-604}, 
abstract={We study the problem of trust inference in signed social networks, in which, in addition to rating items, users can also indicate their disposition towards each other through directional signed links. We explore the problem in a semi-supervised setting, where given a small fraction of signed edges we classify the remaining edges by leveraging contextual information (i.e. the users' ratings). In order to model user behavior, we use deep learning algorithms i.e. a variation of Restricted Boltzmann machine and Autoencoders for user encoding and edge classification respectively. We evaluate our approach on a large-scale real-world dataset and show that it outperforms state-of-the art methods.}, 
keywords={Boltzmann machines;inference mechanisms;learning (artificial intelligence);security of data;social networking (online);autoencoder;contextual information;deep learning algorithm;edge classification;online social network;restricted Boltzmann machine;trust inference;user encoding;Art;Binary codes;Classification algorithms;Encoding;Machine learning;Social network services;Training;autoencoders;edge classification;restricted boltzmann machines;signed social networks;trust}, 
doi={10.1145/2808797.2809418}, 
month={Aug},}
@INPROCEEDINGS{7574681, 
author={J. A. Yang and C. H. Lee and S. W. Yang and V. S. Somayazulu and Y. K. Chen and S. Y. Chien}, 
booktitle={2016 IEEE International Conference on Multimedia Expo Workshops (ICMEW)}, 
title={Wearable social camera: Egocentric video summarization for social interaction}, 
year={2016}, 
pages={1-6}, 
abstract={Wearable social camera is an egocentric camera that summarizes the video of the user's social activities. This paper proposes a core technology of the wearable social camera: egocentric video summarization for social interaction. Different from other works of third-person action/interaction recognition in egocentric videos, which focus on distinguishing different actions, this work finds the common features among all the interactions, which is called interaction features (IF). IF of the third-person is proposed to be composed of three parts: physical information of head, body languages, and emotional expression. Furthermore, hidden Markov model (HMM) is employed to model the interaction sequences, and a summarized video is generated with hidden Markov support vector machine (HM-SVM). Experimental results with a life-log dataset show that the proposed system performs well for summarizing life-log videos.}, 
keywords={cameras;emotion recognition;face recognition;hidden Markov models;image motion analysis;object recognition;support vector machines;video signal processing;HM-SVM;HMM;body languages;egocentric camera;egocentric video summarization;emotional expression;hidden Markov model;hidden Markov support vector machine;interaction features;interaction recognition;interaction sequences;life-log videos;social interaction;third-person action recognition;user social activities;wearable social camera;Cameras;Feature extraction;Hidden Markov models;Magnetic heads;Mouth;Training;Video sequences;Egocentric Video Summarization;Life Log;Social Camera;Social Interaction Detection}, 
doi={10.1109/ICMEW.2016.7574681}, 
month={July},}
@INPROCEEDINGS{5687065, 
author={F. P. Lousame and E. Sánchez}, 
booktitle={2010 10th International Conference on Intelligent Systems Design and Applications}, 
title={Multicriteria predictors using aggregation functions based on item views}, 
year={2010}, 
pages={947-952}, 
abstract={Multicriteria Collaborative Filtering is a promising approach to recommender systems that explores user ratings on item components in order to generate high quality recommendations. This paper focuses on multicriteria collaborative recommender systems and proposes a new algorithm that estimates aggregation functions, which represent the relative importance of individual components, based on the concept of item views. Experiments on a real multicriteria movie dataset demonstrate that our approach outperforms other aggregation models in terms of prediction precision and coverage. Furthermore, the study shows how the concept of item views (i) naturally emerges from the properties of the dataset, (ii) addresses the multicriteria recommendation problem, (iii) provides a mechanism to explain recommendations and (iv) drives the implementation of the rich user interfaces required by this type of recommender systems.}, 
keywords={groupware;information filtering;recommender systems;aggregation functions;item views;multicriteria collaborative filtering;multicriteria predictors;multicriteria recommendation problem;recommender systems;Aggregation Models;Collaborative Filtering;Item Views;Multicriteria Recommender Systems}, 
doi={10.1109/ISDA.2010.5687065}, 
ISSN={2164-7143}, 
month={Nov},}
@INPROCEEDINGS{4677368, 
author={S. Barlowe and Tianyi Zhang and Yujie Liu and J. Yang and D. Jacobs}, 
booktitle={2008 IEEE Symposium on Visual Analytics Science and Technology}, 
title={Multivariate visual explanation for high dimensional datasets}, 
year={2008}, 
pages={147-154}, 
abstract={Understanding multivariate relationships is an important task in multivariate data analysis. Unfortunately, existing multivariate visualization systems lose effectiveness when analyzing relationships among variables that span more than a few dimensions. We present a novel multivariate visual explanation approach that helps users interactively discover multivariate relationships among a large number of dimensions by integrating automatic numerical differentiation techniques and multidimensional visualization techniques. The result is an efficient workflow for multivariate analysis model construction, interactive dimension reduction, and multivariate knowledge discovery leveraging both automatic multivariate analysis and interactive multivariate data visual exploration. Case studies and a formal user study with a real dataset illustrate the effectiveness of this approach.}, 
keywords={data analysis;data mining;data visualisation;differentiation;interactive systems;high dimensional dataset;interactive dimension reduction;multivariate analysis model construction;multivariate data analysis;multivariate data visual explanation;multivariate knowledge discovery;multivariate visualization system;numerical differentiation technique;Computer science;Data analysis;Data visualization;Displays;Jacobian matrices;Optical scattering;Physics;Proteins;Solvents;Statistical analysis;G.3 [Mathematics of Computing]: Probability and Statistics—Multivariate Statistics;H.5.2 [Information Interfaces and Presentation]: User Interfaces—User Centered Design;dimension reduction;multivariate analysis;multivariate model construction;multivariate visualization;visual analysis}, 
doi={10.1109/VAST.2008.4677368}, 
month={Oct},}
@INPROCEEDINGS{7752287, 
author={F. Morstatter and L. Wu and T. H. Nazer and K. M. Carley and H. Liu}, 
booktitle={2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)}, 
title={A new approach to bot detection: Striking the balance between precision and recall}, 
year={2016}, 
pages={533-540}, 
abstract={The presence of bots has been felt in many aspects of social media. Twitter, one example of social media, has especially felt the impact, with bots accounting for a large portion of its users. These bots have been used for malicious tasks such as spreading false information about political candidates and inflating the perceived popularity of celebrities. Furthermore, these bots can change the results of common analyses performed on social media. It is important that researchers and practitioners have tools in their arsenal to remove them. Approaches exist to remove bots, however they focus on precision to evaluate their model at the cost of recall. This means that while these approaches are almost always correct in the bots they delete, they ultimately delete very few, thus many bots remain. We propose a model which increases the recall in detecting bots, allowing a researcher to delete more bots. We evaluate our model on two real-world social media datasets and show that our detection algorithm removes more bots from a dataset than current approaches.}, 
keywords={security of data;social networking (online);Twitter;bot detection;malicious tasks;recall;social media datasets;Feature extraction;Labeling;Pattern matching;Suspensions;Tagging;Twitter}, 
doi={10.1109/ASONAM.2016.7752287}, 
month={Aug},}
@INPROCEEDINGS{6928662, 
author={A. Nika and A. Ismail and B. Y. Zhao and S. Gaito and G. P. Rossi and H. Zheng}, 
booktitle={10th International Conference on Heterogeneous Networking for Quality, Reliability, Security and Robustness}, 
title={Understanding data hotspots in cellular networks}, 
year={2014}, 
pages={70-76}, 
abstract={The unprecedented growth in mobile data usage is posing significant challenges to cellular operators. One key challenge is how to provide quality of service to subscribers when their residing cell is experiencing a significant amount of traffic, i.e. becoming a traffic hotspot. In this paper, we perform an empirical study on data hotspots in today's cellular networks using a 9-week cellular dataset with 734K+ users and 5327 cell sites. Our analysis examines in details static and dynamic characteristics, predictability, and causes of data hotspots, and their correlation with call hotspots. We believe the understanding of these key issues will lead to more efficient and responsive resource management and thus better QoS provision in cellular networks. To the best of our knowledge, our work is the first to characterize in detail traffic hotspots in today's cellular networks using real data.}, 
keywords={cellular radio;quality of service;telecommunication network management;telecommunication traffic;QoS provision;cellular dataset;cellular networks;cellular operators;data hotspots;mobile data usage;quality of service;responsive resource management;traffic hotspot;Cities and towns;Correlation;Distributed databases;Distribution functions;Graphical models;Internet;Quality of service}, 
doi={10.1109/QSHINE.2014.6928662}, 
month={Aug},}
@ARTICLE{6650039, 
author={P. Blanchart and M. Depecker}, 
journal={IEEE Transactions on Neural Networks and Learning Systems}, 
title={A Nonlinear Semantic-Preserving Projection Approach to Visualize Multivariate Periodical Time Series}, 
year={2014}, 
volume={25}, 
number={6}, 
pages={1053-1070}, 
abstract={A major drawback of nonlinear dimensionality reduction (DR) techniques is their inability to preserve some authentic information from the source domain, leading to projections that are often hard to interpret when it comes to observing anything other than the topological structure of the data. In this paper, we propose a nonlinear DR approach enforcing projection constraints resulting from an a priori knowledge about the structure of the data in multivariate periodical time series. We then propose several ways of exploiting this constrained projection to extract user-relevant information, such as the nominal behavior of a periodical dynamical system or the deviant behaviors which may occur at different time scales. The techniques are demonstrated on both a synthetic dataset composed of simulated multivariate data exhibiting a periodical behavior, and a real dataset corresponding to six months of sensor data acquisitions and recordings inside experimental buildings.}, 
keywords={data reduction;data visualisation;time series;authentic information;multivariate periodical time series visualization;nonlinear DR approach;nonlinear dimensionality reduction techniques;nonlinear semantic-preserving projection approach;simulated multivariate data;source domain;synthetic dataset;user-relevant information;Data mining;Data models;Data visualization;Image color analysis;Monitoring;Time series analysis;Visualization;Data mining;deviant behaviors identification;high-dimensional;information visualization;monitoring;nonlinear dimensionality reduction (DR);pseudoperiodical time series;visual analytics;visual analytics.}, 
doi={10.1109/TNNLS.2013.2285928}, 
ISSN={2162-237X}, 
month={June},}
@INPROCEEDINGS{4752326, 
author={L. Gao and C. Li}, 
booktitle={2006 International Technology and Innovation Conference (ITIC 2006)}, 
title={Collaborative filtering recommendation algorithm based on look-ahead selective sampling}, 
year={2006}, 
pages={1948-1952}, 
abstract={Personalized Recommendation System has become an important research item to prove the suitable product and services for individual. And classification of customers becomes the basis to produce recommendation. In a realistic EC system, the magnitudes of customers and products are all huge, so the quality of recommendation decreases dramatically. To improve recommending quantity, a collaborative filtering model was proposed based on look-ahead sampling. In n-dimension Euclid space constituted by users, the proposed algorithm reduces the number of samples while maintaining the quality of classification, through estimating sample’s utility for classifier. At last, experiments were designed at the basis of MoveLens dataset. Compared with general collaborative filtering, the proposed algorithm has higher quality of recommendation.}, 
keywords={Nearest neighbour algorithm;look-ahead algorithm;recommendation system;selective sampling}, 
ISSN={0537-9989}, 
month={Nov},}
@INPROCEEDINGS{6126450, 
author={S. Branson and P. Perona and S. Belongie}, 
booktitle={2011 International Conference on Computer Vision}, 
title={Strong supervision from weak annotation: Interactive training of deformable part models}, 
year={2011}, 
pages={1832-1839}, 
abstract={We propose a framework for large scale learning and annotation of structured models. The system interleaves interactive labeling (where the current model is used to semi-automate the labeling of a new example) and online learning (where a newly labeled example is used to update the current model parameters). This framework is scalable to large datasets and complex image models and is shown to have excellent theoretical and practical properties in terms of train time, optimality guarantees, and bounds on the amount of annotation effort per image. We apply this framework to part-based detection, and introduce a novel algorithm for interactive labeling of deformable part models. The labeling tool updates and displays in real-time the maximum likelihood location of all parts as the user clicks and drags the location of one or more parts. We demonstrate that the system can be used to efficiently and robustly train part and pose detectors on the CUB Birds-200-a challenging dataset of birds in unconstrained pose and environment.}, 
keywords={learning (artificial intelligence);maximum likelihood estimation;object detection;pose estimation;CUB Birds-200;deformable part model;image annotation;interactive labeling;interactive training;large scale learning;maximum likelihood location;online learning;part-based detection;pose detector;structured model annotation;tool updates labeling;Computational modeling;Deformable models;Dynamic programming;Heuristic algorithms;Labeling;Mice;Training}, 
doi={10.1109/ICCV.2011.6126450}, 
ISSN={1550-5499}, 
month={Nov},}
@ARTICLE{6516048, 
author={Z. Sun and P. Yue and L. Hu and J. Gong and L. Zhang and X. Lu}, 
journal={IEEE Transactions on Geoscience and Remote Sensing}, 
title={GeoPWProv: Interleaving Map and Faceted Metadata for Provenance Visualization and Navigation}, 
year={2013}, 
volume={51}, 
number={11}, 
pages={5131-5136}, 
abstract={Visualization of geospatial data provenance aims to provide a user-friendly way for easy navigation and increased understanding of the derivation history of geoscientific results. Most existing work focuses on provenance modeling and management. This paper proposes to interleave map and faceted metadata for geospatial data provenance visualization and navigation. It shows how provenance in Web-geoprocessing workflows can be visualized at four levels: feature, dataset, service, and knowledge. A prototypical system, named GeoPWProv, is developed to demonstrate the applicability of the approach.}, 
keywords={geographic information systems;geophysical techniques;geophysics computing;meta data;GeoPWProv;Web-geoprocessing workflows;faceted metadata;geospatial data provenance navigation;geospatial data provenance visualization;interleaving map;prototypical system;provenance management;provenance modeling;Data visualization;Databases;Geospatial analysis;History;Navigation;Ontologies;Resource description framework;Faceted metadata;geoscientific workflow;geospatial data provenance;provenance visualization;web map}, 
doi={10.1109/TGRS.2013.2248064}, 
ISSN={0196-2892}, 
month={Nov},}
@INPROCEEDINGS{5539880, 
author={R. Li and R. Chellappa}, 
booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, 
title={Group motion segmentation using a Spatio-Temporal Driving Force Model}, 
year={2010}, 
pages={2038-2045}, 
abstract={We consider the `group motion segmentation' problem and provide a solution for it. The group motion segmentation problem aims at analyzing motion trajectories of multiple objects in video and finding among them the ones involved in a `group motion pattern'. This problem is motivated by and serves as the basis for the `multi-object activity recognition' problem, which is currently an active research topic in event analysis and activity recognition. Specifically, we learn a Spatio-Temporal Driving Force Model to characterize a group motion pattern and design an approach for segmenting the group motion. We illustrate the approach using videos of American football plays, where we identify the offensive players, who follow an offensive motion pattern, from motions of all players in the field. Experiments using GaTech Football Play Dataset validate the effectiveness of the segmentation algorithm.}, 
keywords={image motion analysis;image recognition;image segmentation;American football plays;GaTech football play dataset;group motion pattern;group motion segmentation;motion trajectories;multiobject activity recognition problem;spatiotemporal driving force model;Automation;Change detection algorithms;Computer vision;Educational institutions;Legged locomotion;Motion analysis;Motion detection;Motion segmentation;Pattern analysis;Pattern recognition}, 
doi={10.1109/CVPR.2010.5539880}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7344647, 
author={K. Karpouzis and G. N. Yannakakis and N. Shaker and S. Asteriadis}, 
booktitle={2015 International Conference on Affective Computing and Intelligent Interaction (ACII)}, 
title={The platformer experience dataset}, 
year={2015}, 
pages={712-718}, 
abstract={Player modeling and estimation of player experience have become very active research fields within affective computing, human computer interaction, and game artificial intelligence in recent years. For advancing our knowledge and understanding on player experience this paper introduces the Platformer Experience Dataset (PED) - the first open-access game experience corpus - that contains multiple modalities of user data of Super Mario Bros players. The open-access database aims to be used for player experience capture through context-based (i.e. game content), behavioral and visual recordings of platform game players. In addition, the database contains demographical data of the players and self-reported annotations of experience in two forms: ratings and ranks. PED opens up the way to desktop and console games that use video from webcameras and visual sensors and offer possibilities for holistic player experience modeling approaches that can, in turn, yield richer game personalization.}, 
keywords={artificial intelligence;computer games;human computer interaction;psychology;PED;Platformer Experience Dataset;Super Mario Bros players;affective computing;behavioral recordings;console games;demographical data;desktop games;game artificial intelligence;game personalization;human computer interaction;open-access database;open-access game experience corpus;player experience estimation;player modeling;self-reported annotations;visual recordings;visual sensors;webcameras;Analytical models;Cameras;Computational modeling;Data models;Games;Sensors;Visualization;facial expression;head pose;mutimodal data corpus;platform games;player behavior;player experience;visual analysis}, 
doi={10.1109/ACII.2015.7344647}, 
month={Sept},}
@INPROCEEDINGS{6076428, 
author={J. Hoxha and A. Brahaj}, 
booktitle={2011 International Conference on Emerging Intelligent Data and Web Technologies}, 
title={Open Government Data on the Web: A Semantic Approach}, 
year={2011}, 
pages={107-113}, 
abstract={Initiatives of making government data open are continuously gaining interest recently. While this presents immense benefits for increasing transparency, the problem is that the data are frequently offered in heterogeneous formats, missing clear semantics that clarify what the data describe. The data are displayed in ways, which are not always clearly understandable to a broad range of user communities that need to make informed decisions. We address these problems and propose an overall approach, in which raw statistical data independently gathered from the different government institutions are formally and semantically represented, based on an ontology that we present in this paper. We further introduce the approach deployed in publishing these data in alignment with Linked Data principles, as well as present the methods implemented to query single or combined dataset and visualize the results in understandable ways. The introduced approach enables data integration, leading to vast opportunities for information exchange, analysis on combined datasets, simplicity to create mashups, and exploration of innovative ways to use these data creatively.}, 
keywords={government data processing;meta data;ontologies (artificial intelligence);semantic Web;statistical analysis;data integration;decision making;government institutions;information analysis;information exchange;linked data principles;ontology;open government data;semantic approach;statistical data;user communities;Data models;Data visualization;Government;Ontologies;Publishing;Resource description framework;Semantics;linked open data;ontology;open government data;semantic web;statistical metadata}, 
doi={10.1109/EIDWT.2011.24}, 
month={Sept},}
@INPROCEEDINGS{7066836, 
author={S. Ferlito and M. Atrigna and G. Graditi and S. De Vito and M. Salvato and A. Buonanno and G. Di Francia}, 
booktitle={2015 XVIII AISEM Annual Conference}, 
title={Predictive models for building's energy consumption: An Artificial Neural Network (ANN) approach}, 
year={2015}, 
pages={1-4}, 
abstract={Building's energy demand is influenced by many factors, such as: weather conditions, building structure and characteristics, energy consumption of components (lighting and HVAC systems), level of occupancy and user's behavior. As consequence of multi-variable impact on building's energy consumption, theoretical models based on first principles are not able to forecast actual energy demand of a generic building. In this paper, an Artificial Neural Network (ANN) model applied to a real case consisting in a dataset of monthly historical building electric energy consumption is developed. Results show that accuracy of energy consumption forecast runs, in terms of RMSPE (root mean square percentage error), in the range 15.7% to 17.97% and varies slightly according to the prediction horizon (3 months, 6 months and 12 months).}, 
keywords={building management systems;energy consumption;least mean squares methods;load forecasting;neural nets;power engineering computing;ANN;RMSPE;artificial neural network;building energy demand;energy demand forecasting;generic building;monthly historical building electric energy consumption;predictive models;root mean square percentage error;Accuracy;Artificial neural networks;Buildings;Delays;Energy consumption;Forecasting;Predictive models;Artificial Neural Networks;Building's Energy consumption;NAR;Prediction}, 
doi={10.1109/AISEM.2015.7066836}, 
month={Feb},}
@ARTICLE{7055939, 
author={R. Pedarsani and M. A. Maddah-Ali and U. Niesen}, 
journal={IEEE/ACM Transactions on Networking}, 
title={Online Coded Caching}, 
year={2016}, 
volume={24}, 
number={2}, 
pages={836-845}, 
abstract={We consider a basic content distribution scenario consisting of a single origin server connected through a shared bottleneck link to a number of users each equipped with a cache of finite memory. The users issue a sequence of content requests from a set of popular files, and the goal is to operate the caches as well as the server such that these requests are satisfied with the minimum number of bits sent over the shared link. Assuming a basic Markov model for renewing the set of popular files, we characterize approximately the optimal long-term average rate of the shared link. We further prove that the optimal online scheme has approximately the same performance as the optimal offline scheme, in which the cache contents can be updated based on the entire set of popular files before each new request. To support these theoretical results, we propose an online coded caching scheme termed coded least-recently sent (LRS) and simulate it for a demand time series derived from the dataset made available by Netflix for the Netflix Prize. For this time series, we show that the proposed coded LRS algorithm significantly outperforms the popular least-recently used caching algorithm.}, 
keywords={Markov processes;cache storage;file servers;time series;Netflix Prize;basic Markov model;cache contents;coded LRS algorithm;content distribution scenario;demand time series;finite memory cache;least-recently sent;least-recently used caching algorithm;online coded caching;optimal offline scheme;optimal online scheme;single origin server;Cache memory;IEEE transactions;Motion pictures;Multicast communication;Servers;Time series analysis;Vectors;Coded caching;content distribution;online scheme}, 
doi={10.1109/TNET.2015.2394482}, 
ISSN={1063-6692}, 
month={April},}
@INPROCEEDINGS{6115501, 
author={V. W. Bandara and A. P. Jayasumana}, 
booktitle={2011 IEEE 36th Conference on Local Computer Networks}, 
title={Extracting baseline patterns in Internet traffic using Robust Principal Components}, 
year={2011}, 
pages={407-415}, 
abstract={Robust BaseLine (RBL) is a formal technique for extracting the baseline of network traffic to capture the underlying traffic trend. A range of applications such as anomaly detection and load balancing rely on baseline estimation. Once the fundamental period of the pattern for analysis is recognized, e.g., based on user interest or a period detector such as Autocorrelation Function (ACF), the basic extraction is carried out in two steps. First, the common component across the dataset is separated using Robust Principal Component Analysis (RPCA). The fundamental pattern in the common component is extracted using Principal Component Analysis (PCA) in the second step. Scaling factors required to fit the base-pattern back into the data are returned automatically by PCA. Two types of traffic baselines may be extracted: RBL-L captures the common behavior across time on a single link, and RBL-N captures the common behavior across a network of links, i.e., in space. RBL-N is particularly useful for specifying traffic matrices more efficiently over time, which normally requires multiple updates to follow baseline trends. The derived base-patterns for a single link or a single time period is then extended over the entire network or thru the entire observation period with a compressive analysis. The compressed base-pattern provides a smoother baseline and also a filter to separate baseline traffic and the deviations on the fly from traffic measurements. When compared against BLGBA (Baseline for Automatic Backbone Management) the proposed scheme provides a less noisy, more precisely fitting baseline. It is also more effective in revealing anomalies.}, 
keywords={Internet;principal component analysis;telecommunication traffic recording;baseline patterns;compressed base pattern;internet traffic;robust baseline;robust principal component analysis;traffic baselines;traffic measurement;Data mining;Hidden Markov models;Internet;Principal component analysis;Robustness;Sparse matrices;Time frequency analysis;Anomaly detection;Baselining;Internet Traffic;Load balancing;Traffic characterization}, 
doi={10.1109/LCN.2011.6115501}, 
ISSN={0742-1303}, 
month={Oct},}
@ARTICLE{1425668, 
author={T. van Walsum and S. A. M. Baert and W. J. Niessen}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Guide wire reconstruction and visualization in 3DRA using monoplane fluoroscopic imaging}, 
year={2005}, 
volume={24}, 
number={5}, 
pages={612-623}, 
abstract={A method has been developed that, based on the guide wire position in monoplane fluoroscopic images, visualizes the approximate guide wire position in the three-dimensional (3-D) vasculature, that is obtained prior to the intervention with 3-D rotational X-ray angiography (3DRA). The method assumes the position of the guide wire in the fluoroscopic images is known. A two-dimensional feature image is determined from the 3DRA data. In this feature image, the guide wire position is determined in a two-step approach: a mincost algorithm is used to determine a suitable position for the guide wire, and subsequently a snake optimization technique is applied to move the guide wire to a better position. The resulting guide wire can then be visualized in 3-D in combination with the 3DRA dataset. The reconstruction accuracy of the method has been evaluated using a 3DRA image of a vascular phantom filled with contrast, and monoplane fluoroscopic images of the same phantom without contrast and with a guide wire inserted. The evaluation has been performed for different projection angles, and with different parameters for the method. The final result does not appear to be very sensitive to the parameters of the method. The average mean error of the estimated 3-D guide wire position is 1.5 mm, and the average tip distance is 2.3 mm. The effect of inaccurate C-arm geometry information is also investigated. Small errors in geometry information (up to 1°) will slightly decrease the 3-D reconstruction accuracies, with an error of at most 1 mm. The feasibility of this approach on clinical data is demonstrated.}, 
keywords={diagnostic radiography;image reconstruction;medical image processing;optimisation;phantoms;3-D rotational X-ray angiography;guide wire reconstruction;inaccurate C-arm geometry information;mincost algorithm;monoplane fluoroscopic imaging;snake optimization technique;vascular phantom;visualization;Angiography;Data visualization;Image reconstruction;Imaging phantoms;Information geometry;Optical imaging;Performance evaluation;Three dimensional displays;Wire;X-ray imaging;3-D reconstruction;Angiography;fluoroscopy;monoplane;Algorithms;Angiography;Artificial Intelligence;Catheterization;Cluster Analysis;Computer Simulation;Feasibility Studies;Fluoroscopy;Humans;Imaging, Three-Dimensional;Models, Biological;Pattern Recognition, Automated;Phantoms, Imaging;Radiographic Image Enhancement;Radiographic Image Interpretation, Computer-Assisted;Reproducibility of Results;Sensitivity and Specificity;Surgery, Computer-Assisted;User-Computer Interface}, 
doi={10.1109/TMI.2005.844073}, 
ISSN={0278-0062}, 
month={May},}
@INPROCEEDINGS{7469029, 
author={W. Ren and J. Yan}, 
booktitle={2015 8th International Symposium on Computational Intelligence and Design (ISCID)}, 
title={An Improved CMAC Neural Network Model for Web Mining}, 
year={2015}, 
volume={1}, 
pages={614-618}, 
abstract={In recent years, the application of Web technology is more and more mature, and all manner of Web sites and search engines are distributed on the Internet resulting in a huge source of information. Thus, if the Web information is not well organized, it is hard to find the information what a user actually need or interest in. Automatic classification of Web pages is an efficient method in Web Content Mining which can be of great value in the management of Web directories. Based on the analysis done, The Cerebellar Model Articulation Controller (CMAC) is an excellent classification technique, but when it is applied to deal with high-dimensional dataset such as the data on the Internet, the memory required increase intensively. This paper presents an improved CMAC model used in content based Web page classification which requires less memory. The experimental results show that the proposed model is highly effective in Web page classification.}, 
keywords={Web sites;cerebellar model arithmetic computers;data mining;pattern classification;text analysis;Web content mining;Web directory management;Web information;automatic Web page classification;cerebellar model articulation controller;high-dimensional dataset;improved CMAC neural network model;Data models;Feature extraction;Hypercubes;Input variables;Memory management;Neural networks;Web pages;Web page classification;memory required;the improved CMAC}, 
doi={10.1109/ISCID.2015.61}, 
month={Dec},}
@INPROCEEDINGS{6729611, 
author={S. Liu and L. Li and R. Krishnan}, 
booktitle={2013 IEEE 13th International Conference on Data Mining}, 
title={Hibernating Process: Modelling Mobile Calls at Multiple Scales}, 
year={2013}, 
pages={1139-1144}, 
abstract={Do mobile phone calls at larger granularities behave in the same pattern as in smaller ones? How can we forecast the distribution of a whole month's phone calls with only one day's observation? There are many models developed to interpret large scale social graphs. However, all of the existing models focus on graph at one time scale. Many dynamical behaviors were either ignored, or handled at one scale. In particular new users might join or current users quit social networks at any time. In this paper, we propose HiP, a novel model to capture longitudinal behaviors in modeling degree distribution of evolving social graphs. We analyze a large scale phone call dataset using HiP, and compare with several previous models in literature. Our model is able to fit phone call distribution at multiple scales with 30% to 75% improvement over the best existing method on each scale.}, 
keywords={graph theory;mobile computing;social sciences computing;hibernating process;large scale phone call dataset;large scale social graphs;social graphs;social networks;Data models;Hip;Mobile communication;Mobile computing;Mobile handsets;Parametric statistics;Social network services;Mobile phone call graph;churning behavior;heavy tailed distribution;non-parametric model}, 
doi={10.1109/ICDM.2013.82}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{7823894, 
author={Y. Sun and H. Ceker and S. Upadhyaya}, 
booktitle={2016 IEEE International Workshop on Information Forensics and Security (WIFS)}, 
title={Shared keystroke dataset for continuous authentication}, 
year={2016}, 
pages={1-6}, 
abstract={Keystroke dynamics is an effective behavioral biometrics for user authentication at a computer terminal. Continuous or active authentication using keystroke dynamics has raised a lot of interest among researchers. However, there are only a few public datasets available for the research community compared to other biometric modalities primarily because of the difficulty of large scale data collection. Even the existing ones generally suffer from small number of subjects and lack of extensive features. In this paper, we provide the details on the collection of a shared dataset for the study of keystroke dynamics. We have collected raw keystroke data from 157 subjects allowing them to transcribe fixed text and answer questions freely. The dataset is characterized to reflect the temporal variations of typing patterns and the perturbations caused by different keyboard layouts. To show the usability and the quality of our dataset, we apply an existing algorithm, viz. Gaussian mixture model for keystroke analysis on the dataset and report the results.}, 
keywords={Gaussian processes;authorisation;biometrics (access control);mixture models;Gaussian mixture model;active authentication;behavioral biometrics;biometric modalities;computer terminal;continuous authentication;keyboard layouts;keystroke analysis;keystroke dynamics;public datasets;research community;shared keystroke dataset;temporal variations;user authentication;Authentication;Benchmark testing;Data collection;Electronic mail;Heuristic algorithms;Keyboards;Mice}, 
doi={10.1109/WIFS.2016.7823894}, 
month={Dec},}
@ARTICLE{4360140, 
author={J. Pettersson and K. L. Palmerius and H. Knutsson and O. Wahlstrom and B. Tillander and M. Borga}, 
journal={IEEE Transactions on Biomedical Engineering}, 
title={Simulation of Patient Specific Cervical Hip Fracture Surgery With a Volume Haptic Interface}, 
year={2008}, 
volume={55}, 
number={4}, 
pages={1255-1265}, 
abstract={The interest for surgery simulator systems with anatomical models generated from authentic patient data is growing as these systems evolve. With access to volumetric patient data, e.g., from a computer tomography scan, haptic and visual feedback can be created directly from this dataset. This opens the door for patient specific simulations. Hip fracture surgery is one area where simulator systems is useful to train new surgeons and plan operations. To simulate the drilling procedure in this type of surgery, a repositioning of the fractured bone into correct position is first needed. This requires a segmentation process in which the bone segments are identified and the position of the dislocated part is determined. The segmentation must be automatic to cope with the large amount of data from the computer tomography scan. This work presents the first steps in the development of a hip fracture surgery simulation with patient specific models. Visual and haptic feedback is generated from the computer tomography data by simulating fluoroscopic images and the drilling process. We also present an automatic segmentation method to identify the fractured bone and determine the dislocation. This segmentation method is based on nonrigid registration with the Morphon method.}, 
keywords={biomedical education;bone;computer aided instruction;computerised tomography;diagnostic radiography;feedback;haptic interfaces;image registration;image segmentation;medical image processing;orthopaedics;surgery;training;Morphon method;anatomical models;automatic segmentation process;bone dislocation determination;computer tomography scan;drilling procedure;fluoroscopic images;fractured bone repositioning;haptic feedback;nonrigid registration;patient specific cervical hip fracture surgery;surgery simulator systems;visual feedback;volume haptic interface;Bones;Computational modeling;Computer simulation;Drilling;Feedback;Haptic interfaces;Hip;Image segmentation;Surgery;Tomography;Automatic segmentation;Patient specific data;Registration;Simulation;Surgery;Volume haptics;patient specific data;registration;surgery simulation;volume haptics;Computer Simulation;Femoral Neck Fractures;Humans;Imaging, Three-Dimensional;Models, Biological;Radiographic Image Interpretation, Computer-Assisted;Surgery, Computer-Assisted;Tomography, X-Ray Computed;Touch;User-Computer Interface}, 
doi={10.1109/TBME.2007.908099}, 
ISSN={0018-9294}, 
month={April},}
@INPROCEEDINGS{7128918, 
author={F. Belbachir and B. Le Grand}, 
booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
title={Opinion detection: Influence factors}, 
year={2015}, 
pages={522-523}, 
abstract={Many online social networks (like blogs or Twitter) allow users to post and share their opinions on various topics. The detection and interpretation of these subjective comments is strategic for various organizational and business purposes, e.g., product and service benchmarking, ads placement or market intelligence. This article aims at enhancing the opinion detection process, i.e., the identification of documents that reflect an opinion, whatever their polarities - positive or negative. Our contribution consists in analyzing the factors that influence the detection of opinions. In particular, we investigate three factors: document's time, topic, and topic category. We have conducted an experiment to detect opinions in the TREC Blog 06 dataset, using the IMDB data collection as a reference. Our experimental results report that time, topics and topic categories have an impact on the opinion detection process.}, 
keywords={data mining;emotion recognition;information retrieval;social networking (online);IMDB data collection;TREC Blog 06 dataset;Twitter;ads placement;blogs;business purposes;influence factors;market intelligence;online social networks;opinion detection process;opinion sharing;organizational purposes;product benchmarking;service benchmarking;subjective comment detection;subjective comment interpretation;Blogs;Data collection;Data mining;Electronic mail;Internet;Organizations;Social network services;Information retrieval;Opinion detection;Social Networks;categorization;language model}, 
doi={10.1109/RCIS.2015.7128918}, 
ISSN={2151-1349}, 
month={May},}
@INPROCEEDINGS{7758024, 
author={Duong Thi Thu Huyen and Le Hoang Son and Tran Manh Tuan and A. Drogoul}, 
booktitle={2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)}, 
title={Semi-supervised fuzzy co-clustering for hospital-cost analysis from electronic medical records}, 
year={2016}, 
pages={25-30}, 
abstract={It has been widely recognized that decision-making is a crucial part of hospital management which goes through a process of medical behavior. Even though data mining and knowledge discovery techniques have been used to clinical medicine frequently, little research has been conducted on hospital decision-making especially hospital-cost analysis on treatment therapies among inpatients which is considered as an important aspect of annual hospital evaluation and accreditation. In this paper, we propose a novel semi-supervised fuzzy co-clustering method for hospital-cost analysis from electronic medical records. Fuzzy co-clustering is a well-known technique that performs simultaneous fuzzy clustering of objects and features which result in dynamic dimensionality reduction mechanism for categorizing high-dimensional data. However, in many real-world applications, prior knowledge of a dataset is actually available to the users; thus it is necessary to integrate this information in the clustering process. This approach is called semi-supervised fuzzy co-clustering which has been investigated and developed further within an application to hospital-cost analysis of Hanoi Medical University Hospital (HMUH), Vietnam. The findings of the paper suggest the most crucial factors for medical expense in HMUH which are significant to gradually reduce the cost of treatment meanwhile improve the quality of services.}, 
keywords={cost reduction;data integration;data reduction;electronic health records;fuzzy set theory;pattern clustering;HMUH;Hanoi Medical University Hospital;annual hospital accreditation;annual hospital evaluation;decision-making;dynamic dimensionality reduction;electronic medical records;high-dimensional data categorization;hospital management;hospital-cost analysis;information integration;medical behavior;medical expense;quality of services;semisupervised fuzzy co-clustering;treatment cost reduction;treatment therapies;Biomedical imaging;Clustering algorithms;Hospitals;Integrated circuits;Linear programming;Mathematical model;Surgery;Fuzzy co-clustering;Hospital-cost analysis;Medical expense;Medical informatics;Semi-supervised fuzzy clustering}, 
doi={10.1109/KSE.2016.7758024}, 
month={Oct},}
@INPROCEEDINGS{6878959, 
author={Le Quoc Thang and C. Temiyasathit}, 
booktitle={2014 International Conference on Computer, Information and Telecommunication Systems (CITS)}, 
title={Increase performance of four-class classification for motor-imagery based brain-computer interface}, 
year={2014}, 
pages={1-5}, 
abstract={Brain computer interface (BCI) is a system that provide a direct communication between human brain and external devices. BCIs which based on mental tasks of users are widely used for disabled or paralyzed patients, in order to help their mobility. Preprocessing techniques have been extensively developed to increase the signal-to-noise ratio and spatial distribution of the signals. Common Spatial Pattern (CSP) has shown to be a robust and effective method for processing Electroencephalogram (EEG) data. However, the results of CSP filter are still far from being completely explored. CSP was originally designed for two-class problem despite the fact that a practical application of Motor-imagery (MI) based BCI contains numbers of activities. It is necessary to design the classification algorithm which applicable to more than two-class problem. In this paper we investigate the performance of CSP by selecting optimal time slice and components for training CSP filters in four-class BCI by separating the four-class problem into multiple binary classifications. Our method is verified in the testing phase with four different types of classification approaches which are linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), linear support vector machines (LSVM), and support vector machines with radial basis function kernel (RBF-SVM). The result showed that, under the optimal time slice and components, the classification accuracy reach 78.82% for the best untrained subject in this dataset.}, 
keywords={brain-computer interfaces;electrocardiography;medical signal processing;radial basis function networks;signal classification;spatial filters;statistical analysis;support vector machines;BCI;CSP filter;EEG data;LDA;LSVM;MI based BCI;QDA;RBF-SVM;common spatial pattern;electroencephalogram data processing;four-class classification problem;linear discriminant analysis;linear support vector machines;motor-imagery based brain-computer interface;multiple binary classifications;optimal spatial filters;preprocessing techniques;quadratic discriminant analysis;signal-to-noise ratio;spatial signal distribution;support vector machines with radial basis function kernel;testing phase;Accuracy;Brain modeling;Covariance matrices;Electroencephalography;Support vector machines;Testing;Training;Brain-Computer Interface;Classification;Common Spatial Pattern;Motor Imagery;Optimal Spatial Filters}, 
doi={10.1109/CITS.2014.6878959}, 
month={July},}
@INPROCEEDINGS{7799649, 
author={J. Wei and Z. Xu and W. Xia}, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={DQAF: Towards DQV-Based Dataset Quality Annotation Using the Web Annotation Data Model}, 
year={2016}, 
pages={24-27}, 
abstract={The W3C Data on the Web Best Practices Working Group is standardizing the Data Quality Vocabulary (DQV) for expressing data quality of Web-published datasets. As proposed in the DQV specification, quality annotations on datasets, one kind of quality information described using DQV, are achieved through Web annotations. Meanwhile, the W3C Web Annotation Working Group is creating a standard Web Annotation Data Model on the basis of the Open Annotation (OA) Data Model. Despite the significant progress in standardization, there is a lack of systematic research on Web tools for DQV-based dataset quality annotation. This paper therefore proposes a Dataset Quality Annotation Framework (DQAF) that provides annotating users with an interactive visual user interface, through which DQV-based dataset quality annotation can be readily achieved using the OA data model to produce machine-readable quality annotation data. We have implemented a proof-of-concept prototype of DQAF and conducted case study experiments with the prototype. The results indicate that DQAF is feasible and implementable, and annotating users can obtain a better understanding of and more intuitive interaction with the quality annotation data by means of the user interface.}, 
keywords={Internet;data integrity;data models;interactive systems;vocabulary;DQAF;DQV specification;W3C data;Web annotation data model;Web annotation working group;Web best practices working group;Web-published datasets;data quality vocabulary;dataset quality annotation framework;interactive visual user interface;machine-readable quality annotation data;open annotation data model;Computational modeling;Data models;Indexing;Partitioning algorithms;Social network services;Topology;Data Quality Vocabulary (DQV);Web Annotation Data Model;dataset quality annotation;interactive visual user interface;quality metadata;usage-centered design}, 
doi={10.1109/WISA.2016.15}, 
month={Sept},}
@ARTICLE{6600727, 
author={T. Willemen and D. Van Deun and V. Verhaert and M. Vandekerckhove and V. Exadaktylos and J. Verbraecken and S. Van Huffel and B. Haex and J. Vander Sloten}, 
journal={IEEE Journal of Biomedical and Health Informatics}, 
title={An Evaluation of Cardiorespiratory and Movement Features With Respect to Sleep-Stage Classification}, 
year={2014}, 
volume={18}, 
number={2}, 
pages={661-669}, 
abstract={Polysomnography (PSG) is considered the gold standard to assess sleep accurately, but it can be expensive, time-consuming, and uncomfortable, specifically in long-term sleep studies. Actigraphy, on the other hand, is both cheap and user-friendly, but depending on the application lacks detail and accuracy. Our aim was to evaluate cardiorespiratory and movement signals in discriminating between wake, rapid-eye-movement (REM), light (N1N2), and deep (N3) sleep. The dataset comprised 85 nights of PSG from a healthy population. Starting from a total of 750 characteristic variables (features), problem-specific subsets of 40 features were forwardly selected using the combination of a wrapper method (Cohen's kappa statistic on radial basis function (RBF)-kernel support vector machine (SVM) classifier) and filter method (minimum redundancy maximum relevance criterion on mutual information). Final classification was performed using an RBF-kernel SVM. Non-subject-specific wake versus sleep classification resulted in a Cohen's kappa value of 0.695, while REM versus NREM resulted in 0.558 and N3 versus N1N2 in 0.553. The broad pool of initial features gave insight in which features discriminated best between the different classes. The classification results demonstrate the possibility of making long-term sleep monitoring more widely available.}, 
keywords={biomechanics;cardiovascular system;electrocardiography;eye;feature selection;filtering theory;medical signal processing;patient monitoring;radial basis function networks;signal classification;sleep;support vector machines;Cohen kappa statistics;RBF-kernel SVM;actigraphy;cardiorespiratory evaluation;cardiorespiratory signals;characteristic variables;dataset;deep N3 sleep;feature discrimination;feature selection;filter method;final classification;healthy population;kernel support vector machine classifier;light N1N2 sleep;long-term sleep monitoring;minimum redundancy maximum relevance criterion;movement feature evaluation;movement signals;mutual information;nonsubject-specific wake;polysomnography;problem-specific subsets;radial basis function;rapid-eye-movement;sleep classification;sleep-stage classification;user-friendly;wrapper method;Electrocardiography;Feature extraction;Heart rate;Robustness;Sleep apnea;Support vector machines;Training;Biomedical signal processing;data analysis;medical information systems;sleep research;supervised learning;0;Adult;Heart Rate;Humans;Medical Informatics Applications;Movement;Polysomnography;Respiration;Signal Processing, Computer-Assisted;Support Vector Machine;Young Adult}, 
doi={10.1109/JBHI.2013.2276083}, 
ISSN={2168-2194}, 
month={March},}
@INPROCEEDINGS{7363858, 
author={X. He and W. Dai and G. Cao and R. Tang and M. Yuan and Q. Yang}, 
booktitle={2015 IEEE International Conference on Big Data (Big Data)}, 
title={Mining target users for online marketing based on App Store data}, 
year={2015}, 
pages={1043-1052}, 
abstract={It is well known that the key issue of online marketing is to accurately find the target user groups for the corresponding advertisements. Traditionally, the advertising products target user groups based on search keywords (e.g. AdWords), page visiting (e.g. AdSense), and etc. In this work, we explore a new targeting strategy - targeting users based on their downloaded apps. Specifically, we make use of a subset of the data from the Huawei App Store, consisting of 20,169,033 users and 122,875 apps with 453,346,250 downloads during one year. For each marketing job, the advertiser only need to label a small set of apps, usually around 10 apps, that the target users might be interested in. Our system xRank will automatically find a list of top potential target users for the advertiser. We implement xRank with very efficient performance on the top of Hadoop to be capable for a real web-scale dataset, and then conducted our system to several real marketing tasks. The results show that, for each marketing task, with only a few labels, xRank can effectively find a precise target group of users, and can also significantly improved the effectiveness of our online marketing compared to the rule-based approaches in the current system.}, 
keywords={Internet;advertising data processing;data mining;mobile computing;Huawei App Store data;advertising product;online marketing;target user mining;targeting strategy;xRank system;Advertising;Algorithm design and analysis;Collaboration;Google;Labeling;Mathematical model;Web pages}, 
doi={10.1109/BigData.2015.7363858}, 
month={Oct},}
@INPROCEEDINGS{7557544, 
author={H. Wang and X. Zheng}, 
booktitle={2016 IEEE International Conference on Services Computing (SCC)}, 
title={An Online Prediction Approach for Dynamic QoS}, 
year={2016}, 
pages={852-855}, 
abstract={With the rapidly growing number of Web services, how to identify high quality Web services has become a hot topic. Quality of Service (QoS) is a key criterion for the choice of optimal Web services from a set of candidate Web services with similar functions. However, QoS data is acquired through the invocation of services from users. Thus, QoS prediction is critical for building high-quality service-oriented applications. Since QoS is highly related to dynamic factors such as users' or services' status and network environments which are variable over time, it is an important task to predict the unknown QoS values at runtime. In addition, the factors which cause the change of QoS may be various, such as the influence of noise, the change of the network environments. Prediction without taking account of these factors will affect the prediction accuracy. To address the problems above, we propose an online prediction approach for dynamic QoS (OPA-DQ). OPA-DQ extends matrix factorization into an online approach to make the QoS prediction process more efficient. According to the analysis on the factors which will cause the change of QoS, we build a series of processes to make better QoS prediction performance. Experimental results in a real world dataset indicate that our online approach has higher prediction accuracy and efficiency compared with other approaches.}, 
keywords={Web services;matrix decomposition;quality of service;OPA-DQ;QoS prediction performance;Web services;dynamic QoS;high-quality service-oriented applications;matrix factorization;online prediction approach;quality of service;Fluctuations;Heuristic algorithms;Mathematical model;Measurement;Quality of service;Time factors;Web services;QoS prediction;matrix factorization;online learning;quality of service}, 
doi={10.1109/SCC.2016.122}, 
month={June},}
@INPROCEEDINGS{5137283, 
author={V. Menon and W. M. Pottenger}, 
booktitle={2009 IEEE International Conference on Intelligence and Security Informatics}, 
title={A Higher Order Collective Classifier for detecting and classifying network events}, 
year={2009}, 
pages={125-130}, 
abstract={Labeled data is scarce. Most statistical machine learning techniques rely on the availability of a large labeled corpus for building robust models for prediction and classification. In this paper we present a Higher Order Collective Classifier (HOCC) based on higher order learning, a statistical machine learning technique that leverages latent information present in co-occurrences of items across records. These techniques violate the IID assumption that underlies most statistical machine learning techniques and have in prior work outperformed first order techniques in the presence of very limited data. We present results of applying HOCC to two different network data sets, first for detection and classification of anomalies in a border gateway protocol dataset and second for building models of users from network file system calls to perform masquerade detection. The precision of our system has been shown to be 30% better than the standard Naive Bayes technique for masquerade detection. These results indicate that HOCC can successfully model a variety of network events and can be applied to solve difficult problems in security using the general framework proposed.}, 
keywords={Bayes methods;learning (artificial intelligence);pattern classification;security of data;Naive Bayes technique;anomaly classification;anomaly detection;border gateway protocol dataset;higher order collective classifier;machine learning techniques;masquerade detection;network events classification;network events detection;network file system;Computer science;Data security;Event detection;File systems;Machine learning;Phase detection;Predictive models;Problem-solving;Protocols;Robustness}, 
doi={10.1109/ISI.2009.5137283}, 
month={June},}
@INPROCEEDINGS{6307554, 
author={Y. Li and P. Chen}, 
booktitle={2012 Asia-Pacific Power and Energy Engineering Conference}, 
title={A Parallel SVR Model for Short Term Load Forecasting Based on Windows Azure Platform}, 
year={2012}, 
pages={1-4}, 
abstract={Short term load forecasting (STLF) is an important process in electric power operation and control system. Support Vector Regression (SVR) is proved to be a successful application in STLF, and can get great accuracy and efficiency compared to other STLF models. However, when deal large scale sample size, SVR is poor on the performance. With the development of cloud computing, it is changing people's life in more and more areas. Windows Azure Platform is a cloud computing platform developed by Microsoft. It can easily scale up or down to get compute or storage resource according to requirements. Take into account the advantage and convenience, we propose a parallel SVR model based on Windows Azure Platform to solve the large scale dataset problem of SVR. This model is verified with ENUN standard dataset, the results shows that the model of SVR based on Windows Azure Platform has apparently improvement on efficiency than standard SVR model.}, 
keywords={cloud computing;load forecasting;power engineering computing;power system control;regression analysis;support vector machines;user interfaces;ENUN standard dataset;Microsoft development;STLF;cloud computing platform;electric power control system;electric power operation system;large scale dataset problem;parallel SVR model;short term load forecasting;support vector regression;windows azure platform;Cloud computing;Computational modeling;Load forecasting;Load modeling;Predictive models;Support vector machines;Training}, 
doi={10.1109/APPEEC.2012.6307554}, 
ISSN={2157-4839}, 
month={March},}
@INPROCEEDINGS{6909486, 
author={F. Perbet and S. Johnson and M. T. Pham and B. Stenger}, 
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Human Body Shape Estimation Using a Multi-resolution Manifold Forest}, 
year={2014}, 
pages={668-675}, 
abstract={This paper proposes a method for estimating the 3D body shape of a person with robustness to clothing. We formulate the problem as optimization over the manifold of valid depth maps of body shapes learned from synthetic training data. The manifold itself is represented using a novel data structure, a Multi-Resolution Manifold Forest (MRMF), which contains vertical edges between tree nodes as well as horizontal edges between nodes across trees that correspond to overlapping partitions. We show that this data structure allows both efficient localization and navigation on the manifold for on-the-fly building of local linear models (manifold charting). We demonstrate shape estimation of clothed users, showing significant improvement in accuracy over global shape models and models using pre-computed clusters. We further compare the MRMF with alternative manifold charting methods on a public dataset for estimating 3D motion from noisy 2D marker observations, obtaining state-of-the-art results.}, 
keywords={clothing;data structures;edge detection;motion estimation;3D body shape estimation;3D motion;MRMF;clothed users;clothing;data structure;depth maps;horizontal edges;human body shape estimation;local linear models;manifold charting;multiresolution manifold forest;noisy 2D marker observations;on-the-fly building;overlapping partitions;pre-computed clusters;public dataset;synthetic training data;vertical edges;Clothing;Estimation;Manifolds;Optimization;Shape;Three-dimensional displays;Vegetation;3D shape estimation;human body fitting;manifold charting}, 
doi={10.1109/CVPR.2014.91}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{7348016, 
author={G. Bouloukakis and R. Agarwal and N. Georgantas and A. Pathak and V. Issarny}, 
booktitle={2015 IEEE 11th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)}, 
title={Leveraging CDR datasets for context-rich performance modeling of large-scale mobile pub/sub systems}, 
year={2015}, 
pages={596-603}, 
abstract={Large-scale mobile environments are characterized by, among others, a large number of mobile users, intermittent connectivity and non-homogeneous arrival rate of data to the users, depending on the region's context. Multiple application scenarios in major cities need to address the above situation for the creation of robust mobile systems. Towards this, it is fundamental to enable system designers to tune a communication infrastructure using various parameters depending on the specific context. In this paper, we take a first step towards enabling an application platform for large-scale information management relying on `mobile social crowd-sourcing'. To inform the stakeholders of expected loads and costs, we model a large-scale mobile pub/sub system as a queueing network. We introduce additional timing constraints such as (i) mobile user's intermittent connectivity period; and (ii) data validity lifetime period (e.g. that of sensor data). Using our MobileJINQS simulator, we parameterize our model with realistic input loads derived from the D4D dataset (CDR) and varied lifetime periods in order to analyze the effect on response time. This work provides system designers with coarse grain design time information when setting realistic loads and time constraints.}, 
keywords={message passing;middleware;mobile computing;queueing theory;social networking (online);CDR datasets;MobileJINQS simulator;context-rich performance modeling;intermittent connectivity;large-scale mobile pub-sub systems;mobile social crowd-sourcing;queueing network;Analytical models;Antennas;Context;Context modeling;Data models;Mobile communication;Mobile computing;Call Detail Records;Context-Rich;Large-Scale;Publish/Subscribe;Queueing Networks}, 
doi={10.1109/WiMOB.2015.7348016}, 
month={Oct},}
@INPROCEEDINGS{7558033, 
author={M. Shi and J. Liu and D. Zhou and M. Tang and F. Xie and T. Zhang}, 
booktitle={2016 IEEE International Conference on Web Services (ICWS)}, 
title={A Probabilistic Topic Model for Mashup Tag Recommendation}, 
year={2016}, 
pages={444-451}, 
abstract={Mashups are prevalent Service-Oriented Architecture (SOA) based applications consisting of multiple Web Application Programming Interfaces (APIs) and content. Tags have been extensively used to organize and index mashup services. However, people favor manual tags creation in the past. This approach demands user intervention, which is extremely time-consuming and probes to errors. In this paper we propose a novel Mashup-API-Tag model for automatic mashup tag recommendation. The model simultaneously incorporates the composition relationships between mashups and APIs as well as the annotation relationships between APIs and tags to discover the latent topics. Then the semantic similarity between Web APIs and mashups can be acquired. Subsequently, tags of chosen APIs are recommended to a mashup where the mashup and the APIs are most similar. In addition, we develop a tag filtering algorithm to select the most relevant tags for recommendation. The experimental results on a real world dataset prove that our approach outperforms other methods, including frequency-based methods and the methods that only consider the composition relationships and the annotation relationships separately.}, 
keywords={Internet;application program interfaces;indexing;information filtering;recommender systems;service-oriented architecture;Web API;Web application programming interfaces;annotation relationships;composition relationships;frequency-based methods;mashup services;mashup tag recommendation;probabilistic topic model;semantic similarity;service-oriented architecture based applications;tag filtering algorithm;user intervention;Manuals;Mashups;Probabilistic logic;Semantics;Service-oriented architecture;Tagging;Web APIs;mashups;tag recommendation;topic models}, 
doi={10.1109/ICWS.2016.64}, 
month={June},}
@INPROCEEDINGS{7848209, 
author={S. Chakraborty and M. K. Bhowmik and A. K. Ghosh and T. Pal}, 
booktitle={2016 IEEE Region 10 Conference (TENCON)}, 
title={Automated edge detection of breast masses on mammograms}, 
year={2016}, 
pages={1241-1245}, 
abstract={Edge of a breast mass is one of the indicators of breast abnormality detection. In a mammogram, round and circumscribed masses indicate benign changes and malignant masses usually has speculated (irregular) boundary. The paper has encountered a fundamental problem of active contour model which was first proposed by Kass et al. The problem encountered here is generation of initial contour points manually selected by users. Thus the positions of initial contour points will vary with human perspective, which is very difficult to identify actual and accurate contour points. To overcome this problem to some extent, sobel edge detection method is used as a prior step of active contour model. Experiments have been tested on a dataset of 160 mammograms collected from Mini-MIAS benchmark database and compared with sobel edge detection method. In experiments, 92.5% segmentation accuracy has been obtained with sensitivity 93% and 85% specificity where the sobel edge detection method shown very less segmentation accuracy of 84% with 91% sensitivity and 50% specificity. Time complexity and detection error have been also analysed for proposed method, ideal high pass filter, sobel edge detection, hough transform and active contour model.}, 
keywords={Hough transforms;computational complexity;edge detection;high-pass filters;image segmentation;mammography;medical image processing;Hough transform;active contour model;automated edge detection;breast abnormality detection;breast masses;detection error;human perspective;ideal high pass filter;initial contour points;mammograms;mini-MIAS benchmark database;segmentation accuracy;sobel edge detection method;time complexity;Active contours;Breast cancer;Computational modeling;Feature extraction;Image edge detection;Mammography;Automated segmentation;Breast cancer;Edge detection;Greedy active contour model;Mammogram}, 
doi={10.1109/TENCON.2016.7848209}, 
month={Nov},}
@INPROCEEDINGS{4721573, 
author={V. Frias-Martinez and S. J. Stolfo and A. D. Keromytis}, 
booktitle={2008 Annual Computer Security Applications Conference (ACSAC)}, 
title={Behavior-Profile Clustering for False Alert Reduction in Anomaly Detection Sensors}, 
year={2008}, 
pages={367-376}, 
abstract={Anomaly detection (AD) sensors compute behavior profiles to recognize malicious or anomalous activities. The behavior of a host is checked continuously by the AD sensor and an alert is raised when the behavior deviates from its behavior profile. Unfortunately, the majority of AD sensors suffer from high volumes of false alerts either maliciously crafted by the host or originating from insufficient training of the sensor. We present a cluster-based AD sensor that relies on clusters of behavior profiles to identify anomalous behavior. The behavior of a host raises an alert only when a group of host profiles with similar behavior (cluster of behavior profiles) detect the anomaly, rather than just relying on the host's own behavior profile to raise the alert (single-profile AD sensor). A cluster-based AD sensor significantly decreases the volume of false alerts by providing a more robust model of normal behavior based on clusters of behavior profiles. Additionally, we introduce an architecture designed for the deployment of cluster-based AD sensors. The behavior profile of each network host is computed by its closest switch that is also responsible for performing the anomaly detection for each of the hosts in its subnet. By placing the AD sensors at the switch, we eliminate the possibility of hosts crafting malicious alerts. Our experimental results based on wireless behavior profiles from users in the CRAWDAD dataset show that the volume of false alerts generated by cluster-based AD sensors is reduced by at least 50% compared to single-profile AD sensors.}, 
keywords={security of data;anomalous activity;anomalous behavior identification;anomaly detection sensor;behavior-profile clustering;cluster-based AD sensor;false alert reduction;malicious activity;wireless behavior profile;Application software;Collaboration;Communication switching;Computer architecture;Computer networks;Computer security;Performance analysis;Robustness;Sensor phenomena and characterization;Switches;Behavior Profile Clustering;False Alert Reduction;Network-based Anomaly Detection Sensors;Wireless Users}, 
doi={10.1109/ACSAC.2008.30}, 
ISSN={1063-9527}, 
month={Dec},}
@INPROCEEDINGS{7514648, 
author={S. Raghavendra and G. Mara and R. Buyya and V. K. Rajuk and S. Iyengar and L. M. Patnaik}, 
booktitle={2016 International Conference on Computational Techniques in Information and Communication Technologies (ICCTICT)}, 
title={DRSIG: Domain and Range Specific Index Generation for encrypted Cloud data}, 
year={2016}, 
pages={591-596}, 
abstract={One of the most fundamental services of cloud computing is Cloud storage service. Huge amount of sensitive data is stored in the cloud for easy remote access and to reduce the cost of storage. The confidential data is encrypt before uploading to the cloud server in order to maintain privacy and security. All conventional searchable symmetric encryption(SSE) schemes enable the users to search on the entire index file. In this paper, we propose the Domain and Range Specific Index Generation(DRSIG) scheme that minimizes the Index Generation time. This scheme adopts collection sort technique to split the index file into D Domains and R Ranges. The Domain is based on the length of the keyword; the Range splits within the domain based on the first letter of the keyword. A mathematical model is used to encrypt the indexed keyword that eliminates the information leakage. The time complexity of the index generation is O(NT × 3) where NT - Number of rows in index document and 3 is Number of columns in index document. Experiments have been conducted on real world dataset to validate proposed DRSIG scheme. It is observed that DRSIG scheme is efficient and provide more secure data than Ranked Searchable Symmetric Encryption(RSSE) Scheme.}, 
keywords={cloud computing;cryptography;DRSIG;RSSE scheme;SSE;cloud computing;cloud server;cloud storage service;domain and range specific index generation;encrypted cloud data;index document;information leakage;ranked searchable symmetric encryption;searchable symmetric encryption;Cloud computing;Data privacy;Encryption;Indexes;Keyword search;Servers;Cloud Computing;DRSIG;Data Security;Index Generation;Searchable Encryption}, 
doi={10.1109/ICCTICT.2016.7514648}, 
month={March},}
@ARTICLE{7935528, 
author={J. Chen and C. Wang and J. Wang and X. Ying and X. Wang}, 
journal={IEEE Transactions on Image Processing}, 
title={Learning the Personalized Intransitive Preferences of Images}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Most of the previous studies on the user preferences assume there is a personal transitive preference ranking of the consumable media like images. For example, the transitivity of preferences is one of the most important assumptions in the recommender system research. However, the intransitive relations have also been widely observed such as the win/loss relations in online video games, in sport matches, and even in rock-paper-scissors games. It is also found that different subjects demonstrate the personalized intransitive preferences in the pairwise comparisons between the applicants for college admission. Since the intransitivity of preferences on images has barely been studied before and has a large impact on the research of personalized image search and recommendation, it is necessary to propose a novel method to predict the personalized intransitive preferences of images. In this paper, we propose the novel Multi-Criterion preference (MuCri) models to predict the intransitive relations in the image preferences. The MuCri models utilize different kinds of image content features as well as the latent features of users and images. Meanwhile, a new dataset is constructed in this work in order to evaluate the performance of the MuCri models. The experimental evaluation shows that the MuCri models outperform all the baselines. Due to the interdisciplinary nature of this topic, we believe it would widely attract the attention of researchers in the image processing community as well as in other communities such as machine learning, multimedia and recommender system.}, 
keywords={Bayes methods;Games;Learning systems;Media;Predictive models;Recommender systems;Social network services;Intransitive Image Preference;Multi-Criterion Models;Representation Learning}, 
doi={10.1109/TIP.2017.2709941}, 
ISSN={1057-7149}, 
month={},}
@INPROCEEDINGS{6406544, 
author={A. Bindra and S. Pokuri and K. Uppala and A. Teredesai}, 
booktitle={2012 IEEE 12th International Conference on Data Mining Workshops}, 
title={Distributed Big Advertiser Data Mining}, 
year={2012}, 
pages={914-914}, 
abstract={Advertisers and big data mining experts alike are today are dealing with complex datasets of increasing variety (first and third party data), volume (events, impressions, clicks), and velocity (real time bidding). Creating predictive models to customize advertiser requirements and campaign analytics to show targeted ads to users who are most likely to convert has become increasingly challenging. Advertisers often group customers into a segment defined by a given set of demographic or behavioral attributes. Such segments are often very sparse. "Look-Alike Modeling" enables advertisers to enhance the target segment by using predictive models to expand the segment membership by assigning a probability score to users that did not explicitly belong to that segment based on the original segment definition. In this paper accompanied by the demo of a distributed platform, we describe a Look-Alike Modeling framework to expand segment membership using a novel high-dimensional distributed algorithm based on frequent pattern mining. We describe how the distributed algorithm is more efficient than traditional classification techniques that (a) require multiple passes over the dataset and (b) require both positive and negative class labels for training. Our solution is capable of concurrently and continuously processing thousands of segments and includes an efficient grouping operator and a distributed scoring algorithm for predicting multiple segment membership for a given (very large) set of users. This leverages the power of in-database analytics as compared to using standard data mining libraries and is currently deployed on a real-world highly scalable distributed columnar database that powers several hundred campaigns and processes look-alike models for large online display advertisers. The results from the study demonstrate that the proposed algorithm outperforms other comparable techniques for predicting and expanding segments.}, 
keywords={advertising;data mining;database management systems;distributed algorithms;pattern classification;probability;advertiser requirements;behavioral attributes;campaign analytics;classification techniques;clicks;demographic attributes;distributed big advertiser data mining;distributed columnar database;distributed scoring algorithm;events;first party data;frequent pattern mining;grouping operator;high-dimensional distributed algorithm;impressions;in-database analytics;look-alike modeling;negative class labels;online display advertisers;positive class labels;predictive models;probability score;real time bidding;segment membership;standard data mining libraries;third party data;Algorithm design and analysis;Analytical models;Classification algorithms;Data mining;Data models;Prediction algorithms;Predictive models;Audience Segmentation;Big Data for Advertising;In-Database Analytics;Look-Alike Modeling}, 
doi={10.1109/ICDMW.2012.73}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{7800316, 
author={S. N. N. Htun and T. T. Zin and M. Yokota and K. M. M. Tun}, 
booktitle={2016 IEEE 5th Global Conference on Consumer Electronics}, 
title={User-intent visual information ranking system}, 
year={2016}, 
pages={1-2}, 
abstract={Nowadays, the rapid growth of World Wide Web has been resulted in an exponential growth of the data and visual information with full of interesting bits of contents that can be found online. In such situations, finding out images that satisfy user intentions from a huge collection is more and more required, which emphasizes the importance of web image search and visual information ranking system as filters for users. For these reasons, we propose user-centric visual information rank and re-ranking system for web image search to explore the global trends in innovation by combining the sharing patterns of social network. Specifically, we establish an embedded Markov Chain Model along with local and global image features of visual information content for ranking image search engine. In order to evaluate the performance of proposed method, we will conduct a series of experiments based on social media platforms and a real-life social Yelp network dataset with respect to consumer perspectives.}, 
keywords={Internet;Markov processes;image retrieval;search engines;social networking (online);World Wide Web;consumer perspectives;embedded Markov chain model;exponential data growth;image features;image search engine;sharing patterns;social Yelp network dataset;social media platforms;social network;user intentions;user-centric visual information rank;user-intent visual information ranking system;visual information;visual information content;visual information ranking system;web image search;Business;Correlation;Feature extraction;Image color analysis;Markov processes;Social network services;Visualization;Information ranking system;Local and global features;Markov chain;Re-ranking;User-intent;Visual information}, 
doi={10.1109/GCCE.2016.7800316}, 
month={Oct},}
@ARTICLE{6977907, 
author={L. Teijeiro-Mosquera and J. I. Biel and J. L. Alba-Castro and D. Gatica-Perez}, 
journal={IEEE Transactions on Affective Computing}, 
title={What Your Face Vlogs About: Expressions of Emotion and Big-Five Traits Impressions in YouTube}, 
year={2015}, 
volume={6}, 
number={2}, 
pages={193-205}, 
abstract={Social video sites where people share their opinions and feelings are increasing in popularity. The face is known to reveal important aspects of human psychological traits, so the understanding of how facial expressions relate to personal constructs is a relevant problem in social media. We present a study of the connections between automatically extracted facial expressions of emotion and impressions of Big-Five personality traits in YouTube vlogs (i.e., video blogs). We use the Computer Expression Recognition Toolbox (CERT) system to characterize users of conversational vlogs. From CERT temporal signals corresponding to instantaneously recognized facial expression categories, we propose and derive four sets of behavioral cues that characterize face statistics and dynamics in a compact way. The cue sets are first used in a correlation analysis to assess the relevance of each facial expression of emotion with respect to Big-Five impressions obtained from crowd-observers watching vlogs, and also as features for automatic personality impression prediction. Using a dataset of 281 vloggers, the study shows that while multiple facial expression cues have significant correlation with several of the Big-Five traits, they are only able to significantly predict Extraversion impressions with moderate values of R2.}, 
keywords={emotion recognition;face recognition;social networking (online);statistical analysis;video signal processing;CERT temporal signal;YouTube vlogs;big-five traits impressions;computer expression recognition toolbox;correlation analysis;emotion;extraversion impression;face statistics;facial expression;human psychological trait;social video sites;video blogs;Correlation;Face;Face recognition;Facial features;Feature extraction;Hidden Markov models;Video recording;Face processing;Facial Expressions;Personality Prediction;facial expressions;personality prediction;vlogs}, 
doi={10.1109/TAFFC.2014.2370044}, 
ISSN={1949-3045}, 
month={April},}
@INPROCEEDINGS{5231898, 
author={P. Bhattacharyya and A. Garg and S. F. Wu}, 
booktitle={2009 International Conference on Advances in Social Network Analysis and Mining}, 
title={Social Network Model Based on Keyword Categorization}, 
year={2009}, 
pages={170-175}, 
abstract={A user profile on an online social network is characterized by its profile entries (keywords). In this paper, we study the relationship between semantic similarity of user keywords and the social network topology. First, we present a 'forest' model to categorize keywords and define the notion of distance between keywords across multiple categorization trees (i.e., a forest). Second, we use the keyword distance to define similarity functions between a pair of users and show how social network topology can be modeled accordingly. Third, we validate our social network topology model, using a simulated social graph, against a real life social graph dataset.}, 
keywords={social networking (online);text analysis;trees (mathematics);forest model;keyword categorization;keyword distance;multiple categorization trees;profile entries;semantic similarity;similarity function;simulated social graph;social network model;social network topology model;user profile;Cities and towns;Computer science;DSL;Data mining;Facebook;Indexing;Lattices;Network topology;Social network services;Tree graphs}, 
doi={10.1109/ASONAM.2009.46}, 
month={July},}
@INPROCEEDINGS{7914965, 
author={P. Gaikwad and A. Kasliwal and T. Kale and S. Shahani and J. Abraham}, 
booktitle={2016 International Conference on Computing, Analytics and Security Trends (CAST)}, 
title={Privacy preserving vehicular trajectory prediction}, 
year={2016}, 
pages={195-198}, 
abstract={Location based services (LBS) provide many valuable and important services for end users but reveal information about personal location to potentially untrustworthy service providers which could pose privacy concerns. The information about exact path followed by a person is highly personal and can be used to uniquely identify a person. Such information is known as personally identifiable information (PII). Disclosing such information can be considered as breach of privacy. Data needs to be transformed in such a way that either PII is removed or cannot be inferred preserving the accuracy of inferences at the same time. Differential Privacy [1] is a widely used techniques for removing PII from the data. It aims to provide means to maximize the accuracy of queries from statistical databases while minimizing the chances of identifying its records. This work aims at studying the impact of transforming the data on its utility. We have used two differentially privacy techniques Laplace perturbation and sliding moving average noise reduction on trajectories of taxi [2]. This dataset is used to predict the final destination (latitude and longitude) of taxi trips. These differential privacy techniques maintain the utility of this dataset along with the preservation of the trajectory privacy of individuals.}, 
keywords={data privacy;LBS;Laplace perturbation;PII;differential privacy;location based services;personally identifiable information;privacy concerns;privacy preserving vehicular trajectory prediction;privacy techniques;sliding moving average noise reduction;Data privacy;Noise measurement;Predictive models;Privacy;Public transportation;System analysis and design;Trajectory;Data Perturbation;Differential Privacy;Noise Addition;Trajectory Data Privacy;Utility}, 
doi={10.1109/CAST.2016.7914965}, 
month={Dec},}
@INPROCEEDINGS{6785847, 
author={N. U. Rehman and A. Weiler and M. H. Scholl}, 
booktitle={2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)}, 
title={OLAPing social media: The case of Twitter}, 
year={2013}, 
pages={1139-1146}, 
abstract={Social networks are platforms where millions of users interact frequently and share variety of digital content with each other. Users express their feelings and opinions on every topic of interest. These opinions carry import value for personal, academic and commercial applications, but the volume and the speed at which these are produced make it a challenging task for researchers and the underlying technologies to provide useful insights to such data. We attempt to extend the established OLAP(On-line Analytical Processing) technology to allow multidimensional analysis of social media data by integrating text and opinion mining methods into the data warehousing system and by exploiting various knowledge discovery techniques to deal with semi-structured and unstructured data from social media. The capabilities of OLAP are extended by semantic enrichment of the underlying dataset to discover new measures and dimensions for building data cubes and by supporting up-to-date analysis of the evolving as well as the historical social media data. The benefits of such an analysis platform are demonstrated by building a data warehouse for a social network of Twitter, dynamically enriching the underlying dataset and enabling multidimensional analysis.}, 
keywords={data mining;data warehouses;social networking (online);text analysis;Twitter;data cubes;data warehousing system;digital content;historical social media data;knowledge discovery techniques;online analytical processing technology;opinion mining method;social media OLAPing;social media data multidimensional analysis;text mining method;Conferences;Data mining;Data models;Data warehouses;Media;Twitter}, 
doi={10.1145/2492517.2500273}, 
month={Aug},}
@INPROCEEDINGS{6545960, 
author={X. Tang and M. Zhang and C. C. Yang}, 
booktitle={2013 International Conference on Social Intelligence and Technology}, 
title={Leveraging User Interest to Improve Thread Recommendation in Online Forum}, 
year={2013}, 
pages={11-19}, 
abstract={Nowadays thread recommendation is considered to be beneficial to improve the end-user stickiness of an online forum. Given the fact of information overload and the diverse interests of forum users, a recommender system in online forum can satisfy not only forum users' information needs by directing them to what they might be interested in, but also their social needs by connecting them to their friends. Some traditional recommender systems rely on a bipartite graph model to capture users' interests. As an extension, some other content-based methods are proposed to further understand the potential connections between Web users and Web contents. However, due to the prevalence of short and sparse messages in online social media, it is hard for traditional content-based methods to capture Web users' interests. In this paper, we propose a novel graphical model to extract hidden topics from Web contents, cluster Web contents into clusters, and detect users' interests on each cluster. Then we introduce two reran king models which utilize the detected user interest to boost the performance of thread recommendation. Experiment results on a public dataset showed that our proposed methods substantially outperformed the naïve content-based approach. In addition, by testing our approaches with different parameter settings, we observed, to some extent, how forum users' information needs and their social needs interplay to decide which threads they will look for.}, 
keywords={Internet;content-based retrieval;graph theory;information needs;recommender systems;social networking (online);Web content;Web user interest;bipartite graph model;content-based method;end-user stickiness;forum user information needs;graphical model;hidden topic extraction;information overload;online forum;online social media;public dataset;recommender system;social needs;thread recommendation;Bipartite graph;Collaboration;Message systems;Recommender systems;Switches;Vectors;graphical model;personalized recommendation;reranking method;user interest}, 
doi={10.1109/SOCIETY.2013.13}, 
month={May},}
@INPROCEEDINGS{6918274, 
author={H. A. Abdelbary and A. M. ElKorany and R. Bahgat}, 
booktitle={2014 Science and Information Conference}, 
title={Utilizing deep learning for content-based community detection}, 
year={2014}, 
pages={777-784}, 
abstract={Online social networks have been wildly spread in recent years. They enable users to identify other users with common interests, exchange their opinions, and expertise. Discovering user communities from social networks have become one of the major challenges which help its members to interact with relevant people who have similar interests. Community detection approaches fall into two categories: the first one considers user' networks while the other utilizes usergenerated content. In this paper, a multi-layer community detection model based on identifying topics of interest from user published content is presented. This model applies Gaussian Restricted Boltzmann Machine for modeling user's posts within a social network which yields to identify their topics of interest, and finally construct communities. The effectiveness of the proposed multi-layer model is measured using KL divergence which measures similarity between users of the same community. Experiments on the real Twitter dataset show that the proposed deep model outperforms traditional community detection models that directly maps users into corresponding communities using several baseline techniques.}, 
keywords={Boltzmann machines;Gaussian processes;content-based retrieval;learning (artificial intelligence);social networking (online);Gaussian restricted Boltzmann machine;KL divergence;content-based community detection;deep learning;multilayer community detection model;online social networks;user posts modeling;user published content;Communities;Data models;Mathematical model;Probabilistic logic;Probability distribution;Social network services;Vectors;Community detection;K-means;Replicated softmax;Restricted Boltzmann Machines;Topic modeling;deep learning}, 
doi={10.1109/SAI.2014.6918274}, 
month={Aug},}
@INPROCEEDINGS{7226046, 
author={D. Garg and P. Gohil and K. Trivedi}, 
booktitle={2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)}, 
title={Modified Fuzzy K-mean clustering using MapReduce in Hadoop and cloud}, 
year={2015}, 
pages={1-5}, 
abstract={Apache Hadoop is an open source software framework which structures Big data (both structured and unstructured). It is nowadays one of the biggest motivator in market as data storage is inexpensive in it. The storage method of Hadoop uses a distributed file system which lets the user store large amount of data by simply adding more nodes to a Hadoop cluster. Clustering a large amount of data is a point of concern. MapReduce, a programming model used by Hadoop allows a parallelization technique by decomposing a larger problem involving large dataset to smaller portion of data and then executing it. A scalable machine learning library named as Mahout is an approach to clustering which runs on Hadoop. In this paper, the Hadoop multi-node cluster is formed using Amazon EC2. This paper focuses on Fuzzy k-mean clustering algorithm which is modified by centroid generation method using MapReduce in Hadoop. Experimental results depict a decrease in the number of iterations thereby leading to a decrease in the execution time when modification of Fuzzy K-mean clustering algorithm is done using Canopy generation in MapReduce in Hadoop.}, 
keywords={Big Data;cloud computing;fuzzy set theory;learning (artificial intelligence);parallel programming;pattern clustering;public domain software;Amazon EC2;Apache Hadoop;Canopy generation;Hadoop multinode cluster;Mahout;MapReduce;big data;centroid generation method;cloud computing;data clustering;data storage;distributed file system;execution time;modified fuzzy K-mean clustering;open source software framework;parallelization technique;programming model;scalable machine learning library;Classification algorithms;Clustering algorithms;Random access memory;Cloud;Fuzzy K-mean clustering;HDFS;Hadoop;Mahout;MapReduce}, 
doi={10.1109/ICECCT.2015.7226046}, 
month={March},}
@INPROCEEDINGS{7501696, 
author={L. Valerio and A. Passarella and M. Conti}, 
booktitle={2016 IEEE International Conference on Smart Computing (SMARTCOMP)}, 
title={Hypothesis Transfer Learning for Efficient Data Computing in Smart Cities Environments}, 
year={2016}, 
pages={1-8}, 
abstract={It is commonly assumed that in a smart city there will be thousands of mostly mobile/wireless smart devices (e.g. sensors, smart-phones, etc.) that will continuously generate big amounts of data. Data will have to be collected and processed in order to extract knowledge out of it, to feed users' and smart city applications. A typical approach to process such big amounts of data is to i) gather all the collected data on the cloud through wireless pervasive networks, and ii) perform data analysis operations exploiting machine learning techniques. However, according to many studies, this centralised cloud-based approach may not be sustainable from a networking point of view. The joint effect of data-intensive users' multimedia applications and smart cities monitoring and control applications may result in severe network congestions making applications hardly usable. To cope with this problem, in this paper we propose a distributed machine learning approach that does not require to move data in a centralised cloud platform, but processes it directly where it is collected. Specifically, we exploit Hypothesis Transfer Learning (HTL) to build a distributed machine learning framework. In our framework we train a series of partial models, each 'residing' in a location where a subset of the dataset is generated. We then refine the partial models by exchanging them between locations, thus obtaining a unique complete model. Using an activity classification task on a reference dataset as a concrete example, we show that the classification accuracy of the HTL model is comparable with that of a model built out of the complete dataset, but the cost in term of network overhead is dramatically reduced. We then perform a sensitiveness analysis to characterise how the overhead depends on key parameters. It is also worth noticing that the HTL approach is suitable for applications dealing with privacy sensitive data, as data can stay where they are generated, and do not need to be t- ansferred to third parties, i.e., to a cloud provider, to extract knowledge out of it.}, 
keywords={cloud computing;data analysis;data privacy;knowledge acquisition;learning (artificial intelligence);mobile computing;multimedia computing;pattern classification;smart phones;HTL;activity classification task;cloud provider;data analysis;data computing;data privacy;distributed machine learning;hypothesis transfer learning;knowledge extraction;mobile computing;multimedia application;smart city;smart phone;wireless pervasive network;Cloud computing;Data mining;Data models;Mobile communication;Smart cities;Support vector machines;Training}, 
doi={10.1109/SMARTCOMP.2016.7501696}, 
month={May},}
@INPROCEEDINGS{6835592, 
author={Z. Qun}, 
booktitle={2013 International Conference on Computer Sciences and Applications}, 
title={Research and Implementation of Clustering Analysis Algorithms Based on I-MINER}, 
year={2013}, 
pages={254-257}, 
abstract={I-MINER is convenient to establish data mining model and embed other data mining models with I-Miner. DBSCAN algorithm can achieve clustering of any shape of dataset, Fuzzy C-Means is suitable for the dataset which is uniformly distributed around cluster centers and CABOSFV algorithm can be a good clustering for high-dimensional dataset (such as WEB data). In this thesis, DBSCAN, Fuzzy C-Means and CABOSFV clustering analysis algorithms are embedded into I-Miner to enormously satisfy users' needs, establish data mining model and support production decision-making, besides, the three mining models are compared. Through three mining models, mining and comparative analysis are made for examples to get the advantages and disadvantages of the three clustering algorithms.}, 
keywords={data mining;pattern clustering;CABOSFV clustering analysis algorithm;DBSCAN clustering analysis algorithm;I-MINER;cluster centers;data mining model;fuzzy C-means clustering analysis algorithm;high-dimensional dataset;production decision-making;Algorithm design and analysis;Analytical models;Classification algorithms;Clustering algorithms;Data mining;Data models;Software algorithms;CABOSFV;Clustering analysis;DBSCAN;FCM}, 
doi={10.1109/CSA.2013.65}, 
month={Dec},}
@INPROCEEDINGS{7009306, 
author={W. S. Alsharafat}, 
booktitle={The 2014 2nd International Conference on Systems and Informatics (ICSAI 2014)}, 
title={Proposed anticipating learning classifier system for cloud intrusion detection (ALCS-CID)}, 
year={2014}, 
pages={315-318}, 
abstract={Cloud computing is a modern approach in network environment. According to increased number of network users and online systems, there is a need to help these systems to be away from unauthorized resource access and detect any attempts for privacy contravention. For that purpose, Intrusion Detection System is an effective security mechanism to detect any attempts of attacks for cloud resources and their information. In this paper, Cloud Intrusion Detection System has been proposed in term of reducing or eliminating any attacks. This Model concerns about achieving high detection rate after conducting a set of experiments using benchmarks dataset called KDD'99.}, 
keywords={authorisation;cloud computing;computer network security;data privacy;ALCS-CID;cloud computing;cloud intrusion detection system;cloud resources;learning classifier system;network environment;network users;online systems;privacy contravention;security mechanism;unauthorized resource access;Cloud computing;Conferences;Detectors;Genetic algorithms;Hidden Markov models;Intrusion detection;Training;Anticipating classifier system;Cloud computing;IDS}, 
doi={10.1109/ICSAI.2014.7009306}, 
month={Nov},}
@INPROCEEDINGS{7349954, 
author={M. Zhang and Y. Li and A. Zhou and Z. Fang}, 
booktitle={2015 IEEE 12th International Conference on e-Business Engineering}, 
title={A Recommendation System for Travel Services Based on Cyber-Anima}, 
year={2015}, 
pages={114-118}, 
abstract={In recent years, recommendation systems have developed greatly, and is so widely used in online systems such as book, movie or friend recommendation. Current recommendation systems have problems with cold start and new entry. Cyber-Anima is a cyber-image of a user's anima, and we can derive one's cyber-anima by its user input and other observations of the user. With Cyber-Anima, we can build a recommendation system. The system take the concepts in cyber-anima into consideration and build relations between these concepts and user behavior to make further recommendations. We do some experiment with the dataset from travelhub.cn, which is a platform for travel services and is sponsored by the national technology plan. From the experiment, our implementation is better than FISM and Item-based collaborative and the time cost is reasonable.}, 
keywords={recommender systems;tensors;travel industry;Cyber-Anima;recommendation system;travel services;travelhub.cn;user anima cyber-image;Collaboration;Computational modeling;Filtering;History;Knowledge based systems;Motion pictures;Tensile stress;Cyber-Anima;recommendation system;tensor decomposition}, 
doi={10.1109/ICEBE.2015.28}, 
month={Oct},}
@INPROCEEDINGS{5598147, 
author={Y. Higashijima and A. Yamamoto and T. Nakamura and M. Nakamura and M. Matsuo}, 
booktitle={2010 10th IEEE/IPSJ International Symposium on Applications and the Internet}, 
title={Missing Data Imputation Using Regression Tree Model for Sparse Data Collected via Wide Area Ubiquitous Network}, 
year={2010}, 
pages={189-192}, 
abstract={In a ubiquitous/pervasive environment, devices such as sensors and actuators will exist in high density. In this environment, we can acquire a large number of sensor values such as temperature and humidity. We have proposed a ubiquitous data storing architecture called uTupleSpace (uTS), which supports flexible sharing of sensor values with multiple users/software/devices. However, despite a user request, if some values are not stored on the uTS, they should be treated as missing and imputed by estimating such values. We focus on the regression tree imputation method for this problem and show its effectivity for a high-density WAUN environment by regarding multiple sensor values observed at the same time as a spatial dataset. Moreover, we propose a preprocessing method for improving the imputation accuracy in a sparse WAUN environment. We can achieve higher accuracy with our preprocessing method compared to the no-preprocessed and linear interpolation methods. We show the effectivity of our proposed method through experiments.}, 
keywords={regression analysis;sensor fusion;trees (mathematics);ubiquitous computing;wide area networks;missing data imputation;multiple sensor values;pervasive environment;regression tree imputation method;regression tree model;uTupleSpace;ubiquitous data storing architecture;wide area ubiquitous network;Accuracy;Interpolation;Regression tree analysis;Sensor phenomena and characterization;Temperature sensors;Training;Imputation;Regression Tree Imputation;Sparse data;uTupleSpace}, 
doi={10.1109/SAINT.2010.18}, 
month={July},}
@INPROCEEDINGS{6483433, 
author={Y. Deshpande and A. Montanari}, 
booktitle={2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
title={Linear bandits in high dimension and recommendation systems}, 
year={2012}, 
pages={1750-1754}, 
abstract={A large number of online services provide automated recommendations to help users to navigate through a large collection of items. New items (products, videos, songs, advertisements) are suggested on the basis of the user's past history and - when available - her demographic profile. Recommendations have to satisfy the dual goal of helping the user to explore the space of available items, while allowing the system to probe the user's preferences. We model this trade-off using linearly parametrized multi-armed bandits and prove upper and lower bounds that coincide up to constants in the data poor (high-dimensional) regime. We test (a variation of) the scheme used for estabilishing achievability on the Netflix dataset, and obtain results in agreement with the theory.}, 
keywords={information services;recommender systems;sequential estimation;Netflix dataset;automated recommendations;data poor regime;demographic profile;linear bandits;linearly parametrized multiarmed bandits;lower bounds;online services;recommendation systems;upper bounds;user past history;user preferences;Ellipsoids;History;Motion pictures;Noise;Numerical simulation;Upper bound;Vectors}, 
doi={10.1109/Allerton.2012.6483433}, 
month={Oct},}
@INPROCEEDINGS{6011838, 
author={J. C. Wang and Meng-Sung Wu and H. M. Wang and Shyh-Kang Jeng}, 
booktitle={2011 IEEE International Conference on Multimedia and Expo}, 
title={Query by multi-tags with multi-level preferences for content-based music retrieval}, 
year={2011}, 
pages={1-6}, 
abstract={This paper presents a novel content-based music retrieval system that accepts a query containing multiple tags with multiple levels of preference (denoted as an MTML query) to retrieve music from an untagged music database. We select a limited number of popular music tags to form the tag space and design an interface for users to input queries by operating the scroll bars. To effect MTML content-based music retrieval, we introduce a tag-based music aspect model that jointly models the auditory features and tag-based text features of a song. Two indexing methods and their corresponding matching methods, namely pseudo song-based matching and tag co-occurrence pattern-based matching, are incorporated into the pre-learned tag-based music aspect model. Finally, we evaluate the proposed system on the Major Miner dataset. The results demonstrate the potential of using MTML queries to retrieve music from an untagged music database.}, 
keywords={Music retrieval system;query by multi-tags;social tags;tag-based music aspect model}, 
doi={10.1109/ICME.2011.6011838}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{6740377, 
author={C. H. Chen and C. Y. Hsieh and Y. C. Lee}, 
booktitle={2013 IEEE International Conference on Granular Computing (GrC)}, 
title={The YTM-based stock portfolio mining approach by genetic algorithm}, 
year={2013}, 
pages={38-42}, 
abstract={This study proposes a yield-to-maturity (YTM)-based genetic portfolio selection model with user defined constraints, namely YTMGPSM. A set of real numbers are encoded into a chromosome to form a possible portfolio, which presents whether buy or not buy and purchased units of assets. The fitness value of a chromosome is evaluated by return on investment, value at risk and suitability of the respective portfolio. The suitability of a chromosome consists of portfolio penalty and investment capital penalty that are used to reflect the satisfactions of user predefined maximum investment and maximum number of companies, respectively. Experiments on real dataset are made to show the merits of the proposed approach.}, 
keywords={cost-benefit analysis;genetic algorithms;investment;YTM-based stock portfolio mining;YTMGPSM;chromosome;genetic algorithm;investment capital penalty;portfolio penalty;return on investment;yield-to-maturity;Biological cells;Companies;Genetic algorithms;Investment;Portfolios;Sociology;Statistics;M-V model;genetic algorithm;portfolio selection;transaction lots;yield to maturity}, 
doi={10.1109/GrC.2013.6740377}, 
month={Dec},}
@ARTICLE{7926346, 
author={A. Murillo Montes de Oca and R. Bahmanyar and N. Nistor and M. Datcu}, 
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
title={Earth Observation Image Semantic Bias: A Collaborative User Annotation Approach}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-16}, 
abstract={Correctly annotated image datasets are important for developing and validating image mining methods. However, there is some doubt regarding the generalizability of the models trained and validated on available datasets. This is due to dataset biases, which occur when the same semantic label is used in different ways across datasets, and/or when identical object categories are labeled differently across datasets. In this paper, we demonstrate the existence of dataset biases with a sample of eight remote sensing image datasets, first showing they are readily discriminable from a feature perspective, and then demonstrating that a model trained on one dataset is not always valid on others. Past approaches to reducing dataset biases have relied on crowdsourcing, however this is not always an option (e.g., due to public-accessibility restrictions of images), raising the question: How to structure annotation tasks to efficiently and accurately annotate images with a limited number of nonexpert annotators? We propose a collaborative annotation methodology, conducting image annotation experiments where users are placed in either a collaborative or individual condition, and we analyze their annotation performance. Results show the collaborators produce more thorough, precise annotations, requiring less time than the individuals. Collaborators labels show less variance around the consensus point, meaning their assigned labels are more predictable and likely to be generally accepted by other users. Therefore, collaborative image annotation is a promising annotation methodology for creating reliable datasets with a reduced number of nonexpert annotators. This in turn has implications for the creation of less biased image datasets.}, 
keywords={Collaboration;Optical sensors;Remote sensing;Semantics;Synthetic aperture radar;Dataset biases;remote sensing images;semantic image annotation;sensory and semantic gaps;user evaluation}, 
doi={10.1109/JSTARS.2017.2697003}, 
ISSN={1939-1404}, 
month={},}
@INPROCEEDINGS{6927628, 
author={A. Watanabe and R. Sasano and H. Takamura and M. Okumura}, 
booktitle={2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)}, 
title={Generating Personalized Snippets for Web Page Recommender Systems}, 
year={2014}, 
volume={2}, 
pages={218-225}, 
abstract={Web page recommender systems provide users with web pages they might be interested in. Users then select some of the recommended web pages that catch their interest by making a relevance judgment. However, if web page recommender systems do not offer enough useful information for the relevance judgment, users would end up reading irrelevant web pages or overlook relevant web pages. To provide information for the relevance judgment, we propose a novel method for generating personalized snippets for web page recommender systems. Our method directly uses the reasons the web pages are recommended to the user. This use of reasons enables snippets to be selected that better reflect the interest of the user. Moreover, our method can work with various web page recommender systems. It also leverages the maximum coverage summarization model to generate personalized snippets. The results of an experiment on a manually created dataset show that our method is more effective than a personalized summarization model.}, 
keywords={Web sites;recommender systems;relevance feedback;Web page recommender systems;personalized snippet generation;personalized summarization model;relevance judgment;useful information;Bipartite graph;Collaboration;Recommender systems;Redundancy;Search engines;Vectors;Web pages}, 
doi={10.1109/WI-IAT.2014.101}, 
month={Aug},}
@INPROCEEDINGS{6707716, 
author={Y. N. Chen and W. Y. Wang and A. I. Rudnicky}, 
booktitle={2013 IEEE Workshop on Automatic Speech Recognition and Understanding}, 
title={Unsupervised induction and filling of semantic slots for spoken dialogue systems using frame-semantic parsing}, 
year={2013}, 
pages={120-125}, 
abstract={Spoken dialogue systems typically use predefined semantic slots to parse users' natural language inputs into unified semantic representations. To define the slots, domain experts and professional annotators are often involved, and the cost can be expensive. In this paper, we ask the following question: given a collection of unlabeled raw audios, can we use the frame semantics theory to automatically induce and fill the semantic slots in an unsupervised fashion? To do this, we propose the use of a state-of-the-art frame-semantic parser, and a spectral clustering based slot ranking model that adapts the generic output of the parser to the target semantic space. Empirical experiments on a real-world spoken dialogue dataset show that the automatically induced semantic slots are in line with the reference slots created by domain experts: we observe a mean averaged precision of 69.36% using ASR-transcribed data. Our slot filling evaluations also indicate the promising future of this proposed approach.}, 
keywords={grammars;interactive systems;natural language processing;pattern clustering;speech recognition;unsupervised learning;ASR-transcribed data;frame semantic theory;frame-semantic parser;natural language;predefined semantic slot;real-world spoken dialogue dataset;spectral clustering based slot ranking model;spoken dialogue system;unified semantic representation;unlabeled raw audio;unsupervised filling;unsupervised induction;Computational modeling;Hidden Markov models;Iron;Manuals;Probabilistic logic;Semantics;Vectors;Unsupervised slot induction;semantic representation;semantic slot filling}, 
doi={10.1109/ASRU.2013.6707716}, 
month={Dec},}
@INPROCEEDINGS{5379513, 
author={P. Evrard and E. Gribovskaya and S. Calinon and A. Billard and A. Kheddar}, 
booktitle={2009 9th IEEE-RAS International Conference on Humanoid Robots}, 
title={Teaching physical collaborative tasks: object-lifting case study with a humanoid}, 
year={2009}, 
pages={399-404}, 
abstract={This paper presents the application of a statistical framework that allows to endow a humanoid robot with the ability to perform a collaborative manipulation task with a human operator. We investigate to what extent the dynamics of the motion and the haptic communication process that takes place during physical collaborative tasks can be encapsulated by the probabilistic model. This framework encodes the dataset in a Gaussian Mixture Model, which components represent the local correlations across the variables that characterize the task. A set of demonstrations is performed using a bilateral coupling teleoperation setup; then the statistical model is trained in a pure follower/leader role distribution mode between the human and robot alternatively. The task is reproduced using Gaussian Mixture Regression. We present the probabilistic model and the experimental results obtained on the humanoid platform HRP-2; preliminary results assess our theory on switching behavior modes in dyad collaborative tasks: when reproduced with users which were not instructed to behave in either a follower or a leader mode, the robot switched automatically between the learned leader and follower behaviors.}, 
keywords={Gaussian processes;human-robot interaction;humanoid robots;manipulator dynamics;probability;regression analysis;telerobotics;Gaussian mixture model;Gaussian mixture regression;bilateral coupling teleoperation setup;collaborative manipulation task;haptic communication process;humanoid robot;physical collaborative tasks teaching;probabilistic model;pure follower/leader role distribution mode;Collaboration;Education}, 
doi={10.1109/ICHR.2009.5379513}, 
ISSN={2164-0572}, 
month={Dec},}
@INPROCEEDINGS{6885990, 
author={W. F. Yang and M. Wang and Z. Chen}, 
booktitle={2014 IEEE International Conference on Mechatronics and Automation}, 
title={Fast Probabilistic Matrix Factorization for recommender system}, 
year={2014}, 
pages={1889-1894}, 
abstract={In the past decades, with the rapid growth of online user data, it becomes challenging to develop preference learning algorithms that are sufficiently flexible in modeling but also affordable in computation. The enormous datasets and the situation that users who may have few ratings make it extremely hard for many existing approaches to handle. Collaborative filtering[1] is the most successful and popular technology in recommender system. The core of collaborative filtering is collaborative filtering algorithm, while the Probabilistic Matrix Factorization is one of the most useful algorithm. The Probabilistic Matrix Factorization (PMF)[2] model performs well on the large, sparse, and very imbalanced Netflix dataset. However, common methodologies based on error metrics, such as RMSE(Root-Mean-Square Error), are not a natural fit for evaluating the whole recommendation task. In this paper, based on Netflix dataset, we introduce a new Probabilistic Matrix Factorization algorithm called fast PMF, which can get a much better speed results and lower RMSE by updating n% ratings in the top of every movies. By comparing the original PMF, we can get a better understanding of fast PMF. There is a sorting algorithm in fast PMF, we expect a better result by choosing a relatively better sorting algorithm in the future.}, 
keywords={information filtering;learning (artificial intelligence);matrix decomposition;mean square error methods;recommender systems;Netflix dataset;RMSE;collaborative filtering;fast PMF;fast probabilistic matrix factorization;learning algorithm;recommender system;root-mean-square error;Collaboration;Filtering algorithms;Motion pictures;Recommender systems;Sorting;Sparse matrices;fast Probabilistic Matrix Factorization;recommender system;speed results}, 
doi={10.1109/ICMA.2014.6885990}, 
ISSN={2152-7431}, 
month={Aug},}
@INPROCEEDINGS{5176100, 
author={P. Borzymek and M. Sydow and A. Wierzbicki}, 
booktitle={2009 International Conference on Computational Aspects of Social Networks}, 
title={Enriching Trust Prediction Model in Social Network with User Rating Similarity}, 
year={2009}, 
pages={40-47}, 
abstract={Trust management is an increasingly important issue in large social networks, where the amount of data is too extensive to be analysed by ordinary users. Hence there is an urgent need for research aiming at building automated systems that can support users in making their decisions concerning trust. This work is a preliminary implementation of selected ideas described in our previous research proposal which concerns taking a machine learning approach to the problem of trust prediction in social networks.We report experiments conducted on a publicly available social network dataset epinions.com. The results indicate that i) it is possible to predict trust to some extent, but much room for improvement is present; ii) enriching the model with attributes based on similarity between users can significantly improve trust prediction accuracy for more similar users.}, 
keywords={learning (artificial intelligence);security of data;social networking (online);machine learning approach;social network;trust management;trust prediction model;user rating similarity}, 
doi={10.1109/CASoN.2009.30}, 
month={June},}
@INPROCEEDINGS{7349694, 
author={S. Chen and L. Feng and M. R. Rickels and A. Peleckis and O. Sokolsky and I. Lee}, 
booktitle={2015 International Conference on Healthcare Informatics}, 
title={A Data-Driven Behavior Modeling and Analysis Framework for Diabetic Patients on Insulin Pumps}, 
year={2015}, 
pages={213-222}, 
abstract={About 30%-40% of Type 1 Diabetes (T1D) patients in the United States use insulin pumps. Current insulin infusion systems require users to manually input meal carb count and approve or modify the system-suggested meal insulin dose. Users can give correction insulin boluses at any time. Since meal carbohydrates and insulin are the two main driving forces of the glucose physiology, the user-specific eating and pump-using behavior has a great impact on the quality of glycemic control. In this paper, we propose an "Eat, Trust, and Correct" (ETC) framework to model the T1D insulin pump users' behavior. We use machine learning techniques to analyze the user behavior from a clinical dataset that we collected on 55 T1D patients who use insulin pumps. We demonstrate the usefulness of the ETC behavior modeling framework by performing in silico experiments. To this end, we integrate the user behavior model with an individually parameterized glucose physiological model, and perform probabilistic model checking on the user-in-the-loop system. The experimental results show that switching behavior types can significantly improve a patient's glycemic control outcomes. These analysis results can boost the effectiveness of T1D patient education and peer support.}, 
keywords={diseases;drugs;learning (artificial intelligence);medical computing;organic compounds;ETC behavior modeling;Eat-Trust-and-Correct Framework;T1D insulin pump;T1D patient education;data-driven behavior analysis;data-driven behavior modeling;diabetic patient;glucose physiological model;glucose physiology;glycemic control quality;in silico experiments;insulin infusion system;machine learning technique;meal carbohydrate;system-suggested meal insulin dose;type 1 diabetes;user behavior model;user-in-the-loop system;user-specific eating;Analytical models;Biomedical monitoring;Diabetes;Insulin;Physiology;Pumps;Sugar;Blood glucose control;Closed-loop verification;Data-driven modeling and analysis;Data-driven verification;Insulin pump;Medical Cyber-Physical Systems;Patient behavior;Physiological modeling;Probabilistic model checking;Type 1 Diabetes}, 
doi={10.1109/ICHI.2015.33}, 
month={Oct},}
@ARTICLE{7116553, 
author={M. H. Lim and P. C. Yuen}, 
journal={IEEE Transactions on Cybernetics}, 
title={Entropy Measurement for Biometric Verification Systems}, 
year={2016}, 
volume={46}, 
number={5}, 
pages={1065-1077}, 
abstract={Biometric verification systems are designed to accept multiple similar biometric measurements per user due to inherent intrauser variations in the biometric data. This is important to preserve reasonable acceptance rate of genuine queries and the overall feasibility of the recognition system. However, such acceptance of multiple similar measurements decreases the imposter's difficulty of obtaining a system-acceptable measurement, thus resulting in a degraded security level. This deteriorated security needs to be measurable to provide truthful security assurance to the users. Entropy is a standard measure of security. However, the entropy formula is applicable only when there is a single acceptable possibility. In this paper, we develop an entropy-measuring model for biometric systems that accepts multiple similar measurements per user. Based on the idea of guessing entropy, the proposed model quantifies biometric system security in terms of adversarial guessing effort for two practical attacks. Excellent agreement between analytic and experimental simulation-based measurement results on a synthetic and a benchmark face dataset justify the correctness of our model and thus the feasibility of the proposed entropy-measuring approach.}, 
keywords={biometrics (access control);entropy;biometric measurements;biometric verification systems;entropy measurement;security level;Biological system modeling;Biometrics (access control);Entropy;Feature extraction;Security;Sociology;Statistics;Biometric system;entropy measurement;guessing;security}, 
doi={10.1109/TCYB.2015.2423271}, 
ISSN={2168-2267}, 
month={May},}
@ARTICLE{6065027, 
author={C. Turkay and P. Filzmoser and H. Hauser}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Brushing Dimensions - A Dual Visual Analysis Model for High-Dimensional Data}, 
year={2011}, 
volume={17}, 
number={12}, 
pages={2591-2599}, 
abstract={In many application fields, data analysts have to deal with datasets that contain many expressions per item. The effective analysis of such multivariate datasets is dependent on the user's ability to understand both the intrinsic dimensionality of the dataset as well as the distribution of the dependent values with respect to the dimensions. In this paper, we propose a visualization model that enables the joint interactive visual analysis of multivariate datasets with respect to their dimensions as well as with respect to the actual data values. We describe a dual setting of visualization and interaction in items space and in dimensions space. The visualization of items is linked to the visualization of dimensions with brushing and focus+context visualization. With this approach, the user is able to jointly study the structure of the dimensions space as well as the distribution of data items with respect to the dimensions. Even though the proposed visualization model is general, we demonstrate its application in the context of a DNA microarray data analysis.}, 
keywords={data analysis;data visualisation;lab-on-a-chip;set theory;DNA microarray data analysis;brushing dimensions;data item distribution;dual visual analysis model;high-dimensional data;item space;joint interactive visual analysis;multivariate datasets;Analytical models;Computational modeling;Data models;Data visualization;Principal component analysis;High-dimensional data analysis.;Interactive visual analysis;Computer Graphics;Computer Simulation;Data Interpretation, Statistical;Databases, Factual;Humans;Oligonucleotide Array Sequence Analysis;User-Computer Interface}, 
doi={10.1109/TVCG.2011.178}, 
ISSN={1077-2626}, 
month={Dec},}
@INPROCEEDINGS{7791210, 
author={R. Matovu and A. Serwadda}, 
booktitle={2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems (BTAS)}, 
title={Your substance abuse disorder is an open secret! Gleaning sensitive personal information from templates in an EEG-based authentication system}, 
year={2016}, 
pages={1-7}, 
abstract={Given the task of designing an authentication system that uses brain waves as input, researchers typically focus on the sole objective of maximizing authentication accuracy. In this paper we challenge this common wisdom and argue that because brain waves encode a lot of other (potentially sensitive) information about the user, this single-pronged, privacy-agnostic approach can have significant privacy implications. Based on a publicly accessible dataset, we rigorously analyze two EEG-based authentication systems built in accordance with this philosophy and show that such designs could potentially divulge more of the users sensitive personal information than that regarding the intended authentication functionality. The paper argues for privacy-aware designs for systems which take brain signals as input.}, 
keywords={brain-computer interfaces;data privacy;electroencephalography;EEG based authentication systems;authentication accuracy maximization;brain waves;open secret;privacy aware designs;sensitive personal information;substance abuse disorder;Alcoholism;Authentication;Brain modeling;Electrodes;Electroencephalography;Error analysis;Privacy}, 
doi={10.1109/BTAS.2016.7791210}, 
month={Sept},}
@INPROCEEDINGS{7128928, 
author={S. Song and Y. Meng}, 
booktitle={2015 IEEE 9th International Conference on Research Challenges in Information Science (RCIS)}, 
title={Classifying and ranking microblogging hashtags with news categories}, 
year={2015}, 
pages={540-541}, 
abstract={In microblogging, hashtags are used to be topical markers, and they are adopted by users that contribute similar content or express a related idea. However, hashtags are created in a free style and there is no domain category information about them, which make users hard to get access to organized hashtag presentation. In this paper, we propose an approach that classifies hashtags with news categories, and then carry out a domain-sensitive popularity ranking to get hot hashtags in each domain. The proposed approach first trains a domain classification model with news content and news category information, then detects microblogs related to a hashtag to be its representative text, based on which we can classify this hashtag with a domain. Finally, we calculate the domain-sensitive popularity of each hashtag with multiple factors, to get most hotly discussed hashtags in each domain. Preliminary experimental results on a dataset from Sina Weibo, one of the largest Chinese microblogging websites, show usefulness of the proposed approach on describing hashtags.}, 
keywords={information analysis;social networking (online);Chinese microblogging websites;Sina Weibo;domain classification model;domain-sensitive popularity;domain-sensitive popularity ranking;microblogging hashtags;news categories;news category information;news content;organized hashtag presentation;topical markers;Entertainment industry;Feature extraction;Media;Semantics;Training;Twitter;World Wide Web;domain-sensitive popularity ranking;hashtags;microblogging;news categories}, 
doi={10.1109/RCIS.2015.7128928}, 
ISSN={2151-1349}, 
month={May},}
@ARTICLE{4469884, 
author={J. Wang and L. Duan and Q. Liu and H. Lu and J. S. Jin}, 
journal={IEEE Transactions on Multimedia}, 
title={A Multimodal Scheme for Program Segmentation and Representation in Broadcast Video Streams}, 
year={2008}, 
volume={10}, 
number={3}, 
pages={393-408}, 
abstract={With the advance of digital video recording and playback systems, the request for efficiently managing recorded TV video programs is evident so that users can readily locate and browse their favorite programs. In this paper, we propose a multimodal scheme to segment and represent TV video streams. The scheme aims to recover the temporal and structural characteristics of TV programs with visual, auditory, and textual information. In terms of visual cues, we develop a novel concept named program-oriented informative images (POIM) to identify the candidate points correlated with the boundaries of individual programs. For audio cues, a multiscale Kullback-Leibler (K-L) distance is proposed to locate audio scene changes (ASC), and accordingly ASC is aligned with video scene changes to represent candidate boundaries of programs. In addition, latent semantic analysis (LSA) is adopted to calculate the textual content similarity (TCS) between shots to model the inter-program similarity and intra-program dissimilarity in terms of speech content. Finally, we fuse the multimodal features of POIM, ASC, and TCS to detect the boundaries of programs including individual commercials (spots). Towards effective program guide and attracting content browsing, we propose a multimodal representation of individual programs by using POIM images, key frames, and textual keywords in a summarization manner. Extensive experiments are carried out over an open benchmarking dataset TRECVID 2005 corpus and promising results have been achieved. Compared with the electronic program guide (EPG), our solution provides a more generic approach to determine the exact boundaries of diverse TV programs even including dramatic spots.}, 
keywords={digital video broadcasting;video streaming;TV programs;audio scene changes;broadcast video streams;digital video recording;electronic program guide;latent semantic analysis;multimodal scheme;multiscale Kullback-Leibler distance;playback systems;program segmentation;program-oriented informative images;textual content similarity;Broadcast video;TV program segmentation;latent semantic analysis;multimodal fusion}, 
doi={10.1109/TMM.2008.917362}, 
ISSN={1520-9210}, 
month={April},}
@INPROCEEDINGS{5992600, 
author={X. Yu and A. Pan and L. A. Tang and Z. Li and J. Han}, 
booktitle={2011 International Conference on Advances in Social Networks Analysis and Mining}, 
title={Geo-Friends Recommendation in GPS-based Cyber-physical Social Network}, 
year={2011}, 
pages={361-368}, 
abstract={The popularization of GPS-enabled mobile devices provides social network researchers a taste of cyber-physical social network in advance. Traditional link prediction methods are designed to find friends solely relying on social network information. With location and trajectory data available, we can generate more accurate and geographically related results, and help web-based social service users find more friends in the real world. Aiming to recommend geographically related friends in social network, a three-step statistical recommendation approach is proposed for GPS-enabled cyber-physical social network. By combining GPS information and social network structures, we build a pattern-based heterogeneous information network. Links inside this network reflect both people's geographical information, and their social relationships. Our approach estimates link relevance and finds promising geo-friends by employing a random walk process on the heterogeneous information network. Empirical studies from both synthetic datasets and real-life dataset demonstrate the power of merging GPS data and social graph structure, and suggest our method outperforms other methods for friends recommendation in GPS-based cyber-physical social network.}, 
keywords={Global Positioning System;Internet;geographic information systems;graph theory;mobile computing;recommender systems;social networking (online);GPS-based Cyber-physical social network;GPS-enabled mobile devices;Web-based social service users;geo-friends recommendation;geographical information;heterogeneous information network;link prediction methods;pattern-based heterogeneous information network;random walk process;social graph structure;three-step statistical recommendation approach;Correlation;Equations;Global Positioning System;History;Mathematical model;Social network services;Trajectory;cyber-physical;friend recommendation;gps;social network}, 
doi={10.1109/ASONAM.2011.118}, 
month={July},}
@INPROCEEDINGS{6890273, 
author={Y. Liu and H. Liu and Y. Liu and F. Sun}, 
booktitle={2014 IEEE International Conference on Multimedia and Expo (ICME)}, 
title={Outlier-attenuating summarization for user-generated-video}, 
year={2014}, 
pages={1-6}, 
abstract={In this paper, the key-frame extraction problem for user-generated-videos which are captured by smart phones is investigated. A collaborative sparse coding model which incorporates the 1/2, 1 and Li, 2 regularization terms are proposed to select few key-frames while attenuating the influences of the outlier frames. Further, the sensors embedded in the smart phone is used to collect the acceleration values, which can be used to improve the performance of outlier-attenuations. Finally, a real dataset is constructed to test the proposed method and the experimental validation shows promising results.}, 
keywords={feature extraction;image capture;mobile computing;smart phones;video coding;L1,2 regularization;L2,1 regularization;acceleration value;collaborative sparse coding model;keyframe extraction problem;outlier attenuating summarization;sensors;smart phone;user generated video capture;Acceleration;Accelerometers;Encoding;Feature extraction;Optimization;Sensors;Smart phones;User-generated-video;ac-celerometers;sparse coding}, 
doi={10.1109/ICME.2014.6890273}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{5575098, 
author={K. C. Lu and C. W. Hsu and D. L. Yang}, 
booktitle={2010 4th International Conference on Multimedia and Ubiquitous Engineering}, 
title={A Novel Approach for Efficient and Effective Mining of Mobile User Behaviors}, 
year={2010}, 
pages={1-6}, 
abstract={Many people increasingly rely on mobile communication services to carry out daily activities. Due to the limitation of the PCS network architecture, a constantly relocating user may encounter significant delay when requesting data or value-added services. Previous research showed that this inefficiency can be effectively reduced by predicting the user's mobile patterns. However, most research merely focused on the user's moving nodes without considering the traffic times and requested services which could dramatically affect the user's behavior. The research also did not consider how likely the user is going to relocate. Thus, in this work, we extend the hidden Markov model for modeling the behavior of the mobile users with regard to the following important factors: 1) moving node, 2) requested service, 3) user state, and 4) traffic time. Our novel approach requires only one scan of the target dataset. Moreover, the needed memory space and processing time can be independent of the transaction size. A user model can be built to predict the user's mobile patterns at different granularity levels, as well as for decision support and service improvement. Moreover, the built model can be easily adjusted later to reflect the latest user behavior without re-scanning the original dataset. Our approach can also be readily used to mine streaming data.}, 
keywords={consumer behaviour;data mining;hidden Markov models;media streaming;mobile communication;mobile computing;transaction processing;PCS network architecture;decision support;hidden Markov model;mobile communication service;mobile user behaviors mining;personal communications service;service improvement;streaming data mining;value added services;Accuracy;Data mining;Data models;Hidden Markov models;Mobile communication;Mobile computing;Steady-state}, 
doi={10.1109/MUE.2010.5575098}, 
month={Aug},}
@INPROCEEDINGS{6637758, 
author={S. Kim and P. Georgiou and S. Narayanan}, 
booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
title={On-line genre classification of TV programs using audio content}, 
year={2013}, 
pages={798-802}, 
abstract={Automatic genre classification of TV programs can benefit users in various ways such as allowing for rapid selection of multimedia content. In this paper, we introduce an on-line method that can classify genres of TV programs using audio content. We deploy an acoustic topic model (ATM) which was originally designed to capture contextual information embedded within audio segments. With a dataset based on RAI content, we perform both on-line and off-line classification; we segment audio signals with a fixed length and feed into the system for on-line classification tasks, while we use whole audio signals for off-line tasks. The off-line experimental results suggest that the proposed method using audio content yields competitive performance with conventional methods using audio-visual features and outperforms conventional audio-based approaches. The on-line results show promising results in classifying genre of TV programs with short segments and also suggest that ATM performs better than conventional GMM method if the length of audio segments is longer (>1 second).}, 
keywords={audio signal processing;audio-visual systems;television broadcasting;ATM;RAI content;TV programs;acoustic topic model;audio content;audio segments;audio signal segmentation;audio-visual features;automatic genre classification;contextual information;multimedia content;on-line genre classification;Accuracy;Acoustics;Databases;Feature extraction;Multimedia communication;TV;Vectors}, 
doi={10.1109/ICASSP.2013.6637758}, 
ISSN={1520-6149}, 
month={May},}
@INPROCEEDINGS{7886132, 
author={A. Onuean and J. Gim and Y. Jang and H. Jung}, 
booktitle={2017 9th International Conference on Knowledge and Smart Technology (KST)}, 
title={Study on extracting implicit patterns of patent data based on timeline}, 
year={2017}, 
pages={347-353}, 
abstract={Patent documents are provide a significant source of knowledge about future technologies. Many attempts have been conducted to mine important knowledge from patents to analyze new technology trends. In this paper, we will to analyze implicit knowledge derived from the patents dataset of Big Data domain from KIPRIS. Keywords that occur in the title of patents are classified into three categories: Approach, Goal Object, and Goal Predicate, in order to create a model of relations of title patterns. The same keywords found on the timeline interval will be analyzed and illustrated in the patent pattern which are able to depict the relationship of goals and approaches of the patents occurred in different time interval. As a result, implicit trends and knowledge related to of specific keywords of technology reflect of each time gap can be obtained. Search result using `Goal object, Goal predicate and Approach' pattern query is also found efficient and meet the user enquiry related technologies in timeline.}, 
keywords={Big Data;data mining;document handling;pattern classification;Big Data;KIPRIS;approach pattern query;goal object;goal predicate;patent data implicit pattern extraction;patent documents;title patterns;Big Data;Data mining;Databases;Market research;Organizations;Patents;Technological innovation;Implicit;big data;patent;patent pattern;timeline}, 
doi={10.1109/KST.2017.7886132}, 
month={Feb},}
@INPROCEEDINGS{7771716, 
author={S. Singh and A. Yassine and S. Shirmohammadi}, 
booktitle={2016 IEEE Electrical Power and Energy Conference (EPEC)}, 
title={Incremental mining of frequent power consumption patterns from smart meters big data}, 
year={2016}, 
pages={1-6}, 
abstract={The key elements for understanding power consumption of a typical home are related to the activities that users are performing, the time at which appliances are used, and the interdependencies with other appliances that may be used concurrently. This information can be extracted from context rich smart meters big data. However, the main challenge is how to mine complex interdependencies among different appliances usage within a home where multiple concurrent data streams are occurring. Furthermore, generation of energy consumption data from a smart meter is an ongoing continuous process and over period of time inter-appliance associations can change or new ones can establish. In this paper, we propose incremental mining of frequent power consumption patterns from smart meters big data. Our model exploits the benefits of pattern growth strategy and mine in quantum of 24 hour period, i.e. frequent patterns are extracted from data comprising of appliance usage tuples for 24 hours period, in a progressive manner. The details and the results of evaluating the proposed mechanism using real smart meters dataset are presented in this paper.}, 
keywords={Big Data;data mining;domestic appliances;power engineering computing;smart meters;appliances;complex interdependencies;continuous process;data streams;energy consumption data;frequent power consumption patterns;incremental mining;pattern growth strategy;power consumption;smart meters big data;time interappliance associations;Big data;Data mining;Energy consumption;Home appliances;Itemsets;Smart meters}, 
doi={10.1109/EPEC.2016.7771716}, 
month={Oct},}
@ARTICLE{6232028, 
author={A. Raza and L. F. Capretz and F. Ahmed}, 
journal={IET Software}, 
title={Usability bugs in open-source software and online forums}, 
year={2012}, 
volume={6}, 
number={3}, 
pages={226-230}, 
abstract={The unlimited number of open-source software (OSS) users and the importance of end users' experience in determining software quality make usability an even more critical quality attribute for OSS than it is for proprietary software. The research model of this study establishes the relationship between usability errors in OSS and online public forums. The results of this empirical analysis provide evidence about active management of usability-related issues in OSS. To conduct this research, the authors used a dataset consisting of 1753 OSS projects, covering a broad range of categories. The results of the study show that online forums play a significant role in identifying and fixing usability bugs in OSS.}, 
keywords={Internet;program debugging;public domain software;software quality;software reusability;OSS;critical quality attribute;online public forums;open-source software;proprietary software;software quality;usability bugs}, 
doi={10.1049/iet-sen.2011.0105}, 
ISSN={1751-8806}, 
month={June},}
@INPROCEEDINGS{6558439, 
author={L. Ionkov and M. Lang and C. Maltzahn}, 
booktitle={2013 IEEE 29th Symposium on Mass Storage Systems and Technologies (MSST)}, 
title={DRepl: Optimizing access to application data for analysis and visualization}, 
year={2013}, 
pages={1-11}, 
abstract={Until recently most scientific applications produced data that is saved, analyzed and visualized at later time. In recent years, with the large increase in the amount of data and computational power available there is demand for applications to support data access in-situ, or close-to simulation to provide application steering, analytics and visualization. Data access patterns required for these activities are usually different than the data layout produced by the application. In most of the large HPC clusters scientific data is stored in parallel file systems instead of locally on the cluster nodes. To increase reliability, the data is replicated, using standard RAID schemes. Parallel file server nodes usually have more processing power than they need, so it is feasible to off-load some of the data intensive processing to them. DRepl replaces the standard methods of data replication with replicas having different layouts, optimized for the most commonly used access patterns. Replicas can be complete (i.e. any other replica can be reconstructed from it), or incomplete. DRepl consists of a language to describe the dataset and the necessary data layouts and tools to create a user-space file server that provides and keeps the data consistent and up to date in all optimized layouts. DRepl decouples the data producers and consumers and the data layouts they use from the way the data is stored on the storage system. DRepl has shown up to 2x for cumulative performance when data is accessed using optimized replicas.}, 
keywords={RAID;file servers;parallel processing;storage allocation;DRepl;HPC clusters;RAID schemes;application steering;cluster nodes;data access patterns;data layout;data replication;data storage;parallel file server nodes;parallel file systems;user-space file server;Arrays;Data models;Data visualization;Engines;Layout;Reactive power;Servers;DISC;data replication;data storage;exascale;fault tolerance}, 
doi={10.1109/MSST.2013.6558439}, 
ISSN={2160-195X}, 
month={May},}
@INPROCEEDINGS{7351263, 
author={A. Betancourt and P. Morerio and L. Marcenaro and M. Rauterberg and C. Regazzoni}, 
booktitle={2015 IEEE International Conference on Image Processing (ICIP)}, 
title={Filtering SVM frame-by-frame binary classification in a detection framework}, 
year={2015}, 
pages={2552-2556}, 
abstract={Classifying frames, or parts of them, is a common way of carrying out detection tasks in computer vision. However, frame by frame classification suffers from sudden significant variations in image texture, colour and luminosity, resulting in noise in the extracted features and consequently in the decisions taken. Support Vector Machines have been widely validated as powerful tools for frame by frame detection of non-separable datasets, but are extremely sensitive to these variations between adjacent frames, creating as consequence sudden flickering in the classification results. This work proposes a Dynamic Bayesian Network to smooth the classification results of Support Vector Machines (SVM) in detection tasks. The method is evaluated in First Person Vision (FPV) videos, where a SVM is used to decide whether or not the user's hands are in his field of view.}, 
keywords={belief networks;computer vision;feature extraction;image classification;image colour analysis;image filtering;image texture;object detection;support vector machines;video signal processing;FPV videos;SVM frame-by-frame binary classification;computer vision;dynamic Bayesian network;feature extraction;filtering;first person vision videos;image colour;image luminosity;image texture;nonseparable dataset frame by frame detection;support vector machines;Bayes methods;Current measurement;Feature extraction;Kalman filters;Mathematical model;Support vector machines;Videos;Bayesian Filtering;Classification;Detection;Egocentric Vision;First Person Vision;Hand detection;Wearable computing}, 
doi={10.1109/ICIP.2015.7351263}, 
month={Sept},}
@INPROCEEDINGS{7745451, 
author={K. Alzhrani and E. M. Rudd and T. E. Boult and C. E. Chow}, 
booktitle={2016 IEEE Conference on Intelligence and Security Informatics (ISI)}, 
title={Automated big text security classification}, 
year={2016}, 
pages={103-108}, 
abstract={In recent years, traditional cybersecurity safeguards have proven ineffective against insider threats. Famous cases of sensitive information leaks caused by insiders, including the WikiLeaks release of diplomatic cables and the Edward Snowden incident, have greatly harmed the U.S. government's relationship with other governments and with its own citizens. Data Leak Prevention (DLP) is a solution for detecting and preventing information leaks from within an organization's network. However, state-of-art DLP detection models are only able to detect very limited types of sensitive information, and research in the field has been hindered due to the lack of available sensitive texts. Many researchers have focused on document-based detection with artificially labeled “confidential documents” for which security labels are assigned to the entire document, when in reality only a portion of the document is sensitive. This type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents. In this paper, we introduce Automated Classification Enabled by Security Similarity (ACESS), a new and innovative detection model that penetrates the complexity of big text security classification/detection. To analyze the ACESS system, we constructed a novel dataset, containing formerly classified paragraphs from diplomatic cables made public by the WikiLeaks organization. To our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity.}, 
keywords={organisational aspects;pattern classification;security of data;text analysis;ACESS;DLP detection models;US government relationship;WikiLeaks organization;automated big text security classification;automated classification enabled by security similarity;big text security detection;confidential documents;cybersecurity safeguards;data leak prevention;document-based detection;innovative detection model;insider threats;organization network;paragraph granularity;sensitive information leaks;whole-document based security labeling;Clustering algorithms;Cryptography;Feature extraction;Labeling;Monitoring;Sensitivity;Data Leak;Insider Threats;Machine Learning;Security Classification}, 
doi={10.1109/ISI.2016.7745451}, 
month={Sept},}
@INPROCEEDINGS{7488075, 
author={M. M. Salehin and M. Paul}, 
booktitle={2015 18th International Conference on Computer and Information Technology (ICCIT)}, 
title={An efficient method for video summarization using moving object information}, 
year={2015}, 
pages={237-242}, 
abstract={Video surveillance system captures continuous video for the purpose of security, monitoring, investigating and so on. It requires huge memory space to store as well as enormous time to retrieve important information manually from this high volume of videos. In this paper, we propose a novel video summarization scheme using moving object information by considering area of moving objects extracted from dynamic background modeling and frame-to-frame object motion. Through this scheme we rank all frames according to the importance of being key frame by combining moving object features through a fusion method so that users can select desired length of videos for summary. The experimental results show that the proposed method provides better video summary compared to the state-of-the-art method using a publicly available benchmark BL-7F video surveillance dataset.}, 
keywords={feature extraction;image capture;image fusion;image motion analysis;video signal processing;video surveillance;BL-7F video surveillance dataset;continuous video capture;dynamic background modeling;frame-to-frame object motion;fusion method;memory space;moving object area;moving object features;moving object information;video length;video summarization;video surveillance system;Cameras;Dynamics;Feature extraction;Security;Video surveillance;Background modelling;frame difference;video summarization}, 
doi={10.1109/ICCITechn.2015.7488075}, 
month={Dec},}
@INPROCEEDINGS{6268105, 
author={Y. Li and S. Ma and Y. Zhang and R. Huang}, 
booktitle={2012 IEEE 12th International Conference on Advanced Learning Technologies}, 
title={Expertise Network Discovery via Topic and Link Analysis in Online Communities}, 
year={2012}, 
pages={311-315}, 
abstract={Online communities have become important places for people to seek and share expertise. Yet with the increasing number of members and produced artifacts within the communities, it is challenging to find the influential experts who post topic-specific high-quality content. This paper presents an approach to discover expertise network in online communities based on textual information and social links. In addition to computing documents' topic-focus degree, the approach measures the quality of documents according to users' feedback behaviors and topic-specific influence of users who give feedback. In this way, user's expertise rank and social links are both considered to constitute expertise network. Experiments on real dataset have shown that our approach is effective to discover the meaningful expertise networks.}, 
keywords={social networking (online);text analysis;user modelling;documents quality;expertise network discovery;expertise rank;link analysis;online communities;social links;textual information;topic analysis;topic-focus degree;topic-specific high-quality content;topic-specific user influence;user feedback behavior;Algorithm design and analysis;Blogs;Communities;Educational institutions;Humans;Social network services;expertise finding;online community;social link;textual information;topic-specific}, 
doi={10.1109/ICALT.2012.80}, 
ISSN={2161-3761}, 
month={July},}
@INPROCEEDINGS{7280398, 
author={Heng Wang and Z. Abraham}, 
booktitle={2015 International Joint Conference on Neural Networks (IJCNN)}, 
title={Concept drift detection for streaming data}, 
year={2015}, 
pages={1-9}, 
abstract={Common statistical prediction models often require and assume stationarity in the data. However, in many practical applications, changes in the relationship of the response and predictor variables are regularly observed over time, resulting in the deterioration of the predictive performance of these models. This paper presents Linear Four Rates (LFR), a framework for detecting these concept drifts and subsequently identifying the data points that belong to the new concept (for relearning the model). Unlike conventional concept drift detection approaches, LFR can be applied to both batch and stream data; is not limited by the distribution properties of the response variable (e.g., datasets with imbalanced labels); is independent of the underlying statistical-model; and uses user-specified parameters that are intuitively comprehensible. The performance of LFR is compared to benchmark approaches using both simulated and commonly used public datasets that span the gamut of concept drift types. The results show LFR significantly outperforms benchmark approaches in terms of recall, accuracy and delay in detection of concept drifts across datasets.}, 
keywords={data mining;statistical analysis;LFR;batch data;concept drift detection;concept drift type;data point;distribution property;linear four rate;predictive performance;public dataset;statistical prediction model;stream data;streaming data;user-specified parameter;Data models;Radio frequency;Yttrium}, 
doi={10.1109/IJCNN.2015.7280398}, 
ISSN={2161-4393}, 
month={July},}
@INPROCEEDINGS{7123518, 
author={S. Ghiasifard and S. Khadivi and M. Asadpour and A. Zafarian}, 
booktitle={2015 The International Symposium on Artificial Intelligence and Signal Processing (AISP)}, 
title={Improving the quality of overlapping community detection through link addition based on topic similarity}, 
year={2015}, 
pages={182-187}, 
abstract={Community detection in social networks is usually done based on the density of connections between groups of nodes. However, these links do not necessarily represent an actual friendship especially in online social networks. There are users with declared friendship connections but without actual communication and no common interests. Most of the works in this area can be divided into two groups: topology-based and topic-based. The former usually leads to communities each containing diverse topics, and the latter leads to communities each with a consistent topic but with diverse structure. In this paper, we measure the similarity between users using topic models to generate virtual links for users with common interests. Moreover, in order to reduce the effect of useless links between users, we weight the network by measuring similarity of users' topics, so we could generate conforming communities, which contain only one topic or a group of consistent topics. The test results on Enron email dataset have shown the superior performance of our proposed method in the task of community detection.}, 
keywords={social networking (online);topology;Enron e-mail dataset;online social networks;overlapping community detection;topic similarity;topic-based community;topology-based community;virtual links;Clustering algorithms;Communities;Data mining;Electronic mail;Entropy;Image edge detection;Social network services;Social network analysis;Topic modeling;overlapping community detection}, 
doi={10.1109/AISP.2015.7123518}, 
month={March},}
@INPROCEEDINGS{6468674, 
author={Z. X. Liao and W. C. Peng and P. S. Yu}, 
booktitle={2012 IEEE International Conference on Granular Computing}, 
title={A profile-based framework for interaction prediction}, 
year={2012}, 
pages={265-270}, 
abstract={In this paper, we generalize the link prediction problem to an interaction prediction problem. Compared with links in social networks, interactions can occur several times repeatedly. Based on the observation, we formulate an event-triggered interaction prediction problem. For example, we may want to know when a user connects to a website (e.g. Facebook), who will also connect to the website. We propose a Profile-based Interaction Prediction Framework (PIPF) which can solve the event-triggered interaction prediction problem efficiently and effectively. In PIPF, we first transform the interaction log into a Sliding-window Evolving Graph (SEG) to reduce the data volume and incrementally update SEG as interaction log grows. Then, we build profiles designed to present users' behavior by extracting the static and surprising features from SEG. The static (respectively, surprising) feature reflects the regularity of users' behavior (respectively, the temporal behavior). When an event occurs, we compute the similarity between the event and each candidate link. We propose two similarity functions for static and surprising features and an automatic selection strategy to control the influence of the two features. We use a real dataset that records Internet connections to evaluate the scalability, efficiency, and effectiveness of PIPF. The experimental results show that PIPF is far more scalable and efficient than the previous methods to perform real-time prediction.}, 
keywords={Analytical models;Computational modeling;Predictive models;Real-time systems;Solid modeling;data mining;interaction prediction;profile;time-evolving graph}, 
doi={10.1109/GrC.2012.6468674}, 
month={Aug},}
@INPROCEEDINGS{7574720, 
author={Xinhui Song and Ke Chen and Jie Lei and Li Sun and Zhiyuan Wang and Lei Xie and Mingli Song}, 
booktitle={2016 IEEE International Conference on Multimedia Expo Workshops (ICMEW)}, 
title={Category driven deep recurrent neural network for video summarization}, 
year={2016}, 
pages={1-6}, 
abstract={A large number of videos are generated and uploaded to video websites (like youku, youtube) every day and video websites play more and more important roles in human life. While bringing convenience, the big video data raise the difficulty of video summarization to allow users to browse a video easily. However, although there are many existing video summarization approaches, the key frames selected fail to integrate the large video contexts and the qualities of the summarized results are difficult to evaluate because of the lack of ground-truth. Inspired by the previous methods that extract key frames, we propose a deep recurrent neural network model, which learns to extract category-driven key frames. First, we sequentially extract a fixed number of key frames using time-dependent location networks. Second, we utilize recurrent neural network to integrate information of the key frames to classify the category of the video. Therefore, the quality of the extracted key frames could be evaluated by the categorization accuracy. Experiments on a 500-video dataset show that the proposed scheme extracts reasonable key frames and outperforms other methods by quantitative evaluation.}, 
keywords={recurrent neural nets;video signal processing;category driven deep recurrent neural network;category-driven key frames;deep recurrent neural network model;time-dependent location networks;video summarization;video websites;Data mining;Feature extraction;Learning (artificial intelligence);Learning systems;Recurrent neural networks;Training;Recurrent video summarization;reinforcement learning;video categorization}, 
doi={10.1109/ICMEW.2016.7574720}, 
month={July},}
@INPROCEEDINGS{7842140, 
author={X. Zhu and R. Hao and H. Chi and X. Du}, 
booktitle={2016 IEEE Global Communications Conference (GLOBECOM)}, 
title={Personalized Location Recommendations with Local Feature Awareness}, 
year={2016}, 
pages={1-6}, 
abstract={Location-based social networks (LBSNs) make it possible for servers to record users' location histories, mine their life patterns, and infer individual preferences. As an important component of LBSNs, recommender systems gained popularity in recent years. Recommender systems can automatically list candidate locations for users according to their preferences, which is different from traditional search methods. However, making effective recommendations suffers from data sparsity. In order to relieve this problem and achieve high effectiveness, we take context information into consideration and present a personalized location recommender system considering both user preference and local features in this paper. To be specific, we apply Labeled-LDA in user preference learning and local features inference processes, which are denoted as UL-LDA model and CL-LDA model, respectively. Because of this, we can make recommendations even on the condition that users are in a new city and have little information about the city. We evaluate our approach with extensive experiments on a large-scale Foursquare dataset. The experimental results clearly validate the effectiveness of our approach.}, 
keywords={inference mechanisms;learning (artificial intelligence);mobile computing;recommender systems;search problems;social networking (online);CL-LDA model;LBSN;UL-LDA model;data sparsity;labeled-LDA;large-scale Foursquare dataset;local feature awareness;local features inference processes;location-based social networks;personalized location recommendations;personalized location recommender system;search methods;user preference learning;Collaboration;Entertainment industry;Geospatial analysis;History;Recommender systems;Social network services;Urban areas}, 
doi={10.1109/GLOCOM.2016.7842140}, 
month={Dec},}
@INPROCEEDINGS{5572537, 
author={F. Zhang}, 
booktitle={2010 International Conference of Information Science and Management Engineering}, 
title={Analysis of Love-Hate Shilling Attack Against E-commerce Recommender System}, 
year={2010}, 
volume={1}, 
pages={318-321}, 
abstract={Recent research has focus on examining the security of e-commerce collaborative filtering(CF) recommender system. Love/hate attack is one of the most effective model as a nuke attack against the classic user-based CF. In this paper, we examine the effectiveness of Love/hate attack against our topic-level trust based recommendation algorithm that incorporate topic-level trust model into traditional collaborative filtering algorithm. The results of our experiments conducted on well-known dataset show that Love/hate attack is more robust against topic-level trust based recommendation algorithm than against classical user-based CF algorithm.}, 
keywords={electronic commerce;groupware;recommender systems;security of data;e-commerce collaborative filtering recommender system;love-hate shilling attack;topic-level trust model;Algorithm design and analysis;Biological system modeling;Collaboration;Prediction algorithms;Recommender systems;Robustness;collaborative filtering;love/hate attack;recommender system;topic-level trust}, 
doi={10.1109/ISME.2010.116}, 
month={Aug},}
@INPROCEEDINGS{6997682, 
author={H. Su and C. Wang and Y. Zhu and B. Yan and H. Zheng}, 
booktitle={2014 International Conference on Multisensor Fusion and Information Integration for Intelligent Systems (MFI)}, 
title={Parallel collaborative filtering recommendation model based on expand-vector}, 
year={2014}, 
pages={1-6}, 
abstract={The recommendation system based on collaborative filtering is one of the most popular recommendation mechanism. However, with the continuous expansion of the system, several problems that traditional collaborative filtering recommendation algorithm (CF) faced such as speedup, and scalability are worsen. In order to address these issues, a parallel collaborative filtering recommendation model based on expand-vector (PCF-EV) is proposed. Firstly, the eigenvector is expanded reasonably to get the expand-vector based on the expand-vector model. Then, based on the expand-vectors, a series of similarity calculations are expressed. Finally the nearest neighbor item is found and a more accurate recommendation to the target user is given based on the calculation results. On the basis of these, the further optimization makes it applied to the parallel computing framework successfully. Using the MovieLens dataset, the performance of PCF-EV is compared with that of others from both sides of recommendation precision and the speedup ratio. Through experimental results, which are compared with CF, PCF-EV overcomes the problem of cold startup which the CF encounters. Moreover, the accuracy and recall ratio has been doubled. Compared with the serial implementation on the high-end dual-core CPU, the parallel implementation on the low and middle-end GPU reaches nearly 170 times speedup in optimal conditions.}, 
keywords={collaborative filtering;eigenvalues and eigenfunctions;parallel programming;recommender systems;vectors;MovieLens dataset;PCF-EV;eigenvector;expand-vector;nearest neighbor;parallel collaborative filtering recommendation;parallel computing framework;similarity calculation;Algorithm design and analysis;Arrays;Artificial intelligence;Collaboration;Filtering;Graphics processing units;Vectors;GPU;MapReduce;collaborative filtering;data mining;expand-vector}, 
doi={10.1109/MFI.2014.6997682}, 
month={Sept},}
@INPROCEEDINGS{6887952, 
author={Zhibin Zhao and Weisheng Xu and Dazhang Chen}, 
booktitle={2014 IEEE International Conference on System Science and Engineering (ICSSE)}, 
title={EM-LDA model of user behavior detection for energy efficiency}, 
year={2014}, 
pages={295-300}, 
abstract={In energy efficient analysis, user behavior detection related to the dynamic demands of energy is a critical aspect to support the intelligent control schema of Building Management System. In this paper, anomalous occupancy of user behavior tends to be figured out from multiple time-series of occupancy record. The problems in this issue include the time-stamp detection and time-span identification of anomaly events. Most inference model based on Markov Chain can illustrate the time-stamp detection problem reasonably, but the time-span identification problem is just vaguely explained. Therefore, a Latent Dirichlet Allocation (LDA) model is declared to figure out those two problems efficiently. First, the discrete data of occupancy are expressed as mixture model of Poisson distribution, and are transformed to a dataset with several semantic concepts via Expectation-Maximization Algorithm. Then, the denotation of LDA components (including the words, the topic, the document, and the relevant parameters and hyper-parameters) are illustrated, according to the semantic dataset. Finally, particle filter algorithm is leveraged to sample latent variable of topic, according to the conditional posterior probability of word for specific topic. After iterations, the probability of samples is closely approximated the true marginal distribution of words with specific topic. Through the relation matrix of words and topic, the most possible topic can be explained for the specific document. If a document's topic is different with other document's topic, this document can be identified as a bias of point anomaly (noting generally the amount of topics setup to two). Due to a word can involve several time-stamps of the time-series in a time, other contextual anomalies nearby the point anomaly can be marked, and they are the notation of time-spans for anomalous events. With a step by step along the time-series, all time-stamps can be ergodic as the documents, then all the contextual an- malies can be explained as following the happening of point anomalous event.}, 
keywords={Markov processes;Poisson distribution;building management systems;energy conservation;expectation-maximisation algorithm;mixture models;particle filtering (numerical methods);probability;time series;EM-LDA model;Markov chain;Poisson distribution;anomalous occupancy;anomaly events;building management system;conditional posterior probability;contextual anomaly;energy efficiency;expectation-maximization algorithm;latent Dirichlet allocation;marginal distribution;mixture model;multiple time-series;occupancy record;particle filter;semantic concepts;semantic dataset;specific document;time-span identification;time-stamp detection;user behavior detection;Algorithm design and analysis;Energy efficiency;Anomaly Detection;Energy Efficiency;Latent Dirichlet Allocation;Particle Filter;Topic Model;User Behavior Detection}, 
doi={10.1109/ICSSE.2014.6887952}, 
ISSN={2325-0909}, 
month={July},}
@INPROCEEDINGS{7344830, 
author={J. D. Chen and H. Y. Kao}, 
booktitle={2015 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
title={LDA based semi-supervised learning from streaming short text}, 
year={2015}, 
pages={1-8}, 
abstract={With the rapidly growing of real-time social media, like Twitter, many users share and discuss their interest topics through such platforms. Hashtag is a type of metadata tag which allows users to annotate their topics of tweets. For research usage, for example, hashtags can help the performance of event detection by observing the trend of hashtags. Although Twitter grows rapidly, hashtag growth is not as expected. Our dataset shows that there are less than 20% of all tweets containing hashtags. We think that it is caused by that most users may have no idea what hashtags are suitable for tweets they post. If we can recommend suitable hashtags to users, it can be one of the solutions to solve the problem of low usage rate of hashtag. Hashtag recommendation belongs to supervised learning problem. More labeled data for training the learning model can get higher performance in prediction. However, labeled data in hashtag recommendation is not so much due to low usage rate of hashtag. Thus, we want to exploit unlabeled data, i.e. non-hashtag tweets, to solve this problem. Now we have large amount of unlabeled data, but directly adding all non-hashtag tweets may not be helpful to train the model. To overcome this issue, we apply the weight-updating mechanisms to filter out the useless parts of non-hashtag tweets. These mechanisms also have to consider the temporal characteristics of hashtag due to the real-time nature of Twitter. The experimental results in this research show that adding non-hashtag tweets to extend original training data outperforms baseline methods which only exploit labeled data to train the model.}, 
keywords={learning (artificial intelligence);meta data;recommender systems;social networking (online);LDA based semisupervised learning;Twitter;event detection;hashtag recommendation;metadata tag;nonhashtag tweets;real-time social media;streaming short text;weight-updating mechanisms;Data models;Media;Supervised learning;Tagging;Training;Training data;Twitter;hashtag recommendation;semi-supervised learning;social media}, 
doi={10.1109/DSAA.2015.7344830}, 
month={Oct},}
@INPROCEEDINGS{6895511, 
author={H. Zhang and W. Ni and M. Zhao and Y. Liu and Y. Yang}, 
booktitle={Proceedings of the 33rd Chinese Control Conference}, 
title={A hybrid recommendation approach for network teaching resources based on knowledge-tree}, 
year={2014}, 
pages={3450-3455}, 
abstract={Recommender systems could be used to help learners or teachers find useful network teaching resources effectively in technology enhanced learning (TEL), but the quality of recommendations is always limited by cold start, data sparsity, lack of learning or teaching contextual aware, and so on. Considering the features of network teaching resources, a hybrid recommendation approach is presented in this paper. The presented approach takes user context, association rules between resources, association rules between resources and the structure of lessons into consideration, and is mainly composed of five modules. These five modules are: (1) Course model, which is used to express the structure of lessons; (2) Association rules between resources, which are discovered in resources association rule mining module; (3) Association rules between resources and lessons, which are discovered in lessons and resources association rule mining module; (4) User dynamic profile, namely, user context which are found in reasoning user dynamic profile module; (5) Hybrid recommendation, which generates recommended lists in hybrid recommendation module. Finally, experiments have been done on a real dataset from “HHT Education Cloud”, an enterprise education resources sharing platform. The results have shown that our hybrid method can outperform the general recommendation method. The Recall has an improvement ranging from 0.131 to 0.213, and the Precision has an improvement ranging from 0 to 0.152, when the number of recommendations changes from 1 to 40.}, 
keywords={computer aided instruction;data mining;decision trees;educational courses;recommender systems;teaching;HHT Education Cloud;TEL;course model;enterprise education resource sharing platform;hybrid recommendation approach;knowledge-tree;lesson association rule mining module;lesson structure;network teaching resources;precision improvement;real dataset;recall improvement;recommended list generation;recommender systems;resource association rule mining module;technology enhanced learning;user context;user dynamic profile module reasoning;Association rules;Collaboration;Context;Dynamic scheduling;Education;Recommender systems;Association Rule Mining;Dynamic Profile;Knowledge-Tree;Recommender Systems}, 
doi={10.1109/ChiCC.2014.6895511}, 
month={July},}
@INPROCEEDINGS{6722344, 
author={A. Rimsa and M. A. J. Song and L. E. Zárate}, 
booktitle={2013 IEEE International Conference on Systems, Man, and Cybernetics}, 
title={SCGaz - A Synthetic Formal Context Generator with Density Control for Test and Evaluation of FCA Algorithms}, 
year={2013}, 
pages={3464-3470}, 
abstract={An efficient way to evaluate FCA algorithms is through a comparative analysis of their performance in typical contexts. Comparisons are normally conducted using randomly generated contexts that may contain duplicated attributes and objects and other types of redundancies. Failing to acknowledge the presence of these redundancies in formal contexts could lead to erroneous comparison analysis. This paper proposes a tool named SCGaz (Synthetic Context Generator) that randomly fills synthetic formal contexts ensuring the absence of some type of redundancies. At the same time, the tool is able to keep track of the contexts density, allowing users to select any density in the bounds of the minimum and maximum permitted for a type of context. Thus, this approach allows more controllable and reliable simulation environment. In this work, an analysis of the time spent to generate different types of formal contexts, including large ones, is presented. As a case study, a performance comparison between Object Intersection algorithm and its dual version, Attribute Intersections, with contexts generated by SCGaz is discussed. Contexts produced by SCGaz in conjunction with real world dataset allow a more in-depth comparative analysis of FCA algorithms performance.}, 
keywords={formal concept analysis;FCA algorithms;SCGaz;attribute intersections;density control;object intersection algorithm;randomly generated contexts;synthetic formal context generator;synthetic formal contexts;Algorithm design and analysis;Context;Context modeling;Filling;Force;Lattices;Redundancy;Synthetic Formal Context Generato;formal concept analisys}, 
doi={10.1109/SMC.2013.591}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{7727521, 
author={L. G. Hafemann and R. Sabourin and L. S. Oliveira}, 
booktitle={2016 International Joint Conference on Neural Networks (IJCNN)}, 
title={Writer-independent feature learning for Offline Signature Verification using Deep Convolutional Neural Networks}, 
year={2016}, 
pages={2576-2583}, 
abstract={Automatic Offline Handwritten Signature Verification has been researched over the last few decades from several perspectives, using insights from graphology, computer vision, signal processing, among others. In spite of the advancements on the field, building classifiers that can separate between genuine signatures and skilled forgeries (forgeries made targeting a particular signature) is still hard. We propose approaching the problem from a feature learning perspective. Our hypothesis is that, in the absence of a good model of the data generation process, it is better to learn the features from data, instead of using hand-crafted features that have no resemblance to the signature generation process. To this end, we use Deep Convolutional Neural Networks to learn features in a writer-independent format, and use this model to obtain a feature representation on another set of users, where we train writer-dependent classifiers. We tested our method in two datasets: GPDS-960 and Brazilian PUC-PR. Our experimental results show that the features learned in a subset of the users are discriminative for the other users, including across different datasets, reaching close to the state-of-the-art in the GPDS dataset, and improving the state-of-the-art in the Brazilian PUC-PR dataset.}, 
keywords={computer vision;digital signatures;formal verification;handwriting recognition;learning (artificial intelligence);neural nets;Brazilian PUC-PR;GPDS-960;automatic offline handwritten signature verification;computer vision;data generation;deep convolutional neural networks;graphology;signal processing;writer-independent feature learning;Computer vision;Face;Feature extraction;Forgery;Iris recognition;Neural networks;Training}, 
doi={10.1109/IJCNN.2016.7727521}, 
month={July},}
@INPROCEEDINGS{7037677, 
author={O. A. Abdul-Rahman and K. Aida}, 
booktitle={2014 IEEE 6th International Conference on Cloud Computing Technology and Science}, 
title={Towards Understanding the Usage Behavior of Google Cloud Users: The Mice and Elephants Phenomenon}, 
year={2014}, 
pages={272-277}, 
abstract={In the era of cloud computing, users encounter the challenging task of effectively composing and running their applications on the cloud. In an attempt to understand user behavior in constructing applications and interacting with typical cloud infrastructures, we analyzed a large utilization dataset of Google cluster. In the present paper, we consider user behavior in composing applications from the perspective of topology, maximum requested computational resources, and workload type. We model user dynamic behavior around the user's session view. Mass-Count disparity metrics are used to investigate the characteristics of underlying statistical models and to characterize users into distinct groups according to their composition and behavioral classes and patterns. The present study reveals interesting insight into the heterogeneous structure of the Google cloud workload.}, 
keywords={cloud computing;human factors;Google cloud users;Google cluster;cloud computing;heterogeneous structure;mass-count disparity metrics;maximum requested computational resources;user dynamic behavior modeling;Bars;Extraterrestrial measurements;Google;Joints;Random access memory;Shape;Application composition;Mass-Count disparity;User session view;Workload trace analysis}, 
doi={10.1109/CloudCom.2014.75}, 
month={Dec},}
@INPROCEEDINGS{7733025, 
author={J. Zhu and A. Goswami and K. H. Kim and P. Mohapatra}, 
booktitle={2016 13th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)}, 
title={Verification of User-Reported Context Claims with Context Correlation Model}, 
year={2016}, 
pages={1-9}, 
abstract={Context-aware services nowadays offer incentive to user-reported context information , which inevitably solicits malicious users to cheat by submitting fabricated context claims. Conventional countermeasures based on Trusted Computing Base typically focus on particular context of interest, while disregarding the availability of various types of context information and the intrinsic correlation among them. In this work we propose a context claim verification scheme that interrogates correlated contexts of multiple dimensions to corroborate or contradict the reported context. Specifically, it first learns and models the context correlation with a Bayesian Multinet. Given a claim consisting of reported context and witnessing evidence, the scheme performs Bayesian inference with the evidence to verify the reported context. The verification process is light-weight, and can be applied to arbitrary types of context with a single model learnt. Evaluations on Reality Mining dataset and synthetic dataset validates choice of Multinet for data modeling, and demonstrate the feasibility of our scheme in context verification.}, 
keywords={belief networks;ubiquitous computing;Bayesian inference;Bayesian multinet;context claim verification scheme;context correlation model;data modeling;user-reported context claims;Bayes methods;Context;Context modeling;Context-aware services;Correlation;Data models;History}, 
doi={10.1109/SAHCN.2016.7733025}, 
month={June},}
@INPROCEEDINGS{5254654, 
author={A. Zinnen and U. Blanke and B. Schiele}, 
booktitle={2009 International Symposium on Wearable Computers}, 
title={An Analysis of Sensor-Oriented vs. Model-Based Activity Recognition}, 
year={2009}, 
pages={93-100}, 
abstract={Model-based activity recognition has been recently proposed as an alternative to signal-oriented recognition. Such model-based approaches seem attractive due to their ability to enable user-independent activity recognition and due to their improved robustness to signal-variation. The first goal of this paper is therefore to systematically analyze the benefit of body-model derived primitives in different sensor settings for multi activity recognition. Furthermore we propose a new body-model based approach using accelerometer sensors only thereby reducing the sensor requirements significantly. Results on a 20 activity dataset indicate that body-model based approaches consistently improve results over signal-oriented approaches.}, 
keywords={sensors;ubiquitous computing;user interfaces;wearable computers;accelerometer sensors;context awareness;model-based activity recognition;sensor-oriented recognition;user-independent activity recognition;wearable computing;Accelerometers;Humans;Hybrid power systems;Measurement units;Robustness;Sensor phenomena and characterization;Sensor systems;Wearable computers;Wearable sensors;Wrist}, 
doi={10.1109/ISWC.2009.32}, 
ISSN={1550-4816}, 
month={Sept},}
@INPROCEEDINGS{7907450, 
author={N. Sauwen and D. M. Sima and M. Acou and E. Achten and F. Maes and U. Himmelreich and S. V. Huffel}, 
booktitle={2016 12th International Conference on Signal-Image Technology Internet-Based Systems (SITIS)}, 
title={A Semi-Automated Segmentation Framework for MRI Based Brain Tumor Segmentation Using Regularized Nonnegative Matrix Factorization}, 
year={2016}, 
pages={88-95}, 
abstract={Segmentation plays an important role in the clinical management of brain tumors. Clinical practice would benefit from accurate and automated volumetric delineation of the tumor and its subcompartments. We present a semi-automated framework for brain tumor segmentation based on regularized nonnegative matrix factorization (NMF). L1-regularization is incorporated into the NMF objective function to promote spatial consistency and sparseness of the tissue abundance maps. The pathological sources are initialized through user-defined voxel selection. Knowledge about the spatial location of the selected voxels is combined with tissue adjacency constraints in a post-processing step to enhance segmentation quality. The method is applied to the BRATS 2013 Leaderboard dataset, consisting of publicly available multi-sequence MRI data of brain tumor patients. Our method performs well in comparison with state-of-the-art, in particular for the enhancing tumor region, for which we reach the highest Dice score among all participants.}, 
keywords={biomedical MRI;image enhancement;image segmentation;matrix decomposition;medical image processing;BRATS 2013 Leaderboard dataset;Dice score;MRI based brain tumor segmentation;NMF objective function;clinical management;magnetic resonance imaging;pathological sources;regularized nonnegative matrix factorization;semiautomated segmentation framework;tissue abundance maps;tumor region enhancement;user-defined voxel selection;Biomedical imaging;Brain modeling;Image segmentation;Magnetic resonance imaging;Pathology;Tumors;Magnetic resonance imaging;brain tumor;nonnegative matrix factorization;segmentation}, 
doi={10.1109/SITIS.2016.23}, 
month={Nov},}
@INPROCEEDINGS{6785736, 
author={K. Macropol and P. Bogdanov and A. K. Singh and L. Petzold and X. Yan}, 
booktitle={2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)}, 
title={I act, therefore I judge: Network sentiment dynamics based on user activity change}, 
year={2013}, 
pages={396-402}, 
abstract={The study of influence, persuasion, and user sentiment dynamics within online communities has recently emerged as a highly active area of research. In this paper, we focus on analyzing and modeling user sentiment dynamics within a real-world social media such as Twitter. Beyond text and connectivity, we are interested in exploring the level of topical user posting activity and its effect on sentiment change. We perform topic-wise analysis of tweeting behavior that reveals a strong relationship between users' activity acceleration and topic sentiment change. Inspired by this empirical observation, we develop a new generative and predictive model that extends classical neighborhood-based influence propagation with the notion of user activation. We fit the parameters of our model to a large, real-world Twitter dataset and evaluate its utility to predict future sentiment change. Our model outperforms significantly (1 order of magnitude in accuracy) existing alternatives in identifying the individuals who are most likely to change sentiment based on past information. When predicting the next sentiment of users who actually change their opinion (a relatively rare event), our model is twice more accurate than alternatives, while its overall network accuracy is 94% on average. We also study the effect of inactive users on consensus efficiency in the opinion dynamics process both analytically and in simulation within the context of our model.}, 
keywords={Internet;social networking (online);network sentiment dynamics;online communities;real-world Twitter dataset;real-world social media;sentiment dynamics;tweeting behavior;user activation;user activity change;Acceleration;Accuracy;Analytical models;Media;Predictive models;Twitter;Vectors}, 
doi={10.1145/2492517.2492623}, 
month={Aug},}
@INPROCEEDINGS{7745244, 
author={M. E. Cabrera and J. P. Wachs}, 
booktitle={2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)}, 
title={Embodied gesture learning from one-shot}, 
year={2016}, 
pages={1092-1097}, 
abstract={This paper discusses the problem of one shot gesture recognition. This is relevant to the field of human-robot interaction, where the user's intentions are indicated through spontaneous gesturing (one shot) to the robot. The novelty of this work consists of learning the process that leads to the creation of a gesture, rather on the gesture itself. In our case, the context involves the way in which humans produce the gestures - the kinematic and anthropometric characteristics and the users' proxemics (the use of the space around them). In the method presented, the strategy is to generate a dataset of realistic samples based on biomechanical features extracted from a single gesture sample. These features, called “the gist of a gesture”, are considered to represent what humans remember when seeing a gesture and the cognitive process involved when trying to replicate it. By adding meaningful variability to these features, a large training data set is created while preserving the fundamental structure of the original gesture. Having a large dataset of realistic samples enables training classifiers for future recognition. Three classifiers were trained and tested using a subset of ChaLearn dataset, resulting in all three classifiers showing rather similar performance around 80% recognition rate Our classification results show the feasibility and adaptability of the presented technique regardless of the classifier.}, 
keywords={gesture recognition;human-robot interaction;learning (artificial intelligence);pattern classification;ChaLearn dataset;anthropometric characteristics;biomechanical feature extraction;classifier training;cognitive process;gesture learning;human-robot interaction;kinematic characteristics;one shot gesture recognition;training data set;user proxemics;Feature extraction;Gesture recognition;Hidden Markov models;Production;Training;Trajectory;Visualization}, 
doi={10.1109/ROMAN.2016.7745244}, 
month={Aug},}
@INPROCEEDINGS{6341637, 
author={M. Li and Z. Hua and J. Zhao and Y. Zou and B. Xie}, 
booktitle={2012 IEEE 36th Annual Computer Software and Applications Conference Workshops}, 
title={Internet-Based Evaluation and Prediction of Web Services Trustworthiness}, 
year={2012}, 
pages={571-576}, 
abstract={As most Web services are delivered by third parties over unreliable Internet and are late bound at run-time, it is reasonable and useful to evaluate and predict the trustworthiness of Web services. In this paper, we propose a novel approach to evaluate and predict Web services trustworthiness using comprehensive trustworthy evidences collected from the Internet. First, we use an effective way to collect comprehensive trustworthy evidences from the Internet, which include both objective evidences (e.g. QoS) and subjective evidences (e.g. reputation). Second, Web services trustworthiness is evaluated with collected trustworthy evidences on a regular basis. Finally, the cumulative evaluation records are modeled as time series, and we propose a multi-step Web services trustworthiness prediction process, which can automatically and iteratively identify and optimize the model to fit the trustworthiness series data. Experiments conducted on a large-scale real-world dataset show that our method manages to collect comprehensive trustworthy evidences from the Internet and can effectively evaluate and predict the trustworthiness of Web services, which helps users to reuse Web services.}, 
keywords={Web services;iterative methods;service-oriented architecture;time series;trusted computing;Internet-based evaluation;comprehensive trustworthy evidences;iterative method;multistep Web services trustworthiness prediction process;objective evidences;service-oriented computing;subjective evidences;time series;trustworthiness series data;Data models;History;Predictive models;Quality of service;Time series analysis;Web services;ARIMA;QoS;Web services;evaluation;prediction;reputation;trustworthiness}, 
doi={10.1109/COMPSACW.2012.105}, 
month={July},}
@INPROCEEDINGS{7397412, 
author={S. Lu and M. Zhao and H. Zhang and C. Zhang and W. Wang and H. Wang}, 
booktitle={2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)}, 
title={GenderPredictor: A Method to Predict Gender of Customers from E-commerce Website}, 
year={2015}, 
volume={3}, 
pages={13-16}, 
abstract={While e-commerce has grown substantially over last several years, more and more people are utilizing this popular channel to purchase products and services. Thus the ability to predict user demographics, including gender, age and location has important applications in advertising, personalization, and recommendation. In this paper, we aim to automatically predict the users' genders based on their product viewing logs. Our study is based on a dataset from PAKDD'15 data mining competition. We propose an architecture for gender prediction, which consists of the "machine learning model" and the "label updating function". The experimental results show that our proposed method significantly outperform baseline methods. A detailed analysis of features provides an entertaining insight into behavior variation on female and male users.}, 
keywords={Internet;data mining;electronic commerce;gender issues;learning (artificial intelligence);GenderPredictor;PAKDD'15 data mining competition;e-commerce Website;gender prediction;label updating function;machine learning model;Context;Data mining;Feature extraction;Interpolation;Machine learning algorithms;Predictive models;Support vector machines;e-commerce;gender predict}, 
doi={10.1109/WI-IAT.2015.106}, 
month={Dec},}
@ARTICLE{7277026, 
author={M. Hu and Z. Wei and M. Shao and G. Zhang}, 
journal={IEEE Signal Processing Letters}, 
title={3-D Object Recognition via Aspect Graph Aware 3-D Object Representation}, 
year={2015}, 
volume={22}, 
number={12}, 
pages={2359-2363}, 
abstract={This letter addresses the problem of 3-D object recognition, whose aim is to recognize and estimate the pose of user-defined 3-D object when given an image. One difficult problem for 3-D object recognition is false correspondences between input image and 3-D model. To overcome this problem, we propose a novel aspect graph aware 3-D object representation method which enable us to output continuous pose and deal with self-occlusion problem. We also propose a two-stage 2-D to 3-D false correspondence filter based on proposed 3-D representation to achieve more consistent 2-D to 3-D matching pairs. We evaluate our proposed algorithm on Weizman Cars Viewpoint dataset and it demonstrates obvious improvement on localization and pose estimation accuracy compared with traditional methods. Besides, our proposed method accelerates computation time.}, 
keywords={graph theory;object recognition;pose estimation;2D-3D false correspondence filter;3D object recognition;Weizman Cars viewpoint dataset;aspect graph aware 3D object representation;computation time acceleration;pose estimation;pose recognition;Computational modeling;Estimation;Feature extraction;Noise measurement;Object recognition;Solid modeling;Three-dimensional displays;3-D representation;aspect graph;object recognition;pose estimation}, 
doi={10.1109/LSP.2015.2482489}, 
ISSN={1070-9908}, 
month={Dec},}
@INPROCEEDINGS{7264310, 
author={M. Pavan and S. Mizzaro and I. Scagnetto and A. Beggiato}, 
booktitle={2015 16th IEEE International Conference on Mobile Data Management}, 
title={Finding Important Locations: A Feature-Based Approach}, 
year={2015}, 
volume={1}, 
pages={110-115}, 
abstract={We propose a novel approach to address the problem of the recognition of important locations. Our method is organised in two phases: first, a set of candidate stay points is identified by exploiting some state-of-the-art algorithms to filter the GPS-logs, then, the candidate stay points are mapped onto a feature space having as dimensions the area underlying the stay point, its intensity (the time spent in a location) and its frequency (the number of total visits). We conjecture that the features space allows to model aspects/measures that are more semantically related to users and better suited to reason about their similarities and differences than, e.g., Latitude, longitude, and timestamp. An experimental evaluation on the GeoLife public dataset confirms the effectiveness of our approach.}, 
keywords={Global Positioning System;mobility management (mobile radio);GPS-logs;candidate stay points;feature space;public dataset;Acceleration;Accuracy;Geology;Global Positioning System;Heuristic algorithms;Time-frequency analysis;Trajectory;Places}, 
doi={10.1109/MDM.2015.11}, 
ISSN={1551-6245}, 
month={June},}
@INPROCEEDINGS{6977481, 
author={S. Bashbaghi and E. Granger and R. Sabourin and G. A. Bilodeau}, 
booktitle={2014 22nd International Conference on Pattern Recognition}, 
title={Watch-List Screening Using Ensembles Based on Multiple Face Representations}, 
year={2014}, 
pages={4489-4494}, 
abstract={Still-to-video face recognition (FR) is an important function in watch list screening, where faces captured over a network of video surveillance cameras are matched against reference stills of target individuals. Recognizing faces in a watch list is a challenging problem in semi - and unconstrained surveillance environments due to the lack of control over capture and operational conditions, and to the limited number of reference stills. This paper provides a performance baseline and guidelines for ensemble-based systems using a single high-quality reference still per individual, as found in many watch list screening applications. In particular, modular systems are considered, where an ensemble of template matchers based on multiple face representations is assigned to each individual of interest. During enrollment, multiple feature extraction (FE) techniques are applied to patches isolated in the reference still to generate diverse face-part representations that are robust to various nuisance factors (e.g., illumination and pose) encountered in video surveillance. The selection of relevant feature subsets, decision thresholds, and fusion functions of ensembles are achieved using faces of non-target individuals selected from reference videos (forming a universal background model). During operations, a face tracker gradually regroups faces captured from different people appearing in a scene, while each user-specific ensemble generates a decision per face capture. This leads to robust spatio-temporal FR when accumulated ensemble predictions surpass a detection threshold. Simulation results obtained with the Chokepoint video dataset show a significant improvement to accuracy, (1) when performing score-level fusion of matchers, where patches-based and FE techniques generate ensemble diversity, (2) when defining feature subsets and decision thresholds for each individual matcher of an ensemble using non-target videos, and (3) when accumulating positive detections over mul- iple frames.}, 
keywords={face recognition;feature extraction;image matching;image representation;video surveillance;chokepoint video dataset;decision thresholds;detection threshold;ensemble diversity;face tracker;feature extraction techniques;fusion functions;multiple FE techniques;multiple face representations;nontarget individuals;nuisance factors;patches-based techniques;performance baseline;reference videos;relevant feature subsets;robust spatiotemporal FR;semiconstrained surveillance environments;single high-quality reference;still-to-video face recognition;template matchers;unconstrained surveillance environments;universal background model;video surveillance cameras;watchlist screening applications;Cameras;Face;Feature extraction;Iron;Principal component analysis;Robustness;Video sequences}, 
doi={10.1109/ICPR.2014.768}, 
ISSN={1051-4651}, 
month={Aug},}
@INPROCEEDINGS{7523780, 
author={F. Stival and S. Michieletto and E. Pagello}, 
booktitle={2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)}, 
title={Online subject-independent modeling of sEMG signals for the motion of a single robot joint}, 
year={2016}, 
pages={1110-1116}, 
abstract={The interaction with robotic devices by means of physiological human signals has become of great interest in the last years because of the capability of catching human intention of movement and translate it in a coherent action performed by a robotic platform. Due to the complexity of EMG signals, several studies have been carried out about models built on a single subject (subject-specific). However, the execution of a certain task presents a common underlying behaviour, even if it is performed by different people. This common behaviour leads to some constraints that could be extracted by looking to different interpretations of the task, obtaining a subject-independent model. The few attempts in literature showed the possibility of creating a multiuser interface able to adapt to novel users (subject-independent). Nevertheless, the majority of the studies focused on classification problems, that are only able to determine the type of movement. We improved the state-of-the-art by introducing an online subject-independent framework able to compute the actual trajectory of the robot motion through a regression technique. The framework is based on a Gaussian Mixture Model (GMM) trained through Surface Electromyography (sEMG) signals coming from human subjects. Wavelet Transform has been used to elaborate the sEMG signals in real time. The goodness of the proposed framework has been tested with two different dataset involving various joints for both upper and lower limbs. The achieved results show that our framework could obtain high performances in both accuracy and computational time by reaching significant correlation (≥ 0.8). The whole procedure has been tested on two robots, a simulated hand and a humanoid, by remapping the human motion to the robotic platforms in order to verify the proper execution of the original movement.}, 
keywords={Gaussian processes;electromyography;medical robotics;medical signal processing;mixture models;motion control;regression analysis;trajectory control;wavelet transforms;GMM;Gaussian mixture model;human motion remapping;humanoid robot;multiuser interface;online subject-independent modeling;physiological human signals;regression technique;robot motion trajectory;robotic devices;robotic platform;sEMG signals;simulated hand robot;single robot joint motion;surface electromyography;wavelet transform;Adaptation models;Data models;Electromyography;Robots;Training;Wavelet transforms}, 
doi={10.1109/BIOROB.2016.7523780}, 
month={June},}
@INPROCEEDINGS{7544998, 
author={T. Y. Pan and L. Y. Lo and C. W. Yeh and J. W. Li and H. T. Liu and M. C. Hu}, 
booktitle={2016 IEEE Second International Conference on Multimedia Big Data (BigMM)}, 
title={Real-Time Sign Language Recognition in Complex Background Scene Based on a Hierarchical Clustering Classification Method}, 
year={2016}, 
pages={64-67}, 
abstract={Cameras are embedded in many mobile/wearable devices and can be used for gesture recognition or even sign language recognition to help the deaf people communicate with others. In this paper, we proposed a vision-based gesture recognition system which can be used in environments with complex background. We design a method to adaptively update the skin color model for different users and various lighting conditions. Three kinds of features are combined to describe the contours and the salient points of hand gestures. Principle Component Analysis (PCA), Linear Discriminant Analysis (LDA), and Support Vector Machine (SVM) are integrated to construct a novel hierarchical classification scheme. We evaluated the proposed recognition method on two datasets: (1) the CSL dataset collected by ourselves, in which images were captured in complex background. (2) The public ASL dataset, in which images of the same gesture were captured in different lighting conditions. Our method achieves the accuracies of 99.8% and 94%, respectively, which outperforms the existing works.}, 
keywords={cameras;handicapped aids;image classification;image colour analysis;pattern clustering;principal component analysis;sign language recognition;support vector machines;ASL dataset;CSL dataset;LDA;PCA;SVM;cameras;complex background scene;deaf people;hierarchical classification scheme;hierarchical clustering classification method;lighting conditions;linear discriminant analysis;mobile devices;principle component analysis;real-time sign language recognition;skin color model;support vector machine;vision-based gesture recognition system;wearable devices;Assistive technology;Feature extraction;Gesture recognition;Image color analysis;Sensors;Skin;Training;LDA;SVM;gesture recognition;hierarchical clustering;sign language}, 
doi={10.1109/BigMM.2016.44}, 
month={April},}
@INPROCEEDINGS{7870887, 
author={L. A. M. C. Carvalho and K. Belhajjame and C. B. Medeiros}, 
booktitle={2016 IEEE 12th International Conference on e-Science (e-Science)}, 
title={Converting scripts into reproducible workflow research objects}, 
year={2016}, 
pages={71-80}, 
abstract={Scientific discovery and analysis are increasingly computational and data-driven. While scripting languages, such as Python, R and Perl, are the means of choice of the majority of scientists to encode and run their data analysis, scripts are generally not amenable to reuse or reproducibility. Scripts do rarely get reused or even shared with third party scientists. We argue in this paper that the reproducibility of scripts can be promoted by converting them into workflow research objects. A workflow research object encodes a script into a production (executable) workflow that is accompanied by annotations, example datasets and provenance traces of their execution, thereby allowing third party users to understand the data analysis encoded by the original script, run the associated workflow using the same or different dataset, or even repurpose it for a different analysis. To this end, we present a methodology for converting scripts into workflow research objects in a principled manner, guided by requirements that we elicited for this purpose. The methodology exploits tools and standards that have been developed by the community, in particular YesWorkflow, Research Objects and the W3C PROV. It is showcased using a real world use case from the field of Molecular Dynamics.}, 
keywords={authoring languages;formal specification;natural sciences computing;standards;W3C PROV;YesWorkflow;molecular dynamics;requirements;scripting languages;scripts;standards;workflow research objects;Biological system modeling;Computational modeling;Data analysis;Data models;Electronic mail;Proteins;Trajectory}, 
doi={10.1109/eScience.2016.7870887}, 
month={Oct},}
@INPROCEEDINGS{7878251, 
author={X. Xia and Z. Xu and Z. Liu}, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={Towards Interactive Gathering of DUV-Based Dataset Usage Information}, 
year={2016}, 
pages={163-168}, 
abstract={Although many open data portals have been publishing numerous datasets on the Web, there is currently no clear standard way to describe dataset usage on the Web, which is not conducive to the healthy development of the open data ecosystem. The W3C Data on the Web Best Practices Working Group is therefore standardizing the Dataset Usage Vocabulary (DUV) for modeling, conveying and sharing dataset usage information on the Web, aiming at facilitating better communication between data publishers and consumers and promoting the re-use of Web-published data. Despite the significant progress in the standardization, there is a lack of systematic research on approaches and tools for gathering DUV-based dataset usage information. This paper therefore proposes a generic software Framework for Interactively Gathering DUV-based Dataset Usage Information (FIGDUI) on the Web. FIGDUI can provide both data publishers and consumers (collectively referred to as the user) with a friendly user interface designed according to DUV's Citation Model, Usage Model and Feedback Model, and employ automatic algorithms to convert the user-entered data into dataset usage data in a machine-readable RDF format and to publish the RDF data as Linked Open Data on the Web. Our prototype implementation of FIGDUI and preliminary experimental results show that FIGDUI is feasible and implementable.}, 
keywords={Linked Data;interactive systems;portals;user interfaces;vocabulary;DUV citation model;DUV feedback model;DUV usage model;DUV-based dataset usage information;FIGDUI;Linked Open Data;W3C Data;Web-published data reuse;dataset usage vocabulary;friendly user interface;interactive gathering;machine-readable RDF format;open data ecosystem;open data portals;Data models;Ontologies;Portals;Prototypes;Resource description framework;Software;User interfaces;Dataset Usage Vocabulary (DUV);Linked Open Data (LOD);dataset usage information;information gathering;user interface design}, 
doi={10.1109/WISA.2016.41}, 
month={Sept},}
@INPROCEEDINGS{7753403, 
author={S. Karampatakis and C. Bratsas and I. Antoniou}, 
booktitle={2016 11th International Workshop on Semantic and Social Media Adaptation and Personalization (SMAP)}, 
title={Library Linked Data: The case of Public Library of Veroia}, 
year={2016}, 
pages={161-164}, 
abstract={We describe the Public Library of Veroia dataset. This dataset was created by transforming the bibliographic records of the Public Library of Veroia catalogue into Linked Open Data. We present the data model that is used, consisting of a mix of well-established vocabularies such as BIBO, RDA, DC, FOAF between others. We developed a new tool for the transformation process which implies simple configuration for the end user and rich data extraction. Silk Link Discovery Framework was used to make connections with other datasets like DBpedia. Finally, we developed a new service for author profiling using enriched information gathered from the Linked Open Data Cloud.}, 
keywords={bibliographic systems;cloud computing;public libraries;Veroia;bibliographic records;library linked data;linked open data cloud;public library;Cultural differences;Data models;Iris;Libraries;Ontologies;Publishing;Resource description framework}, 
doi={10.1109/SMAP.2016.7753403}, 
month={Oct},}
@INPROCEEDINGS{5693860, 
author={L. L. Presti and M. Morana and M. L. Cascia}, 
booktitle={2010 IEEE International Symposium on Multimedia}, 
title={A Data Association Algorithm for People Re-identification in Photo Sequences}, 
year={2010}, 
pages={318-323}, 
abstract={In this paper, a new system is presented to support the user in the face annotation task. Every time a photo sequence becomes available, the system analyses it to detect and cluster faces in set corresponding to the same person. We propose to model the problem of people re-identification in photos as a data association problem. In this way, the system takes advantage from the assumption that each person can appear at most once in each photo. We propose a fully automated method for grouping facial images, the method does not require any initialization neither a priori knowledge of the number of persons that are in the photo sequence. We compare the results obtained with our method and with standard clustering methods on three personal collections and on a publicly available dataset.}, 
keywords={face recognition;image sequences;sensor fusion;data association algorithm;face annotation;facial images;people reidentification;photo sequences;Data Association;Image databases;Photo Album Management;Re-Identification}, 
doi={10.1109/ISM.2010.55}, 
month={Dec},}
@INPROCEEDINGS{5284947, 
author={X. Chang and Q. Zheng}, 
booktitle={2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology}, 
title={Preference Learning to Rank with Sparse Bayesian}, 
year={2009}, 
volume={3}, 
pages={143-146}, 
abstract={In this paper, we propose a sparse Bayesian approach to learn ranking function from labeled data. The ranking function can be used to define an ordering among documents according to their degree of relevance to the user query. This ranking function is more efficient and accurate than the function leaned by proposed approaches. Experimental results on document retrieval dataset show that the generalization performance of it is competitive with SVM-based ranking method and Gaussian process based method.}, 
keywords={Bayesian methods;Conferences;Gaussian processes;Information retrieval;Intelligent agent;Kernel;Machine learning;Predictive models;Q measurement;Support vector machines;Sparse bayesian;information retrieval;learning to rank}, 
doi={10.1109/WI-IAT.2009.367}, 
month={Sept},}
@INPROCEEDINGS{5170489, 
author={A. H. Chen and C. H. Lin and G. T. Chen and J. C. Hsieh}, 
booktitle={2009 WRI World Congress on Computer Science and Information Engineering}, 
title={GNAnalyzer: A Novel System for Analyzing Gene Networks from Microarray Data with Bayesian Networks}, 
year={2009}, 
volume={5}, 
pages={22-27}, 
abstract={Since the development of biotechnologies such as array-based hybridization, massive amounts of gene expression profiles are quickly accumulating. How to utilize these huge amounts of data has become a major challenge in the post-genomic research era. One approach utilizes a Bayesian network, a graphical model that has been applied toward inferring genetic regulatory networks from microarray experiments. However, a user-friendly system that can display and analyze various gene networks from microarray experimental datasets is now needed. In this paper, we have developed a novel system to construct and analyze various gene networks from microarray datasets. Three aspects characterize the major contributions of this paper. (1) Five Bayesian network algorithm codes were developed and written to construct gene networks of the yeast cell cycle using the information from four different microarray datasets. (2) A gene network analyzing system, GNAnalyzer, consisting of several user-friendly interfaces was implemented. GNAnalyzer is capable of running Bayesian algorithms, constructing gene networks, and analyzing the performance of each network algorithm simultaneously. (3) The system utilizes both the powerful processing ability of MatLab and the dynamic interfaces of LabVIEW in a single platform. This is the first time of this kind of design to be applied in bioinformatics. The system is designed to be extendible. Our next goal is to apply this technique to other real biomedical applications, such as human cancer classification and prognostic prediction.}, 
keywords={Bayes methods;belief networks;biology computing;genetics;human computer interaction;network theory (graphs);user interfaces;Bayesian network algorithm;GNAnalyzer;LabView;Matlab;biotechnology development;gene expression profile;gene regulatory network analysis;graphical model;microarray data;microarray dataset;microarray experiment;user-friendly system;Algorithm design and analysis;Bayesian methods;Biotechnology;Displays;Fungi;Gene expression;Genetics;Graphical models;Information analysis;Performance analysis}, 
doi={10.1109/CSIE.2009.515}, 
month={March},}
@INPROCEEDINGS{5687104, 
author={C. Y. Chang and T. H. Wu}, 
booktitle={2010 10th International Conference on Intelligent Systems Design and Applications}, 
title={Using gait information for gender recognition}, 
year={2010}, 
pages={1388-1393}, 
abstract={Gender recognition is a hot research topic in recent years. Human-machine interfaces or video surveillance can be greatly improved if human gender can be recognized automatically. In this study, an embedded hidden Markov model is used for gender recognition. Video, which is recorded in different angles of view, is utilized to sample properties of each gender. Ten consecutive gait frames are segmented and organized as a composite image, which is used to establish EHMM. For video in each angle of view, two EHMMs are built and trained. The gender of the subject of a testing composite image is decided by the EHMM whose likelihood is most similar to the testing EHMM. We test the proposed approach using the CASIA Gait Database (Dataset B) in this study. Experimental results show that the proposed system can identify the gender of human accurately.}, 
keywords={hidden Markov models;image recognition;image segmentation;user interfaces;video surveillance;CASIA gait database;HMM;composite image segmentation;embedded hidden Markov model;gait information;gender recognition;human-machine interfaces;video surveillance;Gait;embedded hidden Markov model}, 
doi={10.1109/ISDA.2010.5687104}, 
ISSN={2164-7143}, 
month={Nov},}
@INPROCEEDINGS{7782536, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={Towards Interactive Gathering of DUV-Based Dataset Usage Information}, 
year={2016}, 
pages={163-168}, 
abstract={Although many open data portals have been publishing numerous datasets on the Web, there is currently no clear standard way to describe dataset usage on the Web, which is not conducive to the healthy development of the open data ecosystem. The W3C Data on the Web Best Practices Working Group is therefore standardizing the Dataset Usage Vocabulary (DUV) for modeling, conveying and sharing dataset usage information on the Web, aiming at facilitating better communication between data publishers and consumers and promoting the re-use of Web-published data. Despite the significant progress in the standardization, there is a lack of systematic research on approaches and tools for gathering DUV-based dataset usage information. This paper therefore proposes a generic software Framework for Interactively Gathering DUV-based Dataset Usage Information (FIGDUI) on the Web. FIGDUI can provide both data publishers and consumers (collectively referred to as the user) with a friendly user interface designed according to DUV's Citation Model, Usage Model and Feedback Model, and employ automatic algorithms to convert the user-entered data into dataset usage data in a machine-readable RDF format and to publish the RDF data as Linked Open Data on the Web. Our prototype implementation of FIGDUI and preliminary experimental results show that FIGDUI is feasible and implementable.}, 
keywords={Dataset Usage Vocabulary (DUV);Linked Open Data (LOD);dataset usage information;information gathering;user interface design}, 
doi={10.1109/WISA.2016.12}, 
month={Sept},}
@INPROCEEDINGS{5569236, 
author={G. Xu and Weimin Zheng and Haiping Wu and Yujiu Yang}, 
booktitle={2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery}, 
title={Combining topic models and string kernel for deep web categorization}, 
year={2010}, 
volume={6}, 
pages={2791-2795}, 
abstract={Online databases maintain a collection of structured domain-specific documents dynamically generated in response to users' queries instead of being accessed by static URLs. Categorizing deep webs according to their object domains is a critical step to integrate such sources. While existing methods focus on supervised or post-query methodologies, we propose a more practical pre-query algorithm operating in an unsupervised manner. Given the domain number, our two phase approach firstly investigates the hidden domain distribution for each query form using topic models and each query form's object domain can be identified preliminarily. In this phase, we construct our training set composing the query forms deemed to have already been categorized correctly, and beside, the deep webs needed to be reclassified are also selected in this phase. In the second phase, we train a classifier with String Kernel methods to reclassify the uncertain deep webs to improve the overall performance. The advantage of our algorithm over previous ones is that we capture the semantic structure for each query form. Based on the two phase architecture, our framework works in an unsupervised manner and achieves satisfactory results. Experiments on the TEL-8 dataset from the UIUC Web integration repository1 show the effectiveness and efficiency of our algorithm.}, 
keywords={Internet;document handling;query processing;TEL-8 dataset;UIUC Web integration repository;deep Web categorization;online databases;post query methodologies;pre query algorithm;static URL;string kernel;structured domain specific documents;topic models;training set;Atmospheric modeling;Books;Databases;Frequency modulation;Kernel;Semantics;Training}, 
doi={10.1109/FSKD.2010.5569236}, 
month={Aug},}
@INPROCEEDINGS{885071, 
author={Y. Motomura and K. Yoshida and K. Fujimoto}, 
booktitle={Systems, Man, and Cybernetics, 2000 IEEE International Conference on}, 
title={Generative user models for adaptive information retrieval}, 
year={2000}, 
volume={1}, 
pages={665-670 vol.1}, 
abstract={For information retrieval (IR) tasks, user models are used to estimate user's true intention and demand. Unfortunately, most user models are constructed in a specialized form that is not applied to other systems or domains. This specialization makes it difficult to share user models as common resources for developing information retrieval systems and for researching cognitive characteristics in various users. In order to solve this problem, we need a general user modeling method. A user model based on a probabilistic framework is proposed. We call this model a generative user model. The generative user model represents user's mental depth by latent (hidden) variables. It also has visible variables that mean word set and qualifier of each word as a subjective probability distribution. The model can handle uncertainty of the user's subjectivity by a probabilistic framework. Recent statistical studies for such latent models give a learning algorithm. Our generative user model can be constructed from a dataset taken by information retrieval tasks. As an example, we also introduce two different kinds of information retrieval systems, ART MUSEUM (Multimedia Database with Sense of Color and Construction upon the Matter of ART) and DSIU (Decision Support for Internet Users). The generative user model is applied to these systems. The properties of the model and interactive learning mechanism are shown}, 
keywords={information retrieval;information retrieval systems;learning (artificial intelligence);multimedia databases;probability;user modelling;ART MUSEUM;DSIU;Internet;adaptive information retrieval;cognitive characteristics;decision support system;generative user models;information retrieval systems;latent models;learning algorithm;mental depth;multimedia database;probabilistic framework;user intention;user modeling method;Communications technology;Information retrieval;Internet;Learning systems;Mechanical factors;Multimedia databases;Power cables;Probability distribution;Subspace constraints;Uncertainty}, 
doi={10.1109/ICSMC.2000.885071}, 
ISSN={1062-922X}, 
month={},}
@INPROCEEDINGS{5670038, 
author={T. Gong and S. Li and C. L. Tan}, 
booktitle={2010 22nd IEEE International Conference on Tools with Artificial Intelligence}, 
title={A Semantic Similarity Language Model to Improve Automatic Image Annotation}, 
year={2010}, 
volume={1}, 
pages={197-203}, 
abstract={In recent years, with the rapid proliferation of digital images, the need to search and retrieve the images accurately, efficiently, and conveniently is becoming more acute. Automatic image annotation with image semantic content has attracted increasing attention, as it is the preprocess of annotation based image retrieval which provides users accurate, efficient, and convenient image retrieval with image understanding. Different machine learning approaches have been used to tackle the problem of automatic image annotation; however, most of them focused on exploring the relationship between images and annotation words and neglected the relationship among the annotation words. In this paper, we propose a framework of using language models to represent the word-to-word relation and thus to improve the performance of existing image annotation approaches utilizing probabilistic models. We also propose a specific language model - the semantic similarity language model to estimate the semantic similarity among the annotation words so that annotations that are more semantically coherent will have higher probability to be chosen to annotate the image. To illustrate the general idea of using language model to improve current image annotation systems, we added the language model on top of the two specific image annotation models - the translation model (TM) and the cross media relevance model (CMRM). We tested the improved models on a widely used image annotation corpus - the Corel 5K dataset. Our results show that by adding the semantic similarity language model, the performance of image annotation improves significantly in comparison with the original models. Our proposed language model can also be applied to other image annotation approaches using word probability conditioned on image or word-image joint probability as well.}, 
keywords={content-based retrieval;image retrieval;learning (artificial intelligence);probability;Corel 5K image annotation corpus;annotation based image retrieval;automatic image annotation;cross media relevance model;machine learning approach;probabilistic models;semantic similarity language model;translation model;word-image joint probability;Context;Equations;Hidden Markov models;Joints;Mathematical model;Semantics;Training}, 
doi={10.1109/ICTAI.2010.35}, 
ISSN={1082-3409}, 
month={Oct},}
@INPROCEEDINGS{7877398, 
author={P. Kainthura and S. Gupta}, 
booktitle={2016 2nd International Conference on Next Generation Computing Technologies (NGCT)}, 
title={Query driven spatial pattern analysis and visualization through GIS}, 
year={2016}, 
pages={103-105}, 
abstract={Earth features are arranged following one or many rules called spatial patterns. Around the world many features share common patterns. To discover those patterns and analyzing them through some data visualization technique can lead to a good decision supporting system. In this paper analysis and visualization model is designed for the Uttarakhand (India) region. Data like district wise population, schools and hospitals is collected from different sources. To store GIS data PostgreSQL and PostGIS is providing a favorable support and powerful database. GIS maps are created using tool QGIS (Open source) for the visualization purpose. To classify common pattern from the given dataset, clusters are formed from population field. To cluster data OPTICS clustering method is used. Then decision tree by information gain method is used for the supervised learning. To display the result open source software Geoserver is used. Geoserver handle the user queries and display the result on the dynamic maps.}, 
keywords={SQL;data visualisation;decision support systems;decision trees;geographic information systems;learning (artificial intelligence);pattern clustering;public domain software;query processing;Earth features;GIS maps;Geoserver open source software;India;OPTICS clustering method;PostGIS;PostgreSQL;QGIS tool;Uttarakhand region;data visualization;decision supporting system;decision tree;dynamic maps;information gain method;query driven spatial pattern analysis;query driven spatial pattern visualization;supervised learning;Data visualization;Decision trees;Hospitals;Optics;Sociology;Spatial databases;Statistics;Decision Supporting system;Geoserver;OPTICS;PostGIS;QGIS;Spatial data;Spatial patterns;Visualization;supervised learning}, 
doi={10.1109/NGCT.2016.7877398}, 
month={Oct},}
@INPROCEEDINGS{6460545, 
author={B. Ghanem and M. Kreidieh and M. Farra and T. Zhang}, 
booktitle={Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)}, 
title={Context-aware learning for automatic sports highlight recognition}, 
year={2012}, 
pages={1977-1980}, 
abstract={Video highlight recognition is the procedure in which a long video sequence is summarized into a shorter video clip that depicts the most “salient” parts of the sequence. It is an important technique for content delivery systems and search systems which create multimedia content tailored to their users' needs. This paper deals specifically with capturing highlights inherent to sports videos, especially for American football. Our proposed system exploits the multimodal nature of sports videos (i.e. visual, audio, and text cues) to detect the most important segments among them. The optimal combination of these cues is learned in a data-driven fashion using user preferences (expert input) as ground truth. Unlike most highlight recognition systems in the literature that define a highlight to be salient only in its own right (globally salient), we also consider the context of each video segment w.r.t. the video sequence it belongs to (locally salient). To validate our method, we compile a large dataset of broadcast American football videos, acquire their ground truth highlights, and evaluate the performance of our learning approach.}, 
keywords={digital video broadcasting;image sequences;learning (artificial intelligence);sport;video retrieval;video streaming;video surveillance;American football;automatic sport video highlight recognition;content delivery system;content search system;context aware learning;multimedia content;user preferences;video broadcasting;video clip;video segment;video sequence;Clocks;Context;Feature extraction;Games;Hidden Markov models;Training;Visualization}, 
ISSN={1051-4651}, 
month={Nov},}
@INPROCEEDINGS{7498518, 
author={S. h. Zhong and Y. Liu and K. A. Hua and S. Wu}, 
booktitle={2015 International Conference on Orange Technologies (ICOT)}, 
title={Is noise always harmful? Visual learning from weakly-related data}, 
year={2015}, 
pages={181-184}, 
abstract={Noise exists universally in multimedia data, especially in Internet era. For example, tags from web users are often incomplete, arbitrary, and low relevant with the visual information. Intuitively, noise in the dataset is harmful to learning tasks, which implies that huge volumes of image tags from social media can't be utilized directly. To collect the reliable training dataset, labor-intensive manual labeling and various learning based outlier detection techniques are widely used. This paper intends to discuss whether such kind of preprocessing is always needed. We focus on a very normal case in image classification that the available dataset includes a large amount of images weakly related to any target classes. We use deep models as the platform and design a series of experiments to compare the semi-supervised learning performance with/without weakly related unlabeled data. Fortunately, we validate that weakly related data is not always harmful, which is an encouraging finding for research on web image learning.}, 
keywords={Data models;Erbium;Machine learning;Multimedia communication;Standards;Training;Training data;Weakly-related data;deep learning;semi-supervised learning}, 
doi={10.1109/ICOT.2015.7498518}, 
month={Dec},}
@INPROCEEDINGS{4354141, 
author={L. Weng and U. Catalyurek and T. Kurc and G. Agrawal and J. Saltz}, 
booktitle={2007 8th IEEE/ACM International Conference on Grid Computing}, 
title={Optimizing multiple queries on scientific datasets with partial replicas}, 
year={2007}, 
pages={259-266}, 
abstract={We propose strategies to efficiently execute a query workload, which consists of multiple related queries submitted against a scientific dataset, on a distributed-memory system in the presence of partial dataset replicas. Partial replication re-organizes and re-distributes one or more subsets of a dataset across the storage system to reduce I/O overheads and increase I/O parallelism. Our work targets a class of queries, called range queries, in which the query predicate specifies lower and upper bounds on the values of all or a subset of attributes of a dataset. Data elements whose attribute values fall into the specified bounds are retrieved from the dataset. If we think of the attributes of a dataset forming multi-dimensional space, where each attribute corresponds to one of the dimensions, a range query defines a bounding box in this multidimensional space. We evaluate our strategies in two scenarios involving range queries. The first scenario represents the case in which queries have overlapping regions of interest, such as those arising from an exploratory analysis of the dataset by multiple users. In the second scenario, queries represent adjacent rectilinear sections that capture an irregular subregion in the multi-dimensional space. This scenario corresponds to a case where the user wants to query and retrieve a spatial feature from the dataset. We propose cost models and an algorithm for optimizing such queries. Our results using queries for subsetting and analysis of medical image datasets show that effective use of partial replicas can result in reduction in query execution times.}, 
keywords={distributed memory systems;natural sciences computing;query processing;distributed-memory system;multiple query optimization;partial dataset replicas;query workload;scientific datasets;Application software;Biomedical engineering;Biomedical informatics;Computer science;Cost function;Data analysis;Data engineering;Hurricanes;Information retrieval;Parallel processing}, 
doi={10.1109/GRID.2007.4354141}, 
ISSN={2152-1085}, 
month={Sept},}
@INPROCEEDINGS{6722014, 
author={Z. Zhang and S. Poslad}, 
booktitle={2013 IEEE International Conference on Systems, Man, and Cybernetics}, 
title={A New Post Correction Algorithm (PoCoA) for Improved Transportation Mode Recognition}, 
year={2013}, 
pages={1512-1518}, 
abstract={Transportation mode plays an important role in enabling us to derive a mobile user's context, and to adapt intelligent services to this. However, current methods have two key limitations: a low recognition accuracy and coarse-grained recognition capability. In this paper, we propose a new Post Correction Algorithm (PoCoA) that is applied after the use of typical classifiers to address these limitations. We evaluated the use of PoCoA for the following transportation modes, walking, cycling, bus passenger, metro passenger, car passenger, and car driver. PoCoA enhances a typical accelerometer-based transportation recognition method with a more accurate sub-classification of motorized transportation modes when tested on a dataset obtained from 15 individuals. Overall accuracy improved from 69% to 88% when comparing with a state of the art two-stage classifier (Decision Tree + Discrete Hidden Markov Model).}, 
keywords={decision trees;hidden Markov models;intelligent transportation systems;pattern classification;PoCoA;accelerometer-based transportation recognition method;bus passenger;car driver;car passenger;classifiers;coarse-grained recognition capability;cycling;decision tree;discrete hidden Markov model;intelligent services;metro passenger;mobile user context;motorized transportation modes;post correction algorithm;recognition accuracy;subclassification;transportation mode recognition;two-stage classifier;walking;Acceleration;Accelerometers;Accuracy;Global Positioning System;Legged locomotion;Mobile handsets;Transportation;Accelerometer;Hidden Markov Model (HMM);Post Correction Algorithm;Transportation mode recognition}, 
doi={10.1109/SMC.2013.261}, 
ISSN={1062-922X}, 
month={Oct},}
@INPROCEEDINGS{6019754, 
author={R. K. Ayyasamy and B. Tahayna and S. M. Alhashmi and S. Eu-Gene}, 
booktitle={2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)}, 
title={Concept based modeling approach for blog classification using fuzzy similarity}, 
year={2011}, 
volume={2}, 
pages={1007-1011}, 
abstract={As information technology is developing in a faster pace, there is a steep increase in social networking where the user can share their knowledge, views, criticism through various ways such as blogging, facebook, microblogging, news, forums, etc. Among these various ways, blogs play a different role as it is a personal site for each user, and blogger writes lengthy posts on various topics. Several research works are carried out, to classify blogs based on machine learning techniques. In this paper, we describe a method for classifying blog posts automatically using fuzzy similarity. We perform, experiments using TREC dataset and applied our approach to six different fuzzy similarity measures. Experimental results proved that Einstein fuzzy similarity measures performs better than the other measures.}, 
keywords={Internet;fuzzy reasoning;learning (artificial intelligence);pattern classification;social networking (online);Einstein fuzzy similarity;TREC dataset;blog classification;concept based modeling approach;fuzzy similarity;machine learning techniques;social networking;Blogs;Electronic publishing;Encyclopedias;Internet;Machine learning;Text categorization;blog classification;fuzzy similarity;wikipedia}, 
doi={10.1109/FSKD.2011.6019754}, 
month={July},}
@INPROCEEDINGS{7829766, 
author={P. M. Henriques and J. Mendes-Moreira}, 
booktitle={2016 Eleventh International Conference on Digital Information Management (ICDIM)}, 
title={Combining recommendation systems with a dynamic weighted technique}, 
year={2016}, 
pages={203-208}, 
abstract={Recommender systems represent user preferences for items that the user might be interested to view or purchase. These systems have become extremely common in electronic commerce, providing relevant suggestions and directing users towards those items that best meet their needs and preferences. Different techniques have been analysed including content-based, collaborative and hybrid approaches. The last one is used to improve performance prediction combining different recommender systems using the best features of each method, smoothing problems as cold-start. We evaluate our ensemble method using MovieLens dataset with promising results.}, 
keywords={content management;electronic commerce;groupware;recommender systems;user interfaces;MovieLens dataset;collaborative approach;content-based approach;dynamic weighted technique;electronic commerce;recommendation systems;user preferences;Collaboration;History;Motion pictures;Prediction algorithms;Predictive models;Recommender systems;Stacking}, 
doi={10.1109/ICDIM.2016.7829766}, 
month={Sept},}
@INPROCEEDINGS{6928911, 
author={P. He and J. Zhu and Z. Zheng and J. Xu and M. R. Lyu}, 
booktitle={2014 IEEE International Conference on Web Services}, 
title={Location-Based Hierarchical Matrix Factorization for Web Service Recommendation}, 
year={2014}, 
pages={297-304}, 
abstract={Web service recommendation is of great importance when users face a large number of functionally-equivalent candidate services. To recommend Web services that best fit a user's need, QoS values which characterize the non-functional properties of those candidate services are in demand. But in reality, the QoS information of Web service is not easy to obtain, because only limited historical invocation records exist. To tackle this challenge, in recent literature, a number of QoS prediction methods are proposed, but they still demonstrate disadvantages on prediction accuracy. In this paper, we design a location-based hierarchical matrix factorization (HMF) method to perform personalized QoS prediction, whereby effective service recommendation can be made. We cluster users and services into several user-service groups based on their location information, each of which contains a small set of users and services. To better characterize the QoS data, our HMF model is trained in a hierarchical way by using the global QoS matrix as well as several location-based local QoS matrices generated from user-service clusters. Then the missing QoS values can be predicted by compactly combining the results from local matrix factorization and global matrix factorization. Comprehensive experiments are conducted on a real-world Web service QoS dataset with 1,974,675 real Web service invocation records. The experimental results show that our HMF method achieves higher prediction accuracy than the state-of-the-art methods.}, 
keywords={Web services;matrix decomposition;quality of service;recommender systems;HMF;QoS information;QoS prediction methods;QoS values;Web service recommendation;historical invocation records;location-based hierarchical matrix factorization;personalized QoS prediction;quality of service;user-service groups;Accuracy;Predictive models;Quality of service;Sparse matrices;Time factors;Vectors;Web services;QoS prediction;Web service;clustering;location}, 
doi={10.1109/ICWS.2014.51}, 
month={June},}
@ARTICLE{5721820, 
author={L. Zhang and Q. Ji}, 
journal={IEEE Transactions on Image Processing}, 
title={A Bayesian Network Model for Automatic and Interactive Image Segmentation}, 
year={2011}, 
volume={20}, 
number={9}, 
pages={2582-2593}, 
abstract={We propose a new Bayesian network (BN) model for both automatic and interactive image segmentation. A multilayer BN is constructed from an oversegmentation to model the statistical dependencies among superpixel regions, edge segments, vertices, and their measurements. The BN also incorporates various local constraints to further restrain the relationships among these image entities. Given the BN model and various image measurements, belief propagation is performed to update the probability of each node. Image segmentation is generated by the most probable explanation inference of the true states of both region and edge nodes from the updated BN. Besides the automatic image segmentation, the proposed model can also be used for interactive image segmentation. While existing interactive segmentation (IS) approaches often passively depend on the user to provide exact intervention, we propose a new active input selection approach to provide suggestions for the user's intervention. Such intervention can be conveniently incorporated into the BN model to perform actively IS. We evaluate the proposed model on both the Weizmann dataset and VOC2006 cow images. The results demonstrate that the BN model can be used for automatic segmentation, and more importantly, for actively IS. The experiments also show that the IS with active input selection can improve both the overall segmentation accuracy and efficiency over the IS with passive intervention.}, 
keywords={belief networks;image segmentation;interactive systems;probability;BN model;Bayesian network model;VOC2006 cow image;Weizmann dataset;actively IS;belief propagation;edge segment;image measurement;input selection approach;interactive image segmentation;statistical dependency;superpixel region;Image edge detection;Image segmentation;Labeling;Pixel;Shape;Uncertainty;Active labeling;Bayesian network (BN);image segmentation;interactive image segmentation}, 
doi={10.1109/TIP.2011.2121080}, 
ISSN={1057-7149}, 
month={Sept},}
@INPROCEEDINGS{6835627, 
author={X. Wang and S. Li and X. Zou and R. Chen and B. Zhou}, 
booktitle={2013 International Conference on Computer Sciences and Applications}, 
title={An Automatic Tag Recommendation Algorithm for Micro-blogging Users}, 
year={2013}, 
pages={398-401}, 
abstract={Online social networks such as Sina Weibo micro-blogging website allow user to annotate himself (or herself) using tags, which describe the characteristics of the user. Many researches have been done on photos, films, commodities but rare researches on users. In this paper, we try to recommend tags for users who can communicate directly with other users. Our method is based on interactive relations between users and is low cost. We present and evaluate tag recommendation method to support the user self-annotation task by recommending a set of tags for users. Experiment evaluations on real and large Sina Weibo dataset which contains more than 140 million users and distributed processing framework Hadoop shows that our method can effectively recommend relevant tags to users.}, 
keywords={information retrieval;social networking (online);Sina Weibo microblogging Web site;automatic tag recommendation algorithm;distributed processing framework Hadoop;online social network;user self-annotation task;Collaboration;Computers;Equations;Mathematical model;Social network services;Tagging;Web sites;Micro-blogging;Social Network;Tag Recommendation;User Tags}, 
doi={10.1109/CSA.2013.100}, 
month={Dec},}
@ARTICLE{7510475, 
author={L. Wang and Y. Liang and W. Cai and B. Zou}, 
journal={Chinese Journal of Electronics}, 
title={Failure Detection and Correction for Appearance Based Facial Tracking}, 
year={2015}, 
volume={24}, 
number={1}, 
pages={20-25}, 
abstract={The appearance based facial tracking methods, such as active appearance models and candide models, are widely used in intelligent user interface and facial expression recognition. This paper proposes a novel method to detect and correct the failures in appearance based facial tracking. A sparse coding strategy is applied to learn an efficient feature representation for the difference between the warped image and the face template. The features are extracted by directly project the difference image to the space spanned by the dictionary of the parse coding. An iterative regression based method is proposed to detect and correct the failures according to the features. Experimental evaluation on an open dataset shows a global performance improvement of the tracking algorithm.}, 
keywords={face recognition;failure analysis;feature extraction;image coding;iterative methods;object tracking;regression analysis;user interfaces;active appearance models;appearance based facial tracking methods;candide models;face template;facial expression recognition;failure correction;failure detection;feature extraction;intelligent user interface;iterative regression based method;sparse coding strategy}, 
doi={10.1049/cje.2015.01.004}, 
ISSN={1022-4653}, 
month={},}
@INPROCEEDINGS{7758080, 
author={V. M. Vu and H. P. Lai and M. Visani}, 
booktitle={2016 Eighth International Conference on Knowledge and Systems Engineering (KSE)}, 
title={Towards an approach using metric learning for interactive semi-supervised clustering of images}, 
year={2016}, 
pages={357-362}, 
abstract={The problem of unsupervised and semi-supervised clustering is extensively studied in machine learning. In order to involve user in image data clustering, we proposed in [1] a new approach for interactive semi-supervised clustering that translates user feedback (expressed at the level of individual images) into pairwise constraints between groups of images, these groups being formed thanks to the underlying hierarchical clustering solution and user feedback. Recently, the need for appropriate measures of distance or similarity between data led to the emergence of distance metric learning approaches. In this paper1, we propose a method incorporating metric learning in the existing system to improve performance and reduce the computational time. Our preliminary experiments performed on the Wang dataset show that metric learning methods improve the performances and computational time of the existing system.}, 
keywords={image processing;learning (artificial intelligence);pattern clustering;hierarchical clustering;image analysis;image data clustering;interactive semisupervised clustering;machine learning;metric learning;pairwise constraints;unsupervised clustering;user feedback;Clustering algorithms;Covariance matrices;Distortion measurement;Euclidean distance;Linear programming;Mathematical model;image analysis;interactive semi-supervised clustering;metric learning;pairwise constraints}, 
doi={10.1109/KSE.2016.7758080}, 
month={Oct},}
@ARTICLE{7347429, 
author={A. K. Chorppath and T. Alpcan and H. Boche}, 
journal={IEEE Transactions on Mobile Computing}, 
title={Bayesian Mechanisms and Detection Methods for Wireless Network with Malicious Users}, 
year={2016}, 
volume={15}, 
number={10}, 
pages={2452-2465}, 
abstract={Strategic users in a wireless network cannot be assumed to follow the network algorithms blindly. Moreover, some of these users aim to use their knowledge about network algorithms to maliciously gain more resources and also to create interference to other users. We consider a scenario, in which the network and legitimate users gather probabilistic information about the presence of malicious users by observing the network over a long time period. The network (mechanism designer) and legitimate users modify their actions according to this Bayesian information. We consider Bayesian mechanisms, both pricing schemes and auctions, and obtain the Bayesian Nash Equilibrium (BNE) points. The BNE points provide conditions under which, the uncertainty about user's nature (type) is better for regular (legitimate) users. To derive these conditions, we compare the Bayesian case to the complete information case. We obtain the optimal prices and allocations, which counter the malicious users. We also provide detection methods based on machine learning algorithms for the detection of malicious users, by observing the prices and rate allocations. In addition, we provide detection using regression learning by observing the anomalies in the utility functions of malicious users from prices, which is implemented along with the pricing mechanism itself. For the designer and the regular users, in a complementary fashion, the results of the detections provide a better estimate of the statistics of malicious users to implement the pricing mechanisms. We have also proposed a truthful Bayesian mechanism in the presence of malicious users. The numerical studies for malicious user detection are carried out with the model proposed in the paper as well as using real Botnet dataset.}, 
keywords={Bayes methods;computer network security;game theory;invasive software;learning (artificial intelligence);pricing;radio networks;radiofrequency interference;regression analysis;BNE points;Bayesian Nash equilibrium point;Bayesian mechanism;detection method;machine learning algorithm;malicious user;malicious user detection;pricing scheme;real Botnet dataset;regression learning;wireless network;Bayes methods;Interference;Jamming;Pricing;Resource management;Uncertainty;Wireless networks;Bayesian games;Wireless network layer security;game theory;jamming;machine learning;mechanism design}, 
doi={10.1109/TMC.2015.2505724}, 
ISSN={1536-1233}, 
month={Oct},}
@INPROCEEDINGS{6350976, 
author={C. Brockmann and R. Doerffer and S. Sathyendranath and K. Ruddick and V. Brotas and R. Santer and S. Pinnock}, 
booktitle={2012 IEEE International Geoscience and Remote Sensing Symposium}, 
title={The CoastColour dataset}, 
year={2012}, 
pages={2036-2039}, 
abstract={The objective of the ESA DUE CoastColour project is to fully exploit the potential of the MERIS instrument for remote sensing of the coastal zone. The product requirements have been derived from a user consultation process. Users have provided in-situ data from many locations, which were used for algorithm development and validation. The MERIS data archive from 2005 onwards has been processed with the finally selected algorithms for 27 globally distributed coastal sites. The CoastColour dataset comprises an improved Level 1b product (L1P), a product that contains directional and normalised water leaving reflectances (L2R) and a product for water properties (L2W). The total data volume is 100TB. All data are online and available from the CoastColour Website. A near real time service was operated from October 2011 until end of the ENVISAT mission. Plans exist to continue the service with Sentinel data.}, 
keywords={Internet;oceanographic techniques;remote sensing;CoastColour Website;CoastColour dataset;ENVISAT mission;ESA DUE CoastColour project;MERIS data;MERIS instrument;coastal zone;globally distributed coastal sites;normalised water leaving reflectances;ocean colour;remote sensing;water properties;Adaptive optics;Algorithm design and analysis;Atmospheric modeling;Reflectivity;Sea measurements;Uncertainty;Water;MERIS;chlorophyll concentration;coastal zone;ocean colour;total suspended matter}, 
doi={10.1109/IGARSS.2012.6350976}, 
ISSN={2153-6996}, 
month={July},}
@INPROCEEDINGS{1374260, 
author={X. Zhu and X. Wu}, 
booktitle={16th IEEE International Conference on Tools with Artificial Intelligence}, 
title={Data acquisition with active and impact-sensitive instance selection}, 
year={2004}, 
pages={721-726}, 
abstract={Real-world data is never perfect and can often suffer from corruptions or missing values that may impact models created from the data. To build accurate predictive models, data acquisition is usually adopted to complete missing values in the incomplete instances. Due to the significant cost of doing so and the inherent correlations in the dataset, acquiring complete information for all instances is likely prohibitive and unnecessary. An interesting and important problem raises here is to select what kind of instances to complete so the model built from the data can receive significant improvement. We propose two solutions to resolve this problem, and the essential idea is to complete the attributes with higher impacts to the system performance. The first solution is based on an impact-sensitive instance ranking mechanism [X. Zhu et al. (2004)]. We explore the correlation between attributes and the class and use the correlation as weights of the attributes; the larger the weight, the higher the impacts of the attribute. For each incomplete instance, we sum all weights of the attributes with missing values, and the instance with larger sum appears to be more important for users to complete their missing information. In the second solution, active learning, impact-sensitive instance ranking and missing value prediction are combined for data acquisition. Experimental results from real-world datasets demonstrate the effectiveness of our strategies.}, 
keywords={correlation methods;data acquisition;data mining;data models;learning (artificial intelligence);pattern classification;statistical analysis;active learning;data acquisition;instance ranking mechanism;missing attribute value prediction;real-world datasets;Bayesian methods;Computer science;Costs;Data acquisition;Data mining;Filling;Predictive models;Statistics;System performance;Testing}, 
doi={10.1109/ICTAI.2004.46}, 
ISSN={1082-3409}, 
month={Nov},}
@INPROCEEDINGS{7813692, 
author={R. Katarya and O. P. Verma}, 
booktitle={2016 International Conference on Computing, Communication and Automation (ICCCA)}, 
title={Effectivecollaborative movie recommender system using asymmetric user similarity and matrix factorization}, 
year={2016}, 
pages={71-75}, 
abstract={Recommender systems are becoming ubiquitous these days to advise important products to users. Conventional collaborative filtering methods suffer from sparsity, scalability, and cold start problem. In this work, we have implemented a novel and improved method of recommending movies by combining the asymmetric method of calculating similarity with matrix factorization and Tyco (typicality-based collaborative filtering). The asymmetric method describes that similarity of user A with B is not the similar as the similarity of B with A. Matrix factorization shows items (movies) as well as users by vectors of factors derived from rating pattern of items (movies). In Tyco clusters of movies of the same genre are created, and typicality degree (a measure of how much a movie belongs to that genre) of each movie in that cluster was considered and subsequently of each user in a genre was calculated. The similarity between users was calculated by using their typicality in genres rather than co-rated items. We had combined these methods and employed Pearson correlation coefficient method to calculate similarity to optimize results when compared to cosine similarity, Linear Regression to make predictions that gave better results. In this research work stochastic gradient descent is also used for optimization and regularization to avoid the problem of over fitting. All these approaches together provide better prediction and handle problems of sparsity, cold start, and scalability well as compared to conventional methods. Experimental results confirm that our HYBRTyco gives improved results than Tyco regarding mean absolute error (MAE)and mean absolute percentage error (MAPE), especially on the sparse dataset.}, 
keywords={collaborative filtering;matrix decomposition;recommender systems;regression analysis;MAE;MAPE;Pearson correlation coefficient method;Tyco;asymmetric method;asymmetric user similarity;collaborative filtering methods;collaborative movie recommender system;cosine similarity;linear regression;matrix factorization;mean absolute error;mean absolute percentage error;stochastic gradient descent;typicality degree;typicality-based collaborative filtering;user similarity;Automation;Collaboration;Computational modeling;Context;Motion pictures;Recommender systems;Recommender system;asymmetric model;collaborative filtering;matrix factorization;typicality}, 
doi={10.1109/CCAA.2016.7813692}, 
month={April},}
@ARTICLE{7122921, 
author={E. Dede and B. Sendir and P. Kuzlu and J. Weachock and M. Govindaraju and L. Ramakrishnan}, 
journal={IEEE Transactions on Services Computing}, 
title={Processing Cassandra Datasets with Hadoop-Streaming Based Approaches}, 
year={2016}, 
volume={9}, 
number={1}, 
pages={46-58}, 
abstract={The progressive transition in the nature of both scientific and industrial datasets has been the driving force behind the development and research interests in the NoSQL model. Loosely structured data poses a challenge to traditional data store systems, and when working with the NoSQL model, these systems are often considered impractical and costly. As the quantity and quality of unstructured data grows, so does the demand for a processing pipeline that is capable of seamlessly combining the NoSQL storage model and a “Big Data” processing platform such as MapReduce. Although MapReduce is the paradigm of choice for data-intensive computing, Java-based frameworks such as Hadoop require users to write MapReduce code in Java while Hadoop Streaming module allows users to define non-Java executables as map and reduce operations. When confronted with legacy C/C++ applications and other non-Java executables, there arises a further need to allow NoSQL data stores access to the features of Hadoop Streaming. We present approaches in solving the challenge of integrating NoSQL data stores with MapReduce under non-Java application scenarios, along with advantages and disadvantages of each approach. We compare Hadoop Streaming alongside our own streaming framework, MARISSA, to show performance implications of coupling NoSQL data stores like Cassandra with MapReduce frameworks that normally rely on file-system based data stores. Our experiments also include Hadoop-C*, which is a setup where a Hadoop cluster is co-located with a Cassandra cluster in order to process data using Hadoop with non-java executables.}, 
keywords={Big Data;C++ language;Java;parallel processing;pattern clustering;Big Data processing platform;Cassandra dataset processing;Hadoop Streaming module;Hadoop cluster;Hadoop-C;Hadoop-streaming based approach;Java-based frameworks;MARISSA;MapReduce code;MapReduce frameworks;NoSQL data stores;NoSQL storage model;data store systems;data-intensive computing;file-system based data stores;legacy C-C++ applications;loosely structured data;non-Java application scenarios;nonJava executables;Data models;Data processing;Distributed databases;Java;Pipelines;Servers;Cassandra;Hadoop-Streaming;Hadoop-streaming;MapReduce}, 
doi={10.1109/TSC.2015.2444838}, 
ISSN={1939-1374}, 
month={Jan},}
@INPROCEEDINGS{6130272, 
author={S. Z. Masood and C. Ellis and A. Nagaraja and M. F. Tappen and J. J. LaViola and R. Sukthankar}, 
booktitle={2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)}, 
title={Measuring and reducing observational latency when recognizing actions}, 
year={2011}, 
pages={422-429}, 
abstract={An important aspect in interactive, action-based interfaces is the latency in recognizing the action. High latency will cause the system's feedback to lag behind user actions, reducing the overall quality of the user experience. This paper presents a novel dataset and algorithms for reducing the latency in recognizing the action. Latency in classification is minimized with a classifier based on logistic regression that uses canonical poses to identify the action. The classifier is trained from the dataset using a learning formulation that makes it possible to train the classifier to reduce latency. The classifier is compared against both a Bag of Words and a Conditional Random Field classifier and is found to be superior in both pre-segmented and on-line classification tasks.}, 
keywords={gesture recognition;image classification;image segmentation;interactive systems;learning (artificial intelligence);minimisation;pose estimation;regression analysis;user interfaces;action recognition;canonical pose;classification minimization;classifier training;interactive action based interface;learning formulation;logistic regression;observational latency reduction;online classification task;presegmented task;Accuracy;Humans;Joints;Mathematical model;Training;Vectors}, 
doi={10.1109/ICCVW.2011.6130272}, 
month={Nov},}
@ARTICLE{1350896, 
author={P. Haigron and M. E. Bellemare and O. Acosta and C. Goksu and C. Kulik and K. Rioual and A. Lucas}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Depth-map-based scene analysis for active navigation in virtual angioscopy}, 
year={2004}, 
volume={23}, 
number={11}, 
pages={1380-1390}, 
abstract={This work presents an approach dealing with virtual exploratory navigation inside vascular structures. It is based on the notion of active vision in which only visual perception drives the motion of the virtual angioscope. The proposed fly-through approach does not require a premodeling of the volume dataset or an interactive control of the virtual sensor during the fly-through. Active navigation combines the on-line computation of the scene view and its analysis, to automatically define the three-dimensional sensor path. The navigation environment and the camera-like model are first sketched. The basic stages of the active navigation framework are then described: the virtual image computation (based on ray casting), the scene analysis process (using depth map), the navigation strategy, and the virtual path estimation. Experimental results obtained from phantom model and patient computed tomography data are finally reported.}, 
keywords={active vision;computerised tomography;medical image processing;phantoms;virtual reality;visual perception;active vision;depth-map-based scene analysis;fly-through approach;patient computed tomography;phantom model;ray casting;vascular structures;virtual angioscopy;virtual exploratory navigation;virtual image computation;virtual path estimation;virtual sensor;visual perception;Anatomical structure;Automatic control;Biomedical imaging;Computed tomography;Endoscopes;Image analysis;Magnetic resonance imaging;Navigation;Path planning;Surgery;Active virtual sensor;automatic path planning;virtual angioscopy;Algorithms;Angiography;Angioscopy;Humans;Pattern Recognition, Automated;Radiographic Image Enhancement;Radiographic Image Interpretation, Computer-Assisted;Reproducibility of Results;Sensitivity and Specificity;Surgery, Computer-Assisted;User-Computer Interface}, 
doi={10.1109/TMI.2004.836869}, 
ISSN={0278-0062}, 
month={Nov},}
@INPROCEEDINGS{1684458, 
author={M. M. Cristian and B. D. Dan}, 
booktitle={2006 2nd International Conference on Information Communication Technologies}, 
title={From Decision Trees to Classification Rules with Data Representing User Traffic from an e-Learning Platform}, 
year={2006}, 
volume={1}, 
pages={702-707}, 
abstract={The paper presents two state-of-the-art techniques of analyzing data. The employed techniques are decision trees and classification rules. The analyzed data is represented by user traffic gathered from an e-learning platform. User traffic data is represented by actions performed by platform's users. In our analysis we are interested only in student's performed actions. The analysis process creates a decision tree from collected data and then derives the classification rules on the same dataset. We investigate the accuracy and interestingness of the two models}, 
keywords={computer aided instruction;data analysis;decision trees;pattern classification;classification rules;data analysis;decision trees;e-learning;user traffic;Classification tree analysis;Collaboration;Data analysis;Decision trees;Electronic learning;Environmental management;Machine learning;Performance analysis;Software engineering;Traffic control}, 
doi={10.1109/ICTTA.2006.1684458}, 
month={},}
@INPROCEEDINGS{6890944, 
author={A. El Masri and H. Wechsler and P. Likarish and B. B. Kang}, 
booktitle={2014 Twelfth Annual International Conference on Privacy, Security and Trust}, 
title={Identifying users with application-specific command streams}, 
year={2014}, 
pages={232-238}, 
abstract={This paper proposes and describes an active authentication model based on user profiles built from user-issued commands when interacting with GUI-based application. Previous behavioral models derived from user issued commands were limited to analyzing the user's interaction with the *Nix (Linux or Unix) command shell program. Human-computer interaction (HCI) research has explored the idea of building users profiles based on their behavioral patterns when interacting with such graphical interfaces. It did so by analyzing the user's keystroke and/or mouse dynamics. However, none had explored the idea of creating profiles by capturing users' usage characteristics when interacting with a specific application beyond how a user strikes the keyboard or moves the mouse across the screen. We obtain and utilize a dataset of user command streams collected from working with Microsoft (MS) Word to serve as a test bed. User profiles are first built using MS Word commands and identification takes place using machine learning algorithms. Best performance in terms of both accuracy and Area under the Curve (AUC) for Receiver Operating Characteristic (ROC) curve is reported using Random Forests (RF) and AdaBoost with random forests.}, 
keywords={biometrics (access control);human computer interaction;learning (artificial intelligence);message authentication;sensitivity analysis;AUC;AdaBoost;GUI-based application;MS Word commands;Microsoft;RF;ROC curve;active authentication model;application-specific command streams;area under the curve;human-computer interaction;machine learning algorithms;random forests;receiver operating characteristic;user command streams;user identification;user profiles;user-issued commands;Authentication;Biometrics (access control);Classification algorithms;Hidden Markov models;Keyboards;Mice;Radio frequency;Active Authentication;Behavioral biometrics;Intrusion Detection;Machine Learning}, 
doi={10.1109/PST.2014.6890944}, 
month={July},}
@ARTICLE{7530846, 
author={S. Wu and W. Guo and S. Xu and Y. Huang and L. Wang and T. Tan}, 
journal={IEEE Transactions on Human-Machine Systems}, 
title={Coupled Topic Model for Collaborative Filtering With User-Generated Content}, 
year={2016}, 
volume={46}, 
number={6}, 
pages={908-920}, 
abstract={The user-generated content (UGC) is a type of dyadic information that provides description of the interaction between users and items (such as rating, purchasing, etc.). Most conventional methods incorporate either a user profile or the item description, which cannot well utilize this kind of content information. Some other works jointly consider user ratings and reviews, but they are based on the factorization technique and have difficulty in providing explanations on generated recommendations. In this study, a coupled topic model (CoTM) for recommendation with UGC is developed. By combining UGC and ratings, the method discussed in this study captures both the content-based preferences and collaborative preferences and, thus, can explain both the user and item latent spaces using the topics discovered from the UGC. The learned topics in CoTM can also serve as proper explanations for the generated recommendations. Experimental results show that the proposed CoTM model yields significant improvements over the compared competitive methods on two typical datasets, that is, MovieLens-10M and Citation-network V1. The topics discovered by CoTM can be used not only to illustrate the topic distributions of users and items, but also to explain the generated user-item recommendations.},
keywords={collaborative filtering;recommender systems;Citation-network V1 dataset;CoTM model;MovieLens-10M dataset;UGC;collaborative filtering;collaborative preference;content information;content-based preference;coupled topic model;dyadic information;factorization technique;item latent space;user latent space;user-generated content;user-item recommendation;Collaboration;Expectation-maximization algorithms;Filtering;Recommender systems;User-generated content;Collaborative filtering (CF);recommender systems (RS);topic model;user-generated content (UGC)}, 
doi={10.1109/THMS.2016.2586480}, 
ISSN={2168-2291}, 
month={Dec},}
@INPROCEEDINGS{7417841, 
author={Y. Gong and Y. Fang and Y. Guo}, 
booktitle={2015 IEEE Global Communications Conference (GLOBECOM)}, 
title={Privacy-Preserving Collaborative Learning for Mobile Health Monitoring}, 
year={2015}, 
pages={1-6}, 
abstract={Health monitoring is an important category of mobile Health (mHealth) applications. Users generate a large volume of data during health monitoring, which can then be used by the mHealth server for constructing diagnosis or prognosis prediction models. However, these training samples contain private information of data owners, who may be reluctant to share them with the mHealth server. This paper proposes and experimentally studies a scheme that keeps the training samples private while enabling accurate construction of diagnosis and prognosis models. We specifically consider logistic regression models which are widely used in mHealth, and decompose the logistic regression model construction problem into small subproblems that can be executed by each user using their own private data. In this manner, users can keep their raw data locally and only upload encrypted parameters to the mHealth server for model construction. We show that our scheme suits well in mHealth applications by conducting experimental evaluations based on a real-world dataset and analyzing its computation overhead.}, 
keywords={cryptography;patient monitoring;prediction theory;regression analysis;encryption;logistic regression model;mHealth server monitoring;mobile health server monitoring;privacy-preserving collaborative learning;prognosis prediction model;Computational modeling;Data models;Logistics;Monitoring;Sensors;Servers;Training}, 
doi={10.1109/GLOCOM.2015.7417841}, 
month={Dec},}
@INPROCEEDINGS{7906967, 
author={A. Kalaï and A. Wafa and C. A. Zayani and I. Amous}, 
booktitle={2016 14th Annual Conference on Privacy, Security and Trust (PST)}, 
title={LoTrust: A social Trust Level model based on time-aware social interactions and interests similarity}, 
year={2016}, 
pages={428-436}, 
abstract={With the immense growth of online social applications, trust plays a more and more important role in connecting users to each other, sharing their personal information and attracting him to receive recommendations. Therefore, how to obtain trust relationships through mining online social networks became a critical issue. To calculate the level of trust between two users, many computational trust models are proposed which mainly rely on the social network structure, the explicit trust from user to another, the users' behaviors, or the users' similarity, etc. However, the majority of these models ignored the temporal factor. In this paper, we propose a trust relationship detection mechanism from an egocentric social network in order to compute the trust level between an active user and his directed friends. We propose a Level of social Trust model, that we called LoTrust, which is suitable for personalized recommendation purpose. This computational model founded on novel trust metric which is based not only on the users' interests similarity according to their semantic social profiles (RDF/FOAF), but also takes into account the time factor of the users' active interactions (e.g comments, share photo, wall posts, messages). We perform experiments on real life dataset extracted from Facebook. The experimental results demonstrated how our LoTrust model produces satisfactory results than other computational models.}, 
keywords={man-machine systems;semantic networks;social networking (online);Facebook;LoTrust model;egocentric social network;online social applications;online social network;semantic social profiles;social trust level model;time-aware social interaction similarity;time-aware social interest similarity;trust relationship detection mechanism;Computational modeling;Context;Measurement;Psychology;Recommender systems;Reliability;Social network services}, 
doi={10.1109/PST.2016.7906967}, 
month={Dec},}
@INPROCEEDINGS{7816839, 
author={M. Dimaggio and F. Leotta and M. Mecella and D. Sora}, 
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)}, 
title={Process-Based Habit Mining: Experiments and Techniques}, 
year={2016}, 
pages={145-152}, 
abstract={Independently of the specific task to be enacted in a smart space, it is always crucial to mine a set of models representing environmental dynamics and, noteworthy, user habits, desires. Many different formalisms have been proposed to model human habits, but the vast majority of them are either difficult to read, evaluate or their definition requires a huge amount of work from either experts or users. In this paper we propose to employ process mining techniques in order to model human habits,, we experimentally evaluate such an approach on a dataset built adopting the Smart-Home-in-a-Box toolkit with real users.}, 
keywords={data mining;home computing;Smart-Home-in-a-Box toolkit;environmental dynamics;formalisms;habit mining;human habits;process mining;smart space;user habits;Correlation;Data mining;Data models;Decision making;Intelligent sensors;Temperature sensors;habit mining;process mining;smart home}, 
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0043}, 
month={July},}
@INPROCEEDINGS{6337098, 
author={P. Molino and P. Basile and A. Caputo and P. Lops and G. Semeraro}, 
booktitle={2012 IEEE Sixth International Conference on Semantic Computing}, 
title={Exploiting Distributional Semantic Models in Question Answering}, 
year={2012}, 
pages={146-153}, 
abstract={This paper investigates the role of Distributional Semantic Models (DSMs) in Question Answering (QA), and specifically in a QA system called Question Cube. Question Cube is a framework for QA that combines several techniques to retrieve passages containing the exact answers for natural language questions. It exploits Information Retrieval models to seek candidate answers and Natural Language Processing algorithms for the analysis of questions and candidate answers both in English and Italian. The data source for the answer is an unstructured text document collection stored in search indices. In this paper we propose to exploit DSMs in the Question Cube framework. In DSMs words are represented as mathematical points in a geometric space, also known as semantic space. Words are similar if they are close in that space. Our idea is that DSMs approaches can help to compute relatedness between users' questions and candidate answers by exploiting paradigmatic relations between words. Results of an experimental evaluation carried out on CLEF2010 QA dataset, prove the effectiveness of the proposed approach.}, 
keywords={information retrieval;natural language processing;text analysis;DSM;English language;Italian language;distributional semantic models;exploiting distributional semantic models;geometric space;information retrieval models;mathematical points;natural language processing algorithms;natural language questions;question answering;question cube framework;search indices;unstructured text document;Context;Engines;Pipelines;Pragmatics;Search engines;Semantics;Vectors;Distributional Semantic Models;Information Retrieval;Question Answering;Semantics}, 
doi={10.1109/ICSC.2012.53}, 
month={Sept},}
@INPROCEEDINGS{7860469, 
author={B. Rashidi and C. Fung}, 
booktitle={2016 IEEE Conference on Communications and Network Security (CNS)}, 
title={XDroid: An Android permission control using Hidden Markov chain and online learning}, 
year={2016}, 
pages={46-54}, 
abstract={Android devices provide opportunities for users to install third-party applications through various online markets. This brings security and privacy concerns to the users since third-party applications may pose serious threats. The exponential growth and diversity of these applications render conventional defenses ineffective, thus, Android smartphones often remain unprotected from novel malware. In this work, we present XDroid, an Android app and resource risk assessment framework using hidden Markov model. In this framework, we first map the applications' behaviors into an observation set, and we introduce a novel approach to attach timestamp to some observations to improve the accuracy of the model. We show that our HMM can be utilized to generates risk alerts to users when suspicious behaviors are found. Furthermore, an online learning model is introduced to enable the integration of the input from users and provide adaptive risk assessment to meet user's preferences. We evaluate our model through a set of experiments on a benchmark malware dataset DREBIN. Our experimental results demonstrate that the proposed model can assess malicious apps risk-levels with high accuracy. It also provide adaptive risk assessment based on input from users.}, 
keywords={Android (operating system);data privacy;hidden Markov models;invasive software;learning (artificial intelligence);risk management;smart phones;Android app;Android devices;Android permission control;Android smartphones;DREBIN malware dataset;HMM;XDroid;adaptive risk assessment;hidden Markov chain;hidden Markov model;malicious apps risk-level assessment;observation set;observation timestamp;online learning;online markets;resource risk assessment framework;risk alert generation;third-party applications;user privacy;user security;Androids;Computational modeling;Hidden Markov models;Humanoid robots;Risk management;Security;Smart phones}, 
doi={10.1109/CNS.2016.7860469}, 
month={Oct},}
@INPROCEEDINGS{6005968, 
author={Y. Pang and Z. Ma and J. Pan and Y. Yuan}, 
booktitle={2011 Sixth International Conference on Image and Graphics}, 
title={Robust Sparse Tensor Decomposition by Probabilistic Latent Semantic Analysis}, 
year={2011}, 
pages={893-896}, 
abstract={Movie recommendation system is becoming more and more popular in recent years. As a result, it is becoming increasingly important to develop machine learning algorithm on partially-observed matrix to predict users' preferences on missing data. Motivated by the user ratings prediction problem, we propose a novel robust tensor probabilistic latent semantic analysis (RT-pLSA) algorithm that not only takes time variable into account, but also uses the periodic property of data in time attribute. Different from the previous algorithms of predicting missing values on two-dimensional sparse matrix, we formulize the prediction problem as a probabilistic tensor factorization problem with periodicity constraint on time coordinate. Furthermore, we apply the Tsallis divergence error measure in the context of RT-pLSA tensor decomposition that is able to robustly predict the latent variable in the presence of noise. Our experimental results on two benchmark movie rating dataset: Netflix and Movie lens, show a good predictive accuracy of the model.}, 
keywords={cinematography;learning (artificial intelligence);matrix decomposition;probability;recommender systems;sparse matrices;tensors;Movie lens;Netflix;RT-pLSA algorithm;RT-pLSA tensor decomposition;Tsallis divergence error;machine learning algorithm;movie rating;movie recommendation system;partially-observed matrix;periodic property;periodicity constraint;probabilistic tensor factorization problem;robust sparse tensor decomposition;robust tensor probabilistic latent semantic analysis;time coordinate;two-dimensional sparse matrix;user preference;user ratings prediction problem;Algorithm design and analysis;Motion pictures;Noise;Prediction algorithms;Probabilistic logic;Robustness;Tensile stress;movie recommendation;sparse representation;tensor analysis;topic model}, 
doi={10.1109/ICIG.2011.98}, 
month={Aug},}
@INPROCEEDINGS{7353719, 
author={N. Hu and G. Englebienne and Z. Lou and B. Kröse}, 
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
title={A hierarchical representation for human activity recognition with noisy labels}, 
year={2015}, 
pages={2517-2522}, 
abstract={Human activity recognition is an essential task for robots to effectively and efficiently interact with the end users. Many machine learning approaches for activity recognition systems have been proposed recently. Most of these methods are built upon a strong assumption that the labels in the training data are noise-free, which is often not realistic. In this paper, we incorporate the uncertainty of labels into a max-margin learning algorithm, and the algorithm allows the labels to deviate over iterations in order to find a better solution. This is incorporated with a hierarchical approach where we jointly estimate activities at two different levels of granularity. The model is tested on two datasets, i.e., the CAD-120 dataset and the Accompany dataset, and the proposed model shows outperforming results over the state-of-the-art methods.}, 
keywords={image recognition;image representation;learning (artificial intelligence);hierarchical approach;hierarchical representation;human activity recognition;machine learning;max-margin learning algorithm;noisy labels;Cameras;Feature extraction;Labeling;Random variables;Skeleton;Time measurement;Uncertainty}, 
doi={10.1109/IROS.2015.7353719}, 
month={Sept},}
@INPROCEEDINGS{7524567, 
author={E. Baik and A. Pande and Z. Zheng and P. Mohapatra}, 
booktitle={IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications}, 
title={VSync: Cloud based video streaming service for mobile devices}, 
year={2016}, 
pages={1-9}, 
abstract={Synchronizing videos over file-hosting services on personal cloud such as Dropbox, Box or Onedrive leads to wastage in bandwidth and storage, which can be critical, while using mobile devices. Users can alternatively download the video on-the-go, but that leads to high latency, depending on network bandwidth and video file size. In contrast, adaptive video streaming allows near-real-time viewing by streaming the best possible quality in a given network condition. This feature is achieved by keeping multiple versions of video in cloud, leading to additional costs in cloud storage. Moreover, current solutions can only support a small set of bitrates, leading to abrupt switches in video resolution especially when the network condition is unstable, as often experienced by mobile users. This paper introduces Vsync, a framework for cloud based video synchronization for mobile devices. A video content is streamed using a cloud-based real-time transcoding and transmission framework to provide smooth video quality. Built over prediction models for video transcoding sessions and a QoE based adaptive video streaming protocol, Vsync is able to obtain the improvements of 37 ~ 80% than other compared schemes. The dataset and evaluation was done on a pool of 220K video clips.}, 
keywords={cloud computing;mobile computing;transcoding;video coding;video streaming;VSync;adaptive video streaming protocol;cloud based video streaming service;cloud based video synchronization;cloud storage;cloud-based real-time transcoding;file-hosting services;mobile devices;network bandwidth;network condition;smooth video quality;transmission framework;video clips;video file size;video resolution;Bandwidth;Cloud computing;Mobile communication;Mobile handsets;Real-time systems;Streaming media;Transcoding;Cloud Service;MPEG-DASH;Mobile Video Quality;Mobile Video Streaming;Transcoding}, 
doi={10.1109/INFOCOM.2016.7524567}, 
month={April},}
@INPROCEEDINGS{7789590, 
author={J. Wan and S. Z. Li and Y. Zhao and S. Zhou and I. Guyon and S. Escalera}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
title={ChaLearn Looking at People RGB-D Isolated and Continuous Datasets for Gesture Recognition}, 
year={2016}, 
pages={761-769}, 
abstract={In this paper, we present two large video multi-modal datasets for RGB and RGB-D gesture recognition: the ChaLearn LAP RGB-D Isolated Gesture Dataset (IsoGD) and the Continuous Gesture Dataset (ConGD). Both datasets are derived from the ChaLearn Gesture Dataset (CGD) that has a total of more than 50000 gestures for the "one-shot-learning" competition. To increase the potential of the old dataset, we designed new well curated datasets composed of 249 gesture labels, and including 47933 gestures manually labeled the begin and end frames in sequences. Using these datasets we will open two competitions on the CodaLab platform so that researchers can test and compare their methods for "user independent" gesture recognition. The first challenge is designed for gesture spotting and recognition in continuous sequences of gestures while the second one is designed for gesture classification from segmented data. The baseline method based on the bag of visual words model is also presented.}, 
keywords={computer vision;gesture recognition;image sequences;ChaLearn LAP RGB-D isolated gesture dataset;ChaLearn looking;CodaLab platform;ConGD;IsoGD;bag of visual words model;computer vision;continuous gesture dataset;continuous sequences;gesture classification;gesture recognition;one-shot-learning competition;people RGB-D continuous datasets;people RGB-D isolated datasets;user independent gesture recognition;Computer vision;Conferences;Gesture recognition;Indexes;Testing;Training}, 
doi={10.1109/CVPRW.2016.100}, 
month={June},}
@INPROCEEDINGS{5447847, 
author={O. Jurca and S. Michel and A. Herrmann and K. Aberer}, 
booktitle={2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)}, 
title={Continuous query evaluation over distributed sensor networks}, 
year={2010}, 
pages={912-923}, 
abstract={In this paper we address the problem of processing continuous multi-join queries, over distributed data streams. Our approach makes use of existing work in the field of publish/subscribe systems. We show how these principles can be ported to our envisioned architectural model by enriching the common query model with location dependent attributes. We allow users to subscribe to a set of sensor attributes, a service that requires processing multi-join correlation queries. The goal is to decrease the overall network traffic consumption by removing redundant subscriptions and eliminating unrequested events close to the publishing sensors. This is non-trivial, especially in the presence of multi-join queries without any central control mechanism. Our approach is based on the concept of filter-split-forward phases for efficient subscription filtering and placement inside the network. We report on a performance evaluation using a real-world dataset, showing the improvements over the state-of-the-art, as we reduce the overall data traffic by half.}, 
keywords={middleware;query processing;central control mechanism;common query model;continuous query evaluation;distributed data streams;distributed sensor networks;filter-split-forward phases;location dependent attributes;multijoin queries;publish-subscribe systems;redundant subscriptions removal;unrequested events elimination;Communication system traffic control;Humidity measurement;Hydrologic measurements;Large-scale systems;Meteorology;Query processing;Radiation monitoring;Sensor phenomena and characterization;Snow;Subscriptions}, 
doi={10.1109/ICDE.2010.5447847}, 
ISSN={1063-6382}, 
month={March},}
@INPROCEEDINGS{7526951, 
author={A. Dridi and M. Kacimi}, 
booktitle={2015 7th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management (IC3K)}, 
title={KISS MIR: Keep it semantic and social music information retrieval}, 
year={2015}, 
volume={01}, 
pages={433-439}, 
abstract={While content-based approaches for music information retrieval (MIR) have been heavily investigated, user-centric approaches are still in their early stage. Existing user-centric approaches use either music-context or user-context to personalize the search. However, none of them give the possibility to the user to choose the suitable context for his needs. In this paper we propose KISS MIR, a versatile approach for music information retrieval. It consists in combining both music-context and user-context to rank search results. The core contribution of this work is the investigation of different types of contexts derived from social networks. We distinguish semantic and social information and use them to build semantic and social profiles for music and users. The different contexts and profiles can be combined and personalized by the user. We have assessed the quality of our model using a real dataset from Last.fm. The results show that the use of user-context to rank search results is two times better than the use of music-context. More importantly, the combination of semantic and social information is crucial for satisfying user needs.}, 
keywords={Context;Encyclopedias;Semantics;Silicon carbide;Social network services;Tagging;Music Information Retrieval;Music-context;Personalization;Social Information;User-context}, 
month={Nov},}
@INPROCEEDINGS{6787170, 
author={K. Mizuno and H. Y. Wu and S. Takahashi}, 
booktitle={2014 IEEE Pacific Visualization Symposium}, 
title={Manipulating Bilevel Feature Space for Category-Aware Image Exploration}, 
year={2014}, 
pages={217-224}, 
abstract={The demand for interactively designing the image feature space has been increasing due to the ongoing need for image retrieval, recognition, and labeling. Although conventional methods provide an interface for locally rearranging such a feature space, category-level global manipulation is still missing and thus manually rearranging the overall image categorization usually requires a time-consuming task. This paper presents a novel approach to exploring images in the database through the manipulation of bi-level feature space representations, where the upper-and lower-level representations characterize the global categories and local features of the images, respectively. In this approach, the upper-level space describes similarity relationship among the underlying categories extracted from the bag-of-features model, while the lower-level space encodes the closeness between a pair of images within the same category. The key idea behind this approach is to associate the relationship between the two feature spaces with a two-layered graph representation and project it onto 2D screen space using pivot MDS for user manipulation. Experimental results are provided to demonstrate that our approach allows users to understand the entire structure of the given image dataset and reorganize the layout according to their preference both locally and globally.}, 
keywords={feature extraction;graph theory;image classification;2D screen space;bag-of-features model;bilevel feature space manipulation;category-aware image exploration;category-level global manipulation;image categorization;image database;image feature space;image labeling;image recognition;image retrieval;lower-level representations;lower-level space;pivot MDS;two-layered graph representation;upper-level representations;user manipulation;Aerospace electronics;Feature extraction;Histograms;Image edge detection;Semantics;Vectors;Visualization;Image exploration;Methodology and Techniques;Multimedia Information Systems;bag-of-features;dimensionality reduction;feature space manipulation}, 
doi={10.1109/PacificVis.2014.58}, 
ISSN={2165-8765}, 
month={March},}
@INPROCEEDINGS{6680001, 
author={P. Parveen and P. Desai and B. Thuraisingham and L. Khan}, 
booktitle={9th IEEE International Conference on Collaborative Computing: Networking, Applications and Worksharing}, 
title={MapReduce-guided scalable compressed dictionary construction for evolving repetitive sequence streams}, 
year={2013}, 
pages={345-352}, 
abstract={Users' repetitive daily or weekly activities may constitute user profiles. For example, a user's frequent command sequences may represent normative pattern of that user. To find normative patterns over dynamic data streams of unbounded length is challenging. For this, an unsupervised learning approach is proposed in our prior work by exploiting a compressed/quantized dictionary to model common behavior sequences. This work suffers scalability issues. Hence, in this paper, we propose and implement a MapReduce-based framework to construct a quantized dictionary. We show effectiveness of our distributed parallel solution on a benchmark dataset.}, 
keywords={data analysis;human factors;parallel algorithms;unsupervised learning;user interfaces;MapReduce-guided scalable compressed dictionary construction;common behavior sequences;dynamic data streams;quantized dictionary;repetitive sequence streams;unsupervised learning approach;user frequent command sequences;user profiles;user repetitive daily activities;user repetitive weekly activities;Cascading style sheets;Data handling;Dictionaries;Information management;Scalability;Time complexity;Unsupervised learning;Cloud;MapReduce;Sequence;Unsupervised Learning}, 
doi={10.4108/icst.collaboratecom.2013.254135}, 
month={Oct},}
@INPROCEEDINGS{6729618, 
author={W. Shao and X. Shi and P. S. Yu}, 
booktitle={2013 IEEE 13th International Conference on Data Mining}, 
title={Clustering on Multiple Incomplete Datasets via Collective Kernel Learning}, 
year={2013}, 
pages={1181-1186}, 
abstract={Multiple datasets containing different types of features may be available for a given task. For instance, users' profiles can be used to group users for recommendation systems. In addition, a model can also use users' historical behaviors and credit history to group users. Each dataset contains different information and suffices for learning. A number of clustering algorithms on multiple datasets were proposed during the past few years. These algorithms assume that at least one dataset is complete. So far as we know, all the previous methods will not be applicable if there is no complete dataset available. However, in reality, there are many situations where no dataset is complete. As in building a recommendation system, some new users may not have profiles or historical behaviors, while some may not have credit history. Hence, no available dataset is complete. In order to solve this problem, we propose an approach called Collective Kernel Learning to infer hidden sample similarity from multiple incomplete datasets. The idea is to collectively completes the kernel matrices of incomplete datasets by optimizing the alignment of shared instances of the datasets. Furthermore, a clustering algorithm is proposed based on the kernel matrix. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed approach. The proposed clustering algorithm outperforms the comparison algorithms by as much as two times in normalized mutual information.}, 
keywords={learning (artificial intelligence);pattern clustering;recommender systems;collective kernel learning;credit history;incomplete dataset kernel matrices;multiple incomplete dataset clustering;normalized mutual information;real dataset;recommendation systems;synthetic dataset;user historical behavior;user profiles;Algorithm design and analysis;Clustering algorithms;Correlation;Equations;Kernel;Laplace equations;Optimization}, 
doi={10.1109/ICDM.2013.117}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{7284444, 
author={Y. Nishioka and K. Taura}, 
booktitle={2015 IEEE International Parallel and Distributed Processing Symposium Workshop}, 
title={Scalable Task-Parallel SGD on Matrix Factorization in Multicore Architectures}, 
year={2015}, 
pages={1178-1184}, 
abstract={Recommendation is an indispensable technique especially in e-commerce services such as Amazon or Netflix to provide more preferable items to users. Matrix factorization is a well-known algorithm for recommendation which estimates affinities between users and items solely based on ratings explicitly given by users. To handle the large amounts of data, stochastic gradient descent (SGD), which is an online loss minimization algorithm, can be applied to matrix factorization. SGD is an effective method in terms of both convergence speed and memory consumption, but is difficult to be parallelized due to its essential sequentiality. FPSGD by Zhuang et al. Cite fpsgd is an existing parallel SGD method for matrix factorization by dividing the rating matrix into many small blocks. Threads work on blocks, so that they do not update the same rows or columns of the factor matrices. Because of this technique FPSGD achieves higher convergence speed than other existing methods. Still, as we demonstrate in this paper, FPSGD does not scale beyond 32 cores with 1.4GB Netflix dataset because assigning non-conflicting blocks to threads needs a lock operation. In this work, we propose an alternative approach of SGD for matrix factorization using task parallel programming model. As a result, we have successfully overcome the bottleneck of FPSGD and achieved higher scalability with 64 cores.}, 
keywords={electronic commerce;gradient methods;matrix decomposition;minimisation;multiprocessing systems;parallel programming;recommender systems;stochastic programming;Amazon;FPSGD;Netflix dataset;convergence speed;e-commerce services;matrix factorization;memory consumption;multicore architectures;online loss minimization algorithm;recommendation technique;scalable task-parallel SGD;stochastic gradient descent;task parallel programming model;Convergence;Instruction sets;Load management;Radiation detectors;Scalability;Sparse matrices;Synchronization;Matrix factorization;Recommender systems;Stochastic gradient descent;Task parallel model}, 
doi={10.1109/IPDPSW.2015.135}, 
month={May},}
@INPROCEEDINGS{7840821, 
author={Y. Zhu and M. Moh and T. S. Moh}, 
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
title={Multi-layer text classification with voting for consumer reviews}, 
year={2016}, 
pages={1991-1999}, 
abstract={As social media has become increasingly popular in the modern world, people are using these platforms to express their opinions about products, businesses, and services. The need for categorizing these consumer reviews has been prominent. One effective solution is sentiment analysis (SA), which has been an active research topic. The goal of SA is to automatically extracting and classifying user opinions. Pervious research works however have not shown satisfied results. In this paper, a multilayer architecture is proposed to increase the performance of multiclass classification. The framework includes data-preprocessing, feature extraction and selection, and classifier building. The framework is a two-layer classification, choosing from Naïve Bayes, Support Vector Machine, Random Forest, and Logistic Regression as base models, and using a voting scheme to obtain the final predicted class. The proposed model is applied to more than 1.3 million restaurant reviews from the Yelp Challenge dataset. We have achieved a high accuracy of 86% for cross validation, and using real-world online review data as test data, we have achieved an accuracy of 80%. The results show that the proposed framework has greatly improved classification accuracy while comparing with those using single-layer architectures. We believe that the proposed method may be applied to, and would have significant contributions to other areas of opinion mining.}, 
keywords={classification;consumer behaviour;feature extraction;sentiment analysis;support vector machines;Naive Bayes;SA;Yelp Challenge dataset;active research topic;classifier building;consumer reviews;feature extraction;logistic regression;multiclass classification;multilayer architecture;multilayer text classification;opinion mining;random forest;real-world online review data;restaurant reviews;sentiment analysis;single-layer architectures;social media;support vector machine;two-layer classification;voting scheme;Architecture;Business;Computer architecture;Feature extraction;Learning systems;Sentiment analysis;Support vector machines;Yelp restaurant reviews;ensemble learning;multi-class classification;multilayer;opinion mining;sentiment analysis;text classification;voting scheme}, 
doi={10.1109/BigData.2016.7840821}, 
month={Dec},}
@INPROCEEDINGS{6181964, 
author={Chengxu Ye and Kesong Zheng}, 
booktitle={Proceedings of 2011 International Conference on Computer Science and Network Technology}, 
title={Detection of application layer distributed denial of service}, 
year={2011}, 
volume={1}, 
pages={310-314}, 
abstract={In the previous literatures, many methods were designed to defend against IP or TCP layers distributed denial of service attacks instead of the application layer. In this paper, we introduce a simple but effective scheme to detect application layer based ddos attacks. A http request transition matrix is proposed to describe users browsing behavior. We assume normal human user will choose interesting pages and objects. And that forms a pattern - transition probability from one page to another. But a bot can not know what are the popular pages for most people, it will randomly send requests to web server for one scenario so that its request sequence has a very small transition probability, i.e. the sequence is less correlative. At last, simulation experiments are conducted with dataset which shows the scheme is effective.}, 
keywords={Internet;probability;security of data;transport protocols;Web server;application layer based DDoS attacks;application layer distributed denial of service;bot;http request transition matrix;pattern - transition probability;users browsing behavior;Ash;Computational modeling;Computer crime;Humans;IP networks;Servers;Vectors;Application layer DDoS;Correlation analysis;Zipf}, 
doi={10.1109/ICCSNT.2011.6181964}, 
month={Dec},}
@INPROCEEDINGS{5445838, 
author={J. h. Yeh and M. l. Wu}, 
booktitle={2010 Second International Conference on Computer Engineering and Applications}, 
title={Recommendation Based on Latent Topics and Social Network Analysis}, 
year={2010}, 
volume={1}, 
pages={209-213}, 
abstract={In 2007, Netflex provided a large training dataset describing user ratings of movies for KDD Cup contest. Many competitors proposed various kinds of data mining model trying to achieve the best prediction performance. The first place winner among the competitors got the best root mean square error (RMSE) of 0.256. Most of the models applied statistical machines learning techniques with collaborative mining approach to achieve their best performance. In this paper, a hybrid recommendation model is proposed to get better prediction result which combines both content-based and collaborative recommendation approaches with latent topic discovery and social network analysis. This model was tested using 2007 KDD Cup movie dataset and found that either with single content-based approach or single collaborative approach is hard to get better RMSE result than hybrid models. By combining both kinds of approaches, the latent-topic-only approach observed in our experiment achieves only RMSE=0.274, while with Bonacich power centrality in social network get better improvement to 0.252, which proved that our model is better than all of the competitors in the contest.}, 
keywords={data mining;information filters;mean square error methods;social networking (online);Bonacich power centrality;Netflex;collaborative mining approach;hybrid recommendation model;large training dataset;latent topic discovery;recommender system;root mean square error;social network analysis;statistical machines learning techniques;Bonacich Power Centrality;Latent Dirichlet Allocation;latent topic;recommender system;social network}, 
doi={10.1109/ICCEA.2010.48}, 
month={March},}
@ARTICLE{7126952, 
author={R. Irfan and O. Khalid and M. U. S. Khan and C. Chira and R. Ranjan and F. Zhang and S. Khan and B. Veeravalli and K. Li and A. Zomaya}, 
journal={IEEE Transactions on Cloud Computing}, 
title={MobiContext: A Context-aware Cloud-based Recommendation Framework}, 
year={2015}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={In recent years, recommendation systems have seen significant evolution in the field of knowledge engineering. Most of the existing recommendation systems based their models on collaborative filtering approaches that make them simple to implement. However, performance of most of the existing collaborative filtering-based recommendation system suffers due to the challenges, such as: (a) cold start, (b) data sparseness, and (c) scalability. Moreover, recommendation problem is often characterized by the presence of many conflicting objectives or decision variables, such as users’ preferences and venue closeness. In this paper, we proposed MobiContext, a hybrid cloud-based Bi-Objective Recommendation Framework (BORF) for mobile social networks. The MobiContext utilizes multi-objective optimization techniques to generate personalized recommendations. To address the issues pertaining to cold start and data sparseness, the BORF performs data preprocessing by using the Hub-Average (HA) inference model. Moreover, the Weighted Sum Approach (WSA) is implemented for scalar optimization and an evolutionary algorithm (NSGA-II) is applied for vector optimization to provide optimal suggestions to the users about a venue. The results of comprehensive experiments on a large-scale real dataset confirm the accuracy of the proposed recommendation framework.}, 
keywords={Cloud computing;Collaboration;Computer architecture;Electronic mail;Optimization;Scalability;Venus;Collaborative Filtering (CF);Multi-objective optimization;Non-dominated Sorting Genetic Algorithm (NSGA-II)}, 
doi={10.1109/TCC.2015.2440243}, 
ISSN={2168-7161}, 
month={},}
@INPROCEEDINGS{7427438, 
author={K. Wiesneth}, 
booktitle={2016 49th Hawaii International Conference on System Sciences (HICSS)}, 
title={Evolution, Structure and Users' Attachment Behavior in Enterprise Social Networks}, 
year={2016}, 
pages={2038-2047}, 
abstract={Due to the increasing number of organizations who have started to implement Enterprise Social Networks (ESN), their network structure, which is invoked by the users and interactions among them, has gained increasing attention, both by practitioners and researchers. However, prior research has not considered how the network structure of ESN evolves over time through interactions among users. To address this lack in research we investigated two research questions: (1) How do the topological characteristics of ESN evolve in time? (2) How is users' attachment behavior characterized with respect to the creation of social relationships during the evolution of ESN? Drawing on a rich dataset comprising more than 4 years of networking and interactions, we are able to show that ESN do not evolve randomly but follow preferential attachment. Rather, our findings indicate a significant, positive correlation between users' centrality in the network and their number of new social relationships.}, 
keywords={business communication;organisational aspects;social networking (online);ESN;enterprise social networks;network structure;social relationship creation;topological characteristics;user attachment behavior;user centrality;Adaptation models;Atmospheric measurements;Collaboration;Media;Organizations;Particle measurements;Social network services}, 
doi={10.1109/HICSS.2016.257}, 
ISSN={1530-1605}, 
month={Jan},}
@INPROCEEDINGS{6615292, 
author={Zhongyun Ying and Zhurong Zhou and Fengjiao Han and Guofeng Zhu}, 
booktitle={2013 IEEE 4th International Conference on Software Engineering and Service Science}, 
title={Research on personalized web page recommendation algorithm based on user context and collaborative filtering}, 
year={2013}, 
pages={220-224}, 
abstract={Nowadays web portals contain large amount of information that is meant for various visitors or groups of visitors [4]. To effectively navigate within the content the website needs to “know” its users in order to provide personalized content to them. We proposed a personalized web page recommendation model based on user context and collaborative filtering, aimed at predicting the next request of pages that web users are potentially interested in when surfing the web. We proposed an improved Collaborative Filtering (CF) algorithm to discover the similar users' interested web page sets of the target user, based on which, a target user's Collaborative Filtering web Page Set (CFPS) is filtered. To recommend user current interested pages, we introduced context factor to match web pages in the website. And a Merge Sort Algorithm (MSA) is proposed to merge two candidate web page recommendation sets. A thorough experimental evaluation conducted on a large real dataset demonstrates the precision and the accuracy of recommendation results.}, 
keywords={Web sites;collaborative filtering;portals;recommender systems;CFPS;MSA;Web portals;collaborative filtering;collaborative filtering Web page set;merge sort algorithm;personalized Web page recommendation algorithm;user context;Collaboration;Context;Information filters;Matched filters;Ontologies;collaborative filtering;merge sort algorithm (MSA);personalized recommendation;user context}, 
doi={10.1109/ICSESS.2013.6615292}, 
ISSN={2327-0586}, 
month={May},}
@INPROCEEDINGS{7371273, 
author={S. Aminmansour and F. Maire and G. S. Larue and C. Wullems}, 
booktitle={2015 International Conference on Digital Image Computing: Techniques and Applications (DICTA)}, 
title={Improving Near-Miss Event Detection Rate at Railway Level Crossings}, 
year={2015}, 
pages={1-8}, 
abstract={Even though crashes between trains and road users are rare events at railway level crossings, they are one of the major safety concerns for the Australian railway industry. Nearmiss events at level crossings occur more frequently, and can provide more information about factors leading to level crossing incidents. In this paper we introduce a video analytic approach for automatically detecting and localizing vehicles from cameras mounted on trains for detecting near- miss events. To detect and localize vehicles at level crossings we extract patches from an image and classify each patch for detecting vehicles. We developed a region proposals algorithm for generating patches, and we use a Convolutional Neural Network (CNN) for classifying each patch. To localize vehicles in images we combine the patches that are classified as vehicles according to their CNN scores and positions. We compared our system with the Deformable Part Models (DPM) and Regions with CNN features (R-CNN) object detectors. Experimental results on a railway dataset show that the recall rate of of our proposed system is 29% higher than what can be achieved with DPM or R-CNN detectors.}, 
keywords={image sensors;neural nets;object detection;railway safety;Australian railway industry;DPM;R-CNN;cameras;convolutional neural network;deformable part models;level crossing incidents;near-miss event detection rate;railway level crossings;regions with CNN features object detectors;road users;safety concerns;vehicle detection;vehicle localization;video analytic approach;Detectors;Feature extraction;Image segmentation;Proposals;Rail transportation;Rails;Vehicles}, 
doi={10.1109/DICTA.2015.7371273}, 
month={Nov},}
@INPROCEEDINGS{6965232, 
author={S. Agarwal and D. Tomar and Siddhant}, 
booktitle={2014 International Conference on Information Systems and Computer Networks (ISCON)}, 
title={Prediction of software defects using Twin Support Vector Machine}, 
year={2014}, 
pages={128-132}, 
abstract={Considering the current scenario, the crucial need for software developer is the generous enhancement in the quality of the software product we deliver to the end user. Lifecycle models, development methodologies and tools have been extensively used for the same but the prime concern remains is the software defects that hinders our desire for good quality software. A lot of research work has been done on defect reduction, defect identification and defect prediction to solve this problem. This research work focus on defect prediction, a fairly new filed to work on. Artificial intelligence and data mining are the most popular methods researchers have been using recently. This research aims to use the Twin Support Vector Machine (TSVM) for predicting the number of defects in a new version of software product. This model gives a nearly perfect efficiency which compared to other models is far better. Twin Support Vector Machine based software defects prediction model using Gaussian kernel function obtains better performance as compare to earlier proposed approaches of software defect prediction. By predicting the defects in the new version, we thereby attempt to take a step to solve the problem of maintaining the high software quality. This proposed model directly shows its impact on the testing phase of the software product by simply plummeting the overall cost and efforts put in.}, 
keywords={Gaussian processes;data mining;program testing;software product lines;software quality;support vector machines;Gaussian kernel function;TSVM;artificial intelligence;data mining;defect identification;defect prediction;lifecycle models;software developer;software development methodologies;software product quality;software product testing;twin support vector machine based software defect prediction model;Equations;Kernel;Mathematical model;Predictive models;Software measurement;Support vector machines;CM1 dataset;Software defect;Software defects prediction;Twin Support Vector Machine}, 
doi={10.1109/ICISCON.2014.6965232}, 
month={March},}
@ARTICLE{7850970, 
author={M. Lu and C. Lai and T. Ye and J. Liang and X. Yuan}, 
journal={IEEE Transactions on Big Data}, 
title={Visual Analysis of Multiple Route Choices Based on General GPS Trajectories}, 
year={2017}, 
volume={3}, 
number={2}, 
pages={234-247}, 
abstract={There are often multiple routes between regions. Drivers choose different routes with different considerations. Such considerations, have always been a point of interest in the transportation area. Studies of route choice behaviour are usually based on small range experiments with a group of volunteers. However, the experiment data is quite limited in its spatial and temporal scale as well as the practical reliability. In this work, we explore the possibility of studying route choice behaviour based on general trajectory dataset, which is more realistic in a wider scale. We develop a visual analytic system to help users handle the large-scale trajectory data, compare different route choices, and explore the underlying reasons. Specifically, the system consists of: 1. the interactive trajectory filtering which supports graphical trajectory query; 2. the spatial visualization which gives an overview of all feasible routes extracted from filtered trajectories; 3. the factor visual analytics which provides the exploration and hypothesis construction of different factors’ impact on route choice behaviour, and the verification with an integrated route choice model. Applying to real taxi GPS dataset, we report the system’s performance and demonstrate its effectiveness with three cases.}, 
keywords={Big data;Data mining;Data visualization;Global Positioning System;Trajectory;Vehicles;Visualization;Route choice behaviour;interaction;route choice model;visual analysis}, 
doi={10.1109/TBDATA.2017.2667700}, 
month={June},}
@INPROCEEDINGS{6984562, 
author={J. Li and C. Sun and J. Lv}, 
booktitle={2014 IEEE 26th International Conference on Tools with Artificial Intelligence}, 
title={TCMF: Trust-Based Context-Aware Matrix Factorization for Collaborative Filtering}, 
year={2014}, 
pages={815-821}, 
abstract={Trust-aware recommender system (TARS) can provide more relevant recommendation and more accurate rating predictions than the traditional recommender system by taking the trust network into consideration. However, most of the trust-aware collaborative filtering approaches do not consider the influence of contextual information on rating prediction. To the opposite, context-aware matrix factorization approaches as we know do not take trust information into consideration. In this paper, we propose two Trust-based Context-aware Matrix Factorization (TCMF) approaches to fully capture the influence of trust information and contextual information on ratings. We integrate both trust information and contextual information into the baseline predictors (user bias and item bias) and user-item-context-trust interaction. Evaluations based on a real dataset and three semi-synthetic datasets demonstrate that our approaches can improve the accuracy of the trust-aware collaborative filtering and the context-aware matrix factorization models by at least 10.2% in terms of MAE.}, 
keywords={collaborative filtering;matrix decomposition;recommender systems;ubiquitous computing;MAE;TARS;TCMF;collaborative filtering;trust network;trust-aware recommender system;trust-based context-aware matrix factorization;user-item-context-trust interaction;Collaboration;Context;Context modeling;Filtering;Predictive models;Training;Vectors;collaborative filtering;context-aware;matrix factorization;recommender system;trust network;trust-based}, 
doi={10.1109/ICTAI.2014.126}, 
ISSN={1082-3409}, 
month={Nov},}
@INPROCEEDINGS{6855908, 
author={M. Pasinato and C. E. Mello and M. A. Aufaure and G. Zimbrão}, 
booktitle={2013 BRICS Congress on Computational Intelligence and 11th Brazilian Congress on Computational Intelligence}, 
title={Generating Synthetic Data for Context-Aware Recommender Systems}, 
year={2013}, 
pages={563-567}, 
abstract={Context-Aware Recommender Systems (CARS) have emerged as a different way of providing more precise and interesting recommendations through the use of data about the context in which consumers buy goods and/or services. CARS consider not only the ratings given to items by consumers (users), but also the context attributes related to these ratings. Several algorithms and methods have been proposed in the literature in order to deal with context-aware ratings. Although there are lots of proposals and approaches working for this kind of recommendation, adequate and public datasets containing user's context-aware ratings about items are limited, and usually, even these are not large enough to evaluate the proposed CARS very well. One solution for this issue is to crawl this kind of data from e-commerce websites. However, it could be very time-expensive and also complicated due to problems regarding legal rights and privacy. In addition, crawled data from e-commerce websites may not be enough for a complete evaluation, being unable to simulate all possible users' behaviors and characteristics. In this article, we propose a methodology to generate a synthetic dataset for context-aware recommender systems, enabling researchers and developers to create their own dataset according to the characteristics in which they want to evaluate their algorithms and methods. Our methodology enables researchers to define the user's behavior of giving ratings based on the Probability Distribution Function (PDF) associated to their profiles.}, 
keywords={Web sites;data handling;electronic commerce;recommender systems;statistical distributions;ubiquitous computing;CARS;PDF;context attributes;context-aware ratings;context-aware recommender systems;e-commerce Websites;probability distribution function;synthetic data generation;Computational intelligence;Context;Context modeling;Gaussian distribution;Generators;Random variables;Recommender systems;Context-Aware Recommender Systems;Datamining;Synthetic Data Generator}, 
doi={10.1109/BRICS-CCI-CBIC.2013.99}, 
ISSN={2377-0589}, 
month={Sept},}
@INPROCEEDINGS{5671417, 
author={M. Molina and E. Parodi and A. Stent}, 
booktitle={2010 22nd IEEE International Conference on Tools with Artificial Intelligence}, 
title={Combining Text and Graphics for Interactive Exploration of Behavior Datasets}, 
year={2010}, 
volume={2}, 
pages={150-155}, 
abstract={Modern sensor technologies and simulators applied to large and complex dynamic systems (such as road traffic networks, sets of river channels, etc.) produce large amounts of behavior data that are difficult for users to interpret and analyze. Software tools that generate presentations combining text and graphics can help users understand this data. In this paper we describe the results of our research on automatic multimedia presentation generation (including text, graphics, maps, images, etc.) for interactive exploration of behavior datasets. We designed a novel user interface that combines automatically generated text and graphical resources. We describe the general knowledge-based design of our presentation generation tool. We also present applications that we developed to validate the method, and a comparison with related work.}, 
keywords={graphical user interfaces;interactive systems;knowledge based systems;multimedia computing;software tools;text analysis;automatic multimedia presentation generation;behavior dataset;complex dynamic system;graphical resource;image;interactive exploration;knowledge-based design;large dynamic system;map;presentation generation tool;sensor technology;software tool;text resource;user interface;Atmospheric modeling;Graphics;Multimedia communication;Planning;Rivers;Three dimensional displays;User interfaces;intelligent user interface;interactive data exploration;multimedia presentation}, 
doi={10.1109/ICTAI.2010.96}, 
ISSN={1082-3409}, 
month={Oct},}
@INPROCEEDINGS{6932906, 
author={R. Sifa and C. Bauckhage and A. Drachen}, 
booktitle={2014 IEEE Conference on Computational Intelligence and Games}, 
title={The Playtime Principle: Large-scale cross-games interest modeling}, 
year={2014}, 
pages={1-8}, 
abstract={The collection and analysis of behavioral telemetry in digital games has in the past five years become an integral part of game development. One of the key challenges in game analytics is the development of methods for characterizing and predicting player behavior as it evolves over time. Characterizing behavior is necessary for monitoring player populations and gradually improve game design and the playing experience. Predicting behavior is necessary to describe player engagement and prevent future player churn. In this paper, methods and theory from kernel archetype analysis and random process models are utilized to evaluate the playtime behavior, i.e. time spent playing specific games as a function of time, of over 6 million players, across more than 3000 PC and console games from the Steam platform, covering a combined playtime of more than 5 billion hours. A number of conclusions can be derived from this large-scale analysis, notably that playtime as a function of time, across the thousands of games in the dataset, and irrespective of local differences in the playtime frequency distribution, can be modeled using the same model: the Weibull distribution. This suggests that there are fundamental properties governing player engagement as it evolves over time, which we here refer to as the Playtime Principle. Additionally, the analysis shows that there are distinct clusters, or archetypes, in the playtime frequency distributions of the investigated games. These archetypal groups correspond to specific playtime distributions. Finally, the analysis reveals information about player behavior across a very large dataset, showing for example that the vast majority of games are players for less than 10 hours, and very few players spend more than 30-35 hours on any specific game.}, 
keywords={behavioural sciences computing;computer games;Steam platform;Weibull distribution;behavioral telemetry;console games;cross-games interest modeling;digital games;game design;game development;kernel archetype analysis;player behavior;player engagement;playtime behavior;playtime frequency distribution;playtime principle;random process models;Games}, 
doi={10.1109/CIG.2014.6932906}, 
ISSN={2325-4270}, 
month={Aug},}
@INPROCEEDINGS{6883597, 
author={R. Pedarsani and M. A. Maddah-Ali and U. Niesen}, 
booktitle={2014 IEEE International Conference on Communications (ICC)}, 
title={Online coded caching}, 
year={2014}, 
pages={1878-1883}, 
abstract={We consider a basic content distribution scenario consisting of a single origin server connected through a shared bottleneck link to a number of users each equipped with a cache of finite memory. The users issue a sequence of content requests from a set of popular files, and the goal is to operate the caches as well as the server such that these requests are satisfied with the minimum number of bits sent over the shared link. Assuming a basic Markov model for renewing the set of popular files, we characterize approximately the optimal long-term average rate of the shared link. We further prove that the optimal online scheme has approximately the same performance as the optimal offline scheme, in which the cache contents can be updated based on the entire set of popular files before each new request. To support these theoretical results, we propose an online coded caching scheme termed coded least-recently sent (LRS) and simulate it for a demand time series derived from the dataset made available by Netflix for the Netflix Prize. For this time series, we show that the proposed coded LRS algorithm significantly outperforms the popular least-recently used (LRU) caching algorithm.}, 
keywords={Markov processes;cache storage;time series;LRS scheme;LRU caching algorithm;Markov model;Netflix;content distribution scenario;content requests;demand time series;finite memory cache;least-recently sent scheme;least-recently used caching algorithm;online coded caching scheme;Cache memory;Databases;Motion pictures;Multicast communication;Servers;Time series analysis;Vectors}, 
doi={10.1109/ICC.2014.6883597}, 
ISSN={1550-3607}, 
month={June},}
@ARTICLE{7529210, 
author={D. Apiletti and E. Baralis and T. Cerquitelli and P. Garza and D. Giordano and M. Mellia and L. Venturini}, 
journal={IEEE Transactions on Network and Service Management}, 
title={SeLINA: A Self-Learning Insightful Network Analyzer}, 
year={2016}, 
volume={13}, 
number={3}, 
pages={696-710}, 
abstract={Understanding the behavior of a network from a large scale traffic dataset is a challenging problem. Big data frameworks offer scalable algorithms to extract information from raw data, but often require a sophisticated fine-tuning and a detailed knowledge of machine learning algorithms. To streamline this process, we propose self-learning insightful network analyzer (SeLINA), a generic, self-tuning, simple tool to extract knowledge from network traffic measurements. SeLINA includes different data analytics techniques providing self-learning capabilities to state-of-the-art scalable approaches, jointly with parameter auto-selection to off-load the network expert from parameter tuning. We combine both unsupervised and supervised approaches to mine data with a scalable approach. SeLINA embeds mechanisms to check if the new data fits the model, to detect possible changes in the traffic, and to, possibly automatically, trigger model rebuilding. The result is a system that offers human-readable models of the data with minimal user intervention, supporting domain experts in extracting actionable knowledge and highlighting possibly meaningful interpretations. SeLINA's current implementation runs on Apache Spark. We tested it on large collections of real-world passive network measurements from a nationwide ISP, investigating YouTube, and P2P traffic. The experimental results confirmed the ability of SeLINA to provide insights and detect changes in the data that suggest further analyses.}, 
keywords={Big Data;Internet;data analysis;data mining;telecommunication traffic;unsupervised learning;Apache Spark;Big data frameworks;ISP;P2P traffic;SeLINA;YouTube;data analytics;data mining;human-readable models;knowledge extraction;large scale traffic dataset;machine learning;network traffic measurements;parameter tuning;self-learning insightful network analyzer;Algorithm design and analysis;Analytical models;Buildings;Clustering algorithms;Computational modeling;Data mining;Data models;Mining and statistical methods;machine learning;network data analysis}, 
doi={10.1109/TNSM.2016.2597443}, 
ISSN={1932-4537}, 
month={Sept},}
@INPROCEEDINGS{6407373, 
author={G. Loterman and C. Mues}, 
booktitle={2012 IEEE 12th International Conference on Data Mining Workshops}, 
title={Selecting Accurate and Comprehensible Regression Algorithms through Meta Learning}, 
year={2012}, 
pages={953-960}, 
abstract={Data mining tools often include a workbench of algorithms to model a given dataset but lack sufficient guidance to select the most accurate algorithm according to the nature of the dataset. The most accurate algorithm is not known in advance and no single model format is superior for all datasets. An a priori comparison experiment to determine which algorithm leads to the most optimal model fit could offer relief but is rather time consuming. A meta model which is able to predict and explain which model would fit best could address this problem. The relevance of such a meta model increases with the number and runtime of algorithms under consideration and the size of the dataset. In this paper a novel meta model is proposed to automatically provide support to the user about whether to use a linear, spline, tree, linear tree or linear spline model, given a particular dataset. This study distinguishes itself from previous meta learning studies by focusing on comprehensible regression models, regression specific dataset characteristics, artificially generated datasets and a score based user recommendation.}, 
keywords={data analysis;data mining;learning (artificial intelligence);splines (mathematics);trees (mathematics);data mining tools;linear spline model;linear tree;meta learning;regression specific dataset characteristics;score based user recommendation;Accuracy;Correlation;Data mining;Data models;Feature extraction;Prediction algorithms;Splines (mathematics);accuracy;comprehensibility;meta learning;regression}, 
doi={10.1109/ICDMW.2012.68}, 
ISSN={2375-9232}, 
month={Dec},}
@INPROCEEDINGS{7051959, 
author={C. H. Fu and C. S. Chang and D. S. Lee}, 
booktitle={Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)}, 
title={A proximity measure for link prediction in social user-item networks}, 
year={2014}, 
pages={710-717}, 
abstract={Recommendation systems based on historical action logs between users and items are usually formulated as link prediction problems for user-item bipartite networks, and such problems have been studied extensively in the literature. With the advent of on-line social networks, social interactions can also be recorded and used for predicting user's future actions. As such, the link prediction problem based on the union of a social network and a user-item bipartite network, called a social user-item network in this paper, has been a hot research topic recently. One of the key challenges for such a problem is to identify and compute an appropriate proximity (similarity) measure between two nodes in a social user-item network. To compute such a proximity measure, in this paper we propose using a random walk with two different jumping probabilities toward different neighboring nodes. Unlike the simple random walk, our method is able to assign different weights to different paths and thus can lead to a better proximity measure by optimizing the two jumping probabilities. To test our method, we conduct various experiments on the DBLP dataset [21]. With a 3-5 year training period, our method performs significantly better than random guess in terms of minimizing the root mean squared error.}, 
keywords={graph theory;mean square error methods;minimisation;probability;recommender systems;social networking (online);DBLP dataset;historical action logs;jumping probabilities;link prediction problems;neighboring nodes;proximity measure computation;random walk;recommendation systems;root mean squared error minimization;social user-item networks;user future action prediction;user-item bipartite networks;weight assignment;Collaboration;Computational modeling;Filtering;Predictive models;Social network services;Sparse matrices;Training;link prediction;personal recommendation;social networks;user-item networks}, 
doi={10.1109/IRI.2014.7051959}, 
month={Aug},}
@INPROCEEDINGS{5715212, 
author={T. C. Sakata and K. Faceli and M. C. P. d. Souto and A. C. P. L. F. d. Carvalho}, 
booktitle={2010 Eleventh Brazilian Symposium on Neural Networks}, 
title={Improvements in the Partitions Selection Strategy for Set of Clustering Solutions}, 
year={2010}, 
pages={49-54}, 
abstract={No clustering algorithm is guaranteed to find actual groups in any dataset. Thus, the selection of the most suitable clustering algorithm to be applied to a given dataset is not easy. To deal with this problem, one can apply various clustering algorithms to the dataset, generating a set of partitions (solutions). Next, one can choose the best partition generated, according to a given validation measure - such measures are usually biased towards one or more clustering algorithms. However, in many cases, it is interesting to have more than one solution. In a previous work, we proposed a selection strategy able to reduce the number of solutions obtained from Pareto-based multi-objective genetic algorithms. This selection strategy uses the correct Rand index to select a subset of the most different partitions. The size of the solutions' set is controlled by a threshold of the value of this index, given as an external parameter. The reduction of the threshold value decreases the number of solutions. Since the choice of such a threshold value is not intuitive, this paper describes a modification of the original selection algorithm that automatically adjusts this threshold and guarantees the selection of the most evident partitions, which was simultaneously obtained with distinct clustering criteria. The new version does not require any user settings, presents a better number of solutions and maintains the diversity of the partitions in the reduced set.}, 
keywords={data analysis;genetic algorithms;pattern clustering;Pareto based multiobjective genetic algorithm;Rand index;clustering algorithm;data analysis;threshold value;Clustering algorithms;Computational efficiency;Electronic mail;Glass;Indexes;Iris;Partitioning algorithms;cluster analysis;model selection}, 
doi={10.1109/SBRN.2010.17}, 
ISSN={1522-4899}, 
month={Oct},}
@INPROCEEDINGS{7557443, 
author={N. Zhang and J. Wang and K. He and Z. Li}, 
booktitle={2016 IEEE International Conference on Services Computing (SCC)}, 
title={An Approach of Service Discovery Based on Service Goal Clustering}, 
year={2016}, 
pages={114-121}, 
abstract={The increasing amount of services published on the Web makes it difficult to discover relevant services for users. Unlike the SOAP-based services that are described by structural WSDL documents, RESTful services, the most popular type of services, are mainly described using short texts. The keyword-based discovery technology for RESTful services adopted by existing service registries is insufficient to obtain accurate services according to user requirements. Moreover, it remains a difficult task for users to specify queries that perfectly reflect their requirements due to the lack of knowledge of their expected service functionalities. In this paper, we propose a goal-oriented service discovery approach, which aims to obtain accurate RESTful services for user functional goals. The approach first groups existing services into clusters using topic models. It then clusters the service goals extracted from the textual descriptions of services by leveraging the topic model trained for services. Based on the service goal clusters, our approach can help users refine their initial queries by recommending similar service goals. Finally, relevant services are obtained by matching the service goals selected by users with those of existing services. Experiments conducted on a real-world service dataset crawled from ProgrammableWeb show the effectiveness of the proposed approach.}, 
keywords={Internet;document handling;service-oriented architecture;ProgrammableWeb;Web;goal oriented service discovery approach;service functionalities;service goal clustering;structural WSDL documents;textual descriptions;Algorithm design and analysis;Computational modeling;Computers;Data models;Semantics;Skeleton;Web services;RESTful services;Service discovery;clustering;service goal;topic model}, 
doi={10.1109/SCC.2016.22}, 
month={June},}
@INPROCEEDINGS{6683904, 
author={J. Zeng and B. Plale}, 
booktitle={2013 IEEE 9th International Conference on e-Science}, 
title={Data Pipeline in MapReduce}, 
year={2013}, 
pages={164-171}, 
abstract={MapReduce is an effective programming model for large scale text and data analysis. Traditional MapReduce implementation, e.g., Hadoop, has the restriction that before any analysis can take place, the entire input dataset must be loaded into the cluster. This can introduce sizable latency when the data set is large, and when it is not possible to load the data once, and process many times - a situation that exists for log files, health records and protected texts for instance. We propose a data pipeline approach to hide data upload latency in MapReduce analysis. Our implementation, which is based on Hadoop MapReduce, is completely transparent to user. It introduces a distributed concurrency queue to coordinate data block allocation and synchronization so as to overlap data upload and execution. The paper overcomes two challenges: a fixed number of maps scheduling and dynamic number of maps scheduling allows for better handling of input data sets of unknown size. We also employ delay scheduler to achieve data locality for data pipeline. The evaluation of the solution on different applications on real world data sets shows that our approach shows performance gains.}, 
keywords={data analysis;distributed processing;pipeline processing;Hadoop MapReduce;MapReduce analysis;MapReduce implementation;data analysis;data block allocation;data pipeline;data set;health records;log files;programming model;protected texts;Concurrent computing;Delays;Distributed databases;Dynamic scheduling;Equations;Pipelines;Schedules;MapReduce;data pipeline}, 
doi={10.1109/eScience.2013.21}, 
month={Oct},}
@INPROCEEDINGS{7410269, 
author={Jianwei Niu and Danning Wang and Jie Lu}, 
booktitle={2015 IEEE 34th International Performance Computing and Communications Conference (IPCCC)}, 
title={Mining friendships through spatial-temporal features in mobile social networks}, 
year={2015}, 
pages={1-8}, 
abstract={With the rapid popularization of smartphones and tablets, there are thousands of applications based on mobile social networks. The big data from these networks provide a huge potential to shed light on the mobility patterns of users. These big data enable a deeper understanding of users' preferences and behaviors and will help us mine users' friendship in both physical and digital worlds. In this paper, we firstly divide user mobility patterns into different categories to portray the characteristics of user encounter more precisely. Then, with combining proximity data from bluetooth devices and location data from cellular towers, we introduce a set of spatial-temporal features, including the encounter entropy, which measures the probability of encounters between different mobile users. Using these spatial-temporal features, we provide a novel model to infer user friendship by analyzing the social context of users and their encounters. To address the class imbalance problem in the dataset and improve the prediction accuracy of friendship, we employ the sampling method and evaluate our model with three different classifiers. The experimental results show that our encounter entropy feature has a striking effect to infer user friendship, and our model based on these spatial-temporal features can achieve pretty good accuracy in predicting friendship over real human mobility traces without privacy-sensitive information disclosure.}, 
keywords={Big Data;Bluetooth;cellular radio;data mining;entropy;mobile computing;probability;social networking (online);spatiotemporal phenomena;Bluetooth devices;big data;cellular towers;class imbalance problem;encounter probability;entropy;friendship mining;location data;mobile social networks;prediction accuracy improvement;proximity data;spatial-temporal features;user behaviors;user mobility patterns;user preferences;Bluetooth;Entropy;Mobile communication;Poles and towers;Smart phones;Social network services;friendship inferring;mobile social networks;proximity and location data;spatial-temporal features}, 
doi={10.1109/PCCC.2015.7410269}, 
month={Dec},}
@INPROCEEDINGS{6113108, 
author={Z. Borbora and J. Srivastava and K. W. Hsu and D. Williams}, 
booktitle={2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and 2011 IEEE Third International Conference on Social Computing}, 
title={Churn Prediction in MMORPGs Using Player Motivation Theories and an Ensemble Approach}, 
year={2011}, 
pages={157-164}, 
abstract={In this paper, we investigate the problem of churn prediction in Massively multiplayer online role-playing games (MMORPGs) from a social science perspective and develop models incorporating theories of player motivation. The ability to predict player churn can be a valuable resource to game developers designing customer retention strategies. The results from our theory-driven model significantly outperform a diffusion-based churn prediction model on the same dataset. We describe the synthesis between a theory-driven approach and a data-driven approach to a problem and examine the trade-offs involved between the two approaches in terms of prediction accuracy, interpretability and model complexity. We observe that even though the theory-driven model is not as accurate as the data-driven one, the theory-driven model itself can be more interpretable to the domain experts and hence, more preferable over a complex data-driven model. We perform lift analysis of the two models and find that if a marketing effort is restricted in the number of customers it can contact, the theory-driven model would offer much better return-on-investment by identifying more customers among that restricted set who have the highest probability of churn. Finally, we use a clustering technique to partition the dataset and then build an ensemble on the partitioned dataset for better performance. Experiment results show that the ensemble performs notably better than the single classifier in terms of its recall value, which is a highly desirable property in the churn prediction problem.}, 
keywords={computer games;human factors;probability;social sciences;MMORPG;churn prediction;clustering technique;customer retention strategies;interpretability;lift analysis;massively multiplayer online role-playing games;model complexity;player motivation;prediction accuracy;probability;return-on-investment;social science;theory-driven model;Analytical models;Complexity theory;Data mining;Games;Predictive models;Social network services;Training;MMORPGs;churn prediction;machine learning models}, 
doi={10.1109/PASSAT/SocialCom.2011.122}, 
month={Oct},}
@INPROCEEDINGS{7502905, 
author={D. Apiletti and E. Baralis and T. Cerquitelli and P. Garza and L. Venturini}, 
booktitle={NOMS 2016 - 2016 IEEE/IFIP Network Operations and Management Symposium}, 
title={SaFe-NeC: A scalable and flexible system for network data characterization}, 
year={2016}, 
pages={812-816}, 
abstract={Nowadays, large volumes of data and measurements are being continuously generated by computer and telecommunication networks, but such volumes make it difficult to extract meaningful knowledge from them. This paper presents SaFe-NeC, an innovative methodology for analyzing network traffic by exploiting data mining techniques, i.e. clustering and classification algorithms, focusing on self-learning capabilities of state-of-the-art scalable approaches. Self-learning algorithms, coupled with self-assessment indicators and domain-driven semantics enriching data mining results, are able to build a model of the data with minimal user intervention and highlight possibly meaningful interpretations to domain experts. Furthermore, a self-evolving model evaluation phase is included to continuously track the quality degradation of the model itself, whose rebuilding is triggered as soon as quality indicators fall below a threshold of tolerance. The proposed methodology can exploit the computational advantages of distributed computing frameworks, as the current implementation runs on Apache Spark. Preliminary experimental results on a real traffic dataset show the full potential of the proposed methodology to characterize network traffic data.}, 
keywords={data mining;pattern classification;Apache Spark;SaFe-NeC;classification algorithms;data mining techniques;distributed computing frameworks;domain-driven semantics;flexible system;innovative methodology;network data characterization;network traffic data;quality degradation;real traffic dataset;self-assessment indicators;self-evolving model evaluation phase;self-learning algorithms;self-learning capabilities;telecommunication networks;user intervention;Clustering algorithms;Computers;Data analysis;Data mining;Data models;Real-time systems;Semantics}, 
doi={10.1109/NOMS.2016.7502905}, 
month={April},}
@INPROCEEDINGS{7836565, 
author={T. J. Skluzacek and K. Chard and I. Foster}, 
booktitle={2016 1st Joint International Workshop on Parallel Data Storage and data Intensive Scalable Computing Systems (PDSW-DISCS)}, 
title={Klimatic: A Virtual Data Lake for Harvesting and Distribution of Geospatial Data}, 
year={2016}, 
pages={31-36}, 
abstract={Many interesting geospatial datasets are publicly accessible on web sites and other online repositories. However, the sheer number of datasets and locations, plus a lack of support for cross-repository search, makes it difficult for researchers to discover and integrate relevant data. We describe here early results from a system, Klimatic, that aims to overcome these barriers to discovery and use by automating the tasks of crawling, indexing, integrating, and distributing geospatial data. Klimatic implements a scalable crawling and processing architecture that uses an elastic container-based model to locate and retrieve relevant datasets and to extract metadata from headers and within files to build a global index of known geospatial data. In so doing, we create an expansive geospatial virtual data lake that records the location, formats, and other characteristics of large numbers of geospatial datasets while also caching popular data subsets for rapid access. A flexible query interface allows users to request data that satisfy supplied type, spatial, temporal, and provider specifications; in processing such queries, the system uses interpolation and aggregation to combine data of different types, data formats, resolutions, and bounds. Klimatic has so far incorporated more than 10,000 datasets from over 120 sources and has been demonstrated to scale well with data size and query complexity.}, 
keywords={Web sites;data handling;geographic information systems;Klimatic;Web sites;cross repository search;data size;elastic container;geospatial data distribution;geospatial datasets;global index;metadata;online repositories;query complexity;relevant dataset retrieval;virtual data lake;Crawlers;Geospatial analysis;Indexing;Lakes;Metadata;Meteorology}, 
doi={10.1109/PDSW-DISCS.2016.010}, 
month={Nov},}
@INPROCEEDINGS{6295984, 
author={L. Liu and X. Jin and G. Min and L. Xu}, 
booktitle={2012 IEEE 11th International Conference on Trust, Security and Privacy in Computing and Communications}, 
title={Real-Time Diagnosis of Network Anomaly Based on Statistical Traffic Analysis}, 
year={2012}, 
pages={264-270}, 
abstract={Distributed Denial-of-Service (DDoS) attacks are critical threats to both network service providers and legitimate network users. DDoS attacks often overwhelm or exhaust the resources of victims and typically result in abnormal bursty traffic passing through victim systems. In this paper, we develop a mechanism for diagnosing traffic anomalies caused by DDoS attacks on the basis of analyzing the behaviour of network traffic. The traffic in communication networks has been shown to exhibit statistical self-similar phenomena that can be characterized by the so-called Hurst parameter. Therefore, in the proposed mechanism the Hurst parameter coupled by variance and autocorrelation are employed as the key performance metrics to spot the anomalies of network traffic. The proposed diagnosis mechanism is validated through experiments where the datasets consist of two groups. The first group is obtained from the MIT Lincoln Laboratory DOS attack dataset. The second group is collected from our DDoS attack simulation experiments, which cover three representative traffic shapes resulting from three different DDoS attack behaviours, namely, constant intensity, ramp-up behaviour and pulse behaviour. The experimental results show that the developed mechanism can alert the DDoS attack schemes within short respond time.}, 
keywords={computer network security;statistical analysis;telecommunication traffic;DDoS attack behaviours;DDoS attack simulation experiments;Hurst parameter;MIT Lincoln Laboratory DOS attack dataset;abnormal bursty traffic;constant intensity;distributed denial-of-service attacks;network traffic anomaly;network traffic behaviour analysis;pulse behaviour;ramp-up behaviour;real-time diagnosis;statistical self-similar phenomena;statistical traffic analysis;Computer crime;Correlation;Data models;Servers;Standards;Switches;Telecommunication traffic;Anomaly diagnosis;DDoS attack;Intrusion detection;Traffic measurement}, 
doi={10.1109/TrustCom.2012.233}, 
ISSN={2324-898X}, 
month={June},}
@INPROCEEDINGS{5652081, 
author={S. Fritz and I. McCallum and L. See and F. Kraxner and M. Obersteiner}, 
booktitle={2010 IEEE International Geoscience and Remote Sensing Symposium}, 
title={Comparison of global land cover products: community remote sensing to validate areas of high disagreement}, 
year={2010}, 
pages={3740-3743}, 
abstract={Maps of global land cover derived from satellite-based earth observation have existed for almost two decades and represent one of the most important sources of baseline terrestrial information for a wide variety of users, e.g. the Convention on Biological Diversity. More importantly, land cover maps provide critical input data for global models of land use and land use changes. Urgent questions have arisen that depend upon an accurate global land cover dataset, e.g. how much land is available for agricultural use or how high will competition for land be between food and bioenergy, considering increasing needs in the future. Some of these questions could be answered if a global baseline map of land cover would exist. However, at present, a unified and satisfactory solution has not surfaced, owing in part to large disagreements among existing global land cover datasets. This paper compares the three most recent global land cover products, namely GLC-2000, GlobCover, and MODIS. Moreover, it presents a methodology for comparing global land cover maps that allows for differences in legend definitions as well as different spatial resolution between products to be taken into account.}, 
keywords={ecology;geophysics computing;terrain mapping;vegetation mapping;visual databases;GLC-2000;GlobCover;MODIS;bioenergy;biological diversity;community remote sensing;food;global baseline map;global land cover maps;land use changes;satellite-based Earth observation system;Agriculture;Communities;Earth;Estimation;Image resolution;MODIS;Remote sensing}, 
doi={10.1109/IGARSS.2010.5652081}, 
ISSN={2153-6996}, 
month={July},}
@INPROCEEDINGS{5687063, 
author={G. Castellano and D. Dell'Agnello and A. M. Fanelli and C. Mencar and M. A. Torsello}, 
booktitle={2010 10th International Conference on Intelligent Systems Design and Applications}, 
title={A competitive learning strategy for adapting fuzzy user profiles}, 
year={2010}, 
pages={959-964}, 
abstract={In recommender systems, the task of automatically deriving user profiles, encoding the actual preferences of users, covers a fundamental role. In this paper, we propose a strategy for learning and updating user profiles by using fuzzy sets that reveal to be a valid tool to model the vague and imprecise nature of preferences as well as the items to be recommended. The proposed adaptation strategy resembles a competitive learning process in which the user profile is continuously updated in order to make its components as similar as possible to the description of the accessed items. On the same time a mechanism to forget outdated user preferences is proposed in order to describe changes in user interests over time. The strategy was applied on the MovieLens dataset and the obtained results show its effectiveness to learn user profiles reflecting the current preferences of users.}, 
keywords={fuzzy set theory;learning (artificial intelligence);recommender systems;MovieLens dataset;competitive learning strategy;fuzzy sets;fuzzy user profiles;recommender systems}, 
doi={10.1109/ISDA.2010.5687063}, 
ISSN={2164-7143}, 
month={Nov},}
@INPROCEEDINGS{6813944, 
author={K. Zhan and S. Faux and F. Ramos}, 
booktitle={2014 IEEE International Conference on Pervasive Computing and Communications (PerCom)}, 
title={Multi-scale Conditional Random Fields for first-person activity recognition}, 
year={2014}, 
pages={51-59}, 
abstract={We propose a novel pervasive system to recognise human daily activities from a wearable device. The system is designed in a form of reading glasses, named `Smart Glasses', integrating a 3-axis accelerometer and a first-person view camera. Our aim is to classify user's activities of daily living (ADLs) based on both vision and head motion data. This ego-activity recognition system not only allows caretakers to track on a specific person (such as patient or elderly people), but also has the potential to remind/warn people with cognitive impairments of hazardous situations. We present the following contributions in this paper: a feature extraction method from accelerometer and video; a classification algorithm integrating both locomotive (body motions) and stationary activities (without or with small motions); a novel multi-scale dynamic graphical model structure for structured classification over time. We collect, train and validate our system on a large dataset containing 20 hours of ADLs data, including 12 daily activities under different environmental settings. Our method improves the classification performance (F-Score) of conventional approaches from 43.32%(video features) and 66.02%(acceleration features) by an average of 20-40% to 84.45%, with an overall accuracy of 90.04% in realistic ADLs.}, 
keywords={accelerometers;cameras;feature extraction;geriatrics;image classification;image sequences;medical image processing;mobile computing;object recognition;smart phones;video signal processing;3-axis accelerometer;ADL;F-score;activities-of-daily living;classification algorithm;classification performance;cognitive impairments;ego-activity recognition system;feature extraction method;first-person activity recognition;first-person view camera;head motion data;locomotive activities;multiscale conditional random fields;multiscale dynamic graphical model structure;pervasive system;reading glasses;smart glasses;stationary activities;vision data;Acceleration;Accelerometers;Feature extraction;Graphical models;Legged locomotion;Sensors;Vectors}, 
doi={10.1109/PerCom.2014.6813944}, 
month={March},}
@INPROCEEDINGS{7783919, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={Towards Interactive Gathering of DUV-Based Dataset Usage Information}, 
year={2016}, 
pages={163-168}, 
abstract={Although many open data portals have been publishing numerous datasets on the Web, there is currently no clear standard way to describe dataset usage on the Web, which is not conducive to the healthy development of the open data ecosystem. The W3C Data on the Web Best Practices Working Group is therefore standardizing the Dataset Usage Vocabulary (DUV) for modeling, conveying and sharing dataset usage information on the Web, aiming at facilitating better communication between data publishers and consumers and promoting the re-use of Web-published data. Despite the significant progress in the standardization, there is a lack of systematic research on approaches and tools for gathering DUV-based dataset usage information. This paper therefore proposes a generic software Framework for Interactively Gathering DUV-based Dataset Usage Information (FIGDUI) on the Web. FIGDUI can provide both data publishers and consumers (collectively referred to as the user) with a friendly user interface designed according to DUV's Citation Model, Usage Model and Feedback Model, and employ automatic algorithms to convert the user-entered data into dataset usage data in a machine-readable RDF format and to publish the RDF data as Linked Open Data on the Web. Our prototype implementation of FIGDUI and preliminary experimental results show that FIGDUI is feasible and implementable.}, 
keywords={Dataset Usage Vocabulary (DUV);Linked Open Data (LOD);dataset usage information;information gathering;user interface design}, 
doi={10.1109/WISA.2016.12}, 
month={Sept},}
@ARTICLE{7010434, 
author={I. Arnaldo and K. Veeramachaneni and A. Song and U. M. O'Reilly}, 
journal={IEEE Computational Intelligence Magazine}, 
title={Bring Your Own Learner: A Cloud-Based, Data-Parallel Commons for Machine Learning}, 
year={2015}, 
volume={10}, 
number={1}, 
pages={20-32}, 
abstract={We introduce FCUBE, a cloud-based framework that enables machine learning researchers to contribute their learners to its community-shared repository. FCUBE exploits data parallelism in lieu of algorithmic parallelization to allow its users to efficiently tackle large data problems automatically. It passes random subsets of data generated via resampling to multiple learners that it executes simultaneously and then it combines their model predictions with a simple fusion technique. It is an example of what we have named a Bring Your Own Learner model. It allows multiple machine learning researchers to contribute algorithms in a plug-and-play style. We contend that the Bring Your Own Learner model signals a design shift in cloud-based machine learning infrastructure because it is capable of executing anyone's supervised machine learning algorithm. We demonstrate FCUBE executing five different learners contributed by three different machine learning groups on a 100 node deployment on Amazon EC2. They collectively solve a publicly available classification problem trained with 11 million exemplars from the Higgs dataset.}, 
keywords={cloud computing;data handling;learning (artificial intelligence);parallel processing;FCUBE;Higgs dataset;bring your own learner model;community shared repository;data parallel commons;data problems;fusion technique;supervised machine learning algorithm;Algorithm design and analysis;Classification;Cloud computing;Data models;Machine learning algorithms;Parallel processing;Predictive models}, 
doi={10.1109/MCI.2014.2369892}, 
ISSN={1556-603X}, 
month={Feb},}
@INPROCEEDINGS{7816874, 
author={A. D. Salve and B. Guidi and P. Mori and L. Ricci}, 
booktitle={2016 Intl IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)}, 
title={Distributed Coverage of Ego Networks in F2F Online Social Networks}, 
year={2016}, 
pages={423-431}, 
abstract={Although most online social networks rely on a centralized infrastructure, several proposals of Distributed Online Social Networks (DOSNs) have been recently presented. Since in DOSNs user profiles are stored on the peers of the users belonging to the network, one of the main challenges comes from guaranteeing the profile availability when the owner of the data is not online. In this paper, we propose a DOSN based on a friend-to-friend P2P overlay where the user's data is stored only on friend peers. Our approach is based on the ego-network concept, which models the social network from the local point of view of a single user. We propose a distributed algorithm which is based on the notion of coverage of the ego-network, assures that users store their data only on the peers of their friends,, that each online user can retrieve the private data of its offline friends through a common online friend. We formalize this as a Neighbour Dominating Set problem. A set of experimental results conducted on real Facebook dataset show the effectiveness of our approach.}, 
keywords={information retrieval;security of data;social networking (online);DOSN user profiles;F2F online social networks;distributed coverage;distributed online social networks;ego networks;friend-to-friend P2P;neighbour dominating set problem;online user;private data;user data;Cryptography;Data privacy;Distributed databases;Peer-to-peer computing;Privacy;Servers;Social network services;Data Availability;Distributed Online Social Networks;P2P;friend-to-friend networks}, 
doi={10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0078}, 
month={July},}
@INPROCEEDINGS{7756123, 
author={S. Li and D. Yan and X. Li and A. Hao and H. Qin}, 
booktitle={2016 International Conference on Cyberworlds (CW)}, 
title={Detail-Preserving 3D Shape Modeling from Raw Volumetric Dataset via Hessian-Constrained Local Implicit Surfaces Optimization}, 
year={2016}, 
pages={25-32}, 
abstract={Massive routinely-acquired raw volumetric datasets are hard to be deeply exploited by cyber worlds related downstream applications due to the challenges in accurate and efficient shape modeling. This paper systematically advocates an interactive 3D shape modeling framework for raw volumetric datasets by iteratively optimizing Hessian-constrained local implicit surfaces. The key idea is to incorporate contour based interactive segmentation into the generalized local implicit surface reconstruction. Our framework allows a user to flexibly define derivative constraints up to the second order via intuitively placing contours on the cross sections of volumetric images and fine-tuning the eigenvector frame of Hessian matrix. It enables detail-preserving local implicit representation while combating certain difficulties due to ambiguous image regions, low-quality irregular data, close sheets, and massive coefficients involved extra computing burden. Moreover, we conduct extensive experiments on some volumetric images with blurry object boundaries, and make comprehensive, quantitative performance evaluation between our method and the state-of-the-art radial basis function based techniques. All the results demonstrate our method's advantages in the accuracy, detail-preserving, efficiency, and versatility of shape modeling.}, 
keywords={Hessian matrices;eigenvalues and eigenfunctions;image representation;image segmentation;interactive systems;iterative methods;Hessian matrix;blurry object boundaries;close sheets;detail-preserving 3D shape modeling;detail-preserving local implicit representation;eigenvector frame;generalized local implicit surface reconstruction;image regions;interactive 3D shape modeling;interactive segmentation;iterative Hessian-constrained local implicit surface optimization;low-quality irregular data;massive routinely-acquired raw volumetric datasets;quantitative performance evaluation;radial basis function based techniques;Image reconstruction;Image segmentation;Rough surfaces;Shape;Solid modeling;Surface reconstruction;Three-dimensional displays;3D shape modeling;Hessian constraints;implicit surfaces;raw volumetric dataset}, 
doi={10.1109/CW.2016.12}, 
month={Sept},}
@INPROCEEDINGS{7347679, 
author={Min Lu and Chufan Lai and Tangzhi Ye and Jie Liang and Xiaoru Yuan}, 
booktitle={2015 IEEE Conference on Visual Analytics Science and Technology (VAST)}, 
title={Visual analysis of route choice behaviour based on GPS trajectories}, 
year={2015}, 
pages={203-204}, 
abstract={There are often multiple routes between regions. Many factors potentially affect driver's route choice, such as expected time cost, length etc. In this work, we present a visual analysis system to explore driver's route choice behaviour based on taxi GPS trajectory data. With interactive trajectory filtering, the system constructs feasible routes between regions of interest. Using a rank-based visualization, the attributes of multiple routes are explored and compared. Based on a statistical model, the system supports to verify trajectory-related factors' impact on route choice behaviour. The effectiveness of the system is demonstrated by applying to real trajectory dataset.}, 
keywords={Global Positioning System;behavioural sciences computing;data visualisation;driver information systems;interactive systems;statistical analysis;vehicle routing;driver route choice behaviour;interactive trajectory filtering;multiple route attributes;rank-based visualization;real trajectory dataset;regions-of-interest;statistical model;taxi GPS trajectory data;visual analysis system;Global Positioning System;Public transportation;Roads;Trajectory;Vehicles;Visual analytics;H.5.2 [Information Interfaces and Presentation]: Graphical User Interfaces(GUI)-;I.3.6 [Computer Graphics]: Methodology and Interaction Techniques-}, 
doi={10.1109/VAST.2015.7347679}, 
month={Oct},}
@INPROCEEDINGS{6861046, 
author={N. Moseley and C. O. Alm and M. Rege}, 
booktitle={2014 IEEE Eighth International Conference on Research Challenges in Information Science (RCIS)}, 
title={User-annotated microtext data for modeling and analyzing users' sociolinguistic characteristics and age grading}, 
year={2014}, 
pages={1-6}, 
abstract={Information from Twitter messages have become an important area for research in computational analysis of natural language. As yet, much latent user attribute analysis on Twitter is unexplored. One reason is that only few latent attributes are explicitly defined by users on Twitter. This work presents and analyzes a data set annotated by Twitter users themselves for age and other useful attributes for use in latent attribute inference applications. We report on statistical analysis of the collected latent attributes and tweet information using association mining.}, 
keywords={Internet;age issues;computational linguistics;data analysis;data mining;inference mechanisms;natural language processing;social networking (online);statistical analysis;text analysis;Internet;Twitter messages;abbreviation transformations;age grading;association mining;computational natural language analysis;data set analysis;latent attribute inference applications;latent user attribute analysis;statistical analysis;user sociolinguistic characteristics modeling;user-annotated microtext data;users sociolinguistic characteristics analysis;Algorithm design and analysis;Analytical models;Medical services;Abbreviation Transformations;Association Mining;Latent User Annotation;Microblog Dataset}, 
doi={10.1109/RCIS.2014.6861046}, 
ISSN={2151-1349}, 
month={May},}
@ARTICLE{6837425, 
author={H. Jenny and J. Liem and M. S. Lucash and R. M. Scheller}, 
journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 
title={4-D Statistical Surface Method for Visual Change Detection in Forest Ecosystem Simulation Time Series}, 
year={2014}, 
volume={7}, 
number={11}, 
pages={4505-4511}, 
abstract={Rising uncertainties associated with climate change compel forest management planning to include forest ecosystem simulations. The output of such models is often of high spatio-temporal complexity and difficult to interpret for the user. This contribution describes a novel visualization method called four-dimensional (4-D) statistical surfaces, which aims at improving the visual detection of change in time series. The method visualizes attribute values as surfaces, which are interpolated and animated over time; the interactive attribute surfaces are combined with color-coding and contour lines to support absolute and relative height judgment as well as faster perception and better location of change. A design study and prototypical implementation of the visualization method is described in this contribution. Time-series simulation results of LANDIS-II, a commonly used modeling tool in forest ecology, as well as a temporal vegetation index dataset (NDVI) are visualized using 4-D statistical surfaces. Usability challenges are addressed based on explorative interviews with a small group of users. The method is not limited to ecological model output; it can be used to create three-dimensional (3-D) temporal animations of arbitrary time-series datasets where parameters are supplied in regular raster format.}, 
keywords={climatology;ecology;geophysical techniques;statistical analysis;time series;vegetation;4-D statistical surface method;LANDIS-II time-series simulation result;NDVI;absolute height judgment;arbitrary time-series dataset 3D temporal animation;climate change uncertainty;color-coding line;contour line;ecological model output;faster perception;forest ecology modeling tool;forest ecosystem simulation;forest ecosystem simulation time series;forest management planning;four-dimensional statistical surface;high spatiotemporal complexity;interactive attribute surface;novel visualization method;regular raster format;relative height judgment;surface value attribute visualization method;temporal vegetation index dataset;three-dimensional temporal animation;time series change visual detection;visual change detection;visualization method prototypical implementation;Animation;Biological system modeling;Data visualization;Ecosystems;Image color analysis;Meteorology;Visualization;Forestry;simulation software;time-series animation;visualization}, 
doi={10.1109/JSTARS.2014.2324972}, 
ISSN={1939-1404}, 
month={Nov},}
@ARTICLE{7782751, 
author={A. Pezeshk and N. Petrick and W. Chen and B. Sahiner}, 
journal={IEEE Transactions on Medical Imaging}, 
title={Seamless Lesion Insertion for Data Augmentation in CAD Training}, 
year={2017}, 
volume={36}, 
number={4}, 
pages={1005-1015}, 
abstract={The performance of a classifier is largely dependent on the size and representativeness of data used for its training. In circumstances where accumulation and/or labeling of training samples is difficult or expensive, such as medical applications, data augmentation can potentially be used to alleviate the limitations of small datasets. We have previously developed an image blending tool that allows users to modify or supplement an existing CT or mammography dataset by seamlessly inserting a lesion extracted from a source image into a target image. This tool also provides the option to apply various types of transformations to different properties of the lesion prior to its insertion into a new location. In this study, we used this tool to create synthetic samples that appear realistic in chest CT. We then augmented different size training sets with these artificial samples, and investigated the effect of the augmentation on training various classifiers for the detection of lung nodules. Our results indicate that the proposed lesion insertion method can improve classifier performance for small training datasets, and thereby help reduce the need to acquire and label actual patient data.}, 
keywords={computerised tomography;lung;mammography;medical image processing;CAD training;artificial samples;chest CT;classifier performance;data augmentation;image blending tool;lesion insertion method;lung nodule detection;mammography dataset;medical applications;size training sets;source image;target image;training datasets;training sample accumulation;training sample labeling;Biomedical imaging;Labeling;Lesions;Lungs;Shape;Solid modeling;Training;Computer aided diagnosis;data augmentation;poisson editing;pulmonary nodules}, 
doi={10.1109/TMI.2016.2640180}, 
ISSN={0278-0062}, 
month={April},}
@INPROCEEDINGS{7806326, 
author={S. Alias and S. K. Mohammad and G. K. Hoon and T. T. Ping}, 
booktitle={2016 Third International Conference on Information Retrieval and Knowledge Management (CAMP)}, 
title={A Malay text summarizer using pattern-growth method with sentence compression rules}, 
year={2016}, 
pages={7-12}, 
abstract={A text summary is a condensed representation of text where salient information is extracted with the purpose to ease users' readability. However, if the summary was extracted “verbatim” from its source, the sentence may contain inessential information along with salient information that may effect on the overall coherence in the summary generation. The purpose of Sentence Compression in Text Summarization is to produce a compact and informational content by eliminating unnecessary constituent in a sentence. We introduce a Pattern-Growth text representation model named Frequent Adjacent Sequential Pattern (FASP) and Frequent Eliminated Pattern (FASPe) to represent the text using a set of sequence adjacent words or “textual pattern” that are frequently used and eliminated across the Malay news document collection. From the discovered textual pattern, we derived some heuristic Sentence Compression Rules in generating compressed sentences to construct a single extract Malay summary. We conducted experiments on Malay news dataset and the result demonstrates that moderate compressed summary using Sentence Compression Rules has better agreement with human composed summary.}, 
keywords={information resources;natural language processing;text analysis;FASP;FASPe;Malay news document collection;Malay text summarizer;frequent adjacent sequential pattern;frequent eliminated pattern;heuristic sentence compression rules;pattern-growth method;pattern-growth text representation model;salient information extraction;summary generation;text summarization;textual pattern;user readability;verbatim;Data mining;Databases;Information retrieval;Knowledge management;Pragmatics;Semantics;Syntactics;Malay;Pattern-Growth;Sentence Compression;Text Summarization;component}, 
doi={10.1109/INFRKM.2016.7806326}, 
month={Aug},}
@INPROCEEDINGS{7840782, 
author={V. P. Barros and P. Notargiacomo}, 
booktitle={2016 IEEE International Conference on Big Data (Big Data)}, 
title={Big data analytics in cloud gaming: Players' patterns recognition using artificial neural networks}, 
year={2016}, 
pages={1680-1689}, 
abstract={The Cloud Gaming model emerges with the evolution of the Cloud Computing and communication technologies. Through smartphones, PCs, tablets, consoles and other devices, people can access and use games on demand via data streaming, regardless the computing power of these devices. The Internet is the fundamental way of communication between the device and the game, which is hosted on a environment known as Cloud, enabling a large scale offer. The variety, volume, velocity, value and veracity (Big Data 5Vs) of data that is involved in these Cloud environments exceed the limits of analysis and manipulation of conventional tools, therefore, Big Data platforms are required to handle and interpret this data. The model known as Big Data Analytics is an effective and capable way to, not only work with these data, but understand its meaning, providing inputs for assertive analysis and predictive actions. A method is presented in this study to identify and analyze players' patterns in a virtual environment. With this information, it is possible to optimize user experience, revenue for developers and raise the level of control over the environment. Results are presented based on a dataset of the World of Warcraft game. By using a neural network, it was possible to identify with an average of 91% of accuracy the players' assiduity based on patterns in the game. Using the Hadoop technology and visualization tools on a Cloud based cluster, it was possible to map and identify the players' behaviors as well as their gameplay patterns.}, 
keywords={Big Data;cloud computing;computer games;data visualisation;neural nets;Big Data analytics;Hadoop technology;Internet;artificial neural networks;cloud based cluster;cloud computing;cloud gaming model;visualization tools;Analytical models;Avatars;Big data;Cloud gaming;Neural networks;Artificial Neural Networks;Big Data Analytics;Cloud Gaming;Pattern Recognition}, 
doi={10.1109/BigData.2016.7840782}, 
month={Dec},}
@INPROCEEDINGS{7816647, 
author={H. Ziak and R. Kern}, 
booktitle={2016 27th International Workshop on Database and Expert Systems Applications (DEXA)}, 
title={Query Splitting for Context-Driven Federated Recommendations}, 
year={2016}, 
pages={193-197}, 
abstract={Context-driven query extraction for content-based recommender systems faces the challenge of dealing with queries of multiple topics. In contrast to manually entered queries, for automatically generated queries this is a more frequent problem. For instances if the information need is inferred indirectly viathe user's current context. Especially for federated search systems were connected knowledge sources might react vastly differently on such queries, an algorithmic way how to deal with such queries is of high importance. One such method is to split mixed queries into their individual subtopics. To gain insight how a multi topic query can be split into its subtopics we conducted an evaluation where we compared a naive approach against amore complex approaches based on word embedding techniques: One created using Word2Vec and one created using GloVe. To evaluate these two approaches we used the Webis-QSeC-10 query set, consisting of about 5,000 multi term queries. Queries of this set were concatenated and passed through the algorithms with the goal to split those queries again. Hence the naive approach is splitting the queries into several groups, according to the amount of joined queries, assuming the topics are of equal query term count. In the case of the Word2Vec and GloVe based approaches we relied on the already pre-trained datasets. The Google News model and a model trained with a Wikipedia dump and theEnglish Gigaword newswire text archive. The out of this datasets resulting query term vectors were grouped into subtopics usinga k-Means clustering. We show that a clustering approach based on word vectors achieves better results in particular when the query is not in topical order. Furthermore we could demonstrate the importance of the underlying dataset.}, 
keywords={information needs;query processing;recommender systems;English Gigaword newswire text archive;GloVe;Google News model;Webis-QSeC-10 queryset;Wikipedia dump;Word2Vec;content-based recommender systems;context-driven federated recommendations;context-driven query extraction;federated search systems;information need;multiterm queries;multitopic query;query splitting;query term vectors;word vectors;Context;Cultural differences;Estimation;Google;Knowledge based systems;Metasearch;Recommender systems}, 
doi={10.1109/DEXA.2016.049}, 
month={Sept},}
@ARTICLE{7349214, 
author={Z. Fu and K. Ren and J. Shu and X. Sun and F. Huang}, 
journal={IEEE Transactions on Parallel and Distributed Systems}, 
title={Enabling Personalized Search over Encrypted Outsourced Data with Efficiency Improvement}, 
year={2016}, 
volume={27}, 
number={9}, 
pages={2546-2559}, 
abstract={In cloud computing, searchable encryption scheme over outsourced data is a hot research field. However, most existing works on encrypted search over outsourced cloud data follow the model of “one size fits all” and ignore personalized search intention. Moreover, most of them support only exact keyword search, which greatly affects data usability and user experience. So how to design a searchable encryption scheme that supports personalized search and improves user search experience remains a very challenging task. In this paper, for the first time, we study and solve the problem of personalized multi-keyword ranked search over encrypted data (PRSE) while preserving privacy in cloud computing. With the help of semantic ontology WordNet, we build a user interest model for individual user by analyzing the user's search history, and adopt a scoring mechanism to express user interest smartly. To address the limitations of the model of “one size fit all” and keyword exact search, we propose two PRSE schemes for different search intentions. Extensive experiments on real-world dataset validate our analysis and show that our proposed solution is very efficient and effective.}, 
keywords={cloud computing;cryptography;data privacy;ontologies (artificial intelligence);outsourcing;search engines;PRSE;WordNet;cloud computing;data privacy;data usability;encrypted outsourced data;encrypted search;outsourced cloud data;personalized multikeyword ranked search;personalized search;personalized search intention;searchable encryption;semantic ontology;Cloud computing;Computational modeling;Encryption;Indexes;Search problems;Servers;Cloud security;outsourcing security;personalized search;user interest model}, 
doi={10.1109/TPDS.2015.2506573}, 
ISSN={1045-9219}, 
month={Sept},}
@INPROCEEDINGS{7298836, 
author={J. Xu and L. Mukherjee and Y. Li and J. Warner and J. M. Rehg and V. Singh}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Gaze-enabled egocentric video summarization via constrained submodular maximization}, 
year={2015}, 
pages={2235-2244}, 
abstract={With the proliferation of wearable cameras, the number of videos of users documenting their personal lives using such devices is rapidly increasing. Since such videos may span hours, there is an important need for mechanisms that represent the information content in a compact form (i.e., shorter videos which are more easily browsable/sharable). Motivated by these applications, this paper focuses on the problem of egocentric video summarization. Such videos are usually continuous with significant camera shake and other quality issues. Because of these reasons, there is growing consensus that direct application of standard video summarization tools to such data yields unsatisfactory performance. In this paper, we demonstrate that using gaze tracking information (such as fixation and saccade) significantly helps the summarization task. It allows meaningful comparison of different image frames and enables deriving personalized summaries (gaze provides a sense of the camera wearer's intent). We formulate a summarization model which captures common-sense properties of a good summary, and show that it can be solved as a submodular function maximization with partition matroid constraints, opening the door to a rich body of work from combinatorial optimization. We evaluate our approach on a new gaze-enabled egocentric video dataset (over 15 hours), which will be a valuable standalone resource.}, 
keywords={combinatorial mathematics;gaze tracking;optimisation;video signal processing;combinatorial optimization;constrained submodular maximization;gaze tracking information;gaze-enabled egocentric video summarization;partition matroid constraints;submodular function maximization;summarization model;Approximation methods;Cameras;Feature extraction;Gaze tracking;Linear programming;Mutual information;Optimization}, 
doi={10.1109/CVPR.2015.7298836}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{6727233, 
author={C. Remy and N. Pervin and F. Toriumi and H. Takeda}, 
booktitle={2013 International Conference on Signal-Image Technology Internet-Based Systems}, 
title={Information Diffusion on Twitter: Everyone Has Its Chance, But All Chances Are Not Equal}, 
year={2013}, 
pages={483-490}, 
abstract={Twitter is a Web 2.0 social network which attracted much attention recently for its usage as an alternative media for information diffusion. From the recent events in Arab countries, to natural disaster such as earthquakes or tsunamis, Twitter has proven to be a credible alternative to traditional means of information diffusion. Relatively few works have been done on this question of information diffusion, and in particular on the relative importance of different kind of users on this question. In this paper, we show that all users are not equal on the aspect of information diffusion. By investigating thoroughly the retweet chain lengths of users on a large dataset, we found that the number of followers of users plays an important role in their capacity to propagate information. From our observations we propose a very simple model, which is accurate enough to generate realistic length of retweet chains on the network. We consequently show, by studying a Twitter dataset centered on the Japanese Earthquake and Tsunami in March 2011, that such a crisis impact greatly the propagation of information. Finally, we use our results to discuss on the means of improving information diffusion to reach targeted users.}, 
keywords={disasters;earthquakes;information dissemination;social networking (online);tsunami;Arab countries;Japanese earthquake;Japanese tsunami;Twitter dataset;Web 2.0 social network;information diffusion improvement;information propagation;natural disaster;user retweet chain lengths;Correlation;Data models;Earthquakes;Tsunami;Twitter;Web 2.0;Information propagation;Social networks;Twitter;information diffusion}, 
doi={10.1109/SITIS.2013.84}, 
month={Dec},}
@INPROCEEDINGS{7778930, 
author={W. Du and H. Lin and J. Sun and B. Yu and H. Yang}, 
booktitle={2016 First IEEE International Conference on Computer Communication and the Internet (ICCCI)}, 
title={A new trust model for online social networks}, 
year={2016}, 
pages={300-304}, 
abstract={Online social media networks play important roles for people to share opinions, communicate with others. One of important features behind these activities is trust. This paper investigates the trust model in Online social media networks. Considering the interaction between two users and the reputation in the social networks, this trust model gives a definition about the trust value between two users. The Gaussian kernel density estimation is used to describe the user's reputation among a social network. Combined with the interaction relationship, the trust model can provide more accurate trust relation prediction. Experiments on the public dataset shows the method generates high accuracy results.}, 
keywords={Gaussian processes;social networking (online);trusted computing;Gaussian kernel density estimation;online social media networks;public dataset;trust model;trust relation prediction;trust value;Computational modeling;Computers;Estimation;Internet;Kernel;Predictive models;Social network services;interaction;reputation;trust computation}, 
doi={10.1109/CCI.2016.7778930}, 
month={Oct},}
@INPROCEEDINGS{7455842, 
author={S. Onal and X. Chen and S. Lai-Yuen and S. Hart}, 
booktitle={2016 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)}, 
title={Segmentation of sacral curve on dynamic MRI for diagnosis of pelvic organ prolapse}, 
year={2016}, 
pages={90-93}, 
abstract={Pelvic organ prolapse (POP) is a critical health condition for women. Dynamic magnetic resonance imaging (MRI) is currently used for assessing POP and to complement clinical examination. Current studies have shown some evidence on the association between the shape of the sacral curve and the development of POP. However, the sacral curve is currently extracted manually resulting in a time-consuming and subjective process. A new method is proposed to automate the identification and segmentation of the sacral curve on MRI. The proposed method identifies the region of interest without any user input by using our previously developed pelvic floor point identification model. Edges of the sacral structure are detected to identify points along the curve, which are then connected using a proposed adaptive shortest path algorithm. These points are used to finalize the segmentation of the sacral curve using smoothing curve fitting algorithm. Results show that the proposed method can achieve good accuracy for 80% of the dataset used in this study.}, 
keywords={biological organs;biomedical MRI;curve fitting;edge detection;feature extraction;image segmentation;medical image processing;smoothing methods;adaptive shortest path algorithm;complement clinical examination;dataset;dynamic MRI;dynamic magnetic resonance imaging;health condition;manual extraction;pelvic floor point identification model;pelvic organ prolapse diagnosis;sacral curve segmentation;smoothing curve fitting algorithm;Curve fitting;Electronic mail;Heuristic algorithms;Image edge detection;Image enhancement;Image segmentation;Magnetic resonance imaging;Edge Detection;MRI;Pelvic Organ Prolapse;Sacral Curve;Segmentation}, 
doi={10.1109/BHI.2016.7455842}, 
month={Feb},}
@INPROCEEDINGS{7545037, 
author={D. Bountouridis and H. V. Koops and F. Wiering and R. C. Veltkamp}, 
booktitle={2016 IEEE Second International Conference on Multimedia Big Data (BigMM)}, 
title={A data-driven approach to chord similarity and chord mutability}, 
year={2016}, 
pages={275-278}, 
abstract={Assessing the relationship between chord sequences is an important ongoing research topic in the fields of music cognition and music information retrieval. Heuristic and cognitive models of chord similarity have been investigated but none has aimed to capture the collective perception of chord similarity from a large dataset of user-generated content. Devising a largescale experiment to gather sufficient data from human subjects has always been a major stumbling block. We present a novel chord similarity model based on a large amount of crowd-sourced transcriptions from a popular automatic chord estimation service. We show that our model outperforms heuristic-based models in a song identification task. Secondly, a model of chord mutations based on a large amount of crowd-sourced cover songs transcriptions is introduced. From crowd-sourced data, we create substitution matrices that capture the perceived similarity and mutability between chords. These results show that modelling the collective perception can not only substitute alternative, sophisticated models but also further enhance performance in various music information retrieval tasks.}, 
keywords={data mining;information retrieval;music;automatic chord estimation service;chord mutability;chord mutations;chord sequences;chord similarity model;cognitive models;crowd-sourced cover songs transcriptions;crowd-sourced data;crowd-sourced transcriptions;heuristic-based models;music cognition;music information retrieval tasks;song identification task;user-generated content;Computational modeling;Context;Context modeling;Mathematical model;Matrices;Music information retrieval;User-generated content}, 
doi={10.1109/BigMM.2016.18}, 
month={April},}
@INPROCEEDINGS{6421358, 
author={S. Mehta and H. Banati}, 
booktitle={2012 12th International Conference on Hybrid Intelligent Systems (HIS)}, 
title={Trust aware social context filtering using Shuffled frog leaping algorithm}, 
year={2012}, 
pages={342-347}, 
abstract={In the past few years social context filtering (SCF) systems have become trendier to solve the problem of information overload. Conventional SCF approaches utilize preferences of all nearest neighbors to recommend the items. However, in practice preferences of credible peers / true friends with similar interests influence the decision making process. Thus need of trust aware approaches is being increasingly felt. Incorporating user's web of trust information though solves the sparsity and cold start problem prevailing in conventional social context filtering techniques but issue of scalability still remains. The work presents Shuffled frog leaping algorithm (SFLA) based SCF approach to develop trust aware system which is capable of handling all the issues addressed above. The approach performs social context modeling using SFLA based clustering. Subsequently, only the trusted neighbors participate in the process of computing most relevant items. Experimental evaluation over Movielens dataset establishes that SFLA based SCF model significantly outperforms conventional K-means approach. Evaluation over Epinions (rating and trust) dataset further substantiates the accuracy of SFLA based trust aware approach over mean absolute error metric.}, 
keywords={collaborative filtering;decision making;evolutionary computation;pattern clustering;recommender systems;social networking (online);trusted computing;Epinions dataset;Movielens dataset;SFLA-based SCF model;SFLA-based clustering;SFLA-based trust aware approach;cold start problem;decision making;information overload problem;mean absolute error metric;shuffled frog leaping algorithm;trust aware social context filtering systems;trust aware system development;user Web of trust information;Decision support systems;Helium;Hybrid intelligent systems;TV;SFLA based social filtering;Shuffled Frog Leaping Algorithm;Trust aware social filtering;collaborative filtering}, 
doi={10.1109/HIS.2012.6421358}, 
month={Dec},}
@INPROCEEDINGS{6999672, 
author={M. Mansoury and M. Shajari}, 
booktitle={2014 22nd Iranian Conference on Electrical Engineering (ICEE)}, 
title={Subjective implicit trust evaluation in online communities}, 
year={2014}, 
pages={949-954}, 
abstract={A robust trust model helps the users to collect reliable information and overcome the information overloading. Recent researches in trust prediction are extremely rely on users explicit trust, which is based on users past experiences. However, users' explicit trust ratings are not always available and when available, it is very sparse and cannot be used to predict the trust between two unfamiliar users with high accuracy. Therefore, the need for an accurate implicit trust model to calculate trust values between users without any explicit trust network is undeniable. Furthermore, trust is a subjective concept and trust ratings given to a user by others may vary according to the mentality of the raters even when they have similar experiences with a user. In this paper, we propose a subjective implicit trust model to predict trust values between users based on their ratings to different items and without using explicit trust values. Moreover, we compare our approach with another model that did not consider the subjectivity of trust and we show the importance of taking into account the subjectivity in calculating the trust values. We have investigated the accuracy of our approach with some experiments with a real-world dataset collected by epinions.com. The results show that our approach can calculates trust values with high accuracy.}, 
keywords={Internet;security of data;epinions.com;information overloading;online communities;robust trust model;subjective implicit trust evaluation;trust prediction;user explicit trust;Accuracy;Communities;Educational institutions;Equations;Mathematical model;Recommender systems;Web sites;Implicit trust;Subjectivity;Trust;rating}, 
doi={10.1109/IranianCEE.2014.6999672}, 
ISSN={2164-7054}, 
month={May},}
@INPROCEEDINGS{7146509, 
author={H. Xiong and D. Zhang and G. Chen and L. Wang and V. Gauthier}, 
booktitle={2015 IEEE International Conference on Pervasive Computing and Communications (PerCom)}, 
title={CrowdTasker: Maximizing coverage quality in Piggyback Crowdsensing under budget constraint}, 
year={2015}, 
pages={55-62}, 
abstract={This paper proposes a novel task allocation framework, CrowdTasker, for mobile crowdsensing. CrowdTasker operates on top of energy-efficient Piggyback Crowdsensing (PCS) task model, and aims to maximize the coverage quality of the sensing task while satisfying the incentive budget constraint. In order to achieve this goal, CrowdTasker first predicts the call and mobility of mobile users based on their historical records. With a flexible incentive model and the prediction results, CrowdTasker then selects a set of users in each sensing cycle for PCS task participation, so that the resulting solution achieves near-maximal coverage quality without exceeding incentive budget. We evaluated CrowdTasker extensively using a large-scale real-world dataset and the results show that CrowdTasker significantly outperformed three baseline approaches by achieving 3%-60% higher coverage quality.}, 
keywords={budgeting data processing;mobile computing;CrowdTasker;PCS;Piggyback Crowdsensing;budget constraint;flexible incentive model;historical records;incentive budget constraint;maximizing coverage quality;mobile crowdsensing;mobile users;piggyback crowdsensing;Conferences;Mobile communication;Mobile handsets;Poles and towers;Prediction algorithms;Resource management;Sensors}, 
doi={10.1109/PERCOM.2015.7146509}, 
month={March},}
@INPROCEEDINGS{6836062, 
author={Hongyuan Zhu and Jiangbo Lu and Jianfei Cai and Jianming Zheng and N. M. Thalmann}, 
booktitle={IEEE Winter Conference on Applications of Computer Vision}, 
title={Multiple foreground recognition and cosegmentation: An object-oriented CRF model with robust higher-order potentials}, 
year={2014}, 
pages={485-492}, 
abstract={Localizing, recognizing, and segmenting multiple foreground objects jointly from a general user's photo stream that records a specific event is an important task with many useful applications. As argued in recent Multiple Foreground Cosegmentation (MFC) work by Kim and Xing, this task is very challenging in that it contrasts substantially from the classical cosegmentation problem, and aims to parse a set of realistic event photos but each containing irregularly occurring multiple foregrounds with high appearance and scene configuration variations. Inspired by the impressive advance in scene understanding and object recognition, this paper casts the multiple foreground recognition and cosegmentation (MFRC) problem within a conditional random fields (CRFs) framework in a principled manner. We capitalize centrally on the key objective that MFRC is to segment out and annotate foreground objects or “things” rather than “stuff”. To this end, we exploit a few complementary objectness cues (e.g. contours, object detectors and layout) and propose novel and efficient methods to capture object-level information. Integrating object potentials as soft constraints (e.g. robust higher-order potentials defined over detected object regions) with low-level unary and pairwise terms holistically, we solve the MFRC task with a probabilistic CRF model. The inference for such a CRF model is performed efficiently with graph cut based move making algorithms. With a minimal amount of user annotations on just a few example photos, the proposed approach produces spatially coherent, boundary-aligned segmentation results with correct and consistent object labeling. Experiments on the FlickrMFC dataset justify that our method achieves state-of-the-art performance.}, 
keywords={graph theory;image recognition;image segmentation;object recognition;object-oriented programming;FlickrMFC dataset;conditional random fields;graph cut;move making algorithms;multiple foreground cosegmentation;multiple foreground recognition;object recognition;object-level information;object-oriented CRF model;robust higher-order potentials;Abstracts;Image segmentation;Robustness}, 
doi={10.1109/WACV.2014.6836062}, 
ISSN={1550-5790}, 
month={March},}
@INPROCEEDINGS{1241172, 
author={Chih-Ming Chen}, 
booktitle={Proceedings IEEE/WIC International Conference on Web Intelligence (WI 2003)}, 
title={Incremental personalized Web page mining utilizing self-organizing HCMAC neural network}, 
year={2003}, 
pages={47-53}, 
abstract={In recent years, information has grown rapidly, especially on the World Wide Web. Also volume of information found by search engines tends to be large, and these documents are not tailored to a user's actual needs and interests. Thus, to offer the personalized service that includes only user interested information become increasingly important. Web mining techniques have proven themselves as a very useful tool for mining information of interests on the Web. However, past pioneers' studies have indicated that the main challenges in Web mining are in terms of handling high-dimensional data, achieving incremental learning (or incremental mining), providing scalable mining and parallel and distributed mining algorithms. We present a novel self-organizing hierarchical CMAC (HCMAC) neural network composed of two-dimensional weighted grey CMACs (WGCMAC) capable of handling both higher dimensional classification problems and self-organizing memory structure according to the distribution of training patterns. Moreover, a learning algorithm that can learn incrementally from new added data without forgetting prior knowledge is proposed to train the self-organizing HCMAC neural network. It can be applied to incrementally learn user profiles from user feedback for identifying personalized Web pages. A benchmark dataset of Web pages ratings that contains four topics of user profiles is used to demonstrate the effectiveness of the proposed method. Experimental results show that the self-organizing HCMAC neural network has a good incrementally learning ability and can overcome the problem of enormous memory requirement in the conventional CMAC while it is applied to solve the higher dimensional classification problems. Furthermore, experiments also confirm that the self-organizing HCMAC neural network has a better forecasting ability to identify user interesting Web pages than other well-known classifiers do.}, 
keywords={Internet;Web sites;cerebellar model arithmetic computers;data mining;learning (artificial intelligence);online front-ends;search engines;self-organising feature maps;Web page mining;Web sites;World Wide Web;cerebellar model arithmetic computers;incremental learning;incremental personalized service;learning algorithm;scalable mining;search engines;self-organizing hierarchical CMAC;training patterns;weighted grey CMAC;Collaboration;Data mining;Educational institutions;Information filters;Intelligent agent;Neural networks;Search engines;Web mining;Web pages;Web sites}, 
doi={10.1109/WI.2003.1241172}, 
month={Oct},}
@INPROCEEDINGS{6890950, 
author={M. Fazeen and R. Dantu}, 
booktitle={2014 Twelfth Annual International Conference on Privacy, Security and Trust}, 
title={Another free app: Does it have the right intentions?}, 
year={2014}, 
pages={282-289}, 
abstract={Security and privacy holds a great importance in mobile devices due to the escalated use of smart phone applications (app). This has made the user even more vulnerable to malicious attacks than ever before. We aim to address this problem by proposing a novel framework to identify potential Android malware apps by extracting the intention and their permission requests. First, we constructed a dataset consisting of 1,730 benign apps along with 273 malware samples. Then, both datasets were subjected to source code extraction. From there on, we followed a two phase approach to identify potential malware samples. In phase 1, we constructed a machine learning model to group benign apps into different clusters based on their operations known as the task-intention. Once we trained the model, it was used to identify the task-intention of an Android app. Further, in this phase, we only used the benign apps to construct the task-intentions and none of the malware signatures were involved. Therefore, our approach does not use machine learning models to identify malware apps. Then, for each task-intention group, we extracted the permission-requests of the apps and constructed the probability mass functions (PMF). We named the shape of this PMF as Intention-Shape or I-Shape. In phase 2, we used the permission-requests, task-intentions and I-Shapes to identify potential malware apps. We compared the permission-requests of an unknown app with its corresponding I-Shape to identify the potential malware apps. Using this approach, we obtained an accuracy of 89% in detecting potential malware samples. The novelty of our work is to perform potential malware identification without training any models with malware signatures, and utilization of I-Shapes to identify such potential malware samples. Our approach can be utilized to identify the safety of an app before it is installed as it performs static code analysis. Further, it can be utilized in pre-screening or multi-layer security sys- ems. It is also highly useful in screening malware apps when launching in Android markets.}, 
keywords={invasive software;learning (artificial intelligence);mobile computing;smart phones;source code (software);Android app task-intention identification;Android malware apps;I-Shape;PMF;benign apps;clusters;intention-shape;machine learning model;malicious attacks;permission-requests;probability mass functions;smart phone applications;source code extraction;static code analysis;Androids;Feature extraction;Humanoid robots;Malware;Mathematical model;Shape;Unsupervised learning}, 
doi={10.1109/PST.2014.6890950}, 
month={July},}
@INPROCEEDINGS{7026009, 
author={C. Segalin and A. Perina and M. Cristani}, 
booktitle={2014 IEEE International Conference on Image Processing (ICIP)}, 
title={Biometrics on visual preferences: A #x201C;pump and distill #x201D; regression approach}, 
year={2014}, 
pages={4982-4986}, 
abstract={We present a statistical behavioural biometric approach for recognizing people by their aesthetic preferences, using colour images. In the enrollment phase, a model is learnt for each user, using a training set of preferred images. In the recognition/authentication phase, such model is tested with an unseen set of pictures preferred by a probe subject. The approach is dubbed “pump and distill”, since the training set of each user is pumped by bagging, producing a set of image ensembles. In the distill step, each ensemble is reduced into a set of surrogates, that is, aggregates of images sharing a similar visual content. Finally, LASSO regression is performed on these surrogates; the resulting regressor, employed as a classifier, takes test images belonging to a single user, predicting his identity. The approach improves the state-of-the-art on recognition and authentication tasks in average, on a dataset of 40000 Flickr images and 200 users. In practice, given a pool of 20 preferred images of a user, the approach recognizes his identity with an accuracy of 92%, and sets an authentication accuracy of 91% in terms of normalized Area Under the Curve of the CMC and ROC curve, respectively.}, 
keywords={biometrics (access control);image classification;image colour analysis;image matching;regression analysis;CMC curve;Flickr images;LASSO regression;ROC curve;aesthetic preferences;authentication accuracy;authentication task improvement;colour images;enrollment phase;image aggregation;image classifier;image ensemble reduction;normalized area-under-the-curve;preferred images;pump-and-distill regression approach;recognition task improvement;statistical behavioural biometric approach;test images;training image set;unseen picture set;user identity prediction;visual content;visual preferences;Authentication;Biometrics (access control);Image recognition;Probes;Testing;Training;Visualization;LASSO regression;bagging;behavioral biometrics;image preferences}, 
doi={10.1109/ICIP.2014.7026009}, 
ISSN={1522-4880}, 
month={Oct},}
@INPROCEEDINGS{7785395, 
author={I. Kilanioti and G. A. Papadopoulos}, 
booktitle={2016 7th International Conference on Information, Intelligence, Systems Applications (IISA)}, 
title={Efficient content delivery through popularity forecasting on social media}, 
year={2016}, 
pages={1-6}, 
abstract={Ubiquitous social networks have in recent years become significant for sharing of content generated in online video platforms. Our work investigates how the predictability of video sharing is associated with the underlying social network of the initial sharer of the video and the context of the media platform it was uploaded. In particular we combine user-centric data from Twitter with video-centric data from YouTube to give insights that neither dataset (social network and media service dataset) individually gives. We propose a simple model to predict future popularity of a video resource with a small and easily extracted feature set, based on the notion of influence score of a user and its fluctuation through time, as well as the distance of content interests among users of both datasets. We further demonstrate how the incorporation of our prediction model into a mechanism for content delivery results in considerable improvement of the user experience.}, 
keywords={data mining;feature extraction;social networking (online);ubiquitous computing;video retrieval;Twitter;YouTube;content delivery;feature set extraction;media platform;media service dataset;online video platforms;popularity forecasting;social media;social network;ubiquitous social networks;user-centric data;video resource;video sharing;video-centric data;Feature extraction;Predictive models;Support vector machines;Training;Twitter;YouTube;Analytics;Data Mining and Knowledge Extraction;Regression Analysis;Social Cascade;Social Networks;Social Prediction;Social Web;Video Popularity}, 
doi={10.1109/IISA.2016.7785395}, 
month={July},}
@INPROCEEDINGS{5370950, 
author={M. A. Sicilia and S. Sánchez-Alonso and E. García-Barriocanal and D. Rodríguez-García}, 
booktitle={2009 International Conference on Intelligent Networking and Collaborative Systems}, 
title={Exploring Structural Prestige in Learning Object Repositories: Some Insights from Examining References in MERLOT}, 
year={2009}, 
pages={212-218}, 
abstract={Several existing learning object repositories provide mechanisms for users to arrange personal collections with their selection of resources or to provide reviews and ratings for other's resources, creating a kind of community dynamics. The resulting information can be used to build structural prestige models for the creators of the resources. This paper reports preliminary explorations on relational models that could be used to develop metrics of quality and prestige for learning object authors. Concretely, social network analysis tools are used to analyze the overall community structure of a dataset obtained from the MERLOT repository. Networks extracted from the indirect reference between users through references in personal collections and reviews are examined with regards to the position of relevant community members.}, 
keywords={computer aided instruction;social networking (online);MERLOT;learning object repositories;network extraction tools;personal collections;social network analysis tools;structural prestige models;Availability;Buildings;Computer science;Intelligent networks;Intelligent structures;International collaboration;Productivity;Scattering;Social network services;Web pages;MERLOT;learning object;prestige;repositories;social network analysis}, 
doi={10.1109/INCOS.2009.12}, 
month={Nov},}
@INPROCEEDINGS{7009555, 
author={R. Majumdar and K. Alse and S. Iyer}, 
booktitle={2014 IEEE Sixth International Conference on Technology for Education}, 
title={Interactive Stratified Attribute Tracking Diagram for Learning Analytics}, 
year={2014}, 
pages={138-139}, 
abstract={Interactive Stratified Attribute Tracking Diagram (iSAT) is a data visualization and tool to assist interactive visual analytics of multi-attribute learning dataset. The present work reports the evolution of this diagram through a design based research methodology following its three design iterations. There are two output at the current stage i) iSAT and its Web-based interaction. ii) A Learning Analytics method suitable for both researchers and practitioners to trace student attribute value. We received satisfactory user testing score for Pre-test and post-test marks visualization through a two-phase iSAT.}, 
keywords={Internet;data analysis;data visualisation;educational administrative data processing;interactive systems;Web-based interaction;data visualization;iSAT;interactive stratified attribute tracking diagram;interactive visual analytics;learning analytics;multiattribute learning dataset;post-test marks visualization;pretest marks visualization;student attribute value tracing;Context;Data models;Data visualization;Distributed Bragg reflectors;Testing;Usability;Design Based Research methodology;SAT Diagram;State Transition Diagram;iSAT}, 
doi={10.1109/T4E.2014.2}, 
month={Dec},}
@INPROCEEDINGS{7025673, 
author={X. Wu and Q. Han and X. Niu}, 
booktitle={2014 IEEE International Conference on Image Processing (ICIP)}, 
title={An adaptive transfer scheme based on sparse representation for figure-ground segmentation}, 
year={2014}, 
pages={3327-3331}, 
abstract={Figure-ground segmentation benefits lots of tasks in the field of computer vision. Exemplar-based approaches are capable of performing segmenting automatically without user interaction. However, most of them adopt fixed parameters for all the target images, which blocks their segmentation performances. We present a novel sparse representation based transfer scheme to gain adaptive parameters automatically. The proposed scheme transfers the segmentation masks of some windows from training images to obtain the soft mask of the target window from any given test image, when the target window can be represented by the linear combination of those windows. On the challenging PASCAL VOC 2010 segmentation dataset, experimental results and comparisons with the state-of-the-art methods show the effectiveness of the proposed scheme.}, 
keywords={computer vision;image segmentation;PASCAL VOC 2010 segmentation dataset;adaptive transfer scheme;computer vision;figure-ground segmentation;sparse representation;Computational modeling;Computer vision;Conferences;Image segmentation;Labeling;Shape;Training;Figure-ground segmentation;sparse representation;transfer scheme}, 
doi={10.1109/ICIP.2014.7025673}, 
ISSN={1522-4880}, 
month={Oct},}
@INPROCEEDINGS{6206745, 
author={G. Jayanthi and D. P. Lavanya}, 
booktitle={2012 International Conference on Recent Trends in Information Technology}, 
title={An integrated framework of neuro - Fuzzy inference system and MOLAP in knowledge discovery over multidimensional databases}, 
year={2012}, 
pages={17-21}, 
abstract={Data is real fact which undergoes various transformations under knowledge discovery. The computation in various stages of transformation requires parameters for quantifying precision, certainty and error tolerance in decision making process for end user. A multidimensional database serves the need of decision making on the basis of a knowledge discovery. This research work presents an integrated approach of knowledge discovery process and Multidimensional OLAP (MOLAP) tool. The initial step is creating a framework of computation models that formulate hybridized perceptron with fuzzification for multidimensional database management system (MDBMS) to produce crisp dataset to MOLAP. The data retrieved from multidimensional database (MDBMS), upon a structured query to MOLAP, explores the relational model for knowledge discovery process. The information retrieved thus represents a data cube for decision making. The experimental results of the proposed model have proven results for both supervised and unsupervised learning network. The approach has been adopted to serve the need of business logics and business intelligence in various knowledge discovery and hence to create an effective decision making system.}, 
keywords={data mining;database management systems;decision making;fuzzy neural nets;fuzzy reasoning;information retrieval;learning (artificial intelligence);MDBMS;MOLAP;business intelligence;business logics;data cube representation;data retrieval;decision making process;error tolerance;hybridized perceptron;integrated framework;knowledge discovery;multidimensional OLAP;multidimensional database management system;neurofuzzy inference system;structured query;supervised learning;unsupervised learning;Business;Data mining;Data models;Data warehouses;Distributed databases;Neurons}, 
doi={10.1109/ICRTIT.2012.6206745}, 
month={April},}
@INPROCEEDINGS{7839577, 
author={L. P. Queiroz and F. C. M. Rodrigues and J. P. P. Gomes and F. T. Brito and I. C. Brito and J. C. Machado}, 
booktitle={2016 5th Brazilian Conference on Intelligent Systems (BRACIS)}, 
title={Fault Detection in Hard Disk Drives Based on Mixture of Gaussians}, 
year={2016}, 
pages={145-150}, 
abstract={Being able to detect faults in Hard Disk Drives (HDD) can lead to significant benefits to computer manufacturers, users and storage system providers. As a consequence, several works have focused on the development of fault detection algorithms for HDDs. Recently, promising results were achieved by methods using SMART (Self-Monitoring Analysis and Reporting Technology) features and anomaly detection algorithms based on Mahalanobis distance. Nevertheless, the performance of such methods can be seriously degraded when the normality assumption of the data does not hold. As a way to overcome this issue, we propose a new method for fault detection in HDD based on a Gaussian Mixture Model (GMM). The proposed method is tested in a real world dataset and its performance is compared to three other HDD fault detection methods.}, 
keywords={Gaussian processes;disc drives;fault diagnosis;hard discs;mixture models;GMM;Gaussian mixture model;HDD fault detection methods;Mahalanobis distance;SMART features;anomaly detection algorithms;computer manufacturers;hard disk drives;real world dataset;self-monitoring analysis and reporting technology features;storage system providers;Data models;Fault detection;Gaussian mixture model;Hard disks;Mathematical model;Support vector machines;Training data}, 
doi={10.1109/BRACIS.2016.036}, 
month={Oct},}
@INPROCEEDINGS{7050804, 
author={H. Chen and H. Jin and F. Zhao and L. Zhu}, 
booktitle={Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015)}, 
title={Schema adaptive modeling and incremental matching for web interface}, 
year={2015}, 
pages={181-188}, 
abstract={There are so many web data hidden behind so-called deep web and can only be accessed through query interfaces, and the data volume is increasing. We often need to fill forms over alternative interfaces in the same domain to select the best product or service, such as buying a book across various online book websites to choose the most affordable one. Integrating these web interfaces in the same domain to an uniform interface is as a matter of course. One of the most important things for interface integration is interface schema matching. In this paper, we present a new structure and a corresponding algorithm for web interface schema modeling and matching. Using the new schema structure, we could not only easily handle two interfaces schema matching, but also handle incremental schema matching between an existing integrated interface and a new interface. We present a detailed experimental evaluation using UIUC Web Integration Repository dataset. The results show that our approach is effective: it obtains significantly higher accuracy for two schema matching and is more robust than previous techniques. For incremental schema matching, it also performs well.}, 
keywords={Internet;query processing;user interfaces;UIUC Web Integration Repository dataset;Web data;Web interface schema matching;Web interface schema modeling;incremental schema matching;query interfaces;schema adaptive modeling;Accuracy}, 
doi={10.1109/ICOSC.2015.7050804}, 
month={Feb},}
@INPROCEEDINGS{6121638, 
author={C. Birtolo and D. Ronca and R. Armenise}, 
booktitle={2011 11th International Conference on Intelligent Systems Design and Applications}, 
title={Improving accuracy of recommendation system by means of Item-based Fuzzy Clustering Collaborative Filtering}, 
year={2011}, 
pages={100-106}, 
abstract={Predicting user preferences is a challenging task. Different approaches for recommending products to the users are proposed in literature and collaborative filtering has been proved to be one of the most successful techniques. Some issues related to the quality of recommendation and to computational aspects still arise (e.g., scalability and cold-start recommendations). In this paper, we propose an Item-based Fuzzy Clustering Collaborative Filtering (IFCCF) in order to ensure the benefits of a model-based technique improving the quality of suggestions. Experimentation led by predicting ratings of MovieLens and Jester users makes this promising and worth to be further investigated in a cross-domain dataset.}, 
keywords={collaborative filtering;fuzzy set theory;pattern clustering;recommender systems;IFCCF;cross-domain dataset;item-based fuzzy clustering collaborative filtering;model-based technique;recommendation system;user preferences;Accuracy;Clustering algorithms;Collaboration;Filtering;Fuzzy logic;Motion pictures;Prediction algorithms;Collaborative Filtering;Fuzzy Clustering;Pearson correlation;Recommendation System}, 
doi={10.1109/ISDA.2011.6121638}, 
ISSN={2164-7143}, 
month={Nov},}
@INPROCEEDINGS{6289780, 
author={B. Balasingam and P. Willett and Y. Bar-Shalom}, 
booktitle={2012 15th International Conference on Information Fusion}, 
title={Tracking individual behaviors in networks: An experimental demonstration}, 
year={2012}, 
pages={1-8}, 
abstract={Tracking individual behaviors based on observations made from vast personal interaction network has become a major concern and interest for the policing community as well as for the business/commercial players. While the policing community resort to personal networks in order to predict and prevent adverse events, the commercial players want to track opportunities for online advertisement, market identification, personalized product suggestions etc. Recently, revolutionary advances in digital media technology have enabled one to collect, store and analyze massive amounts of personal networking data. Unlike traditional tracking problems, the observations and the inferred targets are highly irregular in nature; they do not evolve or be observed according to established mathematical models. In this paper, we demonstrate an experimental approach for tracking hidden qualities of individuals by observing their closer connections in the personal networks they belong to. We model the hidden features of individuals through hidden Markov random fields (HMRF) and propose a modified observation model in order to simplify the tracking algorithm. We test our algorithm on a fictitious scale-free personal network dataset and report high accuracy through objective performance metrics.}, 
keywords={advertising;hidden Markov models;mathematical analysis;personal area networks;HMRF;business-commercial players;commercial players;experimental demonstration;fictitious scale-free personal network dataset;hidden Markov random fields;hidden qualities tracking;individual behaviors tracking;market identification;mathematical models;online advertisement;personal interaction network;personal networking data;personalized product;Adaptation models;Approximation methods;Data models;Heuristic algorithms;Hidden Markov models;Mathematical model;Social network services;Dynamic networks;graph theory;hidden Markov random field (HMRF);latent states;non-traditional target tracking}, 
month={July},}
@INPROCEEDINGS{7407077, 
author={Z. S. Wang and J. F. Juang and W. G. Teng}, 
booktitle={2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI)}, 
title={Predicting POI visits with a heterogeneous information network}, 
year={2015}, 
pages={388-395}, 
abstract={A point of interest (POI) is a specific location that people may find useful or interesting. Examples include restaurants, stores, attractions, and hotels. With recent proliferation of location-based social networks (LBSNs), numerous users are gathered to share information on various POIs and to interact with each other. POI recommendation is then a crucial issue because it not only helps users to explore potential places but also gives LBSN providers a chance to post POI advertisements. As we utilize a heterogeneous information network to represent a LBSN in this work, POI recommendation is remodeled as a link prediction problem, which is significant in the field of social network analysis. Moreover, we propose to utilize the meta-path-based approach to extract implicit (but potentially useful) relationships between a user and a POI. Then, the extracted topological features are used to construct a prediction model with appropriate data classification techniques. In our experimental studies, the Yelp dataset is utilized as our testbed for performance evaluation purposes. Results of the experiments show that our prediction model is of good prediction quality in practical applications.}, 
keywords={feature extraction;mobile computing;pattern classification;recommender systems;social networking (online);topology;LBSN;POI recommendation;data classification technique;heterogeneous information network;link prediction problem;location-based social network;meta-path-based approach to;point of interest;social network analysis;topological feature extraction;Semantics;POI recommendation;heterogeneous information network;link prediction;meta-path;social network analysis}, 
doi={10.1109/TAAI.2015.7407077}, 
month={Nov},}
@INPROCEEDINGS{7796930, 
author={B. Schreck and K. Veeramachaneni}, 
booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)}, 
title={What Would a Data Scientist Ask? Automatically Formulating and Solving Predictive Problems}, 
year={2016}, 
pages={440-451}, 
abstract={In this paper, we designed a formal language, called Trane, for describing prediction problems over relational datasets, implemented a system that allows data scientists to specify problems in that language. We show that this language is able to describe several prediction problems and even the ones on KAGGLE-a data science competition website. We express 29 different KAGGLE problems in this language. We designed an interpreter, which translates input from the user, specified in this language, into a series of transformation and aggregation operations to apply to a dataset in order to generate labels that can be used to train a supervised machine learning classifier. Using a smaller subset of this language, we developed a system to automatically enumerate, interpret and solve prediction problems. We tested this system on the Walmart Store Sales Forecasting dataset found on KAGGLE, enumerated 1077 prediction problems and built models that attempted to solve them, for which we produced 235 AUC scores. Considering that only one out of those 1077 problems was the focus of a 2.5 month long competition on KAGGLE, we expect this system to deliver a thousandfold increase in data scientist's productivity.}, 
keywords={formal languages;learning (artificial intelligence);pattern classification;KAGGLE;Trane;Walmart store sale forecasting dataset;data science competition Website;data scientists;formal language;supervised machine learning classifier;Complexity theory;Data models;Indexes;Machine learning algorithms;Prediction algorithms;Predictive models;automation;data science;predictive modeling}, 
doi={10.1109/DSAA.2016.55}, 
month={Oct},}
@ARTICLE{6975239, 
author={P. L. St-Charles and G. A. Bilodeau and R. Bergevin}, 
journal={IEEE Transactions on Image Processing}, 
title={SuBSENSE: A Universal Change Detection Method With Local Adaptive Sensitivity}, 
year={2015}, 
volume={24}, 
number={1}, 
pages={359-373}, 
abstract={Foreground/background segmentation via change detection in video sequences is often used as a stepping stone in high-level analytics and applications. Despite the wide variety of methods that have been proposed for this problem, none has been able to fully address the complex nature of dynamic scenes in real surveillance tasks. In this paper, we present a universal pixel-level segmentation method that relies on spatiotemporal binary features as well as color information to detect changes. This allows camouflaged foreground objects to be detected more easily while most illumination variations are ignored. Besides, instead of using manually set, frame-wide constants to dictate model sensitivity and adaptation speed, we use pixel-level feedback loops to dynamically adjust our method's internal parameters without user intervention. These adjustments are based on the continuous monitoring of model fidelity and local segmentation noise levels. This new approach enables us to outperform all 32 previously tested state-of-the-art methods on the 2012 and 2014 versions of the ChangeDetection.net dataset in terms of overall F-Measure. The use of local binary image descriptors for pixel-level modeling also facilitates high-speed parallel implementations: our own version, which used no low-level or architecture-specific instruction, reached real-time processing speed on a midlevel desktop CPU. A complete C++ implementation based on OpenCV is available online.}, 
keywords={image colour analysis;image segmentation;image sequences;object detection;video signal processing;C++;ChangeDetection.net dataset;F-measure;OpenCV;SuBSENSE;adaptation speed;color information;foreground-background segmentation;illumination variations;local adaptive sensitivity;local binary image descriptors;local segmentation noise levels;manually set frame-wide constants;midlevel desktop CPU;model fidelity continuous monitoring;pixel-level feedback loops;spatiotemporal binary features;universal change detection method;universal pixel-level segmentation method;video sequences;Adaptation models;Color;Image color analysis;Lighting;Noise;Sensitivity;Spatiotemporal phenomena;Background subtraction;background subtraction;change detection;foreground segmentation;spatiotemporal features;surveillance;video signal processing}, 
doi={10.1109/TIP.2014.2378053}, 
ISSN={1057-7149}, 
month={Jan},}
@INPROCEEDINGS{5614070, 
author={Z. Zhang and Q. Li and D. Zeng}, 
booktitle={2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology}, 
title={Evolutionary Community Discovery from Dynamic Multi-relational CQA Networks}, 
year={2010}, 
volume={3}, 
pages={83-86}, 
abstract={As a knowledge sharing platform, Community Question Answering (CQA) services have attracted much attention from both academic and industry. This paper studies the problem of mining evolutionary community structures in CQA, through analysis of time-varying, multi-relational data among users and contents. We propose a unified framework for this problem, which makes the following contributions: 1) We propose an AT-LDA model, which combines author-topic model with topological structure analysis, to discover densely connected communities and the community topics in a unified process; 2) Our framework captures community structures and their evolution with temporal smoothing given by historic community structures. Empirical evaluation on real-world dataset shows that interesting communities and their evolution patterns can be detected.}, 
keywords={data mining;information retrieval;relational databases;AT-LDA model;CQA service;author-topic model;community question answering;community topic;dynamic multirelational CQA network;evolutionary community discovery;evolutionary community structure mining;historic community structure;knowledge sharing;multirelational data;time-varying data;topological structure analysis;Analytical models;Communities;Computational modeling;Data models;Driver circuits;Games;Smoothing methods;Community Question Answering;community detection;community evolution;topic model}, 
doi={10.1109/WI-IAT.2010.189}, 
month={Aug},}
@INPROCEEDINGS{5995413, 
author={V. Bychkovsky and S. Paris and E. Chan and F. Durand}, 
booktitle={CVPR 2011}, 
title={Learning photographic global tonal adjustment with a database of input/output image pairs}, 
year={2011}, 
pages={97-104}, 
abstract={Adjusting photographs to obtain compelling renditions requires skill and time. Even contrast and brightness adjustments are challenging because they require taking into account the image content. Photographers are also known for having different retouching preferences. As the result of this complexity, rule-based, one-size-fits-all automatic techniques often fail. This problem can greatly benefit from supervised machine learning but the lack of training data has impeded work in this area. Our first contribution is the creation of a high-quality reference dataset. We collected 5,000 photos, manually annotated them, and hired 5 trained photographers to retouch each picture. The result is a collection of 5 sets of 5,000 example input-output pairs that enable supervised learning. We first use this dataset to predict a user's adjustment from a large training set. We then show that our dataset and features enable the accurate adjustment personalization using a carefully chosen set of training photos. Finally, we introduce difference learning: this method models and predicts difference between users. It frees the user from using predetermined photos for training. We show that difference learning enables accurate prediction using only a handful of examples.}, 
keywords={image reconstruction;learning (artificial intelligence);brightness adjustments;image content;input image pairs;learning photographic global tonal adjustment;output image pairs;photographs adjustment;supervised machine learning;Cameras;Ground penetrating radar;Histograms;Machine learning;Measurement;Supervised learning;Training}, 
doi={10.1109/CVPR.2011.5995413}, 
ISSN={1063-6919}, 
month={June},}
@INPROCEEDINGS{6690747, 
author={A. Sankepally and B. Zhou}, 
booktitle={2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)}, 
title={An Aggregate Search Model for Web Search Engines: An Empirical Study}, 
year={2013}, 
volume={3}, 
pages={288-289}, 
abstract={In this paper, we study a novel aggregate search model for web search engines. Rather than retrieving individual web pages in the search result, our model aggregates relevant web pages and formulates information groups which may capture user's search intents well. An information group may consist of an individual web page, or a set of hyper-linked web pages that are relevant to user's queries. Several meaningful ranking measures are proposed to rank returned information groups. We evaluate the proposed aggregate search model using a large real search log dataset and an open source web search platform. The empirical study indicates that our model is useful to improve the web search quality.}, 
keywords={Internet;public domain software;query processing;search engines;Web page aggregation;Web search engines;Web search quality;aggregate search model;information group formulation;open source Web search platform;ranking measures;search log dataset;user queries;Aggregates;Educational institutions;Engines;Indexes;Web pages;Web search}, 
doi={10.1109/WI-IAT.2013.201}, 
month={Nov},}
@INPROCEEDINGS{5429255, 
author={G. Lucko and K. Swaminathan and P. C. Benjamin and M. G. Madden}, 
booktitle={Proceedings of the 2009 Winter Simulation Conference (WSC)}, 
title={Rapid deployment of simulation models for building construction applications}, 
year={2009}, 
pages={2733-2744}, 
abstract={This paper presents a knowledge based approach to increase the use of simulation in the construction industry without its users having to become or hire experts in simulation techniques. The premise of this approach is to use existing process-related schedule information as inputs to create a functioning simulation model with little or no user intervention. It explains analytical capabilities and limitations of schedules and simulation, reviews previous discrete event simulation studies, and discusses integrating knowledge from this domain. It describes the architecture of the WorkSimÂ® system. A case study presents processes of a real construction project. The conceptual and pragmatic feasibility of converting schedules into simulations is tested with its representative sample dataset. Modeling and analyzing this case study establishes the technical viability of the ideas discussed in the paper. Future work will examine challenges to the quality of such models, e.g. their resolution, required resource data, and duration distributions.}, 
keywords={computational linguistics;construction industry;production engineering computing;scheduling;WorkSimÂ® system;construction industry;functioning simulation model;knowledge based approach;process-related schedule information;rapid deployment;representative sample dataset;simulation models;Analytical models;Application software;Buildings;Civil engineering;Construction industry;Costs;Discrete event simulation;Job shop scheduling;Knowledge based systems;Statistical analysis}, 
doi={10.1109/WSC.2009.5429255}, 
ISSN={0891-7736}, 
month={Dec},}
@INPROCEEDINGS{7179043, 
author={M. Fanaswala and V. Krishnamurthy}, 
booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Meta-level tracking for gestural intent recognition}, 
year={2015}, 
pages={5600-5604}, 
abstract={In this paper, a novel mode-driven switching state space approach is proposed for the joint tracking and recognition of gestural commands. Gestures are modeled as spatio-temporal patterns comprised of syntactic sub-units called gesturelets. These gesturelets are directional vectors modulating a switching state space model. Stochastic context-free grammars (SCFG) are used as generative models for command gestures which impart a scale-invariant modeling framework. This translates into a method that is user-independent and robust to the signing variation between and among users. In addition to the modeling framework, we also design a library of useful gestural patterns that cannot be represented by regular grammars (hidden Markov models). Our approach combines tracking and recognition in a single framework and is able to deal with a high perplexity dataset. We demonstrate the effectiveness of our approach by comparing SCFG models with HMM models on synthetic gesture trajectories.}, 
keywords={gesture recognition;object tracking;stochastic processes;SCFG model;directional vector modulation;generative model;gestural command recognition;gestural command tracking;gestural intent recognition;gesturelet;high perplexity dataset;meta-level tracking;mode-driven switching state space approach;regular grammar;scale-invariant modeling framework;spatiotemporal pattern;stochastic context-free grammar;synthetic gesture trajectory;Atmospheric measurements;Cameras;Computational modeling;Hidden Markov models;Kalman filters;Solid modeling;Three-dimensional displays;gestural command recognition;meta-level tracking;stochastic context-free grammars}, 
doi={10.1109/ICASSP.2015.7179043}, 
ISSN={1520-6149}, 
month={April},}
@INPROCEEDINGS{6528664, 
author={Jiapeng Chen and Lianzhong Liu and Wanli Tian}, 
booktitle={2012 6th International Conference on New Trends in Information Science, Service Science and Data Mining (ISSDM2012)}, 
title={N-triad link prediction model based on structural statistics in social network}, 
year={2012}, 
pages={395-399}, 
abstract={Almost all social networks provide a mechanism for user to evaluate a subject or another user which they trust or distrust (like or dislike). These link data is an efficacious tool to help system understand user's interests and other useful information. Therefore, a lot of research on social networks has focused on it. In this paper, we investigate how the positive and negative link reflect the emotion of node and how the basic triad structures influence the link prediction between any two nodes in the social network. And then we develop a model of link prediction based on structural statistics. Finally, discuss our prediction results in Epinions dataset.}, 
keywords={network theory (graphs);social sciences;statistical analysis;Epinions dataset;N-triad link data prediction model;negative links;positive links;social network nodes;structural statistics;triad structures;link prediction;social network;structural statistics}, 
month={Oct},}
@INPROCEEDINGS{6628784, 
author={M. Rusiñol and T. Benkhelfallah and V. P. dAndecy}, 
booktitle={2013 12th International Conference on Document Analysis and Recognition}, 
title={Field Extraction from Administrative Documents by Incremental Structural Templates}, 
year={2013}, 
pages={1100-1104}, 
abstract={In this paper we present an incremental framework aimed at extracting field information from administrative document images in the context of a Digital Mail-room scenario. Given a single training sample in which the user has marked which fields have to be extracted from a particular document class, a document model representing structural relationships among words is built. This model is incrementally refined as the system processes more and more documents from the same class. A reformulation of the tf-idf statistic scheme allows to adjust the importance weights of the structural relationships among words. We report in the experimental section our results obtained with a large dataset of real invoices.}, 
keywords={document image processing;feature extraction;information retrieval;statistics;administrative document images;digital mail-room scenario;document model;field information extraction;importance weights;incremental structural templates;tf-idf statistic scheme;words structural relationships;Accuracy;Context;Information retrieval;Layout;Optical character recognition software;Text analysis;Field extraction;administrative document images}, 
doi={10.1109/ICDAR.2013.223}, 
ISSN={1520-5363}, 
month={Aug},}
@INPROCEEDINGS{7025119, 
author={E. Zerman and B. Konuk and G. Nur and G. B. Akar}, 
booktitle={2014 IEEE International Conference on Image Processing (ICIP)}, 
title={A parametric video quality model based on source and network characteristics}, 
year={2014}, 
pages={595-599}, 
abstract={The increasing demand for streaming video raises the need for flexible and easily implemented Video Quality Assessment (VQA) metrics. Although there are different VQA metrics, most of these are either Full-Reference (FR) or Reduced-Reference (RR). Both FR and RR metrics bring challenges for on-the-fly multimedia systems due to the necessity of additional network traffic for reference data. No-Reference (NR) video metrics, on the other hand, as the name suggests, are much more flexible for user-end applications. This introduces a need for robust and efficient NR VQA metrics. In this paper, an NR VQA metric considering spatiotemporal information, bit rate, and packet loss rate characteristics of a video content is proposed. The proposed metric is evaluated on EPFL-PoliMI dataset, which includes different video content characteristics. The experimental results show that the proposed metric is a robust and accurate NR VQA metric towards diverse video content characteristics.}, 
keywords={multimedia systems;telecommunication traffic;video signal processing;EPFL-PoliMI dataset;FR metric;NR VQA metric;RR metric;bit rate;full-reference metric;network characteristic;network traffic;no-reference video metric;on-the-fly multimedia system;packet loss rate characteristic;parametric video quality model;reduced-reference metric;source characteristic;spatiotemporal information;user-end applications;video content characteristic;video quality assessment;video streaming;Databases;Feature extraction;Packet loss;Quality assessment;Streaming media;Video recording;Quality of experience (QoE);network condition;no-reference metric;video characteristics;video quality assessment (VQA)}, 
doi={10.1109/ICIP.2014.7025119}, 
ISSN={1522-4880}, 
month={Oct},}
@INPROCEEDINGS{1714425, 
author={Yefei Li and Xianghong Xu and Qinying Qiu}, 
booktitle={2006 6th World Congress on Intelligent Control and Automation}, 
title={FEM-Based Structure Optimization with Grid-Enabled Analysis Environment}, 
year={2006}, 
volume={2}, 
pages={6915-6919}, 
abstract={This paper presents an application of grid-enabled computing technologies in the field of engineering design optimization using finite element method (FEM). Three essential elements in FEM-based structure optimization problems (CAD modeling, mesh and solution, and optimization) are integrated and automated with grid-enabled computation environment, dataset toolkits which is now being developed collect metadata to build a group level to describe the analysis tasks of the whole engineering problem, it allows access to remote computational resources and executing analysis tasks. A FEM-based optimization is exposed to remote users and is applied to help FEM-based excavator working equipment analysis. A case running in this environment is shown in the end to validate design results}, 
keywords={CAD;design engineering;finite element analysis;grid computing;optimisation;structural engineering computing;CAD modeling;FEM-based structure optimization;computing resources;engineering design optimization;engineering system analysis;finite element method;grid application;grid-enabled analysis environment;grid-enabled computing;Application software;Design automation;Design engineering;Design optimization;Distributed computing;Finite element methods;Grid computing;Process design;Research and development;Systems engineering and theory;Computing Resources;Engineering System Analysis;Finite Element Method;Grid Application;Structure Optimization}, 
doi={10.1109/WCICA.2006.1714425}, 
month={},}
@INPROCEEDINGS{7073448, 
author={Q. X. Wang and Y. Ren and N. Q. He and M. Wan and G. B. Lu}, 
booktitle={2014 11th International Computer Conference on Wavelet Actiev Media Technology and Information Processing(ICCWAMTIP)}, 
title={A group attack detecter for collaborative filtering recommendation}, 
year={2014}, 
pages={454-457}, 
abstract={Collaborative filtering recommender systems are now popular both commercially and in the research community. However, they are vulnerable to manipulation by malicious users, where attackers inject into some fake user profiles in order to bias the recommendation results to their benefits. To solve the problem, a lots of methods have been proposed but mainly focus on identification the attacker at the individual level, i.e., to find the fake user one by one, while do not consider the similarity between attack users. In this paper, we present an algorithm to detect the attackers in group level. It works based on an effective algorithm for detecting individual malicious user and an effective clustering algorithm. More precisely, we cluster all users into group, and then find the group characters of attacked items, finally we find the attack user group. We test the algorithm on a benchmark dataset using four kinds of typical attack models, the results show that our solution is both efficient and effective, particularly in the popular attack model and the segment attack model, and the performance is significant in the segment attack model with large attack size.}, 
keywords={collaborative filtering;recommender systems;security of data;attack user similarity;attacker identification;collaborative filtering recommender system;group attack detection;popular attack model;segment attack model;user profile;Algorithm design and analysis;Clustering algorithms;Clustering methods;Collaboration;Detection algorithms;Recommender systems;AP;Collaborative filtering;UnRAP;group detection;malicious users;recommender systems}, 
doi={10.1109/ICCWAMTIP.2014.7073448}, 
month={Dec},}
@INPROCEEDINGS{6076756, 
author={Y. Zhang and Z. Zheng and M. R. Lyu}, 
booktitle={2011 IEEE 30th International Symposium on Reliable Distributed Systems}, 
title={Exploring Latent Features for Memory-Based QoS Prediction in Cloud Computing}, 
year={2011}, 
pages={1-10}, 
abstract={With the increasing popularity of cloud computing as a solution for building high-quality applications on distributed components, efficiently evaluating user-side quality of cloud components becomes an urgent and crucial research problem. However, invoking all the available cloud components from user-side for evaluation purpose is expensive and impractical. To address this critical challenge, we propose a neighborhood-based approach, called CloudPred, for collaborative and personalized quality prediction of cloud components. CloudPred is enhanced by feature modeling on both users and components. Our approach CloudPred requires no additional invocation of cloud components on behalf of the cloud application designers. The extensive experimental results show that CloudPred achieves higher QoS prediction accuracy than other competing methods. We also publicly release our large-scale QoS dataset for future related research in cloud computing.}, 
keywords={cloud computing;groupware;quality of service;storage management;CloudPred;cloud computing;collaborative quality prediction;distributed components;latent features;memory-based QoS prediction;neighborhood-based approach;personalized quality prediction;Accuracy;Cloud computing;Collaboration;Monitoring;Quality of service;Sparse matrices;Vectors;Cloud Computing;Prediction;QoS}, 
doi={10.1109/SRDS.2011.10}, 
ISSN={1060-9857}, 
month={Oct},}
@INPROCEEDINGS{5662712, 
author={J. Wang and J. Wan and Z. Liu and P. Wang}, 
booktitle={2010 Ninth International Conference on Grid and Cloud Computing}, 
title={Data Mining of Mass Storage Based on Cloud Computing}, 
year={2010}, 
pages={426-431}, 
abstract={Cloud computing is an elastic computing model that the users can lease the resources from the rentable infrastructure. Cloud computing is gaining popularity due to its lower cost, high reliability and huge availability. To utilize the powerful and huge capability of cloud computing, this paper is to import it into data mining and machine learning field. As one of the most influential and open competition in machine learning area, Netflix Prize attached with mass storage had driven thousands of teams across the world to attack the problem, among which the final winner was BellKor's Pragmatic Chaos team, who bested Netflix's own algorithm for predicting ratings by 10%. Their solution is an ensemble of a large number of models, each of which specializes in addressing a different aspect of the data. Among such different models, k-nearest neighbors (KNN) and Restricted Boltzmann Machine (RBM) are reported to be two most important and successful models. As a result, we build two predictors based on such two model respectively with the order to testify their performance based on cloud computing platforms. The results show that KNN can achieve root mean square deviation (rmse) with 0:9468 after the Global Effect (GE) data preprocessing, which is better than the Cinematch's performance with rmse being 0:951. The rmse for RBM algorithm is about 0:9670 on the raw dataset, which can be further improved by KNN model.}, 
keywords={Boltzmann machines;cloud computing;data mining;learning (artificial intelligence);Netflix Prize;cloud computing;data mining;k-nearest neighbors;machine learning field;mass storage;restricted Boltzmann machine;root mean square deviation;Cloud Computing;Data Mining;Mass Storage}, 
doi={10.1109/GCC.2010.89}, 
ISSN={2160-4908}, 
month={Nov},}
@INPROCEEDINGS{7780483, 
author={M. Gygli and Y. Song and L. Cao}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Video2GIF: Automatic Generation of Animated GIFs from Video}, 
year={2016}, 
pages={1001-1009}, 
abstract={We introduce the novel problem of automatically generating animated GIFs from video. GIFs are short looping video with no sound, and a perfect combination between image and video that really capture our attention. GIFs tell a story, express emotion, turn events into humorous moments, and are the new wave of photojournalism. We pose the question: Can we automate the entirely manual and elaborate process of GIF creation by leveraging the plethora of user generated GIF content? We propose a Robust Deep RankNet that, given a video, generates a ranked list of its segments according to their suitability as GIF. We train our model to learn what visual content is often selected for GIFs by using over 100K user generated GIFs and their corresponding video sources. We effectively deal with the noisy web data by proposing a novel adaptive Huber loss in the ranking formulation. We show that our approach is robust to outliers and picks up several patterns that are frequently present in popular animated GIFs. On our new large-scale benchmark dataset, we show the advantage of our approach over several state-of-the-art methods.}, 
keywords={Internet;computer animation;video signal processing;GIF creation;Video2GIF;adaptive Huber loss;animated GIF;noisy Web data;photojournalism;ranking formulation;robust deep RankNet;user generated GIF content;video sources;Computational modeling;Neural networks;Noise measurement;Robustness;Social network services;Training;Visualization}, 
doi={10.1109/CVPR.2016.114}, 
month={June},}
@INPROCEEDINGS{7917109, 
author={L. Sun and E. I. Michael and S. Wang and Y. Li}, 
booktitle={2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)}, 
title={A Time-Sensitive Collaborative Filtering Model in Recommendation Systems}, 
year={2016}, 
pages={340-344}, 
abstract={Collaborative filtering is one of the most widely-used algorithms in recommendation systems. In user-based collaborative filtering algorithm, current users' nearest neighbors are used to recommend items because they have similar preference, but users' preference varies with time, which often affects the accuracy of the recommendation. As a result of the varying users' preference, many researches about recommendation systems are focusing on the time factor, to find a way to make up for the change in preferences of users. The existing time-related algorithms usually add time factor in the training phase and make this procedure more complicated. To catch the newest preference of the users and improve the accuracy of the recommendation without complicating the training phase, a timesensitive collaborative filtering model is proposed in this paper, which keeps the original training phase and make some changes in the prediction phase. During the recommendation process, the proposed model orders the items by time for each user as a sequence. The sequence is called time-behavior sequence. First it finds the last item from current user's time-behavior sequence which represents the newest preference of the current user. Secondly, it locates the item in nearest neighbors' timebehavior sequence and saves the timestamp of the item. Lastly, it recommends the items whose timestamps are greater than the saved timestamp from the nearest neighbors' time-behavior sequence. Experiments on the MovieLens dataset show that the proposed time-sensitive collaborative filtering model gives better recommendation quality than the traditional user-based collaborative filtering recommendation algorithm. It can catch the newest preferences of the users and increase the accuracy of recommendation, without changing the training phase.}, 
keywords={filtering theory;prediction theory;recommender systems;training;MovieLens dataset;prediction phase;recommendation systems;time factor;time-behavior sequence;time-related algorithms;time-sensitive collaborative filtering model;timestamps;training phase;Collaboration;Pediatrics;Prediction algorithms;Recommender systems;Time factors;Training;Collaborative filtering;k-nearest-neighborhood;recommendation system;time-sensitive;timestamp}, 
doi={10.1109/iThings-GreenCom-CPSCom-SmartData.2016.81}, 
month={Dec},}
@INPROCEEDINGS{7836821, 
author={U. Khurana and D. Turaga and H. Samulowitz and S. Parthasrathy}, 
booktitle={2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)}, 
title={Cognito: Automated Feature Engineering for Supervised Learning}, 
year={2016}, 
pages={1304-1307}, 
abstract={Feature engineering involves constructing novel features from given data with the goal of improving predictive learning performance. Feature engineering is predominantly a human-intensive and time consuming step that is central to the data science workflow. In this paper, we present a novel system called "Cognito", that performs automatic feature engineering on a given dataset for supervised learning. The system explores various feature construction choices in a hierarchical and non-exhaustive manner, while progressively maximizing the accuracy of the model through a greedy exploration strategy. Additionally, the system allows users to specify domain or data specific choices to prioritize the exploration. Cognito is capable of handling large datasets through sampling and built-in parallelism, and integrates well with a state-of-the-art model selection strategy. We present the design and operation of Cognito, along with experimental results on eight real datasets to demonstrate its efficacy.}, 
keywords={data handling;greedy algorithms;learning (artificial intelligence);Cognito;automated feature engineering;greedy exploration strategy;large dataset handling;model selection strategy;supervised learning;Atmospheric modeling;Data models;Data science;Predictive models;Search problems;Supervised learning;Transforms;Data Science;Feature Construction;Feature Engineering;Machine Learning}, 
doi={10.1109/ICDMW.2016.0190}, 
month={Dec},}
@INPROCEEDINGS{6404834, 
author={A. Condon and J. Veeramony}, 
booktitle={2012 Oceans}, 
title={Development and validation of a coastal surge and inundation prediction system}, 
year={2012}, 
pages={1-8}, 
abstract={The development and validation of a coastal storm surge and inundation prediction system for operational use by the United States Navy is detailed. The system consists of the Delft3D-FLOW hydrodynamic model coupled with the Delft3D-WAVE wave model and a graphical user interface, the Delft Dashboard. The coupled system is used to model storm surge and inundation produced by Hurricane Ike along the Gulf of Mexico coast in September 2008. The hydrodynamic model is run in 2D depth averaged mode to develop a “best” simulation. The best simulation is developed after hundreds of test runs and it consists of a blended elevation dataset for use as the model bathymetry and topography, a spatially varying Manning's N coefficient, multiple nests, atmospheric forcing from re-analysis wind and pressure fields with directional land masking, an initial water level of 0.11 m, an updated air - sea drag coefficient, and dynamically coupled flow and wave fields. The simulation results compare very favorably with observed water level. To assess the system in an operational forecast environment, the system's sensitivity to the elevation dataset, bottom roughness, domain resolution, atmospheric forcing, drag coefficient, initial water level, and wave coupling was investigated and found to vary widely depending on the component.}, 
keywords={atmospheric pressure;atmospheric techniques;bathymetry;computational fluid dynamics;geophysical fluid dynamics;geophysics computing;graphical user interfaces;hydrodynamics;marine systems;ocean waves;sea level;storms;topography (Earth);weather forecasting;wind;AD 2008 09;Delft Dashboard;Delft3D-FLOW hydrodynamic model;Delft3D-WAVE wave model;Gulf of Mexico coast;Hurricane Ike;Manning's coefficient;United States Navy;air-sea drag coefficient;atmospheric forcing;bathymetry;blended elevation dataset;bottom roughness;coastal storm surge;directional land masking;domain resolution;graphical user interface;inundation prediction system;multiple nests;pressure fields;topography;water level;wave coupling;wind fields;Atmospheric modeling;Computational modeling;Couplings;Sea measurements;Sensitivity;Storms;Surges;Delft3D;Forecasting;Hydrodynamic modeling;Inundation;Storm surge}, 
doi={10.1109/OCEANS.2012.6404834}, 
ISSN={0197-7385}, 
month={Oct},}
@INPROCEEDINGS{7113363, 
author={W. Xie and Y. Tian and Y. Sismanis and A. Balmin and P. J. Haas}, 
booktitle={2015 IEEE 31st International Conference on Data Engineering}, 
title={Dynamic interaction graphs with probabilistic edge decay}, 
year={2015}, 
pages={1143-1154}, 
abstract={A large scale network of social interactions, such as mentions in Twitter, can often be modeled as a “dynamic interaction graph” in which new interactions (edges) are continually added over time. Existing systems for extracting timely insights from such graphs are based on either a cumulative “snapshot” model or a “sliding window” model. The former model does not sufficiently emphasize recent interactions. The latter model abruptly forgets past interactions, leading to discontinuities in which, e.g., the graph analysis completely ignores historically important influencers who have temporarily gone dormant. We introduce TIDE, a distributed system for analyzing dynamic graphs that employs a new “probabilistic edge decay” (PED) model. In this model, the graph analysis algorithm of interest is applied at each time step to one or more graphs obtained as samples from the current “snapshot” graph that comprises all interactions that have occurred so far. The probability that a given edge of the snapshot graph is included in a sample decays over time according to a user specified decay function. The PED model allows controlled trade-offs between recency and continuity, and allows existing analysis algorithms for static graphs to be applied to dynamic graphs essentially without change. For the important class of exponential decay functions, we provide efficient methods that leverage past samples to incrementally generate new samples as time advances. We also exploit the large degree of overlap between samples to reduce memory consumption from O(N) to O(logN) when maintaining N sample graphs. Finally, we provide bulk-execution methods for applying graph algorithms to multiple sample graphs simultaneously without requiring any changes to existing graph-processing APIs. Experiments on a real Twitter dataset demonstrate the effectiveness and efficiency of our TIDE prototype, which is built on top of- the Spark distributed computing framework.}, 
keywords={Internet;graph theory;probability;social networking (online);PED model;Spark distributed computing framework;TIDE prototype;Twitter;cumulative snapshot model;dynamic interaction graphs;exponential decay functions;graph analysis algorithm;large scale network;probabilistic edge decay;sliding window model;social interactions;specified decay function;Aggregates;Algorithm design and analysis;Analytical models;Computational modeling;Heuristic algorithms;Probabilistic logic;Twitter}, 
doi={10.1109/ICDE.2015.7113363}, 
ISSN={1063-6382}, 
month={April},}
@INPROCEEDINGS{7439342, 
author={L. Pradhan and C. Zhang and P. Chitrakar}, 
booktitle={2016 IEEE Tenth International Conference on Semantic Computing (ICSC)}, 
title={Multi-view Clustering in Collaborative Filtering Based Rating Prediction}, 
year={2016}, 
pages={250-253}, 
abstract={Collaborative filtering (CF) based rating prediction systems predict unknown ratings of a user for an item, based on the analysis made on how similar users rate similar items. Model based approaches such as cluster models provide efficient means to find out similar users or similar items as they inherently reduce the search space by previously grouping similar users or items into clusters. Hence, the accuracy of such cluster-based approaches can be improved by improving the clustering process itself. As such, in this paper, we present how multi-view clustering can be used to cluster users or items leveraging information from multiple modalities and improve the accuracy of CF-based rating prediction systems. We use Yelp business rating dataset to test our approach on user-restaurant rating prediction. We begin by identifying multiple views for both users and restaurants. Each view represents a distinct source of information regarding the user or the restaurant. These views are used to perform multi-view clustering for both users and restaurants, respectively. To predict the unknown rating of a user for a restaurant, we compute the averages of k-Nearest Neighbors (k-NN) from the respective user-cluster and the restaurant-cluster. Our results suggest that multi-view clustering is capable of leveraging the utility of each view to cluster similar users or items together and to improve the accuracy of CF-based rating prediction systems.}, 
keywords={catering industry;collaborative filtering;pattern clustering;search engines;CF-based rating prediction systems;Yelp business rating dataset;clustering process improvement;collaborative filtering-based rating prediction;information source;k-NN method;k-nearest neighbor method;multiview clustering;restaurant-cluster;search space reduction;unknown user item rating prediction;user-cluster;user-restaurant rating prediction;Business;Collaboration;Computational modeling;Filtering;Heart;Predictive models;Standards;Collaborative filtering;multiview clustering;rating prediction}, 
doi={10.1109/ICSC.2016.40}, 
month={Feb},}
@INPROCEEDINGS{4725029, 
author={C. Sönströd and U. Johansson and U. Norinder and H. Boström}, 
booktitle={2008 Seventh International Conference on Machine Learning and Applications}, 
title={Comprehensible Models for Predicting Molecular Interaction with Heart-Regulating Genes}, 
year={2008}, 
pages={559-564}, 
abstract={When using machine learning for in silico modeling, the goal is normally to obtain highly accurate predictive models. Often, however, models should also bring insights into interesting relationships in the domain. It is then desirable that machine learning techniques have the ability to obtain small and transparent models, where the user can control the tradeoff between accuracy, comprehensibility and coverage. In this study, three different decision list algorithms are evaluated on a dataset concerning the interaction of molecules with a human gene that regulates heart functioning (hERG). The results show that decision list algorithms can obtain predictive performance not far from the state-of-the-art method random forests, but also that algorithms focusing on accuracy alone may produce complex decision lists that are very hard to interpret. The experiments also show that by sacrificing accuracy only to a limited degree, comprehensibility (measured as both model size and classification complexity) can be improved remarkably.}, 
keywords={biochemistry;biology computing;learning (artificial intelligence);classification complexity;decision list algorithms;heart-regulating genes;in silico modeling;machine learning;molecular interaction prediction;random forests;Biochemistry;Biological information theory;Biological system modeling;Drugs;High temperature superconductors;Humans;Informatics;Machine learning;Pharmaceuticals;Predictive models;Comprehensible Models;Data Mining;Decision Lists;hERG}, 
doi={10.1109/ICMLA.2008.130}, 
month={Dec},}
@INPROCEEDINGS{7774563, 
author={D. Hintze and A. Rice}, 
booktitle={2016 IEEE 24th International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems (MASCOTS)}, 
title={Picky: Efficient and Reproducible Sharing of Large Datasets Using Merkle-Trees}, 
year={2016}, 
pages={30-38}, 
abstract={There is growing demand for researchers to share datasets in order to allow others to reproduce results or investigate new questions. The most common option is to simply deposit the data online in its entirety. However, this mechanism of distribution becomes impractical as the size of the dataset increases or if the dataset is frequently changing as new data is collected. In this paper we describe Picky, a new Merkle tree based system for sharing large datasets which allows users to download selected portions and to receive incremental updates. We demonstrate the viability of our approach by quantifying its benefit when applied to a number of large datasets used in the networking and measurement community.}, 
keywords={file organisation;information retrieval;Merkle-trees;Picky;data access;data distribution;large datasets sharing;Computational modeling;Computers;Indexing;Metadata;Ports (Computers);data sharing;repeatable research;selective download;versioning}, 
doi={10.1109/MASCOTS.2016.25}, 
month={Sept},}
@INPROCEEDINGS{7538628, 
author={Shi Yuan and Junjie Wu and Lihong Wang and Qing Wang}, 
booktitle={2016 13th International Conference on Service Systems and Service Management (ICSSSM)}, 
title={A hybrid method for multi-class sentiment analysis of micro-blogs}, 
year={2016}, 
pages={1-6}, 
abstract={With the development of social media, huge volumes of micro-blogs convey not only the factual information, but also the emotional status of individuals, which are crucial for understanding user behaviors in those micro-blogging systems. However, a micro-blog is typically very short and may contain rich sentiments other than the positive and negative, like the anxious, which brings great challenges to the so-called multi-class sentiment analysis. Although the model-based and lexicon-based methods are the two primary approaches extensively investigated and regularly used in this field, it is argued by some researchers that the model-based method provides poor results in multi-class analysis while the lexicon-based method is difficult to reflect the characteristics of short texts. In this paper, we propose a hybrid method for multi-class sentiment analysis of micro-blogs, which combines the model-based approach with the lexicon-based approach. Considering the effect of emoticons, we use emoticons and Naïve-Bayes classification to divide micro-blogs into three sentiments-positive, negative and neutral. After that, we use sentiment dictionaries to identify four negative sentiments-angry, sad, disgusted and anxious. We evaluate our algorithm on a real-life micro-blogging dataset collected from the popular Chinese micro-blogging site, Sina, and the results show that it is effective and efficient for timely sentiment analysis. Our method has been further applied to a Weibo User Profiling System and enabled the sentiment analysis of real-time micro-blogs.}, 
keywords={Bayes methods;classification;sentiment analysis;social networking (online);user interfaces;Naïve-Bayes classification;Sina;Weibo;emotional status;factual information;lexicon-based methods;microblogs;model-based methods;multiclass sentiment analysis;social media;user behaviors;user profiling system;Blogs;Classification algorithms;Dictionaries;Grammar;Micro-blog;Naïve-Bayes;emoticon-based;lexicon-based;multi-class sentiment}, 
doi={10.1109/ICSSSM.2016.7538628}, 
month={June},}
@INPROCEEDINGS{5616206, 
author={R. Krestel and P. Fankhauser}, 
booktitle={2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology}, 
title={Language Models and Topic Models for Personalizing Tag Recommendation}, 
year={2010}, 
volume={1}, 
pages={82-89}, 
abstract={More and more content on the Web is generated by users. To organize this information and make it accessible via current search technology, tagging systems have gained tremendous popularity. Especially for multimedia content they allow to annotate resources with keywords (tags) which opens the door for classic text-based information retrieval. To support the user in choosing the right keywords, tag recommendation algorithms have emerged. In this setting, not only the content is decisive for recommending relevant tags but also the user's preferences. In this paper we introduce an approach to personalized tag recommendation that combines a probabilistic model of tags from the resource with tags from the user. As models we investigate simple language models as well as Latent Dirichlet Allocation. Extensive experiments on a real world dataset crawled from a big tagging system show that personalization improves tag recommendation, and our approach significantly outperforms state-of-the-art approaches.}, 
keywords={Internet;data mining;information retrieval;probability;recommender systems;text analysis;Web;data mining;language models;latent Dirichlet allocation;probabilistic model;search technology;tag recommendation algorithms;tagging system;tagging systems;text-based information retrieval;topic models;Clustering;Data mining;Personalization}, 
doi={10.1109/WI-IAT.2010.29}, 
month={Aug},}
@INPROCEEDINGS{7568598, 
author={Y. Gu and Y. Yao and W. Liu and J. Song}, 
booktitle={2016 25th International Conference on Computer Communication and Networks (ICCCN)}, 
title={We Know Where You Are: Home Location Identification in Location-Based Social Networks}, 
year={2016}, 
pages={1-9}, 
abstract={The rapid spread of smartphones has led to the increasing popularity of Location-Based Social Networks(LBSNs) like Foursquare, Gowalla, Facebook Places and so on where users can publish information about their current location. In LBSNs, identifying home locations of users is very important for various applications like effective location-based advertisement and recommendation. However, this problem is rather challenging because the location information in LBSNs is sparse and noisy: Only a small percentage of users share their home location information due to privacy concerns; users may check in at diverse places far from their home and make friends far away; many users even do not have any check-in information. In this paper, we propose a trust-based influence model, named as TSU to solve the problem. To be specific, TSU is a Trust-based unified probabilistic model that models edges in LBSNs based on signals from Social relationship data(social friendship, social trust) and User-centric data(check-in data) in LBSN. We proposed a Home Location Identification method based on TSU model and evaluate it on a large real-world LBSNs dataset. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art methods.}, 
keywords={data privacy;social networking (online);TSU;check-in information;home location identification;large real-world LBSN dataset evaluation;location-based advertisement;location-based social networks;smart phones;social friendship;social relationship data;social trust;trust-based influence model;trust-based unified probabilistic model;user privacy;user recommendation;user-centric data;Data models;Facebook;Privacy;Probabilistic logic;Smart phones;Twitter}, 
doi={10.1109/ICCCN.2016.7568598}, 
month={Aug},}
@INPROCEEDINGS{7841765, 
author={S. Li and Q. Yu and M. A. Maddah-Ali and A. S. Avestimehr}, 
booktitle={2016 IEEE Global Communications Conference (GLOBECOM)}, 
title={Edge-Facilitated Wireless Distributed Computing}, 
year={2016}, 
pages={1-7}, 
abstract={We propose a framework for edge-facilitated wireless distributed computing, in which several mobile users connected to an access point collaborate for a distributed computing task. We characterize the minimum communication load, both in uplink (from users to the access point) and downlink (from access point to the users), required for distributed computing. In particular, we develop a communication scheme and a dataset placement strategy that induces a particular overlap of computations at the users, which can then be exploited for coding at both users and the access point to significantly reduce the communication load. We demonstrate that the reduction in communication load (compared to uncoded solutions) can scale linearly with the size of the network (i.e., the number of users), hence our proposed scheme can result in a "scalable" design for edge- facilitated wireless distributed computing (i.e., accommodating any number of users without incurring extra communication load). Furthermore, we establish the optimality of the proposed scheme by developing a tight information theoretic outer- bound, and demonstrate that the proposed scheme achieves the minimum uplink and downlink communication load simultaneously. We also generalize the results to a decentralized setting, in which a random and a priori unknown subset of users may participate in distributed computing at each time, and characterize the minimum communication load for uniformly random dataset placement at users.}, 
keywords={mobile computing;quality of service;QoS;dataset placement strategy;decentralized setting;edge-facilitated wireless distributed computing;mobile users;quality of service;Computational modeling;Distributed computing;Downlink;Encoding;Mobile communication;Uplink;Wireless communication}, 
doi={10.1109/GLOCOM.2016.7841765}, 
month={Dec},}
@INPROCEEDINGS{7363441, 
author={V. Kumar and E. Todorov}, 
booktitle={2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)}, 
title={MuJoCo HAPTIX: A virtual reality system for hand manipulation}, 
year={2015}, 
pages={657-663}, 
abstract={Data-driven methods have lead to advances in multiple fields including robotics. These methods however have had limited impact on dexterous hand manipulation, partly due to lack of rich and physically-consistent dataset as well as technology able to collect them. To fill this gap, we developed a virtual reality system combining real-time motion capture, physics simulation and stereoscopic visualization. The system enables a user wearing a CyberGlove to "reach-in" the simulation, and manipulate virtual objects through contacts with a tele-operated virtual hand. The system is evaluated on a subset of tasks in the Southampton Hand Assessment Procedure - which is a clinically validated test of hand function. The system is also being used by performer teams in the DARPA Hand Proprioception & Touch Interfaces program to develop neural control interfaces in simulation. The software is freely available at www.mujoco.org.}, 
keywords={control engineering computing;data visualisation;dexterous manipulators;telerobotics;virtual reality;CyberGlove;MuJoCo HAPTIX;Southampton hand assessment procedure;data-driven methods;dexterous hand manipulation;neural control interfaces;physics simulation;real-time motion capture;stereoscopic visualization;tele-operated virtual hand;virtual object manipulation;virtual reality system;Glass;Monitoring;Physics;Robots;Solid modeling;Three-dimensional displays;Tracking}, 
doi={10.1109/HUMANOIDS.2015.7363441}, 
month={Nov},}
@INPROCEEDINGS{7023444, 
author={Y. T. Wen and P. R. Lei and W. C. Peng and X. F. Zhou}, 
booktitle={2014 IEEE International Conference on Data Mining}, 
title={Exploring Social Influence on Location-Based Social Networks}, 
year={2014}, 
pages={1043-1048}, 
abstract={Recently, with the advent of location-based social networking services (LBSNs), travel planning and location-aware information recommendation based on LBSNs have attracted much research attention. In this paper, we study the impact of social relations hidden in LBSNs, i.e., The social influence of friends. We propose a new social influence-based user recommender framework (SIR) to discover the potential value from reliable users (i.e., Close friends and travel experts). Explicitly, our SIR framework is able to infer influential users from an LBSN. We claim to capture the interactions among virtual communities, physical mobility activities and time effects to infer the social influence between user pairs. Furthermore, we intend to model the propagation of influence using diffusion-based mechanism. Moreover, we have designed a dynamic fusion framework to integrate the features mined into a united follow probability score. Finally, our SIR framework provides personalized top-k user recommendations for individuals. To evaluate the recommendation results, we have conducted extensive experiments on real datasets (i.e., The Go Walla dataset). The experimental results show that the performance of our SIR framework is better than the state-of the-art user recommendation mechanisms in terms of accuracy and reliability.}, 
keywords={recommender systems;social networking (online);user interfaces;LBSN;SIR framework;diffusion-based mechanism;dynamic fusion framework;influential users;location-aware information recommendation;location-based social networking services;location-based social networks;mobility activities;personalized top-k user recommendations;probability score;reliability;social influence-based user recommender framework;social relations;travel planning;user recommendation mechanisms;virtual communities;Cities and towns;Educational institutions;Equations;Heating;Mathematical model;Social network services;Tuning}, 
doi={10.1109/ICDM.2014.66}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{4142629, 
author={W. N. Bhukya and G. S. Kumar and A. Negi}, 
booktitle={TENCON 2006 - 2006 IEEE Region 10 Conference}, 
title={A Study of Effectiveness in Masquerade Detection}, 
year={2006}, 
pages={1-4}, 
abstract={Masquerade attacks are attempts by unauthorized users to gain access to confidential data or greater access privileges, while pretending to be legitimate users. Detection of masquerade attacks is of great importance and is a non-trivial task of system security. While several approaches do exist for masquerade detection, the relative effectiveness of approaches still needs considerable improvement. While in the past certain cost formulations have been used to compute the overall performance of masquerade detection methods, but these formulations appeared to be biased. Hence we present a formulation to compute the effectiveness of masquerade detection and also present a highly effective approach to masquerade detection using hidden Markov models (HMM). Our experimentation is on the well-known Schonalu dataset (SEA). Experimentation shows our approach to be most effective in the set of known approaches}, 
keywords={hidden Markov models;security of data;HMM;SEA;Schonalu dataset;hidden Markov model;masquerade attacks detection;system security;Computer science;Costs;Data security;Detection algorithms;Hidden Markov models;Information security;Protection;Sequences;Standardization;Support vector machines}, 
doi={10.1109/TENCON.2006.344199}, 
ISSN={2159-3442}, 
month={Nov},}
@INPROCEEDINGS{7333810, 
author={S. Gaur and S. Sonkar and P. P. Roy}, 
booktitle={2015 13th International Conference on Document Analysis and Recognition (ICDAR)}, 
title={Generation of synthetic training data for handwritten Indic script recognition}, 
year={2015}, 
pages={491-495}, 
abstract={This paper presents a novel approach to create synthetic dataset for word recognition systems. Our purpose is to improve performance of off-line handwritten text recognizers by providing it with additional synthetic training data. Due to lack of proper data-set for many languages it becomes hard to train recognition systems. To solve such problems synthetic handwriting could be used to expand the existing training dataset. Any available digital data from online newspaper and such sources can be used to generate this synthetic data. The digital data is distorted in such a way that the underlying pattern is conserved for identification of the word by both machine and human user. The images hence produced can be used to train any classification system for handwriting recognition. This data can be used independently to train the system or be combined with natural handwritten data to augment the original dataset and improve the accuracy of the results. We experimented using only synthetic data obtaining high recognition accuracy in both character and word recognition. The data was tested on 3 Indian scripts for numerals- Hindi, Bengali and Telugu, and 1 script-Hindi for words, the results achieved hence are highly promising.}, 
keywords={handwritten character recognition;image classification;natural language processing;text detection;Bengali;Hindi;Telugu;classification system;digital data;handwritten Indic script recognition;natural handwritten data;offline handwritten text recognizer;online newspaper;synthetic handwriting;synthetic training data generation;word recognition systems;Accuracy;Distortion;Handwriting recognition;Image recognition;Principal component analysis;Testing;Trajectory;Hidden Markov Models;Indic Text Recognition;Synthetic Data Generation}, 
doi={10.1109/ICDAR.2015.7333810}, 
month={Aug},}
@ARTICLE{6994839, 
author={J. Bian and Y. Yang and H. Zhang and T. S. Chua}, 
journal={IEEE Transactions on Multimedia}, 
title={Multimedia Summarization for Social Events in Microblog Stream}, 
year={2015}, 
volume={17}, 
number={2}, 
pages={216-228}, 
abstract={Microblogging services have revolutionized the way people exchange information. Confronted with the ever-increasing numbers of social events and the corresponding microblogs with multimedia contents, it is desirable to provide visualized summaries to help users to quickly grasp the essence of these social events for better understanding. While existing approaches mostly focus only on text-based summary, microblog summarization with multiple media types (e.g., text, image, and video) is scarcely explored. In this paper, we propose a multimedia social event summarization framework to automatically generate visualized summaries from the microblog stream of multiple media types. Specifically, the proposed framework comprises three stages, as follows. 1) A noise removal approach is first devised to eliminate potentially noisy images. An effective spectral filtering model is exploited to estimate the probability that an image is relevant to a given event. 2) A novel cross-media probabilistic model, termed Cross-Media-LDA (CMLDA), is proposed to jointly discover subevents from microblogs of multiple media types. The intrinsic correlations among these different media types are well explored and exploited for reinforcing the cross-media subevent discovery process. 3) Finally, based on the cross-media knowledge of all the discovered subevents, a multimedia microblog summary generation process is designed to jointly identify both representative textual and visual samples, which are further aggregated to form a holistic visualized summary. We conduct extensive experiments on two real-world microblog datasets to demonstrate the superiority of the proposed framework as compared to the state-of-the-art approaches.}, 
keywords={Web sites;data visualisation;document handling;multimedia computing;probability;CMLDA;Cross-Media-LDA;cross-media knowledge;cross-media probabilistic model;cross-media subevent discovery process;holistic visualized summary;image relevance probability estimation;information exchange;media type correlation;microblog dataset;microblog stream;microblog summarization;microblogging services;multimedia content;multimedia microblog summary generation process;multimedia social event summarization framework;multiple media types;noise removal approach;potentially noisy image elimination;representative textual samples;representative visual samples;spectral filtering model;visualized summaries;Feature extraction;Media;Multimedia communication;Noise measurement;Semantics;Streaming media;Visualization;Microblog;multimedia summarization;social event}, 
doi={10.1109/TMM.2014.2384912}, 
ISSN={1520-9210}, 
month={Feb},}
@INPROCEEDINGS{6916903, 
author={L. Guo and J. Shao and K. L. Tan and Y. Yang}, 
booktitle={2014 IEEE 15th International Conference on Mobile Data Management}, 
title={WhereToGo: Personalized Travel Recommendation for Individuals and Groups}, 
year={2014}, 
volume={1}, 
pages={49-58}, 
abstract={With the rapid development of GPS-enabled mobile devices, huge amounts of user-contributed data with location information can be collected from the Internet. With this kind of data, one promising application is travel recommendation, which has attracted a considerable number of researches recently. However, most of the previous studies only focus on one aspect of the relations among users and locations or make a coarse linear combination of the relations. Moreover, all the existing work on travel recommendation do not consider recommendation to groups, which is an important characteristic of travelers' behavior. In this paper, we present a personalized travel recommendation system named Where to Go. The novelty of the system is a 3R model which can unify user-location relation, user-user relation and location-location relation into a single framework and perform random walk with restart to analyze the model. We further extend our approach to provide recommendations for groups. To the best of our knowledge, this is the first work to use random walk with restart for group recommendation. We conduct a comprehensive performance evaluation using a real dataset collected from Flickr, which is one of the most popular online photo-sharing sites. Experimental results show that our approach provides significantly superior recommendation quality compared to other state-of-the-art travel recommendation approaches for both individuals and groups.}, 
keywords={Internet;recommender systems;travel industry;3R model;Flickr;GPS-enabled mobile devices;Internet;Where to Go;comprehensive performance evaluation;group recommendation;location-location relation model;online photo-sharing sites;personalized travel recommendation system;random walk;user-location relation model;user-user relation model;Analytical models;Cities and towns;Clustering algorithms;Collaboration;Mathematical model;Social network services;Web sites;geo-tagged photos;personalized travel recommendation;random walk with restart}, 
doi={10.1109/MDM.2014.12}, 
ISSN={1551-6245}, 
month={July},}
@INPROCEEDINGS{6729609, 
author={Z. X. Liao and S. C. Li and W. C. Peng and P. S. Yu and T. C. Liu}, 
booktitle={2013 IEEE 13th International Conference on Data Mining}, 
title={On the Feature Discovery for App Usage Prediction in Smartphones}, 
year={2013}, 
pages={1127-1132}, 
abstract={With the increasing number of mobile Apps developed, they are now closely integrated into daily life. In this paper, we develop a framework to predict mobile Apps that are most likely to be used regarding the current device status of a smartphone. Such an Apps usage prediction framework is a crucial prerequisite for fast App launching, intelligent user experience, and power management of smartphones. By analyzing real App usage log data, we discover two kinds of features: The Explicit Feature (EF) from sensing readings of built-in sensors, and the Implicit Feature (IF) from App usage relations. The IF feature is derived by constructing the proposed App Usage Graph (abbreviated as AUG) that models App usage transitions. In light of AUG, we are able to discover usage relations among Apps. Since users may have different usage behaviors on their smartphones, we further propose one personalized feature selection algorithm. We explore minimum description length (MDL) from the training data and select those features which need less length to describe the training data. The personalized feature selection can successfully reduce the log size and the prediction time. Finally, we adopt the kNN classification model to predict Apps usage. Note that through the features selected by the proposed personalized feature selection algorithm, we only need to keep these features, which in turn reduces the prediction time and avoids the curse of dimensionality when using the kNN classifier. The results based on a real dataset demonstrate the effectiveness of the proposed framework and show the predictive capability for App usage prediction.}, 
keywords={feature selection;graph theory;mobile computing;pattern classification;power aware computing;smart phones;AUG;App usage graph;App usage log data analysis;App usage transitions;IF feature;MDL;built-in sensors;curse of dimensionality;explicit feature;fast App launching;feature discovery;implicit feature;intelligent user experience;kNN classification model;kNN classifier;log size reduction;minimum description length;mobile App usage prediction framework;personalized feature selection algorithm;prediction time reduction;smartphones;training data;Equations;Prediction algorithms;Sensors;Smart phones;Testing;Training;Training data;Mobile Application;Usage prediction;classification;mobile Apps}, 
doi={10.1109/ICDM.2013.130}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{877386, 
author={M. R. Berthold and R. Holve}, 
booktitle={PeachFuzz 2000. 19th International Conference of the North American Fuzzy Information Processing Society - NAFIPS (Cat. No.00TH8500)}, 
title={Visualizing high dimensional fuzzy rules}, 
year={2000}, 
pages={64-68}, 
abstract={In this paper we present an approach to visualize a potentially high-dimensional and large number of (fuzzy) rules in two dimensions. This visualization presents the entire set of rules to the user as one coherent picture. We use a gradient descent based algorithm to generate a 2D-view of the rule set which minimizes the error on the pair-wise fuzzy distances between all rules. This approach is superior to a simple projection and also most non-linear transformations in that it concentrates on the important feature, that is the inter-point distances. In order to make use of the uncertain nature of the underlying fuzzy rules, a new fuzzy distance-measure was developed. The visualizations of a rule set for the well-known IRIS dataset as well as fuzzy models for other benchmark data sets are illustrated and discussed}, 
keywords={data visualisation;fuzzy logic;IRIS dataset;benchmark data sets;fuzzy distance-measure;gradient descent based algorithm;high dimensional fuzzy rules visualization;Data mining;Data visualization;Equations;Euclidean distance;Fuzzy sets;Iris;Marine vehicles;Multidimensional systems;Springs}, 
doi={10.1109/NAFIPS.2000.877386}, 
month={},}
@INPROCEEDINGS{7500265, 
author={X. Li and P. Xu and Y. Shi and M. Larson and A. Hanjalic}, 
booktitle={2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI)}, 
title={Simple tag-based subclass representations for visually-varied image classes}, 
year={2016}, 
pages={1-6}, 
abstract={In this paper, we present a subclass-representation approach that predicts the probability of a social image belonging to one particular class. We explore the co-occurrence of user-contributed tags to find subclasses with a strong connection to the top level class. We then project each image onto the resulting subclass space, generating a subclass representation for the image. The advantage of our tag-based subclasses is that they have a chance of being more visually stable and easier to model than top-level classes. Our contribution is to demonstrate that a simple and inexpensive method for generating sub-class representations has the ability to improve classification results in the case of tag classes that are visually highly heterogenous. The approach is evaluated on a set of 1 million photos with 10 top-level classes, from the dataset released by the ACM Multimedia 2013 Yahoo! Large-scale Flickr-tag Image Classification Grand Challenge. Experiments show that the proposed system delivers sound performance for visually diverse classes compared with methods that directly model top classes.}, 
keywords={image classification;image representation;Flickr-tag Image Classification Grand Challenge;Yahoo;image subclass representation;social image;tag classes;tag-based subclass representations;visually-varied image classes;Flickr;Multimedia communication;Predictive models;Support vector machines;Tagging;Training;Visualization}, 
doi={10.1109/CBMI.2016.7500265}, 
month={June},}
@INPROCEEDINGS{7230954, 
author={L. Wang and T. Lu and H. Gu and X. Ding and N. Gu}, 
booktitle={2015 IEEE 19th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
title={Influential user recommendation through SVD based topic diversification}, 
year={2015}, 
pages={176-181}, 
abstract={With the development of web 2.0, online community websites have become one of the most interactive ways for disabilities to communicate with others and understand the world. However, new disabled users may sometimes find it difficult to learn about existing communities and the topics they feel interested in are drastically different from normal users. On the other hand, influential users have already developed their impact in different topics within communities and are hence able to help disabilities quickly adapt to the communities. As the result, a method that can recommend influential users in diverse topics is necessary. However, traditional approaches to identify influential users are not very comprehensive and accurate without the consideration of topic diversity. In this paper, we present an evaluation index system and a SVD based influence model which can evaluate influence comprehensively. The discussions in online communities are then classified by the topics that attract disabilities most. Finally, a combined influential users recommendation with diverse topics is proposed. We conduct experiments on an online disability community dataset and results show that our approach significantly outperforms other traditional methods.}, 
keywords={Internet;Web sites;recommender systems;user interfaces;SVD;Web 2.0;evaluation index system;influential user recommendation;online community Web sites;topic diversification;SVD;disability;influential user;recommendation;topics}, 
doi={10.1109/CSCWD.2015.7230954}, 
month={May},}
@INPROCEEDINGS{6738595, 
author={P. Lovato and A. Perina and D. S. Cheng and C. Segalin and N. Sebe and M. Cristani}, 
booktitle={2013 IEEE International Conference on Image Processing}, 
title={We like it! Mapping image preferences on the counting grid}, 
year={2013}, 
pages={2892-2896}, 
abstract={Modeling user preferences in photographic images is often reduced to analyzing intermediate explicit representations (e.g. textual tags) as means of capturing the objective and subjective properties of image perception, trying to distill the essence of what gives pleasure. We propose an alternative approach that bypasses the necessity to build an explicit conceptual coding of image preferences, operating directly on the raw properties of the images, extracted with heterogeneous feature descriptors. This is achieved through the counting grid model, which fuses together content-based and aesthetics themes into a 2D map in an unsupervised way. We show that certain locations in this map correspond to perceptually intuitive image classes, even without relying on tags or other user-defined information. Moreover, we show that users' individual preferences can be represented as distributions over the map, allowing us to evaluate the affinity between different users' appreciations. We experiment on a large Flickr dataset, clustering users by affinity, and validating these clusters by checking users that belong to the same Flickr photo groups.}, 
keywords={feature extraction;image coding;photography;2D map;Flickr dataset;Flickr photo group;aesthetics theme;content-based image processing;counting grid model;feature extraction;image classes;image perception;image preference coding;image preference mapping;photographic images;Image preferences;content-based image processing;counting grid;image aesthetics}, 
doi={10.1109/ICIP.2013.6738595}, 
ISSN={1522-4880}, 
month={Sept},}
@INPROCEEDINGS{6405918, 
author={A. T. Hoang and M. T. Tran and A. D. Duong and I. Echizen}, 
booktitle={2012 Eighth International Conference on Computational Intelligence and Security}, 
title={An Indexed Bottom-up Approach for Publishing Anonymized Data}, 
year={2012}, 
pages={641-645}, 
abstract={Sharing information is one of the most important parts of social activities. However, sharing information can leak users' information. Removing all direct identifiers is not enough. Sweeney proposed an approach that applying k-anonymity to protect users' identities from linking attack. Sweeney`s algorithm finds out the optimal anonymized dataset through minimal distortion metric. Other authors proposed other optimal algorithms but their proposals are still impractical due to their high computational cost. Another approach is to release the minimal anonymized dataset by applying some heuristics. Wang and Fung proposed Bottom-up Generalization and Top-down Specialization (TDS) to publish a minimal anonymized dataset with information loss metric, whose performance is more efficient. However, these algorithms still have some limitations. In this paper, we propose an algorithm to publish anonymized datasets through bottom-up generalization approach and information loss data metric. Our algorithm can save time by storing statistical information for later usage. The experimental results is performanced on Adult dataset, which is used in all former algorithms. Experimental results show that our algorithm can process 949,662 records dataset in 42.219s. Classification error on anonymized data, which is created by our algorithm, is lower than Wang's algorithm 3.8%.}, 
keywords={electronic publishing;peer-to-peer computing;security of data;statistical analysis;Sweeney`s algorithm;TDS;Wang algorithm;adult dataset;anonymized data Classification error;anonymized data publishing;fung proposed bottom-up generalization;high computational cost;indexed bottom-up approach;information loss data metric;information sharing;k-anonymity;linking attack;minimal distortion metric;optimal anonymized dataset;social activities;statistical information storage;top-down specialization;users identities protection;Data models;Data privacy;Measurement;Partitioning algorithms;Publishing;Taxonomy;Training;Bottom-up;anonymized data;k-anonymity}, 
doi={10.1109/CIS.2012.148}, 
month={Nov},}
@INPROCEEDINGS{7015444, 
author={C. Pagano and E. Granger and R. Sabourin and A. Rattani and G. L. Marcialis and F. Roli}, 
booktitle={2014 IEEE Symposium on Computational Intelligence in Biometrics and Identity Management (CIBIM)}, 
title={Efficient adaptive face recognition systems based on capture conditions}, 
year={2014}, 
pages={60-67}, 
abstract={In many face recognition (FR) applications, changing capture conditions lead to divergence between facial models stored during enrollment and faces captured during operations. Moreover, it is often costly or infeasible to capture several high quality reference samples a priori to design representative facial models. Although self-updating models using high-confidence face captures appear promising, they raise several challenges when capture conditions change. In particular, face models of individuals may be corrupted by misclassified input captures, and their growth may require pruning to bound system complexity over time. This paper presents a system for self-update of facial models that exploits changes in capture conditions to assure the relevance of templates and to limit the growth of template galleries. The set of reference templates (facial model) of an individual is only updated to include new faces that are captured under significantly different conditions. In a particular implementation of this system, illumination changes are detected in order to select face captures from bio-login to be stored in a gallery. Face captures from a built-in still or video camera are taken at periodic intervals to authenticate the user having accessed a secured computer or network. Experimental results produced with the DIEE dataset show that the proposed system provides a comparable level of performance to the FR system that self-updates the gallery on all high-confidence face captures, but with significantly lower complexity, i.e., number of templates per individual.}, 
keywords={face recognition;DIEE dataset;FR applications;adaptive face recognition systems;capture conditions;face recognition applications;high quality reference samples;periodic intervals;reference templates;representative facial models;self-updating models;system complexity;template galleries;video camera;Adaptation models;Biological system modeling;Distortion measurement;Face;Feature extraction;Indexes;Lighting}, 
doi={10.1109/CIBIM.2014.7015444}, 
ISSN={2325-4300}, 
month={Dec},}
@INPROCEEDINGS{7815192, 
author={L. Mengjuan and W. Wei and Z. Fan and X. Hao and Q. Zhiguang and L. Xucheng}, 
booktitle={2016 International Conference on Advanced Cloud and Big Data (CBD)}, 
title={ActiveRec: A Novel Context-Sensitive Ranking Method for Active Movie Recommendation}, 
year={2016}, 
pages={92-97}, 
abstract={This paper presents a novel context-sensitive ranking algorithm, called ActiveRec, for providing flexible movie recommendations. Typically, ActiveRec can recommend movies to a user according to a specific movie type, or to a group of users satisfying their common interests. Firstly, ActiveRec constructs a multipartite graph where the nodes represent users, movies, and joint information, respectively. And then, a biased random-walk is performed to obtain the similarity between the request vector and every node on the graph. Based on the similarities, ActiveRec sorts out all movies that meet the user requirements and notify the user of a Top-N list. Additionally, a time-decay model following the Ebbinghaus forgetting curve is introduced to imitate the decay process of the importance of users' feedbacks when computing the edges' weights in the graph. Extensive experiments are performed on a real dataset to evaluate the performance. The results demonstrate that ActiveRec not only satisfies the requirements of flexible recommendations, but also achieves higher performance compared to the existing works.}, 
keywords={graph theory;recommender systems;ActiveRec;Ebbinghaus forgetting curve;active movie recommendation;biased random-walk;multipartite graph;novel context-sensitive ranking method;Big data;Context-Sensitive;Graph Recommendation;Random-Walk;Time-decay Model}, 
doi={10.1109/CBD.2016.026}, 
month={Aug},}
@INPROCEEDINGS{6693499, 
author={R. d. S. Villaça and L. B. d. Paula and R. Pasquini and M. F. Magalhães}, 
booktitle={2013 IEEE Seventh International Conference on Semantic Computing}, 
title={A Similarity Search System Based on the Hamming Distance of Social Profiles}, 
year={2013}, 
pages={90-93}, 
abstract={The goal of a similarity search system is to allow users to retrieve data that presents a required similarity level in a certain dataset. For example, such dataset may be applied in the social media scenario, where huge amounts of data represent users in a social network. This paper uses a Vector Space Model (VSM) to represent users' profiles and the Random Hyper plane Hashing (RHH) function to create indexes for them. Both VSM and RHH compose an alternative to address the challenge of performing similarity searches over the huge amount of data present in the social media scenario: the Hamming similarity. In order to evaluate the effectiveness of our proposal, this paper brings examples of reference profiles, used for performing queries, and presents results regarding the correlation between cosine and Hamming similarity and the frequency distribution of Hamming distances among identifiers of users' profiles. In short, the results indicate that Hamming similarity can be useful for the development of similarity search systems for social media.}, 
keywords={query formulation;social networking (online);Hamming distances;Hamming similarity;RHH function;VSM;data retrieval;frequency distribution;queries;random hyper plane hashing;reference profiles;similarity level;similarity search systems;similarity searches;social media;social network;social profiles;users profiles;vector space model;Correlation;Databases;Hamming distance;Measurement;Prototypes;Social network services;Vectors;Hamming distance;RHH;Similarity Search}, 
doi={10.1109/ICSC.2013.24}, 
month={Sept},}
@INPROCEEDINGS{5990561, 
author={A. M. Elmisery and D. Botvich}, 
booktitle={12th IFIP/IEEE International Symposium on Integrated Network Management (IM 2011) and Workshops}, 
title={Private recommendation service for IPTV systems: Protecting user profile privacy}, 
year={2011}, 
pages={571-577}, 
abstract={IPTV providers employ third party content recommendation service to help end users find personalized content, and at the same time increase content sales and gain competitive advantage over other IPTV providers. However current implementations of recommendation services are mostly centralized where all the information about the users' profiles is stored on a dedicated server. A common fear among users is that their user profile data being misused by recommendation provider. Also sharing profile data makes the end users vulnerable to attacks like insider attacks, where an employee of the recommendation service may compromise the confidentiality and integrity of their profiles. For these reasons, privacy aware users intentionally decline to use recommendation or even provide inaccurate or wrong information because they consider it as untrusted service. On the other hand, to build an accurate recommendation model the user must reveal information that is typically considered private such as watching history, previous buying behaviour, content ratings, etc. Further privacy concerns arise when the user data are stored in countries that have privacy laws different from the country where the service is consumed. This poses a severe privacy hazard, since the users profiles are fully under the control of recommendation provider and stored in locations that are not legally bound to ensure the privacy of its users. Due to different legal structures that relate to data privacy laws in different legal jurisdictions maintaining user profile privacy is not a trivial solution. Regardless of the official legal framework requirements, when outsourcing users' profiles the private data should be kept safe when it is in the possession of any third party service. In this paper we introduce a private recommendation method using collaborative filtering techniques. The method preserves the privacy of its users when using the system and allows sharing data among different users in the netwo- - rk. We also introduce two obfuscations algorithms that protect users profile privacy as well as preserve the aggregates in the dataset to maximize the utility of information to provide accurate recommendations. Using these algorithms provides the privacy of users personal profiles.}, 
keywords={IPTV;data privacy;information filtering;law;recommender systems;telecommunication services;IPTV provider;collaborative filtering techniques;data privacy;legal structures;private recommendation method;private recommendation service;recommendation provider;recommendation services;third party content recommendation service;user confidentiality;user profile data;user profile privacy protection;IPTV networks;clustering;privacy;recommender system}, 
doi={10.1109/INM.2011.5990561}, 
ISSN={1573-0077}, 
month={May},}
@INPROCEEDINGS{7551576, 
author={H. L. Cardoso and J. M. Moreira}, 
booktitle={2016 17th IEEE International Conference on Mobile Data Management (MDM)}, 
title={Human Activity Recognition by Means of Online Semi-supervised Learning}, 
year={2016}, 
volume={2}, 
pages={75-77}, 
abstract={Human activity recognition (HAR) is a reasonably recent field of study for the computer science community. It aims at automatically analysing ongoing events and extract their context from the captured data. The detection of human activities, such as walking, running, falling, or even cycling, allows for several heterogeneous applications, from surveillance systems to patient monitoring systems. Despite being a particularly active field of study in the past years, HAR still leaves many strategies left to explore and key aspects left to address. There are two main approaches in terms of data extraction: Video and sensors. The sensor approach is, however, the most promising, due to its extreme portability and unobtrusiveness. Most sensor-based HAR systems are trained in a static dataset with Supervised Learning techniques, generating a classification model with a relatively low error rate. However, these systems commonly ignore one of HAR's challenges, the difference of input signals produced by different people when doing the same activities. Consequently, as a user's movements drift from the generic, the system error increases. The activity classification method should therefore be able to generate adapted results for each different user. This article exhibits and discloses an under explored approach to this problem: By means of Online Semi-supervised Learning, An incremental technique capable of adapting the classification model to the user of the application by continuously updating it as the data from the user's own specific input signals arrives. This is possible due to the nature of Semi-supervised learning, which trains on both labeled and unlabeled data, making it possible to keep learning even after reaching its final user, without the need of any manual input.}, 
keywords={image recognition;learning (artificial intelligence);activity classification method;classification model;computer science community;cycling;data extraction;falling;heterogeneous applications;human activity recognition;incremental technique;online semisupervised learning;patient monitoring systems;portability;running;sensor-based HAR systems;sensors;surveillance systems;unobtrusiveness;video;walking;Data models;Decision trees;Semisupervised learning;Sensors;Supervised learning;Training;Human Activity Recognition;Machine Learning;Online Semi-Supervised Learning}, 
doi={10.1109/MDM.2016.93}, 
month={June},}
@INPROCEEDINGS{6092712, 
author={F. Tian and X. k. Shen and L. Xian-mei and X. Hong-tao}, 
booktitle={2011 International Conference on Virtual Reality and Visualization}, 
title={3D Model Multiple Semantic Automatic Annotation for Small Scale Labeled Data Set}, 
year={2011}, 
pages={193-198}, 
abstract={Automatically assigning keywords to 3D models is of great interest as it allows one to retrieve, index, organize and understand large collections of 3D models. Most Methods require high sample size for training, so the data quality is in high demand. For small scale labeled data set, we propose a semi-supervised method to realize the 3D models multiple semantic annotation, which needs only a small amount of hand tagged information provided by users. The proposed technique utilizes low-level shape features and the keywords are assigned using a graphed-based label transfer mechanism to expand the training dataset. A weighted metric learning method is used to learn the distance measure from the extended dataset. Then multiple semantic annotation task can be completed on the learned distance measure. The proposed method outperforms the current state-of-the-art methods on the small scale labeled dataset and large unlabelled dataset. We believe that such measure will provide a strong platform to label 3D models when a small amount of labeled models were given.}, 
keywords={information retrieval;learning (artificial intelligence);solid modelling;3D model multiple semantic automatic annotation;graphed-based label transfer mechanism;hand tagged information;semi-supervised method;small scale labeled data set;weighted metric learning method;Computational modeling;Data models;Labeling;Measurement;Semantics;Solid modeling;Three dimensional displays;3D model annotation;3D model retrieval}, 
doi={10.1109/ICVRV.2011.54}, 
month={Nov},}
@INPROCEEDINGS{5333431, 
author={Z. Guo and M. O. Ward and E. A. Rundensteiner}, 
booktitle={2009 IEEE Symposium on Visual Analytics Science and Technology}, 
title={Model space visualization for multivariate linear trend discovery}, 
year={2009}, 
pages={75-82}, 
abstract={Discovering and extracting linear trends and correlations in datasets is very important for analysts to understand multivariate phenomena. However, current widely used multivariate visualization techniques, such as parallel coordinates and scatterplot matrices, fail to reveal and illustrate such linear relationships intuitively, especially when more than 3 variables are involved or multiple trends coexist in the dataset. We present a novel multivariate model parameter space visualization system that helps analysts discover single and multiple linear patterns and extract subsets of data that fit a model well. Using this system, analysts are able to explore and navigate in model parameter space, interactively select and tune patterns, and refine the model for accuracy using computational techniques. We build connections between model space and data space visually, allowing analysts to employ their domain knowledge during exploration to better interpret the patterns they discover and their validity. Case studies with real datasets are used to investigate the effectiveness of the visualizations.}, 
keywords={data mining;data visualisation;data space;domain knowledge;linear pattern discovery;model space visualization;multivariate linear trend discovery;Computer science;Data analysis;Data mining;Data visualization;Extraterrestrial phenomena;Navigation;Pattern analysis;Predictive models;Scattering;User interfaces;Knowledge Discovery;model space visualization;multivariate linear model construction;visual analysis}, 
doi={10.1109/VAST.2009.5333431}, 
month={Oct},}
@INPROCEEDINGS{945187, 
author={J. C. Lopez and D. R. O'Hallaron}, 
booktitle={Proceedings 10th IEEE International Symposium on High Performance Distributed Computing}, 
title={Evaluation of a resource selection mechanism for complex network services}, 
year={2001}, 
pages={171-180}, 
abstract={Providing complex (resource-intensive) network services is challenging because the resources they need and the resources that are available can vary significantly from request to request. To address this issue, we have proposed a flexible mechanism, called active frames, that provides a basis for selecting a set of available distributed computing resources, and then mapping tasks onto those resources. As a proof of concept, we have used active frames to build a remote visualization service, called Dv, that allows users to visualize the contents of scientific datasets stored at remote locations. We evaluate the performance of active frames, in the context of Dv. In particular, we address the following two questions: (1) what performance penalty do we pay for the flexibility of the active frames mechanism? (2) can the throughput of a service based on active frames be predicted with reasonable accuracy from micro-benchmarks? The results of the evaluation suggest that the overhead imposed by active frames is reasonable (roughly 5%), and that simple models based on micro-benchmarks can conservatively predict measured throughput with reasonable accuracy (at most 20%)}, 
keywords={Java;client-server systems;data visualisation;local area networks;resource allocation;Dv;Java;active frames;client-server transaction;complex network services;distributed computing resources;local area network;microbenchmarks;performance;remote visualization service;resource selection mechanism;scientific dataset visualization;Accuracy;Complex networks;Distributed computing;Isosurfaces;Java;Network servers;Predictive models;Spatial resolution;Throughput;Visualization}, 
doi={10.1109/HPDC.2001.945187}, 
ISSN={1082-8907}, 
month={},}
@INPROCEEDINGS{7546412, 
author={X. Chen and Y. Guo and Y. Yang and Z. Mi}, 
booktitle={2016 International Conference on Computer, Information and Telecommunication Systems (CITS)}, 
title={Trust-based collaborative filtering algorithm in social network}, 
year={2016}, 
pages={1-5}, 
abstract={In order to improve the accuracy of recommendation algorithm in social network applications, a new recommendation method based on traditional collaborative filtering recommendation algorithm, which called Trust-based Collaborative Filtering, is proposed and verified in this paper. Firstly, we analyze users' behaviors and relationships in social network, and propose a trust calculation method based on Dijkstra's algorithm. Secondly, we integrate users' trust information into the collaborative filtering algorithm to recommend in social network. Finally, we choose Flixster dataset to validate the proposed model and use the Mean Absolute Error (MAE) as the evaluation metric. Experiment results show that Trust-based CF significantly improves the recommendation quality in social network.}, 
keywords={collaborative filtering;recommender systems;social networking (online);trusted computing;Dijkstra algorithm;MAE;evaluation metric;mean absolute error;recommendation algorithm;social network;trust calculation method;trust-based collaborative filtering algorithm;Algorithm design and analysis;Analytical models;Collaboration;Correlation;Motion pictures;Prediction algorithms;Social network services}, 
doi={10.1109/CITS.2016.7546412}, 
month={July},}
@INPROCEEDINGS{7736903, 
author={N. Abe and S. Yamada and T. Shinzaki}, 
booktitle={2016 International Conference of the Biometrics Special Interest Group (BIOSIG)}, 
title={A Novel Local Feature for Eye Movement Authentication}, 
year={2016}, 
pages={1-5}, 
abstract={Eye movement authentication technology has been proposed as a biometric modality, which enables to authenticate a user continuously, and has the counterfeit feature because of the difficulty of the imitation. By using the eye movement authentication, it is possible to realize an automatic authentication system as long as he/she is looking at a display to operate the device. However, the authentication accuracy is still low compared to the other traditional biometric modalities, such as fingerprint, face, and iris. In this paper, we propose the novel local eye movement feature to represent local differences of the captured time-series gazing position data, and we show the proposed feature can work as a complement feature against Mel Frequency Cepstrum Coefficients(MFCC), which is based on the local phase information of eye movement data.We show the classification rate improves from 61% to 82% in the BioEye2015 dataset by using our proposed method on the best case.}, 
keywords={biometrics (access control);eye;image motion analysis;time series;BioEye2015 dataset;MFCC;authentication accuracy;automatic authentication system;biometric modality;counterfeit feature;eye movement authentication technology;eye movement data;mel frequency cepstrum coefficients;phase information;time-series gazing position data;Authentication;Cepstrum;Face;Feature extraction;Iris recognition;Mathematical model;Mel frequency cepstral coefficient}, 
doi={10.1109/BIOSIG.2016.7736903}, 
month={Sept},}
@INPROCEEDINGS{7727431, 
author={Y. Wang and D. Feng and D. Li and X. Chen and Y. Zhao and X. Niu}, 
booktitle={2016 International Joint Conference on Neural Networks (IJCNN)}, 
title={A mobile recommendation system based on logistic regression and Gradient Boosting Decision Trees}, 
year={2016}, 
pages={1896-1902}, 
abstract={Real-life behaviors shown by the mobile users typically exhibit plenty noises, making it hard to construct an effective recommendation engine. In this paper, we present a fused model based on the LR algorithm and the GBDT algorithm to recommend vertical industry commodities in a mobile setting. A set of specifically designed methods are proposed to deal with the data preprocessing and feature extraction problem for the mobile recommendation scenario. The proposed method is evaluated on a large scale real-world dataset provided by the Alibaba mobile shopping department. Result on the F1 score has seen an improvement of 2%-36% compared with the baseline.}, 
keywords={decision trees;feature extraction;gradient methods;mobile commerce;recommender systems;retail data processing;Alibaba mobile shopping department;GBDT algorithm;LR algorithm;data preprocessing;feature extraction problem;gradient boosting decision trees;logistic regression;mobile recommendation system;vertical industry commodities;Computational modeling;Data models;Feature extraction;Logistics;Mobile communication;Prediction algorithms;Predictive models}, 
doi={10.1109/IJCNN.2016.7727431}, 
month={July},}
@BOOK{7464095, 
author={Liqiang Nie and Xuemeng Song and Tat-Seng Chua}, 
booktitle={Learning from Multiple Social Networks}, 
title={Learning from Multiple Social Networks}, 
year={2016}, 
pages={118-}, 
abstract={<p>With the proliferation of social network services, more and more social users, such as individuals and organizations, are simultaneously involved in multiple social networks for various purposes. In fact, multiple social networks characterize the same social users from different perspectives, and their contexts are usually consistent or complementary rather than independent. Hence, as compared to using information from a single social network, appropriate aggregation of multiple social networks offers us a better way to comprehensively understand the given social users. </p><p> Learning across multiple social networks brings opportunities to new services and applications as well as new insights on user online behaviors, yet it raises tough challenges: (1) How can we map different social network accounts to the same social users? (2) How can we complete the item-wise and block-wise missing data? (3) How can we leverage the relatedness among sources to strengt en the learning performance? And (4) How can we jointly model the dual-heterogeneities: multiple tasks exist for the given application and each task has various features from multiple sources? These questions have been largely unexplored to date. </p><p> We noticed this timely opportunity, and in this book we present some state-of-the-art theories and novel practical applications on aggregation of multiple social networks. In particular, we first introduce multi-source dataset construction. We then introduce how to effectively and efficiently complete the item-wise and block-wise missing data, which are caused by the inactive social users in some social networks. We next detail the proposed multi-source mono-task learning model and its application in volunteerism tendency prediction. As a counterpart, we also present a mono-source multi-task learning model and apply it to user interest inference. We seamlessly unify these models with the so-called multi-source multi-ta k learning, and demonstrate several application scenarios, such as occupation prediction. Finally, we conclude the book and figure out the future research directions in multiple social network learning, including the privacy issues and source complementarity modeling. </p><p> This is preliminary research on learning from multiple social networks, and we hope it can inspire more active researchers to work on this exciting area. If we have seen further it is by standing on the shoulders of giants.</p>}, 
doi={10.2200/S00714ED1V01Y201603ICR048}, 
publisher={Morgan & Claypool}, 
isbn={9781627059862}, 
url={http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7464095},}
@ARTICLE{7930456, 
author={Y. Peng and X. Huang and Y. Zhao}, 
journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
title={An Overview of Cross-media Retrieval: Concepts, Methodologies, Benchmarks and Challenges}, 
year={2017}, 
volume={PP}, 
number={99}, 
pages={1-1}, 
abstract={Multimedia retrieval plays an indispensable role in big data utilization. Past eorts mainly focused on single-media retrieval. However, the requirements of users are highly flexible, such as retrieving the relevant audio clips with one query of image. So challenges stemming from the "media gap", which means that representations of dierent media types are inconsistent, have attracted increasing attention. Cross-media retrieval is designed for the scenarios where the queries and retrieval results are of dierent media types. As a relatively new research topic, its concepts, methodologies and benchmarks are still not clear in the literatures. To address these issues, we review more than 100 references, give an overview including the concepts, methodologies, major challenges and open issues, as well as build up the benchmarks including datasets and experimental results. Researchers can directly adopt the benchmarks to promptly evaluate their proposed methods. This will help them to focus on algorithm design, rather than the time-consuming compared methods and results. It is noted that we have constructed a new dataset XMedia, which is the first publicly available dataset with up to five media types (text, image, video, audio and 3D model). We believe this overview will attract more researchers to focus on cross-media retrieval and be helpful to them.}, 
keywords={Benchmark testing;Bridges;Extraterrestrial measurements;Logic gates;Media;Solid modeling;Three-dimensional displays;Cross-media retrieval;benchmarks;challenges;concepts;methodologies;overview}, 
doi={10.1109/TCSVT.2017.2705068}, 
ISSN={1051-8215}, 
month={},}
@INPROCEEDINGS{5457534, 
author={D. Okwechime and E. J. Ong and R. Bowden}, 
booktitle={2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops}, 
title={Real-time motion control using pose space probability density estimation}, 
year={2009}, 
pages={2056-2063}, 
abstract={The ability to control the movements of an object or person in a video sequence has applications in the movie and animation industries, and in HCI. In this paper, we introduce a new algorithm for real-time motion control and demonstrate its application to pre-recorded video clips and HCI. Firstly, a dataset of video frames are projected into a lower dimension space. A k-mediod clustering algorithm with a distance metric is used to determine groups of similar frames which operate as cut points, segmenting the data into smaller subsequences. A multivariate probability distribution is learnt and probability density estimation is used to determine transitions between the subsequences to develop novel motion. To facilitate real-time control, conditional probabilities are used to derive motion given user commands. The motion controller is extended to HCI using speech Mel-Frequency Ceptral Coefficients (MFCCs) to trigger movement from an input speech signal. We demonstrate the flexibility of the model by presenting results ranging from datasets composed of both vectorised images and 2D point representation. Results show plausible motion generation and lifelike blends between different types of movement.}, 
keywords={cepstral analysis;human computer interaction;image motion analysis;image representation;image sequences;pattern clustering;pose estimation;probability;real-time systems;speech processing;video signal processing;2D point representation;HCI;MFCC;conditional probability;distance metric;input speech signal;k-mediod clustering algorithm;lower dimension space;motion controller;motion generation;multivariate probability distribution;pose space probability density estimation;prerecorded video clips;real-time motion control;speech mel-frequency ceptral coefficients;vectorised images;video frames;video sequence;Animation;Clustering algorithms;Human computer interaction;Industrial control;Motion control;Motion estimation;Motion pictures;Probability distribution;Speech;Video sequences}, 
doi={10.1109/ICCVW.2009.5457534}, 
month={Sept},}
@INPROCEEDINGS{7230958, 
author={L. G. Montané-Jiménez and E. Benítez-Guerrero and C. Mezura-Godoy and J. A. Pino}, 
booktitle={2015 IEEE 19th International Conference on Computer Supported Cooperative Work in Design (CSCWD)}, 
title={Measuring Social Presence in groupware systems}, 
year={2015}, 
pages={200-205}, 
abstract={Software developers have been recently interested in building next generation groupware systems that provide environments adaptable to various contexts of users, thus, offering information and services in a timely way depending on the developed activity. However, the existing solutions for building such systems are mostly limited to modeling and representing individual indicators, neglecting the importance of the social indicators. By contrast, in this paper we present a mechanism for designing and measuring an indicator called Social Presence (SP), which allows to represent the relevance of users when they are gradually participating in a collaborative activity. Thus, teams of users can be supported for decision-making with an indicator that mostly reflects teamwork and not only individual work. In order to propose this mechanism, we designed an algorithm to measure social presence from the results of the objectives and tasks associated to a collaborative activity. For evaluating this algorithm, we implemented an experiment with 24 users, which allowed us to obtain a dataset with team information from a groupware called AssaultCube - a collaborative First-Person-Shooter type videogame. The obtained results evidence that performance measurement on teams can still be improved.}, 
keywords={computer games;decision making;groupware;AssaultCube;collaborative activity;collaborative first-person-shooter type videogame;decision-making;next generation groupware systems;social indicators;social presence;teamwork;Collaborative software;Collaborative work;Collaborative Activity;Groupware;Social Presence;Team Performance;Videogames}, 
doi={10.1109/CSCWD.2015.7230958}, 
month={May},}
@INPROCEEDINGS{5646297, 
author={J. Li and S. Huang and G. Li and R. Cao and X. Pei and H. Zheng and G. Song and Y. Wu}, 
booktitle={2010 3rd International Congress on Image and Signal Processing}, 
title={Reconstrucion and visualization of 3D surface model from serial-sectioned contour points}, 
year={2010}, 
volume={5}, 
pages={2396-2400}, 
abstract={In three-dimensional accurate radiotherapy treatment planning system, one of the key steps in the whole planning is to reconstruct and visualize the surface model of tumor target and organs at risk (OAR) from a serial of cross-sectioned contour points rapidly and accurately. This study designed a fast 3D-reconstruction and visualization pipeline comprising the following four main steps: (1) pre-processing of contour points dataset; (2) extraction and simplification of Iso-surface; (3) linear transformation of surface model; (4) Smoothing the surface model. An open source Visualization Toolkit (VTK) was applied to implement this method and a friendly user interface was developed on the Visual C++ development platform. Several clinic patients' CT datasets were chosen for test data. The results show that this method could effectively avoid the “ladder effect” of standard marching cubes (MC) algorithm due to inconsistency between slice spacing and image resolution. The reconstruction surface is simplified to speed up the rendering time. Furthermore, it has been successfully integrated into domestic Accurate/Advanced Radiotherapy Treatment Planning System (ARTS) for clinical use.}, 
keywords={C++ language;data visualisation;feature extraction;graphical user interfaces;image reconstruction;medical image processing;radiation therapy;rendering (computer graphics);solid modelling;tumours;3D reconstruction;3D surface model;Visual C++ development platform;clinical use;cross-sectioned contour point;friendly user interface;iso-surface extraction;iso-surface simplification;linear transformation;open source visualization toolkit;organs at risk;radiotherapy treatment planning system;rendering time;serial-sectioned contour point;surface model reconstruction;surface model smoothing;surface model visualization;tumor target;visualization pipeline;Data visualization;Image reconstruction;Planning;Rendering (computer graphics);Surface reconstruction;Surface treatment;Three dimensional displays;3D-reconstruction;ARTS;VTK;Visualization;surface model}, 
doi={10.1109/CISP.2010.5646297}, 
month={Oct},}
@INPROCEEDINGS{6785750, 
author={S. Chang and A. Pal}, 
booktitle={2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)}, 
title={Routing questions for collaborative answering in Community Question Answering}, 
year={2013}, 
pages={494-501}, 
abstract={Community Question Answering (CQA) service enables its users to exchange knowledge in the form of questions and answers. By allowing the users to contribute knowledge, CQA not only satisfies the question askers but also provides valuable references to other users with similar queries. Due to a large volume of questions, not all questions get fully answered. As a result, it can be useful to route a question to a potential answerer. In this paper, we present a question routing scheme which takes into account the answering, commenting and voting propensities of the users. Unlike prior work which focuses on routing a question to the most desirable expert, we focus on routing it to a group of users - who would be willing to collaborate and provide useful answers to that question. Through empirical evidence, we show that more answers and comments are desirable for improving the lasting value of a question-answer thread. As a result, our focus is on routing a question to a team of compatible users.We propose a recommendation model that takes into account the compatibility, topical expertise and availability of the users. Our experiments over a large real-world dataset shows the effectiveness of our approach over several baseline models.}, 
keywords={Internet;collaborative filtering;question answering (information retrieval);CQA service;collaborative answering;collaborative effort;community question answering;compatibility;compatible users team;question routing scheme;question-answer thread;real-world dataset;topical expertise;user availability;Availability;Collaboration;Communities;Computational modeling;Java;Knowledge discovery;Routing}, 
doi={10.1109/ASONAM.2013.6785750}, 
month={Aug},}
@ARTICLE{7464124, 
author={L. Zhang and T. Zhou and Qi Zhixin and L. Guo and L. Xu}, 
journal={China Communications}, 
title={The research on e-mail Users' behavior of participating in Subjects based on social network analysis}, 
year={2016}, 
volume={13}, 
number={4}, 
pages={70-80}, 
abstract={The e-mail network is a type of social network. This study analyzes user behavior in e-mail subject participation in organizations by using social network analysis. First, the Enron dataset and the position-related information of an employee are introduced, and methods for deletion of false data are presented. Next, the three-layer model (User, Subject, Keyword) is proposed for analysis of user behavior. Then, the proposed keyword selection algorithm based on a greedy approach, and the influence and propagation of an e-mail subject are defined. Finally, the e-mail user behavior is analyzed for the Enron organization. This study has considerable significance in subject recommendation and character recognition.}, 
keywords={behavioural sciences computing;character recognition;electronic mail;greedy algorithms;social networking (online);Enron organization;character recognition;e-mail network;e-mail user behavior;greedy approach;social network analysis;subject recommendation;three-layer model;Algorithm design and analysis;Analytical models;Character recognition;Communication networks;Electronic mail;Organizations;Social network services;e-mail network;keyword selection;social network analysis;user behavior analysis}, 
doi={10.1109/CC.2016.7464124}, 
ISSN={1673-5447}, 
month={April},}
@INPROCEEDINGS{6424349, 
author={A. Ghaffar and M. A. Niazi and F. Mustafa}, 
booktitle={2012 10th International Conference on Frontiers of Information Technology}, 
title={A Critical Examination of Complex Network File Formats for Bioinformatics Data Sources}, 
year={2012}, 
pages={354-359}, 
abstract={Recent work has demonstrated the effectiveness of using complex networks for studying complex biological interactions. The process often involves conversion of biological data such as gene expressions or biochemical interactions into one of numerous complex network file formats. However, the exact selection of an appropriate file format for a particular dataset often poses a non-trivial decision problem, biologists are non-specialist end-users and find it difficult to select a particular format for data storage and conversion. In this paper, we propose a solution to this problem of the selection of a suitable network format by means of a critical evaluation based on performance analysis of empirical experiments on biological data sets of different sizes. Experimental results substantiate the hypothesis of being extra careful in the selection of a suitable complex network format based primarily on the size of the biological dataset.}, 
keywords={biochemistry;bioinformatics;complex networks;electronic data interchange;genetics;performance evaluation;storage management;biochemical interactions;bioinformatics data sources;biological data sets;complex biological interactions;complex network file formats;data conversion;data storage;gene expressions;nontrivial decision problem;performance analysis;Adaptation models;Bioinformatics;Biology;Complex networks;Computational modeling;Loading;Software;Bioinformatics;Cognitive Agent-based Computing;Complex Adaptive Systems;Complex Networks}, 
doi={10.1109/FIT.2012.70}, 
month={Dec},}
@INPROCEEDINGS{7929951, 
author={L. Guo and D. Zhang and H. Wu and B. Cui and K. L. Tan}, 
booktitle={2017 IEEE 33rd International Conference on Data Engineering (ICDE)}, 
title={From Raw Footprints to Personal Interests: Bridging the Semantic Gap via Trip Intention Aggregation}, 
year={2017}, 
pages={123-126}, 
abstract={User-generated trajectories (UGT), such as GPS footprints from wearable devices or travel records from bus companies, capture rich information of human mobility and urban dynamics in the offline world. In this paper, our objective is to enrich these raw footprints and discover the users' personal interests by utilizing the semantic information contained in the spatial-and temporal-aware user-generated contents (STUGC) published in the online world. We design a novel probabilistic framework named CO2 to connect the offline world with the online world in order to discover the users' interests directly from their raw footprints in UGT. In particular, we first propose a latent probabilistic generative model named STLDA to infer the intention attached with each trip, and then aggregate the extracted trip intentions to discover the users' personal interests. To tackle the inherent sparsity and noisiness problems of the tags in STUGC, STLDA considers the inner correlation between tags (i.e., semantic, spatial and temporal correlation) on the topic-level. To evaluate the effectiveness of CO2, we utilize a dataset containing three months of data with 5.3 billion bus records and a Twitter dataset with 1.5 million tweets published in 6 months in Singapore as a case study. Experimental results on these two real-world datasets show that CO2 is effective in discovering user interests and improves the precision of the state-of-the-art method by 280%. In addition, we also conduct a questionnaire survey in Singapore to evaluate the effectiveness of CO2. The results further validate the superiority of CO2.}, 
keywords={Bars;Bridges;Correlation;Noise measurement;Probabilistic logic;Semantics;Trajectory}, 
doi={10.1109/ICDE.2017.55}, 
month={April},}
@INPROCEEDINGS{6103474, 
author={M. A. Ghazanfar and S. Szedmak and A. Prugel-Bennett}, 
booktitle={2011 IEEE 23rd International Conference on Tools with Artificial Intelligence}, 
title={Incremental Kernel Mapping Algorithms for Scalable Recommender Systems}, 
year={2011}, 
pages={1077-1084}, 
abstract={Recommender systems apply machine learning techniques for filtering unseen information and can predict whether a user would like a given item. Kernel Mapping Recommender (KMR) system algorithms have been proposed, which offer state-of-the-art performance. One potential drawback of the KMR algorithms is that the training is done in one step and hence they cannot accommodate the incremental update with the arrival of new data making them unsuitable for the dynamic environments. From this line of research, we propose a new heuristic, which can build the model incrementally without retraining the whole model from scratch when new data (item or user) are added to the recommender system dataset. Furthermore, we proposed a novel perceptron-type algorithm, which is a fast incremental algorithm for building the model that maintains a good level of accuracy and scales well with the data. We show empirically over two datasets that the proposed algorithms give quite accurate results while providing significant computation savings.}, 
keywords={learning (artificial intelligence);perceptrons;recommender systems;KMR algorithm;data making;dynamic environment;incremental kernel mapping algorithm;kernel mapping recommender system algorithm;machine learning technique;perceptron-type algorithm;scalable recommender system;state-of-the-art performance;Algorithm design and analysis;Computational modeling;Data models;Kernel;Motion pictures;Recommender systems;Vectors;Incremental Algorithm;Kernel;Maximum Margin;Perceptron;Recommender Systems}, 
doi={10.1109/ICTAI.2011.183}, 
ISSN={1082-3409}, 
month={Nov},}
@INPROCEEDINGS{7870960, 
author={P. Nair and M. Moh and T. S. Moh}, 
booktitle={2016 International Conference on Collaboration Technologies and Systems (CTS)}, 
title={Using Social Media Presence for Alleviating Cold Start Problems in Privacy Protection}, 
year={2016}, 
pages={11-17}, 
abstract={Recommender systems play an important role in most modern e-commerce applications. They have allowed users to become aware of the myriad choices available to them. The ease of information and the abundance of options have helped users make educated decisions. A recommender system studies a user's preferences and continues learning the user's changing interests, so as to suggest items that incline with the user's interests. In cases where a user is new to the application, or the user prefers not to discourse preferences, the recommender system is unable to gather the user's preference on any item. This is called the cold start problem; wherein the system can make valid recommendations only once the user starts informing the system about his/her choices. In this paper, we discuss the challenges faced by the cold start problem and how this problem may be alleviated using social media. We suggest an approach where we collect public information from users' social media accounts and analyze this information to understand their preferences. In particular, we gather the new user's information using their Twitter profile; i.e., the user's interest and preferences are extracted from his/her Twitter profile by analyzing his/her tweets. These interests will help the system understand what kind of movies the user will be most interested in. We compare these preferences with the metadata about the individual items. Using this approach, we develop a movie recommendation system wherein we produce top-N movie recommendations for a user. We used the MovieTweetings dataset to model the application. Two sets of results have been produced. In the first, smaller set of 770 users, 72.67% of users have received 100% accurate movie recommendations while nearly 80% of users got more than 75% accuracy. For the second, larger set of more than 3,500 users, 53 % of users have received 100% accurate recommendations while 72% of users got more than 75% accuracy. The- e encouraging results have demonstrated that the approach is effectively in alleviating cold start problems in recommendation systems, and may be applicable to many other e-commerce applications.}, 
keywords={data privacy;human computer interaction;meta data;recommender systems;social networking (online);MovieTweetings dataset;Twitter profile;cold start problem alleviation;metadata;movie recommendation system;privacy protection;social media presence;Collaboration;Correlation;Motion pictures;Recommender systems;Twitter;cold start problem;collaborative filtering;recommender systems}, 
doi={10.1109/CTS.2016.0022}, 
month={Oct},}
@INPROCEEDINGS{7581331, 
author={J. Wang and X. Chen and Z. Chen and L. Mao}, 
booktitle={2016 IEEE International Conference on Intelligent Transportation Engineering (ICITE)}, 
title={Cluster algorithm based on LDA model for public transport passengers' trip purpose identification in specific area}, 
year={2016}, 
pages={186-192}, 
abstract={A better understanding of travel demand will enable transit authorities to evaluate the services they offer, adjust marketing strategies and improve overall transit performance. In this paper, we aim to develop a method to identify the trip purpose of passenger flow who have trips to commercial district. While the same region always has the different functions, it is fairly challenging to identify travel patterns for individual transit riders in a large dataset. To this end, we use the Latent Dirichlet Allocation algorithm to generate users' trip topic. And then, with the extraction of user topic distribution as the eigenvectors of the user, we cluster users into groups that have different trip purposes. The performance of the algorithm is compared with those of other prevailing classification algorithms. The results indicate that the proposed method outperforms other commonly used data-mining algorithms in terms of accuracy and efficiency.}, 
keywords={eigenvalues and eigenfunctions;public transport;LDA model;cluster algorithm;eigenvectors;latent Dirichlet allocation;public transport passengers trip purpose identification;transit services;travel demand;travel patterns;user topic distribution extraction;Algorithm design and analysis;Analytical models;Classification algorithms;Clustering algorithms;Hospitals;Transportation;Urban areas;LDA model;cluster algorithm;public transport;trip purpose identification}, 
doi={10.1109/ICITE.2016.7581331}, 
month={Aug},}
@ARTICLE{7862767, 
author={K. Diederichs and A. Qiu and G. Shaker}, 
journal={IEEE Sensors Letters}, 
title={Wireless Biometric Individual Identification Utilizing Millimeter Waves}, 
year={2017}, 
volume={1}, 
number={1}, 
pages={1-4}, 
abstract={Biometrics offer a personal and convenient way of keeping our identities and our data secure. Here, we introduce a method of using mm-wave sensors to identify various individuals. In our system prototype, the compact radar sensor has two transmit antennas and four receive ones. The transmitter(s) send a sequence of signals which are reflected and scattered from a nearby part of the body of a user (a hand in our demo case). Different signal processing algorithms are applied to the received signals in order to create a rich feature dataset. In our demo system, the resulting dataset is classified using a random forest machine learning model, which is shown to facilitate identifying a group of individuals with high accuracy. This technology has promising implications in terms of using mm-wave radars as an independent or an auxiliary tool for biometric authentication.}, 
keywords={biometrics (access control);learning (artificial intelligence);millimetre wave detectors;random processes;signal processing;wireless sensor networks;compact radar sensor;millimeter wave sensors;random forest machine learning model;signal processing;wireless biometric individual identification;Authentication;Biological system modeling;Fingerprint recognition;Radar;Radar antennas;Sensors;Solid modeling;Microwave/Millimeter Wave Sensors;biometrics;biosensors;identification;internet of things;radars;security}, 
doi={10.1109/LSENS.2017.2673551}, 
ISSN={2475-1472}, 
month={Feb},}
@INPROCEEDINGS{4119187, 
author={B. B. Bederson and B. Lee and C. Plaisant and H. Kang}, 
booktitle={Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)}, 
title={Exploring content-actor paired network data using iterative query refinement with NetLens}, 
year={2006}, 
pages={372-372}, 
abstract={Networks have remained a challenge for information retrieval and visualization because of the rich set of tasks that users want to accomplish. This paper demonstrates a tool, NetLens, to explore a content-actor paired network data model. The NetLens interface was designed to allow users to pose a series of elementary queries and iteratively refine visual overviews and sorted lists. This enables the support of complex queries that are traditionally hard to specify in node-link visualizations. NetLens is general and scalable in that it applies to any dataset that can be represented with our abstract content-actor data model}, 
keywords={data models;data visualisation;digital libraries;query processing;user interface management systems;NetLens;content-actor pair network data model;data visualization;information retrieval;iterative query refinement;Computer networks;Computer science;Data models;Data visualization;Educational institutions;Graphical user interfaces;Law;Legal factors;Software libraries;User interfaces;digital library;human-computer interaction;incremental data exploration;iterative query refinement;network visualization;piccolo;user interfaces}, 
doi={10.1145/1141753.1141868}, 
month={June},}
@INPROCEEDINGS{7449189, 
author={M. Trupthi and S. Pabboju and G. Narasimha}, 
booktitle={2016 International Conference on Advances in Human Machine Interaction (HMI)}, 
title={Improved feature extraction and classification #8212; Sentiment analysis}, 
year={2016}, 
pages={1-6}, 
abstract={Sentiment analysis is becoming a popular area of research and social media analysis, especially around user reviews and tweets. It is a special case of text mining generally focused on identifying opinion polarity, People are usually interested to seek positive and negative opinions containing likes and dislikes, shared by users. Therefore reviews or product features have got significant role in sentiment analysis. In addition to sufficient work being performed in text analytics, feature extraction in sentiment analysis is now becoming an active area of research. Feature based sentiment analysis include feature extraction, sentiment classification and finally the sentiment evaluation. In this paper, explored the machine learning classification approaches with different feature selection schemes to obtain a sentiment analysis model for the movie review dataset and the results obtained are compared to identify the best possible approach.}, 
keywords={classification;feature extraction;learning (artificial intelligence);social networking (online);text analysis;feature extraction;feature selection scheme;machine learning classification;sentiment analysis;sentiment classification;social media analysis;text analytics;Classification algorithms;Feature extraction;Motion pictures;Probability;Semantics;Sentiment analysis;Text mining;Bag of Words;Bigram Collocation;Classification;Evaluation;Feature Extraction;Information Features}, 
doi={10.1109/HMI.2016.7449189}, 
month={March},}
@INPROCEEDINGS{6595844, 
author={W. Di and C. Wah and A. Bhardwaj and R. Piramuthu and N. Sundaresan}, 
booktitle={2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops}, 
title={Style Finder: Fine-Grained Clothing Style Detection and Retrieval}, 
year={2013}, 
pages={8-13}, 
abstract={With the rapid proliferation of smartphones and tablet computers, search has moved beyond text to other modalities like images and voice. For many applications like Fashion, visual search offers a compelling interface that can capture stylistic visual elements beyond color and pattern that cannot be as easily described using text. However, extracting and matching such attributes remains an extremely challenging task due to high variability and deformability of clothing items. In this paper, we propose a fine-grained learning model and multimedia retrieval framework to address this problem. First, an attribute vocabulary is constructed using human annotations obtained on a novel fine-grained clothing dataset. This vocabulary is then used to train a fine-grained visual recognition system for clothing styles. We report benchmark recognition and retrieval results on Women's Fashion Coat Dataset and illustrate potential mobile applications for attribute-based multimedia retrieval of clothing items and image annotation.}, 
keywords={Internet;clothing;electronic commerce;image retrieval;learning (artificial intelligence);mobile computing;multimedia systems;object recognition;retail data processing;smart phones;user interfaces;attribute vocabulary;attribute-based multimedia retrieval;clothing item deformability;clothing item variability;e-commerce;fine-grained clothing dataset;fine-grained clothing style detection;fine-grained clothing style retrieval;fine-grained learning model;fine-grained visual recognition system;human annotations;image annotation;interface;mobile applications;multimedia retrieval framework;online stores;smartphones;style finder;stylistic visual elements;tablet computers;visual search;women fashion coat dataset;Clothing;Fasteners;Image color analysis;Materials;Mobile communication;Visualization;Vocabulary;attribute-based image classification;fashion;fine-grained image recognition;image retrieval;image search;mobile application}, 
doi={10.1109/CVPRW.2013.6}, 
ISSN={2160-7508}, 
month={June},}
@ARTICLE{7444160, 
author={S. Jiang and X. Qian and T. Mei and Y. Fu}, 
journal={IEEE Transactions on Big Data}, 
title={Personalized Travel Sequence Recommendation on Multi-Source Big Social Media}, 
year={2016}, 
volume={2}, 
number={1}, 
pages={43-56}, 
abstract={Big data increasingly benefit both research and industrial area such as health care, finance service and commercial recommendation. This paper presents a personalized travel sequence recommendation from both travelogues and community contributed photos and the heterogeneous metadata (e.g., tags, geo-location, and date taken) associated with these photos. Unlike most existing travel recommendation approaches, our approach is not only personalized to user's travel interest but also able to recommend a travel sequence rather than individual Points of Interest (POIs). Topical package space including representative tags, the distributions of cost, visiting time and visiting season of each topic, is mined to bridge the vocabulary gap between user travel preference and travel routes. We take advantage of the complementary of two kinds of social media: travelogue and community contributed photos. We map both user's and routes' textual descriptions to the topical package space to get user topical package model and route topical package model (i.e., topical interest, cost, time and season). To recommend personalized POI sequence, first, famous routes are ranked according to the similarity between user package and route package. Then top ranked routes are further optimized by social similar users' travel records. Representative images with viewpoint and seasonal diversity of POIs are shown to offer a more comprehensive impression. We evaluate our recommendation system on a collection of 7 million Flickr images uploaded by 7,387 users and 24,008 travelogues covering 864 travel POIs in nine famous cities, and show its effectiveness. We also contribute a new dataset with more than 200 K photos with heterogeneous metadata in nine famous cities.}, 
keywords={Big Data;meta data;recommender systems;social networking (online);travel industry;Big Data;heterogeneous metadata;multisource big social media;personalized POI sequence;personalized travel sequence recommendation approaches;route package;topical package space;user package;Big data;Global Positioning System;Media;Planning;Space exploration;Trajectory;Urban areas;Travel recommendation;geo-tagged photos;multimedia information retrieval;social media}, 
doi={10.1109/TBDATA.2016.2541160}, 
month={March},}
@INPROCEEDINGS{6137240, 
author={F. Lemmerich and F. Puppe}, 
booktitle={2011 IEEE 11th International Conference on Data Mining}, 
title={Local Models for Expectation-Driven Subgroup Discovery}, 
year={2011}, 
pages={360-369}, 
abstract={Subgroup discovery (also known as Pattern Mining or Supervised Descriptive Rule Discovery) searches for descriptions of subsets in a dataset that differ from the total population with respect to a given target concept. In this paper we argue that in the traditional approach potentially interesting complex patterns with an unexpected relative increase of the target share remain undiscovered while on the other hand less surprising patterns are returned. Therefore, we present a generalized approach on subgroup discovery, in which the target share in the subgroup is not compared to the target share in the total population, but to the expectations a user has given the knowledge of more general (simpler) patterns. We claim that the resulting complex patterns are more interesting for the user and are less biased towards simpler patterns with a positive influence on the target concept. In order to estimate these expectations we utilize local models, i.e., fragments of Bayesian Networks. The proposed approach is evaluated using data from the UCI repository as well as on two totally different real world applications that investigate university student drop-out rates and identify spammers in a social book marking system.}, 
keywords={belief networks;data mining;learning (artificial intelligence);Bayesian Networks;complex patterns;expectation driven subgroup discovery;pattern mining;simpler patterns;social book marking system;supervised descriptive rule discovery;Additives;Bayesian methods;Data mining;Estimation;Itemsets;Knowledge engineering;Mathematical model;interestingness measures;pattern mining;subgroup discovery}, 
doi={10.1109/ICDM.2011.94}, 
ISSN={1550-4786}, 
month={Dec},}
@INPROCEEDINGS{1640955, 
author={A. Altinok and M. El-Saban and A. J. Peck and L. Wilson and S. C. Feinstein and B. S. Manjunath and K. Rose}, 
booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
title={Activity Analysis in Microtubule Videos by Mixture of Hidden Markov Models}, 
year={2006}, 
volume={2}, 
pages={1662-1669}, 
abstract={We present an automated method for the tracking and dynamics modeling of microtubules -a major component of the cytoskeleton- which provides researchers with a previously unattainable level of data analysis and quantification capabilities. The proposed method improves upon the manual tracking and analysis techniques by i) increasing accuracy and quantified sample size in data collection, ii) eliminating user bias and standardizing analysis, iii) making available new features that are impractical to capture manually, iv) enabling statistical extraction of dynamics patterns from cellular processes, and v) greatly reducing required time for entire studies. An automated procedure is proposed to track each resolvable microtubule, whose aggregate activity is then modeled by mixtures of Hidden Markov Models to uncover dynamics patterns of underlying cellular and experimental conditions. Our results support manually established findings on an actual microtubule dataset and illustrate how automated analysis of spatial and temporal patterns offers previously unattainable insights to cellular processes.}, 
keywords={Application software;Biological system modeling;Biology computing;Cells (biology);Computer vision;Data analysis;Data mining;Hidden Markov models;Pattern analysis;Videos}, 
doi={10.1109/CVPR.2006.48}, 
ISSN={1063-6919}, 
month={},}
@INPROCEEDINGS{7488130, 
author={M. M. Hasan and N. H. Shaon and A. A. Marouf and M. K. Hasan and H. Mahmud and M. M. Khan}, 
booktitle={2015 18th International Conference on Computer and Information Technology (ICCIT)}, 
title={Friend recommendation framework for social networking sites using user's online behavior}, 
year={2015}, 
pages={539-543}, 
abstract={Social network sites (SNS's) have connected millions of users creating the social revolution. Users' social behavior influences them to connect with others with same mentality. Social networks are constituted because of its user or organizations common interest in some social emerging issues. The popular social networking sites are Facebook, Twitter, MySpace, Orkut, LinkedIn, Google plus etc. which are actually online social networking (OSN) sites. However, the large amount of online users and their diverse and dynamic interests possess great challenges to support recommendation of friends on SNS's for each of the users. In this paper, we proposed a novel friend recommendation framework (FRF) based on the behavior of users on particular SNS's. The proposed method is consisted of the following stages: measuring the frequency of the activities done by the users and updating the dataset according to the activities, applying FP-Growth algorithm to classify the user behavior with some criteria, then apply multilayer thresholding for friend recommendation. The proposed framework shows good accuracy for social graphs used as model dataset.}, 
keywords={behavioural sciences computing;information retrieval;recommender systems;social networking (online);FP-growth algorithm;FRF;Facebook;Google plus;LinkedIn;MySpace;OSN sites;Orkut;SNS;Twitter;dataset update;friend recommendation framework;multilayer thresholding;online social networking sites;online users;user behavior;user online behavior;user social behavior;users activity frequency measurement;Facebook;Games;MySpace;Nonhomogeneous media;Postal services;Twitter;FP-Growth Algorithm;Friend Recommendation Framework (FRF);Multilayer Thresholding;Social Entities;Social Networking Sites (SNS's)}, 
doi={10.1109/ICCITechn.2015.7488130}, 
month={Dec},}
@INPROCEEDINGS{7814492, 
author={X. Hu and Z. Mai and H. Zhang and Y. Xue and W. Zhou and X. Chen}, 
booktitle={2016 IEEE/WIC/ACM International Conference on Web Intelligence Workshops (WIW)}, 
title={A Hybrid Recommendation Model Based on Weighted Bipartite Graph and Collaborative Filtering}, 
year={2016}, 
pages={119-122}, 
abstract={Recommender systems are designed to solve the overload information. Collaborative filtering based on the entire user network is by far most widely recommended algorithm, but it produced large amounts of operational data and it has difficulty to analyze characteristics of products and deal with data sparsity problem. To solve this problem, we employed a hybrid recommendation model which combined the weighted bipartite network with item based collaborative filtering. The experiment was implemented on dataset BookCrossing. Compared with traditional recommendation algorithms, the results proved that the proposed algorithm shows better performance and higher accuracy.}, 
keywords={collaborative filtering;graph theory;recommender systems;BookCrossing;hybrid recommendation model;item based collaborative filtering;weighted bipartite graph;weighted bipartite network;Algorithm design and analysis;Analytical models;Bipartite graph;Collaboration;Filtering;Mathematical model;Resource management;Weighted bipartite Graph; collaborative filtering; recommendation system}, 
doi={10.1109/WIW.2016.042}, 
month={Oct},}
@INPROCEEDINGS{7849919, 
author={A. Al-Molegi and M. Jabreel and B. Ghaleb}, 
booktitle={2016 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
title={STF-RNN: Space Time Features-based Recurrent Neural Network for predicting people next location}, 
year={2016}, 
pages={1-7}, 
abstract={This paper proposes a novel model called Space Time Features-based Recurrent Neural Network (STF-RNN) for predicting people next movement based on mobility patterns obtained from GPS devices logs. Two main features are involved in model operations, namely, the space which is extracted from the collected GPS data and also the time which is extracted from the associated timestamps. The internal representation of space and time features is extracted automatically in the proposed model rather than relying on handcraft representation. This enables the model to discover the useful knowledge about people behaviour in more efficient way. Due to the ability of RNN structure to represent the sequences, it is utilized in the proposed model in order to keep track of user movement history. These tracks help the model to discover more meaningful dependencies and as consequence, enhancing the model performance. The results show that STF-RNN model provides good improvements in predicting people's next location compared with the state-of-the-art models when applied on a large real life dataset from Geo-life project.}, 
keywords={behavioural sciences computing;feature extraction;recurrent neural nets;GPS devices logs;STF-RNN;feature extraction;peoples next location prediction;space features internal representation;space time features-based recurrent neural network;time features internal representation;Artificial neural networks;Buildings;Computational modeling;Hidden Markov models;Mobile communication;Predictive models;Recurrent neural networks}, 
doi={10.1109/SSCI.2016.7849919}, 
month={Dec},}
@INPROCEEDINGS{6766035, 
author={Jin Wang and Min Zhang and X. Yang and Keping Long and Chimin Zhou}, 
booktitle={2013 19th Asia-Pacific Conference on Communications (APCC)}, 
title={HTTP-sCAN: Detecting HTTP-flooding attaCk by modeling multi-features of web browsing behavior from noisy dataset}, 
year={2013}, 
pages={677-682}, 
abstract={HTTP-flooding attack disables the victimized Web server by sending a large number of HTTP Get requests. Recent research tends to detect the attacks with the anomaly-based approaches, which detect the HTTP-flooding by modeling the behavior of normal Web users. However, most of the existing anomaly-based detection approaches usually cannot filter the Web crawling traces of the unknown search bots mixed in the normal Web browsing logs. These Web-crawling traces can bias the detection model in the training phase, thus further influencing the performance of the anomaly-based detection schemes. This paper proposes a novel anomaly-based HTTP-flooding detection scheme (HTTP-sCAN), which can eliminate the influence of the Web-crawling traces with the cluster algorithm. The simulation results show that HTTP-sCAN is immune to the interferences of unknown search sessions, and can detect all HTTP-flooding attacks.}, 
keywords={Internet;file servers;online front-ends;telecommunication security;transport protocols;HTTP get requests;HTTP-flooding attack;HTTP-sCAN;Web browsing behavior;Web crawling traces;anomaly-based detection;cluster algorithm;detection model;noisy dataset;normal Web browsing logs;normal Web users;training phase;unknown search bots;victimized Web server;Analytical models;Cluster algorithm;DDoS;IP network;Relative Entropy}, 
doi={10.1109/APCC.2013.6766035}, 
ISSN={2163-0771}, 
month={Aug},}
@ARTICLE{7214410, 
author={F. Zhang and S. Sun and H. Yi}, 
journal={IET Information Security}, 
title={Robust collaborative recommendation algorithm based on kernel function and Welsch reweighted M-estimator}, 
year={2015}, 
volume={9}, 
number={5}, 
pages={257-265}, 
abstract={The existing collaborative recommendation algorithms based on matrix factorisation (MF) have poor robustness against shilling attacks. To address this problem, in this study the authors propose a robust collaborative recommendation algorithm based on kernel function and Welsch reweighted M-estimator. They first propose a median-based method to calculate user and item biases, which can reduce the influence of shilling attacks on user and item biases because median is insensitive to outliers. Then, they present a method of similarity computation based on kernel function, which can obtain the information of similar users by non-linear inner product operation. Finally, they combine the user and item biases based on median and the similarity based on kernel function with MF model, and introduce the Welsch reweighted M-estimator to realise the robust estimation of user feature matrix and item feature matrix. The experimental results on the MovieLens dataset show that the proposed algorithm outperforms the existing algorithms in terms of both recommendation accuracy and robustness, and the improvement of its robustness is not at the expense of recommendation accuracy.}, 
keywords={groupware;matrix algebra;recommender systems;MF;MovieLens dataset;Welsch reweighted M-estimator;kernel function;matrix factorisation;median based method;nonlinear inner product operation;robust collaborative recommendation algorithm;shilling attacks;similarity computation}, 
doi={10.1049/iet-ifs.2014.0488}, 
ISSN={1751-8709}, 
month={},}
@INPROCEEDINGS{6881946, 
author={J. Gutierrez-Cáceres and C. Portugal-Zambrano and C. Beltrán-Castañón}, 
booktitle={2014 IEEE 27th International Symposium on Computer-Based Medical Systems}, 
title={Computer Aided Medical Diagnosis Tool to Detect Normal/Abnormal Studies in Digital MR Brain Images}, 
year={2014}, 
pages={501-502}, 
abstract={This work presents a model to support medical diagnosis through the classification of abnormality normality in medical brain images, in order to help to specialist as a previous step in the brain pathology diagnosis. Our proposal was incorporated into a content-based image retrieval system, thus we developed a useful tool for radiologists. The first step produces the features vector of MR image using Gabor Filter for the data train and test, then as second step features vector of training data are indexed into CBIR module. The third step makes the training of SVM and as four step the test dataset is classified with the SVM trained. Finally, the result of classification are presented with a set of similar images product of a KNN query. This model was implemented as a software tool with graphical interface. We obtained 94.12% of correct classification. Our medical image dataset is composed of 187 MRI images collected from a medical diagnosis company and selected by medical specialist. The result shows that the proposed model is robust and effective as a software tool to aid support to medical diagnostic.}, 
keywords={Gabor filters;biomedical MRI;brain;computer aided analysis;content-based retrieval;graphical user interfaces;image classification;image retrieval;medical image processing;neural nets;support vector machines;CBIR module;Gabor filter;KNN query;MRI images;SVM training;abnormality classification;abnormality detection;brain pathology diagnosis;computer aided medical diagnosis tool;content-based image retrieval system;digital MR brain images;feature vector;graphical interface;medical brain images;medical image dataset;normality classification;normality detection;radiologists;software tool;Brain;Computers;Feature extraction;Magnetic resonance imaging;Medical diagnostic imaging;Support vector machines;cbir;computer aided diagnosis;pattern recognition;svm}, 
doi={10.1109/CBMS.2014.110}, 
ISSN={1063-7125}, 
month={May},}
@ARTICLE{6203585, 
author={A. Hero and B. Rajaratnam}, 
journal={IEEE Transactions on Information Theory}, 
title={Hub Discovery in Partial Correlation Graphs}, 
year={2012}, 
volume={58}, 
number={9}, 
pages={6064-6078}, 
abstract={One of the most important problems in large-scale inference problems is the identification of variables that are highly dependent on several other variables. When dependence is measured by partial correlations, these variables identify those rows of the partial correlation matrix that have several entries with large magnitudes, i.e., hubs in the associated partial correlation graph. This paper develops theory and algorithms for discovering such hubs from a few observations of these variables. We introduce a hub screening framework in which the user specifies both a minimum (partial) correlation $rho $ and a minimum degree $delta $ to screen the vertices. The choice of $rho $ and $delta $ can be guided by our mathematical expressions for the phase transition correlation threshold $rho _{c}$ governing the average number of discoveries. They can also be guided by our asymptotic expressions for familywise discovery rates under the assumption of large number $p$ of variables, fixed number $n$ of multivariate samples, and weak dependence. Under the null hypothesis that the dispersion (covariance) matrix is sparse, these limiting expressions can be used to enforce familywise error constraints and to rank the discoveries in order of increasing statistical significance. For $nll p$, the computational complexity of the proposed partial correlation screening method is low and is therefore highly scalable. Thus, it can be applied to significantly large- problems than previous approaches. The theory is applied to discovering hubs in a high-dimensional gene microarray dataset.}, 
keywords={Approximation methods;Artificial neural networks;Correlation;Covariance matrix;Dispersion;Matrix converters;Sparse matrices;${p}$-value trajectories;Asymptotic Poisson limits;Gaussian graphical models (GGMs);correlation networks;discovery rate phase transitions;nearest neighbor dependence;node degree and connectivity}, 
doi={10.1109/TIT.2012.2200825}, 
ISSN={0018-9448}, 
month={Sept},}
@INPROCEEDINGS{4385309, 
author={Z. Bankovic and S. Bojanic and O. Nieto-Taladriz and A. Badii}, 
booktitle={The International Conference on Emerging Security Information, Systems, and Technologies (SECUREWARE 2007)}, 
title={Increasing Detection Rate of User-to-Root Attacks Using Genetic Algorithms}, 
year={2007}, 
pages={48-53}, 
abstract={An extensive set of machine learning and pattern classification techniques trained and tested on KDD dataset failed in detecting most of the user-to-root attacks. This paper aims to provide an approach for mitigating negative aspects of the mentioned dataset, which led to low detection rates. Genetic algorithm is employed to implement rules for detecting various types of attacks. Rules are formed of the features of the dataset identified as the most important ones for each attack type. In this way we introduce high level of generality and thus achieve high detection rates, but also gain high reduction of the system training time. Thenceforth we re-check the decision of the user-to- root rules with the rules that detect other types of attacks. In this way we decrease the false-positive rate. The model was verified on KDD 99, demonstrating higher detection rates than those reported by the state- of-the-art while maintaining low false-positive rate.}, 
keywords={genetic algorithms;pattern classification;security of data;KDD 99;KDD dataset;genetic algorithms;machine learning;pattern classification;system training time;user-to-root attack detection;Benchmark testing;Filters;Genetic algorithms;Information security;Intrusion detection;Machine learning;Machine learning algorithms;Pattern recognition;Protection;System testing}, 
doi={10.1109/SECUREWARE.2007.4385309}, 
ISSN={2162-2108}, 
month={Oct},}
@INPROCEEDINGS{5593355, 
author={T. Mahlmann and A. Drachen and J. Togelius and A. Canossa and G. N. Yannakakis}, 
booktitle={Proceedings of the 2010 IEEE Conference on Computational Intelligence and Games}, 
title={Predicting player behavior in Tomb Raider: Underworld}, 
year={2010}, 
pages={178-185}, 
abstract={This paper presents the results of an explorative study on predicting aspects of playing behavior for the major commercial title Tomb Raider: Underworld (TRU). Various supervised learning algorithms are trained on a large-scale set of in-game player behavior data, to predict when a player will stop playing the TRU game and, if the player completes the game, how long will it take to do so. Results reveal that linear regression models and other non-linear classification techniques perform well on the tasks and that decision tree learning induces small yet well-performing and informative trees. Moderate performance is achieved from the prediction models, which indicates the complexity of predicting player behavior based on a constrained set of gameplay metrics and the noise existent in the dataset examined, a generic problem in large-scale data collection from millions of remote clients.}, 
keywords={computer games;learning (artificial intelligence);pattern classification;solid modelling;TRU game;Tomb Raider:Underworld;constrained set;decision tree learning;gameplay metrics;generic problem;informative tree;large scale data collection;linear regression model;nonlinear classification technique;player behavior prediction;remote client;supervised learning algorithm;Classification algorithms;Data mining;Feature extraction;Games;Machine learning algorithms;Measurement;Prediction algorithms;Player modeling;Tomb Raider: Underworld;classification;supervised learning}, 
doi={10.1109/ITW.2010.5593355}, 
ISSN={2325-4270}, 
month={Aug},}
@INPROCEEDINGS{5284216, 
author={S. Budalakoti and D. DeAngelis and K. S. Barber}, 
booktitle={2009 International Conference on Computational Science and Engineering}, 
title={Expertise Modeling and Recommendation in Online Question and Answer Forums}, 
year={2009}, 
volume={4}, 
pages={481-488}, 
abstract={Question and answer forums provide a method of connecting users and resources that can leverage both the static and dynamic (live) capabilities of a network of human users. We present a recommender for selecting the most appropriate responders given a question. The goal of this work is to encourage expert participation in QA forums by reducing the time investment needed by an expert to find a suitable question, decrease responder load, and to increase questioner confidence in the responses of others. The two primary contributions of this work are: 1. a generative model for characterizing the production of content in an online question and answer forum and 2. a decision theoretic framework for recommending expert participants while maintaining questioner satisfaction and distributing responder load. We have also developed two new metrics tailored to QA forums: responder load and questioner satisfaction. These metrics are used to evaluate the performance of our recommender system on datasets harvested from Yahoo! Answers. Experiments across several topic domains demonstrate our systems ability to predict responder identities and suggest new responders that are likely to have the appropriate expertise.}, 
keywords={information filtering;social networking (online);Yahoo! Answers dataset;content production;decision theoretic framework;expertise modeling;expertise recommendation;human users networking;network dynamic capability;network static capability;online question-answer forum;questioner confidence;questioner satisfaction;recommender system;responder load distribution;responder load reduction;time investment reduction;Character generation;Computational intelligence;Computer networks;Humans;Intelligent networks;Intelligent systems;Investments;Joining processes;Laboratories;Production;expertise;question and answer;recommendation}, 
doi={10.1109/CSE.2009.62}, 
month={Aug},}
@INPROCEEDINGS{7492921, 
author={J. Tadrous and A. Sabharwal}, 
booktitle={2016 14th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt)}, 
title={Interactive app traffic: An action-based model and data-driven analysis}, 
year={2016}, 
pages={1-8}, 
abstract={Many popular smartphone apps involve human interaction through the entire session; e.g. apps for web browsing, making reservations and online gaming. In this work, we characterize bi-directional interactive app traffic in the timescale of seconds, that is shaped by the human interaction. We collect and analyze a dataset comprising 1500 interactive app sessions. The combined uplink and downlink traffic bursts are the outcome of user-server interactions, which we label as actions. Within each action, we discover high correlation between the number of uplink and downlink packets reaching 0.98. Our study reveals that the distribution of action duration and interarrival can be approximated with exponential or gamma distributions. The analysis provides insights on the temporal characteristics of bi-directional packet bursts associated with actions, during an app session. We show that action-based service at access points (APs), where actions constitute the service units rather than packets, can reduce service delay by 50%.}, 
keywords={exponential distribution;gamma distribution;smart phones;telecommunication traffic;AP;access points;action-based model;bi-directional interactive app traffic;data-driven analysis;exponential distributions;gamma distributions;smartphone apps;user-server interactions;Correlation;Downlink;IEEE 802.11 Standard;Mobile communication;Mobile computing;Servers;Uplink}, 
doi={10.1109/WIOPT.2016.7492921}, 
month={May},}
@INPROCEEDINGS{6726202, 
author={S. Huangfu and B. Guo and Z. Yu and D. Li}, 
booktitle={2013 IEEE 10th International Conference on Ubiquitous Intelligence and Computing and 2013 IEEE 10th International Conference on Autonomic and Trusted Computing}, 
title={Using the Model of Markets with Intermediaries as an Incentive Scheme for Opportunistic Social Networks}, 
year={2013}, 
pages={142-149}, 
abstract={The popularity of smart phones and other intelligent devices has made it possible for us to organize social activities by means of opportunistic social networks. However, message dissemination performance in opportunistic social networks is highly dependent on the cooperation between different users. In this paper, we put forward the model of markets with intermediaries as an incentive mechanism. In this model, users can overcome their selfishness in order to get profits, thus cooperation can be strengthened. Based on this incentive mechanism, we propose a broker selection algorithm: the Ranger Algorithm. Rangers are those users who not only have met with users in other communities for multiple times, but also have a high probability of meeting these users. Experiments are implemented using the MIT Reality Mining Dataset. Results show that the model of markets with intermediaries can serve as an incentive mechanism to stimulate collaboration, and Ranger Algorithm outperforms other baseline algorithms in improving message dissemination performance. On the basis of the above work, a prototype system is built to help organize offline social activities.}, 
keywords={data mining;information dissemination;probability;smart phones;social networking (online);MIT reality mining dataset;Ranger Algorithm;broker selection algorithm;incentive scheme;intelligent devices;message dissemination performance;opportunistic social networks;smart phones;Communities;Incentive schemes;Organizing;Receivers;Routing;Smart phones;Social network services;broker selection algorithm;incentive scheme;markets with intermediaries;opportunistic social networks}, 
doi={10.1109/UIC-ATC.2013.82}, 
month={Dec},}
@INPROCEEDINGS{7573737, 
author={T. Pan and W. Zhang and Z. Wang and L. Xu}, 
booktitle={2016 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)}, 
title={Recommendations Based on LDA Topic Model in Android Applications}, 
year={2016}, 
pages={151-158}, 
abstract={With the popularity of smart phones, mobile applications have become an essential element in people's lives. Wherever you are, the Android market can allow people to download these applications. When you open Android mobile phone market in a mobile application program interface, we can see not only the application content presentation but also other information. The user can also see the scoring for this application, comments and so on. The text describes the application content information and user reviews, no more than 200 words, the contents of which are closely related to mobile applications. How to use these text data to obtain valuable information for the users has been an important research field of data mining, it is also the focus of this study. The topics of different mobile applications are the summary of their contents. This generalization to some extent reflects the different mobile application content core idea. Therefore mobile applications relating to mining user interest analysis has important significance, the results relating to mining can provide data support for topic-based personalized recommendation applications. This article extracts the contents and users' descriptions from a real Android Market dataset, and builds a new topic model called combineLDA to analyze different topics of each mobile application. By combineLDA model we can analyze the topic probability distribution of each mobile application, and then we can calculate the similarity and recommend to users with high similarity applications.}, 
keywords={Android (operating system);data mining;information retrieval;mobile computing;smart phones;statistical distributions;text analysis;Android Market dataset;Android application;Android market;LDA topic model based recommendation;application content information;application content presentation;combineLDA;data mining;high similarity application;mobile application program interface;smart phones;topic probability distribution;topic-based personalized recommendation application;user description;user interest analysis mining;user review;Conferences;Security;Software quality;Software reliability;Android mobile phone market;combineLDA;mining theme;personalized recommendations;similarity calculation;users' descriptions}, 
doi={10.1109/QRS-C.2016.24}, 
month={Aug},}
@ARTICLE{7345559, 
author={V. C. Magaña and M. Muñoz-Organero}, 
journal={IEEE Transactions on Mobile Computing}, 
title={Artemisa: A Personal Driving Assistant for Fuel Saving}, 
year={2016}, 
volume={15}, 
number={10}, 
pages={2437-2451}, 
abstract={In this paper, we propose a driving assistant that makes recommendations in order to reduce the fuel consumption. The solution only requires a smartphone and an OBD/Bluetooth device. Eco-driving advices try to avoid situations that cause an increase in the fuel consumption such as inappropriate speed or slow reaction to the detection of traffic signs and traffic incidents. The main contribution of this paper is the use of artificial intelligence techniques in order to issue the eco-driving tips that are best adapted to the user profile, the characteristics of the vehicle, and the road state conditions. This is very important because the driver may lose the interest due to the high requirements that tend to be provided by general use eco-driving assistants. In order to properly assess and validate the proposed solution, it has been implemented on several Android mobile devices and has been validated using a dataset of 2,250 driving tests using three different models of vehicles with 25 different drivers on three distinct routes. The results show that the system reduces the fuel consumption by 11.04 percent on average and even, in certain cases, the fuel saving is greater than 15 percent.}, 
keywords={Bluetooth;energy conservation;energy consumption;fuel economy;intelligent transportation systems;mobile computing;road traffic;road vehicles;smart phones;Android mobile devices;Artemisa;OBD/Bluetooth device;artificial intelligence techniques;driving tests;eco-driving advices;eco-driving assistants;eco-driving tips;fuel consumption reduction;fuel saving;intelligent vehicle system;personal driving assistant;road state conditions;smartphone;traffic incidents;traffic signs;user profile;vehicle characteristics;Algorithm design and analysis;Fuels;Games;Proposals;Roads;Telemetry;Vehicles;Intelligent vehicle systems and telematics;eco-driving;fuel optimal control}, 
doi={10.1109/TMC.2015.2504976}, 
ISSN={1536-1233}, 
month={Oct},}
@INPROCEEDINGS{6857840, 
author={A. C. Enache and V. Sgârciu}, 
booktitle={2014 IEEE International Conference on Automation, Quality and Testing, Robotics}, 
title={Spam host classification using PSO-SVM}, 
year={2014}, 
pages={1-5}, 
abstract={Search engines have become a de facto place to start information acquisition on the Internet. Sabotaging the quality of the results retrieved by search engines can lead users to doubt the search engine provider. Spam websites can serve as means of phishing. This paper shows a spam host detection approach that uses support vector machines(SVM) for classification. We create a parallel version of standard Particle Swarm Optimization(PSO) to determine free parameters of the SVM classifier and apply our proposed model to a content web spamming dataset, WEBSPAM-UK2011. Our implementation of the parallel PSO is constructed on a pool of threads and each thread executes tasks associated to a particle from the swarm. Experiments showed that our proposed model can achieve a higher accuracy than regular SVM and outperforms other classifiers (C4.5, Naive Bayes). Furthermore, parallel version of standard Particle Swam Optimization(PSO) can efficiently select parameters for SVM.}, 
keywords={Internet;Web sites;parallel algorithms;particle swarm optimisation;search engines;security of data;support vector machines;unsolicited e-mail;Internet;PSO;SVM;Web spamming dataset;particle swarm optimization;phishing;search engines;spam Websites;spam host classification;spam host detection;support vector machines;Accuracy;Kernel;Particle swarm optimization;Sensitivity;Standards;Support vector machines;Unsolicited electronic mail;Particle Swarm Optimization;Support Vector Machine;parallelism;spam host}, 
doi={10.1109/AQTR.2014.6857840}, 
month={May},}
@INPROCEEDINGS{6379581, 
author={N. Lerthirunwong and I. Shimizu}, 
booktitle={The 1st IEEE Global Conference on Consumer Electronics 2012}, 
title={Interactive metric learning system for similar image search using Linear Discriminant Analysis}, 
year={2012}, 
pages={206-209}, 
abstract={Similar image search algorithm is important technique for efficiently search the images of our interests in the large pool of image data. In this paper, we propose an interactive metric learning search system using Linear Discriminant Analysis (LDA). We rank the search result based on the Mahalanobis distance calculated from SIFT feature vectors and use a LDA which enables users to update a search result until the returned results are relevant or satisfied by users. Moreover, we also examine the efficient learning rate and dimension size of feature vector for this model to enhance the search results. The efficiency of our model is confirmed through the intensive experiment using dataset from Caltech-256 which shows that the third updated result's accuracy can be increased by more than 57% from the default result's accuracy using our method.}, 
keywords={image retrieval;learning (artificial intelligence);learning systems;statistical analysis;transforms;Caltech-256;LDA;Mahalanobis distance;SIFT feature vectors;dimension size;image data;interactive metric learning search system;learning rate;linear discriminant analysis;similar image search algorithm;Covariance matrix;Feature extraction;Image retrieval;Linear discriminant analysis;USA Councils;Vectors;Interactive Metric Learning;Linear Discriminant Analysis;Similar Image Search}, 
doi={10.1109/GCCE.2012.6379581}, 
ISSN={2378-8143}, 
month={Oct},}
@ARTICLE{6704276, 
author={Q. Fang and J. Sang and C. Xu and Y. Rui}, 
journal={IEEE Transactions on Multimedia}, 
title={Topic-Sensitive Influencer Mining in Interest-Based Social Media Networks via Hypergraph Learning}, 
year={2014}, 
volume={16}, 
number={3}, 
pages={796-812}, 
abstract={Social media is emerging as a new mainstream means of interacting around online media. Social influence mining in social networks is therefore of critical importance in real-world applications such as friend suggestion and photo recommendation. Social media is inherently multimodal, including rich types of user contributed content and social link information. Most of the existing research suffers from two limitations: 1) only utilizing the textual information, and/or 2) only analyzing the generic influence but ignoring the more important topic-level influence. To address these limitations, in this paper we develop a novel Topic-Sensitive Influencer Mining (TSIM) framework in interest-based social media networks. Specifically, we take Flickr as the study platform. People in Flickr interact with each other through images. TSIM aims to find topical influential users and images. The influence estimation is determined with a hypergraph learning approach. In the hypergraph, the vertices represent users and images, and the hyperedges are utilized to capture multi-type relations including visual-textual content relations among images, and social links between users and images. Algorithmwise, TSIM first learns the topic distribution by leveraging user-contributed images, and then infers the influence strength under different topics for each node in the hypergraph. Extensive experiments on a real-world dataset of more than 50 K images and 70 K comment/favorite links from Flickr have demonstrated the effectiveness of our proposed framework. In addition, we also report promising results of friend suggestion and photo recommendation via TSIM on the same dataset.}, 
keywords={data mining;graph theory;learning (artificial intelligence);social networking (online);Flickr;TSIM framework;friend suggestion;generic influence analysis;hypergraph learning;influence estimation;interest-based social media networks;online media;photo recommendation;social influence mining;social link information;textual information;topic-sensitive influencer mining;user contributed content;visual-textual content relations;Electronic mail;Media;Nominations and elections;Videos;YouTube;Hypergraph learning;influencer mining;topic modeling}, 
doi={10.1109/TMM.2014.2298216}, 
ISSN={1520-9210}, 
month={April},}
@INPROCEEDINGS{6475405, 
author={M. Przepiora and R. Karimpour and G. Ruhe}, 
booktitle={Proceedings of the 2012 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement}, 
title={A hybrid release planning method and its empirical justification}, 
year={2012}, 
pages={115-118}, 
abstract={Background: The use of Constraint Programming (CP) has been proposed by Regnell and Kuchcinski to model and solve the Release Planning Problem. However, they did not empirically demonstrate the advantages and disadvantages of CP over existing release planning methods. Aims: The aims of this paper are (1) to perform a comparative analysis between CP and ReleasePlanner (RP), an existing release planning tool, and (2) to suggest a hybrid approach combining the strengths of each individual method. Method: (1) An empirical evaluation was performed, evaluating the efficiency and effectiveness of the individual methods to justify their hybrid usage. (2) A proof of concept for a hybrid release planning method is introduced, and a real-world dataset including more than 600 features was solved using the hybrid method to provide evidence of its effectiveness. Results: (1) Use of RP was found to be more efficient and effective than CP. However, CP is preferred when advanced planning objectives and constraints exist. (2) The hybrid method (RP&CP) greatly outperformed the individual approach (CP), increasing computational solution quality by 87%. Conclusion: We were able to increase the expressiveness and thus applicability of an existing, efficient and effective release planning method. We presented evidence for its computational effectiveness, but more work is needed to make this result significant.}, 
keywords={constraint handling;software development management;CP;RP;ReleasePlanner;computational solution quality;constraint programming;empirical justification;hybrid release planning method;software development management;Educational institutions;Planning;Programming;Search problems;Software;Software engineering;Systematics;Release planning;efficiency of use;hybrid algorithm;performance evaluation;user satisfaction}, 
doi={10.1145/2372251.2372271}, 
ISSN={1949-3770}, 
month={Sept},}
@INPROCEEDINGS{5413590, 
author={S. Zhang and Q. Tian and Q. Huang and W. Gao and S. Li}, 
booktitle={2009 16th IEEE International Conference on Image Processing (ICIP)}, 
title={Utilizing affective analysis for efficient movie browsing}, 
year={2009}, 
pages={1853-1856}, 
abstract={Because of the fast increasing number of movies and long time span each movie lasts, novel methods should be developed to help users browse movies and find their desired clips effectively. Affective information in movies is closely related with users' experiences and preferences. Therefore, in this paper, we analyze the affective states of movies and propose affective information based movie browsing. Affective movie content analysis is challenging due to the great variety of movie contents and styles. To address this challenge, we first extract rich audio-visual features. Then, feature selection and affective modeling are carried out to select and map effective features into corresponding affective states. Finally, we propose novel Affective Visualization techniques which intuitively visualize affective states to achieve efficient and user-friendly movie browsing. Experiments on representative movie dataset demonstrate the effectiveness of our proposed methods.}, 
keywords={audio-visual systems;feature extraction;video signal processing;affective information;audio visual features;feature selection;map effective features;movie browsing;movie content analysis;user friendly movie browsing;utilizing affective analysis;visualization techniques;Analysis of variance;Asia;Content addressable storage;Data mining;Feature extraction;Hidden Markov models;Humans;Information analysis;Motion pictures;Visualization;Affective Video Content Analysis;Affective Visualization;Dimensional Affective Model}, 
doi={10.1109/ICIP.2009.5413590}, 
ISSN={1522-4880}, 
month={Nov},}
@INPROCEEDINGS{7883095, 
author={M. Iqbal and M. M. Abid and M. Ahmad}, 
booktitle={2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS)}, 
title={Catching Webspam traffic with Artificial Immune System (AIS) classification algorithm}, 
year={2016}, 
pages={402-405}, 
abstract={Everyday a large number of internet users are being encountered with web spamming where the search engines produce false ranking to web sites due to the use of unethical methods of Search Engine Optimization (SEO). The objective of the paper is to identify the spam traffic by using Artificial Immune System (AIS) classification algorithm. The paper presents chi square method for attribute selection of different machine learning methods, including proposed biological inspired Artificial Immune System for Spam Classification (AISSC) method, which provides a solution for supervised classification problem. In order to show the efficiency of proposed algorithm results are compared and analyzed with well-known classifiers i.e., Naive Bayes and J48. Experimental work is performed on Webspam-uk-2007 dataset. The results of proposed method show prominent achievements with increase in number of features to train the model.}, 
keywords={Internet;Web sites;feature selection;learning (artificial intelligence);optimisation;pattern classification;search engines;statistical distributions;AISSC;SEO;Web site;Web spam traffic;artificial immune system for spam classification;attribute selection;chi square method;machine learning;search engine optimization;Algorithm design and analysis;Classification algorithms;Feature extraction;Immune system;Mathematical model;Niobium;Web pages;AISSC;Spam detection;Web Spam;supervised algorithms}, 
doi={10.1109/ICSESS.2016.7883095}, 
month={Aug},}
@ARTICLE{4376169, 
author={G. Weber and P. T. Bremer and V. Pascucci}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Topological Landscapes: A Terrain Metaphor for Scientific Data}, 
year={2007}, 
volume={13}, 
number={6}, 
pages={1416-1423}, 
abstract={Scientific visualization and illustration tools are designed to help people understand the structure and complexity of scientific data with images that are as informative and intuitive as possible. In this context the use of metaphors plays an important role since they make complex information easily accessible by using commonly known concepts. In this paper we propose a new metaphor, called "topological landscapes," which facilitates understanding the topological structure of scalar functions. The basic idea is to construct a terrain with the same topology as a given dataset and to display the terrain as an easily understood representation of the actual input data. In this projection from an n-dimensional scalar function to a two-dimensional (2D) model we preserve function values of critical points, the persistence (function span) of topological features, and one possible additional metric property (in our examples volume). By displaying this topologically equivalent landscape together with the original data we harness the natural human proficiency in understanding terrain topography and make complex topological information easily accessible.}, 
keywords={data visualisation;software tools;user interfaces;feature detection;illustration tools;scientific data visualization;terrain metaphor;terrain topography;topological landscapes;user interfaces;visual analytics;Computer vision;Data visualization;Histograms;Humans;Isosurfaces;Laboratories;Surfaces;Topology;Two dimensional displays;User interfaces;Contour Tree;Feature Detection (primary keyword);SOAR;Terrain;Topology;User Interfaces;Visual Analytics}, 
doi={10.1109/TVCG.2007.70601}, 
ISSN={1077-2626}, 
month={Nov},}
@INPROCEEDINGS{6890172, 
author={J. Y. Liu and S. Y. Liu and Y. H. Yang}, 
booktitle={2014 IEEE International Conference on Multimedia and Expo (ICME)}, 
title={LJ2M dataset: Toward better understanding of music listening behavior and user mood}, 
year={2014}, 
pages={1-6}, 
abstract={Recent years have witnessed a growing interest in modeling user behaviors in multimedia research, emphasizing the need to consider human factors such as preference, activity, and emotion in system development and evaluation. Following this research line, we present in this paper the LiveJournal two-million post (LJ2M) dataset to foster research on user-centered music information retrieval. The new dataset is characterized by the great diversity of real-life listening contexts where people and music interact. It contains blog articles from the social blogging website LiveJournal, along with tags self-reporting a user's emotional state while posting and the musical track that the user considered as the best match for the post. More importantly, the data are contributed by users spontaneously in their daily lives, instead of being collected in a controlled environment. Therefore, it offers new opportunities to understand the interrelationship among the personal, situational, and musical factors of music listening. As an example application, we present research investigating the interaction between the affective context of the listener and the affective content of music, using audio-based music emotion recognition techniques and a psycholinguistic tool. The study offers insights into the role of music in mood regulation and demonstrates how LJ2M can contribute to studies on real-world music listening behavior.}, 
keywords={Web sites;emotion recognition;human factors;information retrieval;music;LJ2M dataset;LiveJournal two-million post dataset;audio-based music emotion recognition techniques;blog articles;human factors;mood regulation;multimedia research;music listening behavior;psycholinguistic tool;social blogging Web site;system development;system evaluation;user behavior modeling;user mood;user-centered music information retrieval;Blogs;Context;Correlation;Emotion recognition;Mood;Music;Vectors;User-centered approach;listening context;music emotion;music-listening dataset;user mood}, 
doi={10.1109/ICME.2014.6890172}, 
ISSN={1945-7871}, 
month={July},}
@INPROCEEDINGS{7899008, 
author={H. M. Al-Barhamtoshy and A. Al-Ghamdi}, 
booktitle={2017 International Conference on Informatics, Health Technology (ICIHT)}, 
title={Toward cloud-based mixed reality e-learning system}, 
year={2017}, 
pages={1-6}, 
abstract={This paper presents design and implementation of a mixed reality e-learning architecture integrated with cloud computing technology. Consequently, the paper discusses a cloud based mixed reality computing framework for e-learning collaboration model. The proposed framework for the cloud-based mixed reality is designed to open the mixed reality dataset courses and related contents. The framework employees' four modules, each module implements various services related to this module. Also, the framework provides an environment of online mixed reality learning courses. The structure of the framework includes (1) learner, (2) mixed reality, (3) learning objects and styles, and (4) manager with user friendly modules. Consequently, ways of deploying such cloud computing can be defined as public, private or hybrid followed by the security and privacy techniques to protect the proposed framework. So, confidentiality, integrity and availability of material of courses will satisfied, to examine the threats and security issues for the cloud computing framework.}, 
keywords={augmented reality;cloud computing;computer aided instruction;data integrity;data protection;educational courses;security of data;cloud computing technology;cloud-based e-learning system;cloud-based mixed reality e-learning system;course material availability;course material confidentiality;course material integrity;online mixed reality learning courses;privacy techniques;security techniques;user friendly modules;Cloud computing;Electronic learning;Servers;Virtualization;Virtual reality;augmented reality;cloud;e-learning framework;mixed reality}, 
doi={10.1109/ICIHT.2017.7899008}, 
month={Feb},}
@INPROCEEDINGS{6287821, 
author={J. C. Wang and H. M. Wang and S. K. Jeng}, 
booktitle={2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
title={Playing with tagging: A real-time tagging music player}, 
year={2012}, 
pages={77-80}, 
abstract={Visualizing audio signals during playback has long been a fundamental function of music players. However, most visual effects are generated by audio signal processing directly and render meaningless or incomprehensible displays to users. In this paper, we present an intelligent music player called the Playing with Tagging (PWT) music player. By integrating a real-time music tagger, the PWT player can display dynamic tag distributions via a set of tag bars that move in sync with the music. To synchronize the tag distributions, the music tagger must be able to online recognize the music tags. We utilize a Gaussian mixture model (GMM) as an auditory feature encoding reference and a mixture of tag-based aspect models (TBAMs) to predict the tag distribution for a short sliding chunk of the music played. To evaluate the real-time tagging function, we simulate tag prediction on short music chunks. The results of experiments on the MajorMiner dataset demonstrate the potential and effectiveness of the proposed music tagging method.}, 
keywords={Gaussian processes;audio coding;GMM;Gaussian mixture model;MajorMiner dataset;PWT music player;TBAM;audio signal processing;auditory feature encoding reference;intelligent music player;real-time tagging music player;short music chunks;short sliding chunk;tag-based aspect models;Feature extraction;Music;Real time systems;Synchronization;Tagging;Training;Vectors;Online music tag annotation;probabilistic tag-based aspect model;real-time music visualization}, 
doi={10.1109/ICASSP.2012.6287821}, 
ISSN={1520-6149}, 
month={March},}
@INPROCEEDINGS{7878252, 
author={Z. Liu and Z. Xu and X. Xia}, 
booktitle={2016 13th Web Information Systems and Applications Conference (WISA)}, 
title={Towards Systematic Analysis and Summary of DUV-Based Dataset Usage Information}, 
year={2016}, 
pages={169-172}, 
abstract={The forthcoming Dataset Usage Vocabulary (DUV) standard under development by the W3C Data on the Web Best Practices Working Group can be used to describe and share dataset usage information on the Web, thus facilitating better communication between data publishers and consumers. Despite the significant progress in the standardization, there is a lack of systematic research on approaches and tools for analyzing and summarizing DUV-based dataset usage information to promote the re-use of data. This paper therefore proposes a Framework for Analysis and Summary of DUV-based Dataset Usage Information (FASDUI). FASDUI can provide users (data publishers and consumers) with a friendly user interface designed according to DUV's Citation Model, Usage Model, and Feedback Model, and employ automatic algorithms to systematically analyze dataset usage data and then present the summarized usage information in the user interface. Our prototype implementation of FASDUI and preliminary experimental results show that FASDUI is feasible and implementable.}, 
keywords={data analysis;user interfaces;vocabulary;DUV citation model;DUV feedback model;DUV usage model;DUV-based dataset usage information;FASDUI;W3C Data;Web Best Practices Working Group;data reuse;dataset usage data analysis;dataset usage vocabulary;friendly user interface;Algorithm design and analysis;Data models;Ontologies;Prototypes;User interfaces;Vocabulary;W3C;Datasets Usage Vocabulary (DUV);dataset usage information;information summary;systematic analysis;user interface design}, 
doi={10.1109/WISA.2016.42}, 
month={Sept},}
@ARTICLE{7515294, 
author={T. Chen and R. Xu and Y. He and Y. Xia and X. Wang}, 
journal={IEEE Computational Intelligence Magazine}, 
title={Learning User and Product Distributed Representations Using a Sequence Model for Sentiment Analysis}, 
year={2016}, 
volume={11}, 
number={3}, 
pages={34-44}, 
abstract={In product reviews, it is observed that the distribution of polarity ratings over reviews written by different users or evaluated based on different products are often skewed in the real world. As such, incorporating user and product information would be helpful for the task of sentiment classification of reviews. However, existing approaches ignored the temporal nature of reviews posted by the same user or evaluated on the same product. We argue that the temporal relations of reviews might be potentially useful for learning user and product embedding and thus propose employing a sequence model to embed these temporal relations into user and product representations so as to improve the performance of document-level sentiment analysis. Specifically, we first learn a distributed representation of each review by a one-dimensional convolutional neural network. Then, taking these representations as pretrained vectors, we use a recurrent neural network with gated recurrent units to learn distributed representations of users and products. Finally, we feed the user, product and review representations into a machine learning classifier for sentiment classification. Our approach has been evaluated on three large-scale review datasets from the IMDB and Yelp. Experimental results show that: (1) sequence modeling for the purposes of distributed user and product representation learning can improve the performance of document-level sentiment classification; (2) the proposed approach achieves state-of-the-art results on these benchmark datasets.}, 
keywords={learning (artificial intelligence);neural net architecture;pattern classification;recurrent neural nets;sentiment analysis;IMDB dataset;Yelp dataset;benchmark datasets;document level sentiment analysis performance improvement;gated recurrent units;large-scale review datasets;machine learning classifier;one-dimensional convolutional neural network;polarity rating distribution;pretrained vectors;product distributed representation learning;product information;product reviews;recurrent neural network;review sentiment classification;sequence model;temporal relations;user distributed representation learning;user information;Analytical models;Classification algorithms;Computational modeling;Neural networks;Semantics;Sentiment analysis;Support vector machines}, 
doi={10.1109/MCI.2016.2572539}, 
ISSN={1556-603X}, 
month={Aug},}
@INPROCEEDINGS{6978600, 
author={Y. Xu and J. Yin and Z. Wu and D. He and Y. Tan}, 
booktitle={2014 IEEE 7th International Conference on Service-Oriented Computing and Applications}, 
title={Reliability Prediction for Service Oriented System via Matrix Factorization in a Collaborative Way}, 
year={2014}, 
pages={125-130}, 
abstract={The reliability prediction of service-oriented system is a key problem, and has become increasingly important along with the wide utilization of service-oriented architecture. In this paper, we aim to improve the prediction accuracy of reliability in a collaborative way. First, we estimate the failure probability of each component through two independent models extended from Matrix Factorization. For each service and user, we identify the similar neighbors through similarity computation. Then, we build the service neighborhood-based MF model (SN-MF) and user neighborhood-based MF model (UN-MF). In the two models, each unknown failure probability is learned out assisted by similar neighbors' historical failure records collaboratively. Further, we combine the two models together to build an ensemble model, and explicate the way of calculating the final failure probability of the whole system. Afterwards, the failure probability is mapped to reliability with a classical function. Finally, experiments conducted in a real-world dataset demonstrate the effectiveness of our models.}, 
keywords={matrix decomposition;service-oriented architecture;software reliability;SN-MF;UN-MF;failure probability;matrix factorization;reliability prediction;service neighborhood-based MF model;service oriented architecture;service oriented system;similarity computation;user neighborhood-based MF model;Accuracy;Collaboration;Computational modeling;Predictive models;Probability;Software reliability;Matrix Factorization;Reliability Prediction;Service-Oriented System;Similarity Computation;Software Failure}, 
doi={10.1109/SOCA.2014.11}, 
ISSN={2163-2871}, 
month={Nov},}
@INPROCEEDINGS{7195564, 
author={X. Liu and A. Kale and J. Wasani and C. Ding and Q. Yu}, 
booktitle={2015 IEEE International Conference on Web Services}, 
title={Extracting, Ranking, and Evaluating Quality Features of Web Services through User Review Sentiment Analysis}, 
year={2015}, 
pages={153-160}, 
abstract={Quality of Service (QoS) has become a standard way of evaluating web services and selecting the one that suites user interests the best. Traditional methods adopt a fixed set of QoS parameters and typical ones include response time, fee, and availability. There currently lacks an effective way of identifying quality features that users are actually interested in when choosing a service. Meanwhile, the traditional way of collecting QoS values relies on either public information released by service providers or test results from repeatedly invoking a service. Therefore, the values can be heavily affected by authenticity of the provider offered information or the quality/configuration of the test code/environment. As a result, existing QoS evaluation methods are not applicable to subject features, such as usability and affordability, where the values depend on user personal judgement. In this paper, we propose a novel approach to extracting domain-related QoS features, ranking those features based on their interestingness, evaluating the value of these features through sentiment analysis on user reviews. More specifically, we leverage natural language processing techniques and machine learning approaches to identify top QoS features that users are interested in and simultaneously learn their sentiment orientation towards those features. We model the problem as sentiment classification, where relevant terms in a review are modeled as features that determine whether a review is positive or negative. Logistic regression is used so that the impact of these terms are learned simultaneously when the classifier is learned through a supervised learning process. The nontrivial terms are selected as the candidate QoS featured. A comprehensive experiment has been conducted on a real-world dataset and the result demonstrates the effectiveness of our approach.}, 
keywords={Web services;data mining;emotion recognition;feature extraction;natural language processing;quality of service;regression analysis;QoS evaluation methods;QoS features;QoS parameters;QoS values;Web services;domain-related QoS features;logistic regression;machine learning approach;natural language processing techniques;public information;quality feature evaluation;quality feature extraction;quality feature ranking;sentiment classification;service providers;supervised learning process;user personal judgement;user review sentiment analysis;Accuracy;Feature extraction;Logistics;Quality of service;Sentiment analysis;Tagging;Web services;QoS;natural language processing;sentiment analysis;supervised learning;web services}, 
doi={10.1109/ICWS.2015.30}, 
month={June},}
@INPROCEEDINGS{7043535, 
author={F. H. Khan and M. E. Ali and H. Dev}, 
booktitle={2015 International Conference on Networking Systems and Security (NSysS)}, 
title={A hierarchical approach for identifying user activity patterns from mobile phone call detail records}, 
year={2015}, 
pages={1-6}, 
abstract={With the increasing use of mobile devices, now it is possible to collect different data about the day-to-day activities of personal life of the user. Call Detail Record (CDR) is the available dataset at large-scale, as they are already constantly collected by the mobile operator mostly for billing purpose. By examining this data it is possible to analyze the activities of the people in urban areas and discover the human behavioral patterns of their daily life. These datasets can be used for many applications that vary from urban and transportation planning to predictive analytics of human behavior. In our research work, we have proposed a hierarchical analytical model where this CDR Dataset is used to find facts on the daily life activities of urban users in multiple layers. In our model, only the raw CDR data are used as the input in the initial layer and the outputs from each consecutive layer is used as new input combined with the original CDR data in the next layers to find more detailed facts, e.g., traffic density in different areas in working days and holidays. So, the output in each layer is dependent on the results of the previous layers. This model utilized the CDR Dataset of one month collected from the Dhaka city, which is one of the most densely populated cities of the world. So, our main focus of this research work is to explore the usability of these types of dataset for innovative applications, such as urban planning, traffic monitoring and prediction, in a fashion more appropriate for densely populated areas of developing countries.}, 
keywords={mobile handsets;telecommunication network planning;Dhaka city;mobile devices;mobile operator;mobile phone call detail records;traffic monitoring;transportation planning;urban planning;Analytical models;Cities and towns;Data models;Employment;Mobile handsets;Poles and towers;Transportation}, 
doi={10.1109/NSysS.2015.7043535}, 
month={Jan},}
@INPROCEEDINGS{5546417, 
author={L. T. De Paolis and M. Pulimeno and G. Aloisio}, 
booktitle={Proceedings of the ITI 2010, 32nd International Conference on Information Technology Interfaces}, 
title={Visualization and interaction systems for surgical planning}, 
year={2010}, 
pages={269-274}, 
abstract={The visualization of 3D models of the patient's body emerges as a priority in surgery. In this paper two different visualization and interaction systems are presented: a virtual interface and a low cost multi-touch screen. The systems are able to interpret in real-time the user's movements and can be used in the surgical pre-operative planning for the navigation and manipulation of 3D models of the human body built from CT images. The surgeon can visualize both the traditional patient information, as the CT image dataset, and the 3D models of the patient's organs built from these images. The developed virtual interface is the first prototype of a system designed to avoid any contact with the computer so that the surgeon will be able to visualize models of the patient's organs and to interact with these moving the finger in the free space. The developed multi-touch screen provides a user interface customized for doctor requirements that allows users to interact at the same time with 3D models of the human body built from CT images for surgical pre-operative planning purpose.}, 
keywords={computerised tomography;human computer interaction;interactive systems;medical image processing;solid modelling;surgery;user interfaces;3D models visualization;CT image;doctor requirement;interaction system;multitouch screen;patients body;surgical preoperative planning;user interface;virtual interface;visualization system;Medical Imaging;Multi-Touch Screen;Surgical Planning;User Interface}, 
ISSN={1330-1012}, 
month={June},}
@INPROCEEDINGS{6624009, 
author={A. K. Saha and R. K. Saha and K. A. Schneider}, 
booktitle={2013 10th Working Conference on Mining Software Repositories (MSR)}, 
title={A discriminative model approach for suggesting tags automatically for Stack Overflow questions}, 
year={2013}, 
pages={73-76}, 
abstract={Annotating documents with keywords or `tags' is useful for categorizing documents and helping users find a document efficiently and quickly. Question and answer (Q&A) sites also use tags to categorize questions to help ensure that their users are aware of questions related to their areas of expertise or interest. However, someone asking a question may not necessarily know the best way to categorize or tag the question, and automatically tagging or categorizing a question is a challenging task. Since a Q&A site may host millions of questions with tags and other data, this information can be used as a training and test dataset for approaches that automatically suggest tags for new questions. In this paper, we mine data from millions of questions from the Q&A site Stack Overflow, and using a discriminative model approach, we automatically suggest question tags to help a questioner choose appropriate tags for eliciting a response.}, 
keywords={Web sites;data mining;document handling;question answering (information retrieval);Q&A site;Stack Overflow questions;automatic question categorization;automatic question tagging;automatic tag suggestion;data mining;discriminative model approach;document annotation;document categorization;document keywords;document tags;question-and-answer sites;test dataset;training dataset;Accuracy;Prediction algorithms;Predictive models;Support vector machines;Tagging;Training;Vectors;Machine learning;automatic tagging;discriminative model}, 
doi={10.1109/MSR.2013.6624009}, 
ISSN={2160-1852}, 
month={May},}
@INPROCEEDINGS{7552039, 
author={R. Khan and R. Hasan}, 
booktitle={2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC)}, 
title={The Story of Naive Alice: Behavioral Analysis of Susceptible Internet Users}, 
year={2016}, 
volume={1}, 
pages={390-395}, 
abstract={The Internet has become an integral part of our everyday life. Unfortunately, not all of us are equally aware of the threats when we use online services. Naive users are generally less aware of security and privacy practices on the Internet and are susceptible to online predators. In this paper, we present a behavioral analysis of Internet users and their susceptibility to online malpractices. We have considered the dataset from the Global Internet User Survey for 10789 respondents to perform a security-oriented statistical analysis of correlated user behavior. We constructed logistic regression models to analyze the statistical predictability of susceptible and not-so-susceptible identity theft victims based on their behavior and knowledge of security and privacy practices. We posit that such a study can be used to assess the vulnerability of Internet users and can hence be used to leverage institutional and personal safety on the Internet by promoting online security education, threat awareness, and guided Internet-safe behavior.}, 
keywords={Internet;data privacy;human factors;regression analysis;security of data;Internet privacy;Internet security;Internet users vulnerability;Naive Alice;guided Internet-safe behavior;identity theft victims;logistic regression models;online predators;online security education;security-oriented statistical analysis;susceptible Internet users behavioral analysis;threat awareness;Electronic mail;Integrated circuits;Internet;Malware;Privacy;Probability distribution;Security;Behavioral Analysis;Linear Regression Model;Naive Alice;Susceptible User;User Model}, 
doi={10.1109/COMPSAC.2016.206}, 
month={June},}
@INPROCEEDINGS{5706867, 
author={T. Giannakopoulos and S. Petridis and S. Perantonis}, 
booktitle={2010 Fifth International Workshop Semantic Media Adaptation and Personalization}, 
title={User-driven recognition of audio events in news videos}, 
year={2010}, 
pages={44-49}, 
abstract={We propose a method for user-driven recognition of events in audio streams, aiming to assist journalists towards easily annotate unedited audiovisual content. Nonlocal information provided by the user, as for example that the sound of applause exists within the video, is used for adapting the audio event classifiers so as to detect the exact position of these events in the video. Towards this end, each audio class is modeled using a Support Vector Machine (SVM) and the final automatic decision is taken on a mid-term audio basis, using an alternative of the One Vs All architecture. A weighting function is generated based on the user input and it is applied on the soft-output decision of the respective SVMs, thus adapting the final decision to the user's provided knowledge. To evaluate our method, we have used a large dataset of real news videos, provided by the German international broadcaster (DW - Deutsche Welle) and the Portugese broadcaster (Lusa - Agłncia de Notcias de Portuga) where five audio classes, often met in the particular dataset, are defined. Results show that the above process leads to significant raise of the audio tracking performance.}, 
keywords={audio signal processing;audio streaming;decision making;image classification;support vector machines;video signal processing;German international broadcaster;Portugese broadcaster;audio class;audio event classifier;audio tracking;automatic decision;journalists assist;midterm audio basis;news video;nonlocal information;one vs all architecture;soft output decision;support vector machine;unedited audiovisual content;user driven recognition;weighting function;Entropy;Event detection;Feature extraction;Mathematical model;Speech;Support vector machines;Videos}, 
doi={10.1109/SMAP.2010.5706867}, 
month={Dec},}
@INPROCEEDINGS{7166119, 
author={M. A. Bode and S. A. Oluwadare and B. K. Alese and A. F. B. Thompson}, 
booktitle={2015 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)}, 
title={Risk analysis in cyber situation awareness using Bayesian approach}, 
year={2015}, 
pages={1-12}, 
abstract={The unpredictable cyber attackers and threats have to be detected in order to determine the outcome of risk in a network environment. This work develops a Bayesian network classifier to analyse the network traffic in a cyber situation. It is a tool that aids reasoning under uncertainty to determine certainty. It further analyze the level of risk using a modified risk matrix criteria. The classifier developed was experimented with various records extracted from the KDD Cup'99 dataset with 490,021 records. The evaluations showed that the Bayesian Network classifier is a suitable model which resulted in same performance level for classifying the Denial of Service (DoS) attacks with Association Rule Mining while as well as Genetic Algorithm, the Bayesian Network classifier performed better in classifying probe and User to Root (U2R) attacks and classified DoS equally. The result of the classification showed that Bayesian network classifier is a classification model that thrives well in network security. Also, the level of risk analysed from the adapted risk matrix showed that DoS attack has the most frequent occurrence and falls in the generally unacceptable risk zone.}, 
keywords={Bayes methods;belief networks;computer network security;data mining;inference mechanisms;pattern classification;risk analysis;Bayesian approach;Bayesian network classifier;DoS attacks;KDD Cup 99 dataset;U2R attacks;association rule mining;classified DoS equally;cyber attackers;cyber situation;cyber situation awareness;cyber threats;denial of service attacks;genetic algorithm;modified risk matrix criteria;network environment;network security;network traffic analysis;risk analysis;user to root attacks;Bayes methods;Intrusion detection;Risk management;Telecommunication traffic;Uncertainty;Bayesian approach;Cyber Situation Awareness;KDD Cup'99;Risk matrix}, 
doi={10.1109/CyberSA.2015.7166119}, 
month={June},}
@INPROCEEDINGS{6614291, 
author={J. Patel and S. Okamoto and S. M. Dascalu and F. C. Harris}, 
booktitle={2013 10th International Conference on Information Technology: New Generations}, 
title={A Web-Enabled Approach for Generating Data Processors}, 
year={2013}, 
pages={71-76}, 
abstract={Researchers in environmental sciences work with datasets that have a large variety of data structures and file formats. Consequently, for data and model interoperability, it is essential to have the right tools for converting from a given data structure to another, and from a specific file format to another. In this paper, we propose an original web-enabled approach for generating data processors capable of handling a multitude of data operations, including numerous data conversion and processing activities. The proposed approach emphasizes end-user, direct manipulation-based definition of data processors and automated code generation of their associated code, thus freeing the scientists from specialized and often tedious programming tasks. This, in turn, translates into an increased efficiency of the scientific research work. Details of the proposed approach and its supporting web-based software tool are presented in this paper, together with an application example in which an input dataset in the CSV format is converted into an output dataset in the XML format. The applicability of the proposed approach goes beyond the domain of environmental sciences, for which it was initially created, as it can be used in many other areas of research that emphasize data-intensive exploration and processing.}, 
keywords={Web services;XML;data models;electronic data interchange;open systems;program compilers;software tools;CSV format;Web enabled approach;Web-based software tool;XML format;automated code generation;data conversion;data intensive exploration;data intensive processing;data interoperability;data operation handling;data processor;data structure;environmental science;file format;model interoperability;Data models;Data structures;Interoperability;Meteorology;Program processors;Software tools;data and model interoperability;data processors;graphical interface;web-enabled software tool}, 
doi={10.1109/ITNG.2013.19}, 
month={April},}
@INPROCEEDINGS{7828594, 
author={A. Samara and M. L. R. Menezes and L. Galway}, 
booktitle={2016 15th International Conference on Ubiquitous Computing and Communications and 2016 International Symposium on Cyberspace and Security (IUCC-CSS)}, 
title={Feature Extraction for Emotion Recognition and Modelling Using Neurophysiological Data}, 
year={2016}, 
pages={138-144}, 
abstract={The ubiquitous computing paradigm is becoming a reality; we are reaching a level of automation and computing in which people and devices interact seamlessly. However, one of the main challenges is the difficulty users have in interacting with these increasingly complex systems. Ultimately, endowing machines with the ability to perceive users' emotions will enable a more intuitive and reliable interaction. Consequently, using the electroencephalogram (EEG) as a bio-signal sensor, the affective state of a user can be modelled and subsequently utilised in order to achieve a system that can recognise and react to the users emotions. In this context, this paper investigates feature vector generation from EEG signals for the purpose of affective state modelling based on Russells Circumplex Model. Investigations are presented that aim to provide the foundation for future work in modelling user affect and interaction experiences through exploitation of different input modalities. The DEAP dataset was used within this work, along with a Support Vector Machine, which yielded reasonable classification accuracies for Valence and Arousal using feature vectors based on statistical measurements, band power from the , β, δ and θ waves, and High Order Crossing of the EEG signal.}, 
keywords={biosensors;electroencephalography;emotion recognition;feature extraction;medical signal processing;neurophysiology;signal classification;statistical analysis;support vector machines;ubiquitous computing;vectors;Arousal classification;EEG signals;Russells circumplex model;Valence classification;biosignal sensor;electroencephalogram;emotion recognition;feature extraction;feature vector;neurophysiological data;statistical measurements;support vector machine;ubiquitous computing;Brain models;Context;Electroencephalography;Emotion recognition;Feature extraction;Support vector machines;EEG;affective state modelling;bio-signal sensor;feature extraction}, 
doi={10.1109/IUCC-CSS.2016.027}, 
month={Dec},}
@INPROCEEDINGS{6743541, 
author={N. Alam and M. Sarim and A. B. Shaikh}, 
booktitle={2013 IEEE 9th International Conference on Emerging Technologies (ICET)}, 
title={A global non-parametric sampling based image matting}, 
year={2013}, 
pages={1-6}, 
abstract={Image matting is a process of separating the foreground objects from an image along with opacity values for each pixel. It is an under-constraint problem hence a user interaction is required to identify the definite foreground, background and semi-transparent pixels. In general the information in the definite foreground and background regions is modeled locally to estimate the foreground and background color of the pixels in the semi-transparent region which are then used to estimate opacity values. A global non-parametric sampling based approach is presented which incorporates not only the color information in the foreground and background regions but also utilizes the local structure of an image to improve the quality of the estimate matte. This global sampling approach reduces the segmentation mis-classification that is incorporated in the resulting alpha matte by considering only the local color information. The results obtained are comparable to the state of the art image matting techniques on a standard dataset.}, 
keywords={image colour analysis;image sampling;image segmentation;alpha matte;background color;background regions;definite foreground;foreground color;foreground objects;global nonparametric sampling;image matting techniques;local color information;opacity values;segmentation misclassification;semitransparent pixels;Equations;Estimation;Image color analysis;Mathematical model;Optimization;Robustness;Wires;Alpha matte;Global sampling method;Non-parametric}, 
doi={10.1109/ICET.2013.6743541}, 
month={Dec},}
@INPROCEEDINGS{7129580, 
author={F. Xia and Q. Zhang and C. Wang and W. Qian and A. Zhou}, 
booktitle={2015 31st IEEE International Conference on Data Engineering Workshops}, 
title={On the rise and fall of Sina Weibo: Analysis based on a fixed user group}, 
year={2015}, 
pages={224-231}, 
abstract={Micro-blogging service Sina Weibo in China has become the country's most free-flowing and important source of news and opinions just a few years ago. Following its launch in the summer of 2009, Sina Weibo grew quickly, attracting hundreds of millions of users and saw its biggest boom around 2011. However, several reports indicate a decrease in activity on Sina Weibo. In our study, we reveal the prosperity and decline of Sina Weibo by analyzing how a fixed user group's collective behaviors change throughout the whole development process. A huge dataset based on Sina Weibo along with search engine data is used in this study. In this paper we model the popularity of single tweet and multiple tweets. Then we define the statistic representing the capability of information propagation of Sina Weibo. The well-known time series prediction model, ARMA, is used to model and predict its trend. In addition, we extract both internal features, i.e. features of Sina Weibo, and external features, i.e. public's attention. Their trends are presented and analyzed. Then detailed experiments are conducted to measure the correlation and causality between them and our proposed statistic. The approaches we present in this paper clearly show the prosperity and decline of this microblogging community.}, 
keywords={search engines;social networking (online);time series;ARMA;China;Sina Weibo;information propagation capability;microblogging community;microblogging service;search engine data;time series prediction model;tweet;Autoregressive processes;Feature extraction;Market research;Media;Predictive models;Search engines;Time series analysis}, 
doi={10.1109/ICDEW.2015.7129580}, 
month={April},}
@INPROCEEDINGS{6064412, 
author={P. Salvador and A. Nogueira and R. Valadas and A. Pacheco}, 
booktitle={SoftCOM 2011, 19th International Conference on Software, Telecommunications and Computer Networks}, 
title={Joint modeling of network related events with multi-dimensional Markov Modulated Deterministic Processes}, 
year={2011}, 
pages={1-7}, 
abstract={Multiple proposals for modeling networking characteristics have been proposed in the past. However, these proposals have focused in a particular networking characteristic and do not cope with modern requirements for integrated characterization and prediction of network related events. This integrated network characterization should include not only data flow statistics but also higher-level application and protocol statistics and users behaviors (including network actions and mobility profiles). Possible examples of utilizations are the joint modeling of: (i) traffic statistics with lower (wireless) network characteristics (e.g. upload and download packet and byte counts with transmitted and received signal power), (ii) flow statistics with application level statistics (e.g. number of active flows with number of active BitTorrent's multi-source downloads) and (iii) traffic characteristics with network radio statistics and with user location (e.g. amount of generated traffic, receiving signal-to-noise ratio and user distance and angle to a radio base station). Towards this objective, this paper proposes a novel multidimensional discrete Markov Modulated Deterministic Processes (dMMDPs) model, and an associate parameter fitting procedure, that leads to accurate joint estimation of the first and second order statistics of multiple network related events. The procedure matches simultaneously both the multidimensional density distribution function and the autocovariance functions of the univariate marginal statistics. One of the main features of this model is that the number of states is not fixed a priori and can be adapted to the particular dataset and to the network events that are being modeled. The individual autocovariance tail of each univariate marginal statistic can be adjusted to capture the long-range dependence characteristics of the network related events. The procedure then fits the dMMDP parameters in order to match the multi-dimensional distribution, wit- - hin the constraints imposed by multiple autocovariance matchings. The number of states is also determined as part of this step. As a proof of concept, we applied the inference procedure to a BitTorrent traffic trace, modeling simultaneously the packet count both in the upload and download directions. These two traffic statistics exhibit long-range dependence. Very good results were obtained in terms of approximating the first and second order statistics of the traffic characteristics.}, 
keywords={Markov processes;covariance analysis;peer-to-peer computing;telecommunication traffic;BitTorrent traffic trace;application level statistics;associate parameter fitting procedure;autocovariance functions;autocovariance matchings;data flow statistics;integrated network characterization;mobility profiles;multidimensional Markov modulated deterministic processes;network radio statistics;packet count;protocol statistics;signal-to-noise ratio;traffic statistics;univariate marginal statistic;user distance;users behaviors;Adaptation models;Computational modeling;Data models;Hidden Markov models;Joints;Markov processes;Mathematical model}, 
month={Sept},}
@INPROCEEDINGS{6425640, 
author={F. G. Davoodi and O. Fatemi}, 
booktitle={2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining}, 
title={Tag Based Recommender System for Social Bookmarking Sites}, 
year={2012}, 
pages={934-940}, 
abstract={It is often essential for people to consult with others and ask them about their past experience and thoughts when making choices. Exchanging ideas among people has become more meaningful since the extensive growth of information on the World Wide Web (WWW). People have access to tremendous amount of information, but choosing the most relevant information is of high effort. It was when recommender systems came into existence in 1992 in order to assist users in the process of finding the most appropriate information on WWW, and identify sets of items which are likely to be interesting for the users. Recommender systems have used different sources of data in order to identify users' interests. In addition, by growth of social resource sharing like social book marking sites, tagging activities can be considered as explicit knowledge for user and item modeling. Existing recommender systems lack use of external source of information for recommending the most appropriate item. They mainly use the information of their own website, while there is valuable information on the web which could improve the performance of the predictions. In this paper, we use Open Directory Project (ODP) data as external knowledge about web pages in addition to tagging activities of users in a social book marking site. We have designed a content based recommender system which can recommend the most relevant web pages for each user based on the user's profile and gathered information about web pages from ODP as implicit data. We empirically evaluate effect of ODP data on the predictions using Delicious dataset in order to analyze the performance of the proposed method. The results show that our recommender system outperforms when it uses ODP information as external source of data.}, 
keywords={content management;recommender systems;social networking (online);WWW;Web page;World Wide Web;content based recommender system;item modeling;open directory project;social bookmarking site;social resource sharing;tag based recommender system;Collaboration;Recommender systems;Search engines;Tagging;Web pages;World Wide Web;Content based Recommender;Open Directory Project;Recommender System;Social bookmarking;User modelling}, 
doi={10.1109/ASONAM.2012.166}, 
month={Aug},}
@INPROCEEDINGS{6877500, 
author={S. Kumar and A. Saha and V. Vishwanath and P. Carns and J. A. Schmidt and G. Scorzelli and H. Kolla and R. Grout and R. Latham and R. Ross and M. E. Papka and J. Chen and V. Pascucci}, 
booktitle={2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)}, 
title={Characterization and modeling of PIDX parallel I/O for performance optimization}, 
year={2013}, 
pages={1-12}, 
abstract={Parallel I/O library performance can vary greatly in response to user-tunable parameter values such as aggregator count, file count, and aggregation strategy. Unfortunately, manual selection of these values is time consuming and dependent on characteristics of the target machine, the underlying file system, and the dataset itself. Some characteristics, such as the amount of memory per core, can also impose hard constraints on the range of viable parameter values. In this work we address these problems by using machine learning techniques to model the performance of the PIDX parallel I/O library and select appropriate tunable parameter values. We characterize both the network and I/O phases of PIDX on a Cray XE6 as well as an IBM Blue Gene/P system. We use the results of this study to develop a machine learning model for parameter space exploration and performance prediction.}, 
keywords={input-output programs;learning (artificial intelligence);parallel processing;software libraries;software performance evaluation;Cray XE6;I/O phases;IBM Blue Gene/P system;PIDX parallel I/O characterization;PIDX parallel I/O modeling;aggregation strategy;aggregator count;file count;file system;hard constraints;machine learning techniques;network phases;parallel I/O library performance prediction;parameter space exploration;performance optimization;target machine;user-tunable parameter value selection;Adaptation models;Data visualization;Laboratories;Libraries;Memory management;Predictive models;Throughput;I/O & Network Characterization;Performance Modeling}, 
doi={10.1145/2503210.2503252}, 
ISSN={2167-4329}, 
month={Nov},}
@INPROCEEDINGS{6609212, 
author={K. Kapoor and D. Sharma and J. Srivastava}, 
booktitle={2013 IEEE 2nd Network Science Workshop (NSW)}, 
title={Weighted node degree centrality for hypergraphs}, 
year={2013}, 
pages={152-155}, 
abstract={Many real-world social interactions involve multiple people, for e.g., authors collaborating on a paper, email exchanges made in a company and task-oriented teams in workforce. Simple graph representation of these activities destroys the group structure present in them. Hypergraphs have recently emerged as a better tool for modeling group interactions. However, methods in social hypernetwork analysis haven't kept pace. In this work, we extend the concept of node degree centrality to hypergraphs. We validate our proposed measures using alternate measures of influence available to us using two datasets namely, the DBLP dataset of scientific collaborations and the group network in a popular Chinese multi-player online game called CR3. We discuss several schemes for assigning weights to hyperedges and compare them empirically. Finally, we define separate weak and strong tie node degree centralities which improves performance of our models. Weak tie degree centrality is found to be a better predictor of influence in hypergraphs than strong tie degree centrality.}, 
keywords={computer games;graph theory;social networking (online);CR3;DBLP dataset;graph representation;hypergraphs;popular Chinese multi player online game;real-world social interactions;scientific collaborations;social hypernetwork analysis;weighted node degree centrality;Biological system modeling;Collaboration;Correlation;Electronic mail;Social network services;Time measurement;Weight measurement;Hypergraph;centrality;degree}, 
doi={10.1109/NSW.2013.6609212}, 
month={April},}
@INPROCEEDINGS{6036752, 
author={K. H. Y. Lin and C. J. Wang and H. H. Chen}, 
booktitle={2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology}, 
title={Predicting Next Search Actions with Search Engine Query Logs}, 
year={2011}, 
volume={1}, 
pages={227-234}, 
abstract={Capturing users' future search actions has many potential applications such as query recommendation, web page re-ranking, advertisement arrangement, and so on. This paper predicts users' future queries and URL clicks based on their current access behaviors and global users' query logs. We explore various features from queries and clicked URLs in the users' current search sessions, select similar intents from query logs, and use them for prediction. Because of an intent shift problem in search sessions, this paper discusses which actions have more effects on the prediction, what representations are more suitable to represent users' intents, how the intent similarity is measured, and how the retrieved similar intents affect the prediction. MSN Search Query Log excerpt (RFP 2006 dataset) is taken as an experimental corpus. Three methods and the back-off models are presented.}, 
keywords={query processing;search engines;user modelling;MSN search query log;URL clicks;access behaviors;back-off models;information retrieval;intent shift problem;search action prediction;search engine;user representation;Flow graphs;Indexing;Prediction methods;Search engines;Testing;Training;action prediction;intent mining;query logs anallysis}, 
doi={10.1109/WI-IAT.2011.15}, 
month={Aug},}
@INPROCEEDINGS{4498374, 
author={Young Ae Kim and Minh-Tam Le and H. W. Lauw and Ee-Peng Lim and Haifeng Liu and Jaideep Srivastava}, 
booktitle={2008 IEEE 24th International Conference on Data Engineering Workshop}, 
title={Building a web of trust without explicit trust ratings}, 
year={2008}, 
pages={531-536}, 
abstract={A satisfactory and robust trust model is gaining importance in addressing information overload, and helping users collect reliable information in online communities. Current research on trust prediction strongly relies on a web of trust, which is directly collected from users based on previous experience. However, the web of trust is not always available in online communities and even though it is available, it is often too sparse to predict the trust value between two unacquainted people with high accuracy. In this paper, we propose a framework to derive degree of trust based on users' expertise and users' affinity for certain contexts (topics), using users rating data which is available and much more dense than direct trust data. In experiments with a real-world dataset, we show that our model can predict trust connectivity with a high degree of accuracy. With this framework, we can predict trust connectivity and degree of trust without a web of trust and then apply it to online community applications, e.g. e-commerce environments with users rating data.}, 
keywords={Internet;security of data;information overload;online community;robust Web trust model;Computer science;History;Motion pictures;Predictive models;Reliability engineering;Robustness;Sparse matrices;Uncertainty;Writing}, 
doi={10.1109/ICDEW.2008.4498374}, 
month={April},}
@INPROCEEDINGS{7559615, 
author={Y. Fei and C. Lv and Y. Feng and D. Zhao}, 
booktitle={2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)}, 
title={Real-time filtering on interest profiles in Twitter stream}, 
year={2016}, 
pages={263-264}, 
abstract={The advent of Twitter has led to the ubiquitous information overload problem with a dramatic increase in the amount of tweets a user is exposed to. In this paper, we consider real-time tweet filtering with respect to users' interest profiles in public Twitter stream. While traditional filtering methods mainly focus on judging relevance of a document, we aim to retrieve relevant and novel documents to address the high redundancy of tweets. An unsupervised approach is proposed to model relevance between tweets and different profiles adaptively and a neural network language model is employed to learn semantic representation for tweets. Experiments on TREC 2015 dataset demonstrate the effectiveness of the proposed approach.}, 
keywords={document handling;information filtering;neural nets;social networking (online);unsupervised learning;TREC 2015 dataset;interest profiles;neural network language model;novel document retrieval;public Twitter stream;real-time tweet filtering;relevant document retrieval;semantic representation learning;ubiquitous information overload problem;unsupervised approach;Adaptation models;Filtering;Neural networks;Real-time systems;Redundancy;Semantics;Twitter;Adaptive Thresholding;Neural Network Language Model;Real-time Filtering}, 
month={June},}
@INPROCEEDINGS{6785770, 
author={X. Long and J. Joshi}, 
booktitle={2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM 2013)}, 
title={A HITS-based POI recommendation algorithm for Location-Based Social Networks}, 
year={2013}, 
pages={642-647}, 
abstract={Location-Based Social Networks (LBSNs), (also called as Geo-Social Networks), has been attracting more and more users by providing services that integrate social activities with location information. LBSN systems usually provide support for indicating various Points of Interest (POIs) but there is no straightforward rating mechanism for POIs in most LBSNs [1]. POI recommendations in LBSNs, thus, is an important and challenging research topic. In this paper, we first investigate the dataset crawled from Foursquare to explore the features that attract and influence users to check in at various POIs. Based on the analysis results, we propose a HITS (Hypertext Induced Topic Search)-based POI recommendation algorithm to recommend POIs to LBSN users that can also incorporate the impact of the social relationships on recommendations. We evaluate our proposed model on Foursquare dataset and compare our results with the latest POI recommendation algorithm. The experimental results show that our approach performs better.}, 
keywords={hypermedia;mobile computing;recommender systems;social networking (online);Foursquare dataset;HITS-based POI recommendation algorithm;LBSN;geo-social networks;hypertext induced topic search;location information;location-based social networks;points of interest;social activities;social relationships;Algorithm design and analysis;Collaboration;Conferences;Educational institutions;Entropy;Social network services;Vectors;Geo-social Networks;HITS;Location-based Social Networks;recommendation}, 
doi={10.1109/ASONAM.2013.6785770}, 
month={Aug},}
